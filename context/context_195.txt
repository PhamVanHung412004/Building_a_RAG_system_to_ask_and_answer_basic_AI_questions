Extra Class
Introduction to Transformer
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Attention
(2) â€“ Transformer-Encoder
(3) â€“ Text Classification
(4) â€“ Vision Transformer
1 â€“ Attention
!
3
RNNs Model
X1
X2
XN
Embedding Layer
Dense vector
Hidden State
Input
RNN
RNN
RNN
Output Sequence
Last Hidden State
1 â€“ Attention
!
4
Sequence-to-Sequence Architecture
Input Sequence (Source)
Output Sequence (Target)
ENCODER
DECODER
â–Encoder: encoding the inputs into state (thought vector)
â–Decoder: the state is passed into the decoder to generate 
the output
RNN
!
5
Scaled Dot-Product Attention
1 â€“ Attention
Encoder
Decoder
k1
k2
k3
k4
q1
q2
q3
q4
!
6
Scaled Dot-Product Attention
1 â€“ Attention
q!k!
"/ d#
Encoder
Attention 
Score
e! = q"k"
#
d$
, q"k%
#
d$
, â€¦ q"k%
#
d$
Decoder
k1
k2
k3
k4
q!k$
"/ d#
q!k%
"/ d# q!k&
"/ d#
q1
q2
q3
q4
!
7
Scaled Dot-Product Attention
1 â€“ Attention
q!k!
"/ d#
Encoder
Attention 
Score
e! = q"k"
#
d$
, q"k%
#
d$
, â€¦ q"k%
#
d$
Decoder
Attention
distribution
Î±"
Î±%
Î±&
Î±'
Î±! = softmax(e!)
k1
k2
k3
k4
q!k$
"/ d#
q!k%
"/ d# q!k&
"/ d#
q1
q2
q3
q4
!
8
Scaled Dot-Product Attention
1 â€“ Attention
q!k!
"/ d#
Encoder
Attention 
Score
e! = q"k"
#
d$
, q"k%
#
d$
, â€¦ q"k%
#
d$
Decoder
Attention
distribution
Î±"
Î±%
Î±&
Î±'
Attention 
output
a! = 2
"
(
Î±)
!v) ; v) = k)
Î±! = softmax(e!)
k1
k2
k3
k4
q!k$
"/ d#
q!k%
"/ d# q!k&
"/ d#
q1
q2
q3
q4
o1
!
9
Scaled Dot-Product Attention
1 â€“ Attention
Encoder
Decoder
Attention 
output
k1
k2
k3
k4
q1
q2
q3
q4
Scaled Dot-Product Attention
Attention Q, K, V = softmax QK#
d$
V; K = V
Loss - Prediction
!
10
Scaled Dot-Product Attention
1 â€“ Attention
Encoder
Decoder
Attention 
output
k1
k2
k3
k4
q1
q2
q3
q4
Attention Q, K, V = softmax QK#
d$
V; K = V
Loss - Prediction
Scaled Dot-Product Attention
o1
o2
o3
o4
!
11
Scaled Dot-Product Attention - Example
1 â€“ Attention
Encoder
Decoder
k1
k2
k3
k4
q1
q2
q3
q4
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
0
0
1
0
0
Attention 
Score
Attention Q, K, V = softmax QK#
d$
V; K = V
0.5
0.0
0.5
0.5
!
12
Scaled Dot-Product Attention - Example
1 â€“ Attention
Encoder
Decoder
k1
k2
k3
k4
q1
q2
q3
q4
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
0
0
1
0
0
Attention 
Score
Attention Q, K, V = softmax QK#
d$
V; K = V
0.5
0.0
0.5
0.5
0.28
0.16
0.28
0.28
Attention
distribution
1.00
0.83
0.72
0.44
!
13
Scaled Dot-Product Attention - Example
1 â€“ Attention
Encoder
Decoder
k1
k2
k3
k4
q1
q2
q3
q4
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
0
0
1
0
0
Attention 
Score
Attention Q, K, V = softmax QK#
d$
V; K = V
0.5
0.0
0.5
0.5
0.28
0.16
0.28
0.28
Attention
distribution
1.00
0.83
0.72
0.44
Attention 
output
!
14
Scaled Dot-Product Attention - Example
1 â€“ Attention
Encoder
Decoder
k1
k2
k3
k4
q1
q2
q3
q4
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
0
0
1
0
0
1.00
0.83
0.72
0.44
0
1
0
0
1.00
0.83
0.72
0.44
0
1
0
0
1.00
0.83
0.72
0.44
0
0
1
0
1.00
0.72
0.83
0.55
Scaled Dot-Product Attention
o1
o2
o3
o4
Attention Q, K, V = softmax QK#
d$
V; K = V
!
15
Scaled Dot-Product Attention - Demo
1 â€“ Attention
!
16
Transformer
2 â€“ Transformer-Encoder
Encoder
Decoder
â–Architecture:
N Encoder Layer
N Decoder Layer
â–Core technique: attention
â–Loss function: cross-entropy
!
17
Transformer-Encoder
2 â€“ Transformer-Encoder
â–Input Embedding
â–Positional Encoding
â–Multi-Head Attention
â–Feed Forward
â–Add & Norm
!
18
Input Embedding
2 â€“ Transformer-Encoder
â–Input Embedding: Embedding Layer
x1
x2
x3
x4
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
0
Embedding Layer
Scaled Dot-Product Attention
QUERY â€“ KEY â€“ VALUE ?
!
19
Self-Attention
2 â€“ Transformer-Encoder
x1
x2
x3
x4
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
0
Embedding Layer
Scaled Dot-Product Attention
QUERY = KEY = VALUE = EMBEDDED
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
20
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
x1
x2
x3
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
21
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
1.7
0.6
0.0
0.7
0.2
0.1
x1
x2
x3
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
22
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
0
1
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
1
1
1
1.7
0.6
0.0
0.7
0.2
0.1
0.7
0.9
0.7
x1
x2
x3
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
23
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0.7
0.9
0.7
x1
x2
x3
0
1
0
0.6
0.6
0.0
0.4
0.4
0.2
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
24
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0.7
0.9
0.7
x1
x2
x3
0
1
0
0.6
0.6
0.0
0.4
0.4
0.2
0.4
0.8
0.4
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
25
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0.7
0.9
0.7
x1
x2
x3
0
1
0
0.4
0.8
0.4
0.0
0.0
0.0
0.3
0.3
0.3
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
26
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0.7
0.9
0.7
x1
x2
x3
0
1
0
0.0
0.0
0.0
0.3
0.3
0.3
0.3
0.6
0.3
0.4
0.8
0.4
q3
k3
v3
q2
k2
v2
q1
k1
v1
!
27
Self-Attention
2 â€“ Transformer-Encoder
0
1
0
0
0
0
1
1
1
0
1
0
0
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0.7
0.9
0.7
x1
x2
x3
0
1
0
0.7
0.9
0.7
0.3
0.6
0.3
0
0
0
Self-Attention
Scaled Dot-Product Attention
â–To learn the relationship between word in the sentence
Ignore the order of words in the sentence ?
!
28
Positional Encoding
2 â€“ Transformer-Encoder
â–The position of a token in a sentence as unique representation â€“ each position is mapped to a vector
â–Methods: Sinusoid; Learned positional embedding (as learned input embedding)
1
1
1
0
0
0
1
1
1
x1
x2
x3
Embedding 
Layer #1
1
2
1
0
1
0
2
1
0
x1
x2
x3
Embedding 
Layer #2
2
3
2
0
1
0
3
2
1
+
Self-Attention
0.2
0.3
1.2
0.7
1.9
0.8
0.3
2.1
1.2
!
29
Positional Encoding â€“ Demo
2 â€“ Transformer-Encoder
O1
!
30
Multi-Head Attention
2 â€“ Transformer-Encoder
â–Split into the multiple attention heads (process independently) => self-attention => concat
Attention
Score
H: Heads
S: Sequence Length
D: Embedding Dim
S x D
Q
V
K
Split
H x S x (D/H)
H x S x (D/H)
H x S x (D/H)
Q1
Q2
Q3
K1
K2
K3
V1
V2
V3
H x S x (D/H)
O2
O3
concat
S x D
!
31
Multi-Head Attention â€“ Demo
2 â€“ Transformer-Encoder
!
32
Transformer-Encoder
2 â€“ Transformer-Encoder
!
33
Layer Normalization
2 â€“ Transformer-Encoder
!
34
Feed Forward
2 â€“ Transformer-Encoder
â–2 FC Layer
!
35
Transformer-Encoder â€“ Demo
2 â€“ Transformer-Encoder
!
36
NTC-SCV Dataset
3 â€“ Text Classification
Positive Example
Negative Example
MÃ¬nh Ä‘Æ°á»£c 1 cÃ´ báº¡n giá»›i_thiá»‡u Ä‘áº¿n Ä‘Ã¢y , tÃ¬m
Ä‘á»‹a_chá»‰khÃ¡ dá»…. Menu nÆ°á»›c uá»‘ng cháº¥t khá»i nÃ³i
. MÃ¬nh muá»‘n cÅ©ng Ä‘c 8 loáº¡i nÆ°á»›c á»ŸÄ‘Ã¢y , mÃ³n
nÃ o cÅ©ng ngon vÃ  bá»•_dÆ°á»¡ng cáº£.
QuÃ¡n cháº¿_biáº¿n Ä‘á»“_Äƒn lÃ¢u , CÃ¡_Sapa nÆ°á»›ng 
uá»›p ráº¥t dá»Ÿ , sÃ² LÃ´ng ko tÆ°Æ¡i , nÆ°á»›c_cháº¥m ko 
ngon\n TÃ³m_láº¡i sáº½ ko bao_giá» ghÃ© ná»¯a , Äƒn_dá»Ÿ 
mÃ  uá»•ng tiá»n
Má»—i láº§n thÃ¨m trÃ  sá»¯a lÃ  lÃ m 1 ly . QuÃ¡n dá»…
kiáº¿m , khÃ´ng_gian láº¡i rá»™ng_rÃ£i . NhÃ¢n_viÃªn thÃ¬
dá»…_thÆ°Æ¡ng gáº§n_gÅ©i . NÃ³i_chung thÃ¨m trÃ  sá»¯a
lÃ  mÃ¬nh ghÃ© QuÃ¡n á»ŸÄ‘Ã¢y vÃ¬ gáº§n nhÃ  .
QuÃ¡n nÃ y tháº¥y khÃ¡ nhiá»u ngÆ°á»i báº£o mÃ¬nh nÃªn 
mÃ¬nh Ä‘Ã£ Ä‘i Äƒn thá»­ , nhÆ°ng thá»±c_sá»± Äƒn xong 
tháº¥y khÃ´ng Ä‘Æ°á»£c nhÆ° mong_Ä‘á»£i láº¯m .
â–Sentiment Analysis
!
37
Modeling
3 â€“ Text Classification
Transformer-Encoder
1
1
0
0
1
1
1
1
1
1
0
0
x1
x2
x3
x4
Token and Positional Embedding Layer
Average Pooling
Classifier
!
38
Modeling â€“ Demo
3 â€“ Text Classification
!
39
Training
3 â€“ Text Classification
â–Testing: 83.66%
!
40
ViT
4 â€“ Vision Transformer
Transformers are so successful in NLP, 
Can we use them for images?
!
41
From text to image
4 â€“ Vision Transformer
Transformer-Encoder
1
1
0
0
1
1
1
1
1
1
0
0
I 
Love
AI 
VN
Token and Positional Embedding Layer
Average Pooling
â€œI Love AI VNâ€
Tokenization
!
42
From text to image
4 â€“ Vision Transformer
Can we tokenize an image?
!
43
From text to image
4 â€“ Vision Transformer
Can we tokenize an image?
!
44
From text to image
4 â€“ Vision Transformer
Can we tokenize an image?
â€¦
ğ‘¥!
ğ‘¥$
ğ‘¥%
ğ‘¥&
ğ‘¥'
ğ‘¥(
ğ‘¥)*%
ğ‘¥)*$
ğ‘¥)*!
ğ‘¥)
Flattening
!
45
ViT Architecture
4 â€“ Vision Transformer
!
46
Patch embedding
4 â€“ Vision Transformer
!
47
Patch embedding
4 â€“ Vision Transformer
!
48
Patch embedding
4 â€“ Vision Transformer
1  2
4
2
2
3
3
2
1
0
2
1
2
1
1
1
2
2
3
4
3
4
1
3
2
1
3
0
0
2
3
0
3
3
4
0
2
0
2
2
1
4
4
3
4
0
4
0
1
2
0
0
0
3
2
3
4
1
4
1
0
0
0
0
Patch size: 4
1
0
0
1
0
1
1
0
1
0
0
1
1
1
0
1
1
1
0
0
0
0
1
0
0
1
0
0
1
1
0
0
14
10
14
12
18
17
11
9
!
49
Positional embedding
4 â€“ Vision Transformer
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
76
80
Embed + add
!
50
Positional embedding
4 â€“ Vision Transformer
!
51
Positional embedding
4 â€“ Vision Transformer
14
10
14
12
18
17
11
9
0
0
1
1
1
1
0
0
+
Positional 
Embedding
=
14
10
15
13
19
18
11
9
!
52
[CLS] Token
4 â€“ Vision Transformer
!
53
[CLS] Token â€“ Why?
4 â€“ Vision Transformer
Alternatives?
- Global Average Pooling
- Max Pooling
- â€¦ 
1
1
0
0
1
1
1
1
1
Average Pooling
1
1
0
0
1
1
1
1
1
1
0
0
Attention 1
CLS
t1
t2
t3
â€¦
Attention n
Have info of  
other tokens
t1
t2
t3
!
54
[CLS] Token â€“ Demo
4 â€“ Vision Transformer
!
55
Modeling
4 â€“ Vision Transformer
Change Token 
Embedding with 
Patch Embedding
[CLS] token instead 
of pooling (can still 
use pooling)
!
56
Training
4 â€“ Vision Transformer
Thanks!
Any questions?
57
