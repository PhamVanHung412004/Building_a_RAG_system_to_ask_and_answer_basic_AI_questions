<<<<<<< HEAD
Node ID: 1929341e-79f2-4a69-bf33-2e7122d404a5
Text: Dataset Chúng ta sử dụng bộ dữ liệu openai_summarize_tldr. Nối
phần văn bản và phần bản tóm tắt để huấn luyện mô hình. 1 from
transformers import AutoTokenizer 2 3 ppo_ds_name = ’ CarperAI /
openai_summarize_tldr ’ 4 ppo_ds = load_dataset ( sft_ds_name , split
=" train ") 5 6 def build_dataset (ds , tokenizer , max_length =200) :
7 ds = ds. filt...
=======
Dataset
Chúng ta sử dụng bộ dữ liệu openai_summarize_tldr. Nối phần văn bản và phần bản tóm tắt để huấn
luyện mô hình.
1 from transformers import AutoTokenizer
2
3 ppo_ds_name = ’ CarperAI / openai_summarize_tldr ’
4 ppo_ds = load_dataset ( sft_ds_name , split =" train ")
5
6 def build_dataset (ds , tokenizer , max_length =200) :
7 ds = ds. filter ( lambda x: len (x[" prompt "]) > max_length , batched = False )
8
9 def tokenize ( sample ):
10 sample [" text "] = sample [" prompt "] + sample [" label "]
11 sample [" input_ids "] = tokenizer . encode ( sample [" text "]) [: max_length ]
12 sample [" query "] = tokenizer . decode ( sample [" input_ids "])
13 return sample
14
15 ds = ds. map ( tokenize , batched = False )
16 ds. set_format ( type =" torch ")
17 return ds
18
19 tokenizer = AutoTokenizer . from_pretrained (" facebook /opt -350 m")
20 tokenizer . pad_token = tokenizer . eos_token
21 ppo_ds = build_dataset ( ppo_ds , tokenizer )
2.2. Model
Chúng ta sử dụng mô hình CausalLMWithValueHead để khởi tạo mô hình
1 from trl import AutoModelForCausalLMWithValueHead
2
3 from peft import LoraConfig
4
5 peft_config = LoraConfig (
6 r=16 ,
7 lora_alpha =32 ,
8 lora_dropout =0.05 ,
11
>>>>>>> HEAD@{1}
