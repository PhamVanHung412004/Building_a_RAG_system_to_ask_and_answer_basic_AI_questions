1
AI VIETNAM
All-in-One Course
1
Introduction to OpenAI’s Sora
Vinh Dinh Nguyen
PhD in Computer Science
Video generated by Sora
2
AI VIETNAM
All-in-One Course
2
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
3
AI VIETNAM
All-in-One Course
3
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
4
AI VIETNAM
All-in-One Course
4
Vinh Dinh Nguyen- PhD in Computer Science
Objective
1
• Understand a Diffusion Model
2
• Understand a Transformer Architecture
3
• Understand a Vision Transformer
4
• Understand a Diffusion Transformer
5
• Understand a Sora Architecture
5
AI VIETNAM
All-in-One Course
5
Vinh Dinh Nguyen- PhD in Computer Science
What Can OpenAI’ Sora Does?
Demo: https://seo.ai/blog/openai-sora-examples
6
AI VIETNAM
All-in-One Course
6
Vinh Dinh Nguyen- PhD in Computer Science
Some related works about the video generation 
tasks
7
AI VIETNAM
All-in-One Course
7
Vinh Dinh Nguyen- PhD in Computer Science
History of Generative AI in Vision Domain
8
AI VIETNAM
All-in-One Course
8
Vinh Dinh Nguyen- PhD in Computer Science
Introduction to Open AI’s Sora
An example of the video quality Sora is capable of producing
9
AI VIETNAM
All-in-One Course
9
Vinh Dinh Nguyen- PhD in Computer Science
Sora’s Definition
Sora is a generative text to video model. Basically, it’s a machine learning model that takes in text and spits out video.
10
AI VIETNAM
All-in-One Course
10
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
11
AI VIETNAM
All-in-One Course
11
Vinh Dinh Nguyen- PhD in Computer Science
Diffusion Model: Review
12
AI VIETNAM
All-in-One Course
12
Vinh Dinh Nguyen- PhD in Computer Science
Diffusion Model: Review
Once the model learns to get really good at turning noise into images, it can be used to 
generate new images based on random noise.
13
AI VIETNAM
All-in-One Course
13
Vinh Dinh Nguyen- PhD in Computer Science
Diffusion Model: Review
A conceptual diagram of a diffusion model being trained to generate images based on a caption.
14
AI VIETNAM
All-in-One Course
14
Vinh Dinh Nguyen- PhD in Computer Science
Sora: Motivation
The idea of a diffusion model in a nutshell; they take in text, and use that text to 
turn noise into images. Sora uses a variation of the diffusion model called a 
“Diffusion Transformer”
Original Diffusion Model
Denoising Difffusion Model
Stable Difffusion
What and Why?
15
AI VIETNAM
All-in-One Course
15
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
16
AI VIETNAM
All-in-One Course
16
Vinh Dinh Nguyen- PhD in Computer Science
Transformer Motivation for OD
English
Vietnamese
I watch movie
Tôi xem phim
1.9 0.1
-1.5 1.5
1.6 -1.6
0.0 2.0
1.2 1.2
1.7 1.8
Input
Output
Word embeddeding
Encoder 
Decoder 
Transformer Encoder 
17
AI VIETNAM
All-in-One Course
17
Vinh Dinh Nguyen- PhD in Computer Science
Word Embedding Using Neural Network
1
0
0
Sum
× 1.87
1.87
f(x)= x
0
× -1.45
× -0.78
× 2.21
I
watch
movie
[EOS]
× 0.09
× 1.50
× -0.27
× -0.64
Sum
0.09
f(x)= x
0
1
0
Sum
× 1.87
−0.78
f(x)= x
0
× -1.45
× -0.78
× 2.21
I
watch
movie
[EOS]
× 0.09
× 1.50
× -0.27
× -0.64
Sum
0.27
f(x)= x
18
AI VIETNAM
All-in-One Course
18
Vinh Dinh Nguyen- PhD in Computer Science
Word Ordering
Vinh eats fish 
Fish eats Vinh 
Positional Encoding
19
AI VIETNAM
All-in-One Course
19
Vinh Dinh Nguyen- PhD in Computer Science
Word Ordering
Peter eats fish 
https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/
20
AI VIETNAM
All-in-One Course
20
Vinh Dinh Nguyen- PhD in Computer Science
Word Ordering
0.0
1.0
-0.9
0.4
Positional Encoding
1.87
1.09
-1.68
0.67
21
AI VIETNAM
All-in-One Course
21
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Idea
The pizza came out of the oven and it tasted good!
Seft-attention works by seeing how similar each word is to all of the words in the sentense, including itself
The pizza came out of the oven and it tasted good!
If we looked at a lot of sentenses abut pizza and the word it was more commonly associated with pizza than oven
22
AI VIETNAM
All-in-One Course
22
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Detail
× 1.1
× 2.8
−1.0
× 0.6
× 2.4
3.7
Query (Q)
× −1.4
× 0.5
1.9
× −1.7
× 0.9
-4.7
Key(k)
× −1.4
× 0.5
× −1.7
× 0.9
−0.2
-1.9
Key(k)
Dot product: Calculate similarities between 
the Query and the Keys
23
AI VIETNAM
All-in-One Course
23
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Detail
3.7
-1.0
Query (Q)
1.9
-4.7
Key(k)
−0.2
-1.9
Key(k)
Dot product: Calculate 
similarities between 
the Query and the Keys
11.7
Dot product: Calculate 
similarities between 
the Query and the Keys
−2.6
’I’ is much more similar to itself 
than it is to the word ‘watch’ 
Softmax Function
0.0
1.0
24
AI VIETNAM
All-in-One Course
24
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Detail
3.7
-1.0
Query (Q)
Dot product: Calculate 
similarities between 
the Query and the Keys
11.7
Dot product: Calculate 
similarities between 
the Query and the Keys
−2.6
Summation = combine encoding ‘I’ and ‘watch’ & Compute similarity to ‘I’ = selft-attention value for ‘I’ 
Softmax Function
0.0
1.0
2.5
-2.1
Share the same weights
25
AI VIETNAM
All-in-One Course
25
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Detail
× 1.1
× −2.8
−1.0
× 0.6
× 2.4
3.7
Query (Q)
× −1.4
× 0.5
1.9
× −1.7
× 0.9
-4.7
Key(k)
× −1.4
× 0.5
× −1.7
× 0.9
−0.2
-1.9
Key(k)
1.5
-2.7
Query (Q)
× −2.8
× 0.6
× 1.1
× 2.4
26
AI VIETNAM
All-in-One Course
26
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Detail
Word and 
Position
Encoding
Q,K and V
2.5
−2.1
Q,K and V
2.8
−2.2
Pay attention: Self-attention values for each word 
contain input from all of the other words
Self-Attention
Word and 
Position
Encoding
Self-Attention
Stack of Self-Attentions to capture different 
relationship among the words im complicated 
sentenses and pagraphs.
The pizza came out of the oven and it tasted good!
27
AI VIETNAM
All-in-One Course
27
Vinh Dinh Nguyen- PhD in Computer Science
Relationship Among Words: Detail
Tranformer: 8 self-attention was used
GPT-v1: 12 sefl-attention was used
Stack of Self-Attentions to capture different 
relationship among the words im complicated 
sentenses and pagraphs.
28
AI VIETNAM
All-in-One Course
28
Vinh Dinh Nguyen- PhD in Computer Science
Residual Connections
Q,K and V
2.8
−2.2
Position
Encoding
Self-Attention
Residual Connection
Transformer Encoders:
 Encode the input
Word 
embedding
Q,K and V
2.5
−2.1
Position
Encoding
Self-Attention
Residual Connection
Word 
embedding
Encode words into numbers
Encode the position of the words
Encode the relationship of words
Easy to train
−1.01
4.37
−1.43
0.82
29
AI VIETNAM
All-in-One Course
29
Vinh Dinh Nguyen- PhD in Computer Science
Transformer Encoders
0
0
0
f(x)= x
1
Tôi
xem
phim
[EOS]
2.70
−0.34
Position Encoding
Word embedding
Q,K and V
−2.8
−2.3
−0.1
−2.6
Self-Attention
Residual Connection
Transformer Encoder-Decoder
Transformer Decoder
• Don’t watch movie
• Watch movie
Encoder-
Decoder 
Attention
Keep tracking 
significant words
I  watch movie [EOS]
I  watch movie [EOS]
30
AI VIETNAM
All-in-One Course
30
Vinh Dinh Nguyen- PhD in Computer Science
Encoder-Decoder Attention
Transformer Decoder
x1.5
x0.3
0.9
x1.5
x −1.0
x −0.3
2.6
Query
−4.5
2.1
Keys
−0.5
1.4
Dot product
9.5
Dot product
4.1
Softmax
1.0
0.0
Decoder determine what should be the first translated word.
Keys
I  watch movie [EOS]
I  watch movie [EOS]
I  watch movie [EOS]
31
AI VIETNAM
All-in-One Course
31
Vinh Dinh Nguyen- PhD in Computer Science
Encoder-Decoder Attention
1.0
0.0
6
1.0
0.0
3.1
x1.5
x0.3
0.9
x1.5
x −1.0
x −0.3
2.6
Query
Transformer Decoder
I  watch movie [EOS]
I  watch movie [EOS]
32
AI VIETNAM
All-in-One Course
32
Vinh Dinh Nguyen- PhD in Computer Science
Encoder-Decoder Attention
Q,K and V
6
3.1
Residual Connection
Residual Connection
5.9
0.5
I  watch movie [EOS]
I  watch movie [EOS]
33
AI VIETNAM
All-in-One Course
33
Vinh Dinh Nguyen- PhD in Computer Science
Decoder Output
5.9
0.5
x-0.6
x-2.0
-0.6
5.1
x0.8
x-0.9
1.4
5.7
x-0.1
x-1.1
2.5
-3.6
x-1.0
x1.6
2.5
-4.6
Softmax function
1
0
0
0
Tôi
xem
phim
[EOS]
Translation is correct, but the Decoder does not stop until it outputs an [EOS] token.
dk: Number of embedding values for ech token
34
AI VIETNAM
All-in-One Course
34
Vinh Dinh Nguyen- PhD in Computer Science
What is Self-Attention?
35
AI VIETNAM
All-in-One Course
35
Vinh Dinh Nguyen- PhD in Computer Science
What is Self-Attention?
36
AI VIETNAM
All-in-One Course
36
Vinh Dinh Nguyen- PhD in Computer Science
What is Self-Attention?
37
AI VIETNAM
All-in-One Course
37
Vinh Dinh Nguyen- PhD in Computer Science
Attention: Summary
Two inputs, called the “key” and “query” get 
multiplied together to create the attention 
matrix. The key is usually transposed (rotated) 
to make the matrix multiplication work out right
Then, the attention matrix is used as a filter to transform the value matrix 
into the final output. 
The attention matrix acts like a filter, which transforms an input called the 
“value” into the final result.
the attention mechanism uses some inputs to filter other inputs
38
AI VIETNAM
All-in-One Course
38
Vinh Dinh Nguyen- PhD in Computer Science
Transformer: Summary
The encoder converts an input into an abstract representation which the decoder uses to iteratively generate output.
39
AI VIETNAM
All-in-One Course
39
Vinh Dinh Nguyen- PhD in Computer Science
Decoder Only Transformers (GPT)
Encoder-only style models are good at extracting information from text for tasks like classification and regression, 
while decoder-only style models focus on generating text. GPT, being a model focused on text generation, is a decoder 
only style model.
40
AI VIETNAM
All-in-One Course
40
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
41
AI VIETNAM
All-in-One Course
41
Vinh Dinh Nguyen- PhD in Computer Science
Attention in Text and Image
Three patches are described 
the concept of cat.
High attention
Low attention
The self-attention mechanism can be applied to the feature maps of a convolutional neural network (CNN) in order to allow 
the network to selectively focus on important image regions while suppressing noise and irrelevant information.
The pizza came out of the oven and it tasted good!
Seft-attention works by seeing how similar each word is to all of the words in the sentense, including itself
42
AI VIETNAM
All-in-One Course
42
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
An Image is Worth 16*16 Words: Transformers for Image Recognition at Scale," 
published at ICLR 2021.
1.Split an image into patches
2.Flatten the patches
3.Produce lower-dimensional linear embeddings from the flattened patches
4.Add positional embeddings
5.Feed the sequence as an input to a standard transformer encoder
6.Pretrain the model with image labels (fully supervised on a huge dataset)
7.Finetune on the downstream dataset for image classification
The ViT model represents an input image as a series of image patches, like the series of word 
embeddings used when using transformers to text, and directly predicts class labels for the image.
43
AI VIETNAM
All-in-One Course
43
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer for Classification
An Image is Worth 16*16 Words: Transformers for Image Recognition at Scale," 
published at ICLR 2021.
1.Split an image into patches
2.Flatten the patches
3.Produce lower-dimensional linear embeddings from the flattened patches
4.Add positional embeddings
5.Feed the sequence as an input to a standard transformer encoder
6.Pretrain the model with image labels (fully supervised on a huge dataset)
7.Finetune on the downstream dataset for image classification
The ViT model represents an input image as a series of image patches, like the series of word 
embeddings used when using transformers to text, and directly predicts class labels for the image.
44
AI VIETNAM
All-in-One Course
44
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer for Classification
256
256
65536
tokens 
(pixels)
What are tokens in images?
256
256
16
16
Linear 
Projection
9 tokens
To much expensive!  
Much cheaper!
45
AI VIETNAM
All-in-One Course
45
Vinh Dinh Nguyen- PhD in Computer Science
Linear Projection of Flattened Patches
1
2
3
4
5
6
7
8
9
0
Transformer Encoder
Patch + Position
Embedding
*Extra learnable
[class] embedding
MLP 
Head
Class
Bird
Cat
Dog
...
Embedded 
Patches
Norm
Multi-Head
Attention
Norm
MLP
+
+
Vision Transformer (ViT)
Transformer Encoder
Vision Transformer for Classification
L x
46
AI VIETNAM
All-in-One Course
46
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
Image to Patches
𝔁∈ℝ!×#×$
𝔁𝒑𝒊∈ℝ'×'×$, 𝑖= 1 … 9
In the original paper they use 
total 196 tokens.
47
AI VIETNAM
All-in-One Course
47
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
Linear Projection Patches
Convert patches to feature vectors
Flatten the patches from 2D to 1D vector
Linear Projection of Flattened Patches
𝑧/
/,/ 𝑧/
/,1
.
.
.
𝑧/
/,2
𝑧/
3,/ 𝑧/
3,1
.
.
.
𝑧/
3,2
𝔁𝒑𝒊∈ℝ'×'×$, 𝑖= 1 … 9
𝒛𝟏
𝒊∈ℝ)×*, 𝑖= 1 … 9
...
...
Normal Linear Projection Patches
For each patch:
• 𝔁𝒑𝒊∈ℝ#×#×% →𝔁𝒑𝒊∈ℝ&×#!% →𝑅𝑒𝑠ℎ𝑎𝑝𝑒
• 𝔁𝒑𝒊∈ℝ&×#!% + 𝑾 ∈ℝ#!%×' = 𝔁𝒑𝒊𝑾= 𝒛𝟏
𝒊∈ℝ&×'
For all patches at a time, N is total tokens (N=P*P):
• 𝔁∈ℝ)×#!% + 𝑾 ∈ℝ#!%×' = 𝒙𝑾= 𝒛𝟏∈ℝ)×'
48
AI VIETNAM
All-in-One Course
48
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
MLP
*no bias
Features 𝑧0
𝔁∈ℝ)×#!% →𝒛𝟏∈ℝ)×'
𝔁
For each patch:
• 𝔁𝒑𝒊∈ℝ#×#×% →𝔁𝒑𝒊∈ℝ&×#!% →𝑅𝑒𝑠ℎ𝑎𝑝𝑒
• 𝔁𝒑𝒊∈ℝ&×#!% + 𝑾 ∈ℝ#!%×' = 𝔁𝒑𝒊𝑾= 𝒛𝟏
𝒊∈ℝ&×'
For all patches at a time:
• 𝔁∈ℝ)×#!% + 𝑾 ∈ℝ#!%×' = 𝒙𝑾= 𝒛𝟏∈ℝ)×'
Normal Linear Projection Patches
𝑁×𝐷
Patch Linear 
Projection
Image Linear 
Projection
W
49
AI VIETNAM
All-in-One Course
49
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
Normal Linear Projection Patches
...
Linear Projection of Flattened Patches
1D vectors
(flattened patches)
Lower dimensional 
vectors
50
AI VIETNAM
All-in-One Course
50
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
For each patch:
•
𝔁𝒑𝒊∈ℝ)×'!$ 1 𝑾𝟏 ∈ℝ'×'×$ = 𝔁𝒑𝒊𝑾𝟏= 𝒛𝟏
𝒊,𝟏∈ℝ
•
...
•
𝔁𝒑𝒊∈ℝ)×'!$ 1 𝑾𝑫 ∈ℝ'×'×$ = 𝔁𝒑𝒊𝑾𝑫= 𝒛𝟏
𝒊,𝑫∈ℝ
Linear Projection Patches w/ Convolution
Conv2D
Kernel = W
Stride = P
Out_channels = D
Features 𝑧0
𝔁
• We can use Conv2D to project 
patches into features instead. 
• Each kernel (filter) has the same 
size as a patch.
Number 
of filters
𝑁×𝐷
51
AI VIETNAM
All-in-One Course
51
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
Linear Projection Patches w/ Convolution
0.1
0.2
0.2
0.3
0.5
0.4
0.1
0.2
0.3
Kernel 1
0.2
0.7
0.1
0.1
0.6
0.1
0.3
0.1
0.2
Kernel D
...
...
0.2
0.7
0.1
0.1
0.6
0.1
0.3
0.1
0.2
0.1
0.2
0.2
0.3
0.5
0.4
0.1
0.2
0.3
0.1
0.2
0.2
0.3
0.5
0.4
0.1
0.2
0.3
0.1
0.2
0.2
0.3
0.5
0.4
0.1
0.2
0.3
𝑃×𝑃×𝐷
𝑃×𝑃
𝑃×𝑃
flatten
Features 𝑧0
𝑁×𝐷
52
AI VIETNAM
All-in-One Course
52
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
Linear Projection Patches w/ Convolution
Normal Linear Projection Patches
53
AI VIETNAM
All-in-One Course
53
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
shuffle
cat
cat
The model still predict the correct class of an 
input image, but it losses the meaning because 
the input had shuffled. 
Without Position Embedding
54
AI VIETNAM
All-in-One Course
54
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer
With Position Embedding
Possition
1
2
3
4
5
6
7
8
9
...
Linear Projection of Flattened Patches
1D vectors
(flattened patches)
Lower dimensional 
vectors + Positional 
Encoding
𝑁×𝐷
55
AI VIETNAM
All-in-One Course
55
Vinh Dinh Nguyen- PhD in Computer Science
Linear Projection of Flattened Patches
1
2
3
4
5
6
7
8
9
0
Transformer Encoder
Patch + Position
Embedding
*Extra learnable
[class] embedding
MLP 
Head
Class
Bird
Cat
Dog
...
Embedded 
Patches
Norm
Multi-Head
Attention
Norm
MLP
+
+
Vision Transformer (ViT)
Transformer Encoder
Vision Transformer
L x
56
AI VIETNAM
All-in-One Course
56
Vinh Dinh Nguyen- PhD in Computer Science
*Extra learnable
[class] embedding
Vision Transformer
Linear Projection of Flattened Patches
1
2
3
4
5
6
7
8
9
0
Transformer Encoder
Patch + Position
Embedding
MLP 
Head
Class
Bird
Cat
Dog
...
Embedded 
Patches
Norm
Multi-Head
Attention
Norm
MLP
+
+
Vision Transformer (ViT)
Transformer Encoder
L x
57
AI VIETNAM
All-in-One Course
57
Vinh Dinh Nguyen- PhD in Computer Science
Vision Transformer: Summary
1.Broke images into patches
2.Flattened those patches into vectors
3.Added some information about where in the image the 
chunks came from (positional encoding)
4.Passed those vectors through a transformer
5.Took the output, put it into a dense neural network, and 
predicted what was in the image.
58
AI VIETNAM
All-in-One Course
58
Vinh Dinh Nguyen- PhD in Computer Science
Local Network Deployment: Real Device 
Flask Rest API and Mobile
59
AI VIETNAM
All-in-One Course
59
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
60
AI VIETNAM
All-in-One Course
60
Vinh Dinh Nguyen- PhD in Computer Science
Diffusion Transformer: Motivation
GPT (as in ChatGPT) pioneered the “decoder only transformer”, which is a modification 
of the original transformer that kind of acts like a filter
Naturally, with the success of transformers, people wanted to see what happened if you 
tried to use them for diffusion.
Using a transformer, which can be thought of as a really complicated 
filter, to turn noise into image
61
AI VIETNAM
All-in-One Course
61
Vinh Dinh Nguyen- PhD in Computer Science
Diffusion Transformer: Motivation
The results were shockingly good, entering us into the era of large scale and performant image generation tools like Stable 
Diffusion and MidJourney.
62
AI VIETNAM
All-in-One Course
62
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
The paper Scalable Diffusion Models with Transformers was the landmark paper which popularized transformers in diffusion models. From 
it’s highest level, the paper takes the pre-existing vision transformer, and the pre-existing idea of diffusion models, and combines them 
together to make a diffusion model which leverages transformers.
63
AI VIETNAM
All-in-One Course
63
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Combining diffusion (on the left) and transformers (on the right) to make a diffusion transformer.
Diffusion Process
Transformer
64
AI VIETNAM
All-in-One Course
64
Vinh Dinh Nguyen- PhD in Computer Science
Question 1: How do we get a prompt into the transformer, so we can tell the 
transformer what image we want it to generate?
Question 2: How do we get the model to output a less noisy image?
The Architecture Behind Diffusion Transformers
65
AI VIETNAM
All-in-One Course
65
Vinh Dinh Nguyen- PhD in Computer Science
Question 1: How do we get a prompt into the transformer, so we can tell the 
transformer what image we want it to generate?
The Architecture Behind Diffusion Transformers
66
AI VIETNAM
All-in-One Course
66
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
The Scalable Diffusion Models with Transformers paper covers three approaches to injecting text into the diffusion process:
Normal 
Transformer
What and Why?
67
AI VIETNAM
All-in-One Course
67
Vinh Dinh Nguyen- PhD in Computer Science
Patchify Architecture
Patchify is very simple; it divides z into a grid 
(32 / p) x (32 / p) x 4, where each grid 
element p x p x 4 is then linearly projected to 
become 1 x d, where d is a hyperparameter
68
AI VIETNAM
All-in-One Course
68
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
The Scalable Diffusion Models with Transformers paper covers three approaches to injecting text into the diffusion process:
Normal 
Transformer
What and Why?
What and Why?
69
AI VIETNAM
All-in-One Course
69
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
A conceptual diagram of a DiT Block with In-Context Conditioning. i.e. just combining the text and image information together at the input.
Solution 1:
This is the simplest approach, but also the least performant. In various domains it’s been shown that mixing important 
information at various levels of a model is better than just sticking everything into the beginning and hoping the model 
sorts it out.
70
AI VIETNAM
All-in-One Course
70
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
The Scalable Diffusion Models with Transformers paper covers three approaches to injecting text into the diffusion process:
What and Why?
What and Why?
71
AI VIETNAM
All-in-One Course
71
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Another approach, which is inspired by other multimodal problems, is to use cross attention to incrementally inject the textual conditioning into 
the vision transformer throughout various steps.
Solution 2:
Cross attention does the same thing but with two different inputs, allowing the model to 
create a highly contextualized and abstract representation based on both inputs.
how cross attention works
attention mechanisms ain’t 
cheap, and while this is a 
highly performant strategy, 
it comes at a steep price 
tag.
Q
K,V
72
AI VIETNAM
All-in-One Course
72
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
A third option, which the Scalable Diffusion Models with Transformers paper used, was adaLN-Zero.
Solution 3:
A conceptual 
diagram of layer 
norm (or, layer 
normalization)
adaLN-Zero (adaptive layer normalization-Zero) allows a text to 
interact with image information in a very cost-efficient way
73
AI VIETNAM
All-in-One Course
73
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
A third option, which the Scalable Diffusion Models with Transformers paper used, was adaLN-Zero.
Solution 3:
How Layer Normalization (LN) works
74
AI VIETNAM
All-in-One Course
74
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
A third option, which the Scalable Diffusion Models with Transformers paper used, was adaLN-Zero.
Solution 3:
A conceptual diagram of how adaLN allows text to control distributions from within the transformer
75
AI VIETNAM
All-in-One Course
75
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Question 2: How do we get the model to output a less noisy image?
So, instead of the hefty amount of parameters in 
cross attention, adLN employs three: mean, 
standard deviation, and scale (as well as the 
parameters in the text encoder, naturally).
76
AI VIETNAM
All-in-One Course
76
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Question 2: How do we get the model to output a less noisy image?
Transformers employ skip connections which allow old data to be combined 
with new data; a strategy that’s been shown to improve the performance of 
large models. By allowing the text to scale the importance of information 
before the addition of the skip connection, adaLN effectively lets the text 
decide how much a certain operation should contribute to the data.
77
AI VIETNAM
All-in-One Course
77
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Question 2: How do we get the model to output a less noisy image?
The Scalable Diffusion Models with Transformers paper doesn’t recommend adaLN, but 
“adaLN-Zero”. The only real difference here is in initialization. Some research has suggested 
that setting certain key values to zero at the beginning of training can improve performance.
78
AI VIETNAM
All-in-One Course
78
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Question 2: How do we get the model to output a less noisy image?
A naive approach to training a diffusion transformer. Feed in a noisy image, and expect a slightly less noisy output.
79
AI VIETNAM
All-in-One Course
79
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Question 2: How do we get the model to output a less noisy image?
An example of, if a diffusion transformer were trained naively, what it might do to denoise an image.
80
AI VIETNAM
All-in-One Course
80
Vinh Dinh Nguyen- PhD in Computer Science
The Architecture Behind Diffusion Transformers
Question 2: How do we get the model to output a less noisy image?
Diffusion transformers don’t work in images, but in “latent image embeddings”
81
AI VIETNAM
All-in-One Course
81
Vinh Dinh Nguyen- PhD in Computer Science
Diffusion Transformer
q how to compress the video spatially and temporally 
to a latent space for efficient denoising; 
q how to convert the compressed latent to patches 
and feed them to the transformer; 
q how to handle long-range temporal and spatial 
dependencies and ensure content consistency.
82
AI VIETNAM
All-in-One Course
82
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
83
AI VIETNAM
All-in-One Course
83
Vinh Dinh Nguyen- PhD in Computer Science
Sora Achitecture
Basically, instead of feeding noisy images in and getting images out like in the traditional diffusion transformer, Sora puts in noisy videos in and 
gets video out.
84
AI VIETNAM
All-in-One Course
84
Vinh Dinh Nguyen- PhD in Computer Science
Sora : Data Pre-processing
Variable Durations, Resolutions, Aspect Ratios. Traditional methods often resize, crop, or adjust the aspect ratios of videos to fit a uniform standard
Sora can generate images in flexible sizes or resolutions ranging from 1920x1080p to 1080x1920p and 
anything in between.
85
AI VIETNAM
All-in-One Course
85
Vinh Dinh Nguyen- PhD in Computer Science
Sora : Data Pre-processing
Variable Durations, Resolutions, Aspect Ratios. Traditional methods often resize, crop, or adjust the aspect ratios of videos to fit a uniform standard
Sora can generate images in flexible sizes or resolutions ranging from A comparison between Sora (right) and a 
modified version of the model (left), which crops videos to square shapes—a common practice in model training—
highlights the advantages to 1080x1920p and anything in between.
86
AI VIETNAM
All-in-One Course
86
Vinh Dinh Nguyen- PhD in Computer Science
Sora turns videos into patches by first compressing videos into a lowerdimensional latent space, and subsequently decomposing 
the representation into spacetime patches
Sora: Unified Visual Representation
87
AI VIETNAM
All-in-One Course
87
Vinh Dinh Nguyen- PhD in Computer Science
Sora: Video Compression Network
Spatial patchification simply samples nt frames and embeds each 2D 
frame independently following ViT
Spatial-temporal patchification extracts and linearly embeds non-
overlapping or overlapping tubelets that span the spatiotemporal 
input volume
88
AI VIETNAM
All-in-One Course
88
Vinh Dinh Nguyen- PhD in Computer Science
Sora: Video Compression Network
Patch packing enables variable resolution images or videos with preserved aspect ratio.6 Token dropping 
somehow could be treated as data augmentation
89
AI VIETNAM
All-in-One Course
89
Vinh Dinh Nguyen- PhD in Computer Science
Sora: Video Compression Network
The overall framework of Masked Diffusion Transformer 
(MDT). Solid/dotted lines indicate each time step’s 
training/inference 
process. 
Masking 
and 
side-
interpolater are only used during training and are 
removed during inference.
90
AI VIETNAM
All-in-One Course
90
Vinh Dinh Nguyen- PhD in Computer Science
Sora Architecture
91
AI VIETNAM
All-in-One Course
91
Vinh Dinh Nguyen- PhD in Computer Science
Generating videos provided an 
image and prompt as input
Image
Video
Diffusion 
Transformer
1
0
0
0
0
Xt
C
Mask
92
AI VIETNAM
All-in-One Course
92
Vinh Dinh Nguyen- PhD in Computer Science
Video Generation
Give a model a bunch of noise, a description of some video you want, 
and Sora attempts to build that video. 
93
AI VIETNAM
All-in-One Course
93
Vinh Dinh Nguyen- PhD in Computer Science
Animating Images
You can feed Sora an image, followed by a bunch of noise.
94
AI VIETNAM
All-in-One Course
94
Vinh Dinh Nguyen- PhD in Computer Science
Extending Videos
You can put a sequence of images into the input of the model, surrounded by noise
95
AI VIETNAM
All-in-One Course
95
Vinh Dinh Nguyen- PhD in Computer Science
Connecting Videos
we stick noise between two videos, the model will naturally attempt to reconstruct a video which respects all surrounding video
96
AI VIETNAM
All-in-One Course
96
Vinh Dinh Nguyen- PhD in Computer Science
Image Generation
You can just ask the model to build a video consisting of one frame, effectively turning the model into a typical image 
generation model.
97
AI VIETNAM
All-in-One Course
97
Vinh Dinh Nguyen- PhD in Computer Science
Qualitative results on unified 
video generation tasks
https://arxiv.org/pdf/2305.13311.pdf
98
AI VIETNAM
All-in-One Course
98
Vinh Dinh Nguyen- PhD in Computer Science
Small Latent Diffusion Transformer
99
AI VIETNAM
All-in-One Course
99
Vinh Dinh Nguyen- PhD in Computer Science
Small Latent Diffusion Transformer
100
AI VIETNAM
All-in-One Course
100
Vinh Dinh Nguyen- PhD in Computer Science
Dicussion: Talk with Sora Developers
https://www.youtube.com/watch?v=eBvvJUYtnEA
101
AI VIETNAM
All-in-One Course
101
Vinh Dinh Nguyen- PhD in Computer Science
Applications of Sora
102
AI VIETNAM
All-in-One Course
102
Vinh Dinh Nguyen- PhD in Computer Science
Ø Objective
Ø Diffusion Probability Model
Ø Transformer and ChatGPT Architecture
Ø Vision Transformer Architecture
Ø Diffusion and Transformer
Ø OpenAI’s Sora Architecture
Ø Summary
Outline
103
AI VIETNAM
All-in-One Course
103
Vinh Dinh Nguyen- PhD in Computer Science
Summary
1
• Know how Do Diffusions Model Work
2
• Know how Do Transformer and ChatGP work
3
• Know how Does a Vision Transformer Work
4
• Know how Does a Diffusion Transformer Work
5
• Know how Does an OpenAI’s Sora Work
104
AI VIETNAM
All-in-One Course
