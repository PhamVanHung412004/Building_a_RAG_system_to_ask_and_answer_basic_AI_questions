{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29423,"status":"ok","timestamp":1742631043224,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"57K85Yx6ff0J","outputId":"9bc60a19-bc2a-4e8c-f40f-8508045232fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7365,"status":"ok","timestamp":1742631052885,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"olsk3yfDgFGb","outputId":"3903542c-423b-4143-ed11-6ec5cb041e67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-index\n","  Downloading llama_index-0.12.25-py3-none-any.whl.metadata (12 kB)\n","Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n","  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n","Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n","  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n","Collecting llama-index-core<0.13.0,>=0.12.25 (from llama-index)\n","  Downloading llama_index_core-0.12.25-py3-none-any.whl.metadata (2.5 kB)\n","Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n","Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n","  Downloading llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl.metadata (3.6 kB)\n","Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_llms_openai-0.3.26-py3-none-any.whl.metadata (3.3 kB)\n","Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n","  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n","Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n","Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n","Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n","  Downloading llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)\n","Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n","  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n","Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.66.3)\n","Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (6.0.2)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.39)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.11.14)\n","Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.18)\n","Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n","Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2025.3.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.28.1)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (11.1.0)\n","Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.10.6)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (9.0.0)\n","Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.12.2)\n","Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.17.2)\n","Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n","  Downloading llama_cloud-0.1.16-py3-none-any.whl.metadata (902 bytes)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n","Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n","  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n","  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n","Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n","  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.18.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n","Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.14.0)\n","Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n","  Downloading llama_cloud_services-0.6.7-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.1.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n","Downloading llama_index-0.12.25-py3-none-any.whl (7.0 kB)\n","Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n","Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n","Downloading llama_index_core-0.12.25-py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n","Downloading llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl (14 kB)\n","Downloading llama_index_llms_openai-0.3.26-py3-none-any.whl (16 kB)\n","Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n","Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n","Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n","Downloading llama_index_readers_file-0.4.6-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n","Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n","Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading llama_cloud-0.1.16-py3-none-any.whl (251 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n","Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading llama_cloud_services-0.6.7-py3-none-any.whl (28 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n","Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.16 llama-cloud-services-0.6.7 llama-index-0.12.25 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.25 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.9 llama-index-llms-openai-0.3.26 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.6 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 marshmallow-3.26.1 mypy-extensions-1.0.0 pypdf-5.4.0 python-dotenv-1.0.1 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n"]}],"source":["!pip install llama-index"]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"/content/drive/MyDrive/LLM/chatbot/ChatbotAIO\")\n"],"metadata":{"id":"KwpkM0ya7jyi","executionInfo":{"status":"ok","timestamp":1742631488392,"user_tz":-420,"elapsed":1,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-eOS3lSi-7j"},"outputs":[],"source":["# import sys\n","# sys.path.append('/content/drive/MyDrive/LLM/chatbot/ChatbotAIO')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97347,"status":"ok","timestamp":1742631151879,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"SwNRFdTJkH4n","outputId":"b72e642d-8009-4bc6-84c4-5151de1c8d73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.25)\n","Collecting llama-index-embeddings-huggingface\n","  Downloading llama_index_embeddings_huggingface-0.5.2-py3-none-any.whl.metadata (767 bytes)\n","Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n","Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n","Requirement already satisfied: llama-index-core<0.13.0,>=0.12.25 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.25)\n","Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n","Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.9)\n","Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.26)\n","Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n","Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n","Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n","Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n","Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.29.3)\n","Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.14)\n","Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.66.3)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.39)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.18)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.8)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.28.1)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (11.1.0)\n","Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.10.6)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (9.0.0)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.17.2)\n","Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.16)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n","Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n","Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n","Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.4.post1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.49.0)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.18.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n","Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.14.0)\n","Requirement already satisfied: llama-cloud-services>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.7)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.26.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n","Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.2)\n","Downloading llama_index_embeddings_huggingface-0.5.2-py3-none-any.whl (8.9 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, llama-index-embeddings-huggingface\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed llama-index-embeddings-huggingface-0.5.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}],"source":["# %cd /content/drive/MyDrive/LLM/chatbot/ChatbotAIO/Get_datas\n","# %ls\n","!pip install llama-index llama-index-embeddings-huggingface"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1746,"status":"ok","timestamp":1742631515238,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"OMpLv5vFXeYo"},"outputs":[],"source":["from Chunking import Chunking_Data\n","from Get_datas import Get_Data"]},{"cell_type":"markdown","metadata":{"id":"RywsYXd6gICS"},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/LLM/chatbot/ChatbotAIO/Get_datas')\n","sys.path.append('/content/drive/MyDrive/LLM/chatbot/ChatbotAIO/Chunking')"]},{"cell_type":"markdown","metadata":{"id":"NTrRTfwKriAW"},"source":["# Get data"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"mxdbqjp2hJ-D","executionInfo":{"status":"ok","timestamp":1742631608440,"user_tz":-420,"elapsed":76787,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"}}},"outputs":[],"source":["documents = Get_Data(\"/content/drive/MyDrive/LLM/chatbot/ChatbotAIO/data\").run()\n","# documents.run()"]},{"cell_type":"markdown","metadata":{"id":"gmdV9vmVj5PV"},"source":["# **Chunking data**"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"jCg4_19_iEaZ","outputId":"c7d8b367-0372-4fbe-8a1d-cae4b386e95e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742631650728,"user_tz":-420,"elapsed":3718,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: llama_index in /usr/local/lib/python3.11/dist-packages (0.12.25)\n","Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.6)\n","Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.1)\n","Requirement already satisfied: llama-index-core<0.13.0,>=0.12.25 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.12.25)\n","Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.1)\n","Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.6.9)\n","Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.26)\n","Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.3)\n","Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.1)\n","Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.0)\n","Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.6)\n","Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.0)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n","Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.66.3)\n","Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (6.0.2)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama_index) (2.0.39)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (3.11.14)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (1.2.18)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (1.0.8)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (1.2.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (2025.3.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (0.28.1)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (11.1.0)\n","Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (2.10.6)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (9.0.0)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (4.12.2)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama_index) (1.17.2)\n","Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (0.1.16)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (4.13.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.2.2)\n","Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (5.4.0)\n","Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (0.0.26)\n","Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama_index) (0.6.4.post1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama_index) (1.18.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.6)\n","Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.1.31)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama_index) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama_index) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama_index) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.25->llama_index) (0.14.0)\n","Requirement already satisfied: llama-cloud-services>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (0.6.7)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama_index) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama_index) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama_index) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama_index) (2.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama_index) (3.1.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama_index) (1.0.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama_index) (3.26.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.1)\n","Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.0.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama_index) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (1.17.0)\n"]}],"source":["!pip install --upgrade llama_index\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"p3Qfeluno_-O","outputId":"40cd7c77-f478-4e83-f0b5-dc26f1b2e076","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742631661791,"user_tz":-420,"elapsed":11062,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-index-llms-huggingface\n","  Downloading llama_index_llms_huggingface-0.4.2-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface) (0.29.3)\n","Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface) (0.12.25)\n","Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n","  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface) (2.6.0+cu124)\n","Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.49.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.0.39)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.11.14)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.2.18)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.0.8)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.2.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.28.1)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.4.2)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (11.1.0)\n","Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.10.6)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (9.0.0)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.9.0)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.17.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.5.3)\n","Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (1.5.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.18.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.4.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2025.1.31)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.1.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.0.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.26.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.14.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.0.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.3.1)\n","Downloading llama_index_llms_huggingface-0.4.2-py3-none-any.whl (11 kB)\n","Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n","Installing collected packages: text-generation, llama-index-llms-huggingface\n","Successfully installed llama-index-llms-huggingface-0.4.2 text-generation-0.7.0\n","Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.11/dist-packages (0.5.2)\n","Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.29.3)\n","Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (0.12.25)\n","Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.14)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.0.39)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.18)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.8)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.0)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.28.1)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.4.2)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (11.1.0)\n","Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.10.6)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (9.0.0)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.17.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.49.0)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.18.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2024.11.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.1.31)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.26.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.14.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.2)\n"]}],"source":["!pip install llama-index-llms-huggingface\n","!pip install llama-index-embeddings-huggingface"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"37I-igOiqHde","outputId":"721291ff-b50e-4ecb-9b79-34a535f5d6d4","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3f931f2157ee4b30a2b5d99a356e55df","1d352a50e82c46b0bdacc82e09112106","d462778df6e34e3580a474e78c0164d9","48ff92440c804f5a842f8f421cb23f1e","ef60bd5e3d684327a9233a125821de4b","5ba0d8390a0c40049e85f3e5b97083dd","5d876bd50e00416d80e425391e906ab9","66fa533c442245ddbe597b6023a5f08f","28c79584ab9a4f5da834c0bc43fe17ed","d669f1dcff9243dc88f523d81df35910","3cec7962a79942c1849a2d11b8ec5b38","6bf48812a75c4c588ad7ac78d5f24f5f","4f1930e19619455f8072ac98f558d5aa","a82a424c9e264c6cb17e5a8bb80461fd","7cff5a88f08442c380faa0e1fa13b89e","b790f3c270e648bca3c34c7626c2d22d","4da2b8b0772e47aaaacb5a3c0aa21a70","d0e48649c2f4405aa22e6e62d7d12924","234127b0aed44acd94af31847a31f349","d5821541f0634209afeace6ddf129521","9f3908be699746d9bb888c1792f10666","1d56ca7a1ff845a585325f1573c933b1","cc0523b8934a41e28d67d4ce31405dce","89f818259b384468ace74f60eed18c94","0443ca611c224f879f85f05f05195472","f71aa89c29d447d2ae6b26dfe48008d7","61b145aaf04843739562e7bbb5cd736c","5869905b3a2145759d30164cea51604b","1472943eddcf49d591fff2cd26ca3b4c","cdc54f2767f4425982f861177484fd12","d50d0722b27346058b82937177b67a5e","24360bf86e8c4bc992181ea3523f0782","4a880d3b271c4f0cb182abd22bb278fe"]},"executionInfo":{"status":"ok","timestamp":1742631773552,"user_tz":-420,"elapsed":44530,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f931f2157ee4b30a2b5d99a356e55df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bf48812a75c4c588ad7ac78d5f24f5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc0523b8934a41e28d67d4ce31405dce"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Node ID: af9d1060-e081-42b1-a735-f50b6df29db1\n","Text: AI VIETNAM aivietnam.edu.vn 5 !pip install selenium 6 !apt-get\n","install -y chromium-browser 7 !apt-get install -y chromium-\n","chromedriver 8 9 from selenium import webdriver 10 from\n","selenium.webdriver.common.by import By 11 from\n","selenium.webdriver.support.ui import WebDriverWait 12 from\n","selenium.webdriver.support import expected_conditions as EC 13 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4ef38e9c-cda3-446f-a6af-4916a87b17f8\n","Text: • Thiết kế class dùng để Crawl url từ website dựa vào các đặc\n","điểm của html. • Xử lí đa luồng khi crawl url từ website. Trước tiên\n","chúng ta sẽ quan sát bố cục của trang web này trước khi thực hiện đi\n","sâu phân tích về cấu trúc HTML của nó. Hình 23: Bố cục trang tìm kiếm\n","Có thể thấy flickr là trang web có bố cục khá đơn giản, khi chúng ta\n","nhập một...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a6948a2c-170c-425e-b3af-3839c1f1cd19\n","Text: AI VIETNAM aivietnam.edu.vn search, chúng ta hoàn toàn có thể\n","thay đổi từ khóa \"Cat\" trong đường dẫn bằng từ khóa mong muốn. Từ đây,\n","chúng ta có thể liên hệ tìm kiếm với trang web flickr thông qua đường\n","link: 1 https://www.flickr.com/search/?text={search_term} Từ đường dẫn\n","URL, chúng ta có thể lấy được toàn bộ nội dung mà trang web hiển thị:\n","Hìn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2d8a60bb-2e38-4cef-ae1c-c5c1b3ef0660\n","Text: • Bước 2:Click vào vị trí số 1 ở ảnh bên dưới, sau đó trỏ tới 1\n","bức ảnh bất kì (2). • Bước 3:Quan sát cấu trúc HTML chứa đường link\n","của bức ảnh (3). 22\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0ad7ed4f-f695-4137-b024-66d6ebbea30b\n","Text: AI VIETNAM aivietnam.edu.vn Hình 25: Tìm hiểu cấu trúc trang web\n","( Các bức ảnh trên các trang web thường được lưu trữ trên các server\n","riêng biệt. Khi bạn truy cập một trang web, trang web đó sẽ sử dụng\n","URL của bức ảnh để gửi yêu cầu (request) đến các server chứa bức ảnh.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 19ffb1d1-45a4-42b8-913f-b8088395a55c\n","Text: Server sau đó sẽ gửi bức ảnh trở lại để hiển thị trên trang web.\n","Nói cách khác, trang web chỉ lưu trữ đường dẫn đến bức ảnh, còn bức\n","ảnh thực tế được lưu trữ và phục vụ từ server khác.) Một cách khác để\n","nhìn thấy rõ ràng hơn cấu trúc html của trang web, bấm tổ hợp phím\n","Ctrl + U. Và như quan sát, tất cả các đường dẫn hình ảnh đều nằm trong\n","khối m...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d8693dd2-ba9c-4173-81d8-9d5e7c672322\n","Text: AI VIETNAM aivietnam.edu.vn 7 self.setup_environment() # Call\n","for set up environment 8 9 # Set up environment for selenium 10 def\n","setup_environment(self): 11 os.environ[’PATH’] += ’:/usr/lib/chromium-\n","browser/’ 12 os.environ[’PATH’] += ’:/usr/lib/chromium-\n","browser/chromedriver/’ Ta quan sát được rằng, thẻ chứa url của bức ảnh\n","là thẻ <img/> với thu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f700a5eb-4215-4699-a0f6-c01d4da789d4\n","Text: Đồng thời ở một mức nào đó, trang web sẽ yêu cầu người dùng bấm\n","vào button \"Load more results\" để hiển thị thêm ảnh. Hình 27: Tính\n","chất lazy loading giúp trang web tối ưu hiệu năng hiển thị ảnh Nắm\n","được đặc tính này, chúng ta sẽ sử dụng selenium để scroll trang web\n","xuống dưới để nhận thêm nhiều nội dung HTML, đồng thời click vào\n","button \"Load mor...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6d046ec7-4550-409e-9508-f266d546dfc2\n","Text: AI VIETNAM aivietnam.edu.vn 20 driver.get(url) 21 22 # Start\n","crawl urls of image like brute force - the same mechanism with this\n","but add some feature 23 urls = [] 24 more_content_available = True 25\n","26 pbar = tqdm(total=self.max_images, desc=f\"Fetching images for\n","{term}\") # Set up for visualize progress 27 28 while len(urls) <\n","self.max_images an...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ffb9e54e-dc25-4e3d-ae5a-102acb61a93f\n","Text: AI VIETNAM aivietnam.edu.vn 1 def scrape_urls(self, categories):\n","2 \"\"\" 3 Call get_url_images method to get all urls of any object in\n","categories\\ 4 5 Parameter: 6 categories (dictionary): the dict of all\n","object we need to collect image with format categories{\"name_object\":\n","[value1, value2, ...]} 7 8 Returns: 9 all_urls (dictionary):\n","Dictionary of...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 051ce325-2307-428b-9520-d26a2ec97ab3\n","Text: 7 filename (str): The name of the JSON file. 8 9 Returns: 10\n","None 11 \"\"\" 12 with open(filename, ’w’) as file: 13 json.dump(data,\n","file, indent=4) 14 print(f\"Data saved to {filename}\") Sau khi đã khai\n","báo xong, thực hiện khởi tạo đối lượng từ class, đưa vào các tham số\n","cần thiết. Gọi đến hàm scrape_images để tải tất cả các đường dẫn cần\n","thiết về. ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 52ab7a45-a8df-4ea0-8423-ca73bcae76ce\n","Text: AI VIETNAM aivietnam.edu.vn Caterpillar\", \"Cheetah\", \"Chicken\",\n","\"Dragonfly\", \"Duck\", \"panda\", \"Giraffe\"], 3 \"plant\": [\"Bamboo\",\n","\"Apple\", \"Apricot\", \"Banana\", \"Bean\", \"Wildflower\", \"Flower\", \"\n","Mushroom\", \"Weed\", \"Fern\", \"Reed\", \"Shrub\", \"Moss\", \"Grass\",\n","\"Palmtree\", \"Corn\", \" Tulip\", \"Rose\", \"Clove\", \"Dogwood\", \"Durian\",\n","\"Ferns\", \"Fig\", \"Flax\", \"F...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e634ff5a-d8d5-4ebd-b7d5-6c4e2dfa3dc9\n","Text: AI VIETNAM aivietnam.edu.vn Hình 28: Cấu trúc lưu trữ các đường\n","dẫn ảnh của file json Trước tiên, khai báo lớp ImageDownloader với các\n","tham số đầu vào như sau: • json_file: đường dẫn tới file json •\n","download_dir: folder chứa các ảnh sẽ download • max_worker: số luồng\n","xử lý khi tải ảnh • delay: thời gian nghỉ giữa các lần request •\n","filename: lưu ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 46dcd36e-56f9-47ae-8a49-a70985cc4b2c\n","Text: AI VIETNAM aivietnam.edu.vn 1 def read_json(self): 2 \"\"\" 3 Read\n","the JSON file and return the data. 4 5 Returns: 6 data (dict): The\n","data read from the JSON file. 7 \"\"\" 8 with open(self.json_file, ’r’)\n","as file: 9 data = json.load(file) 10 return data 11 12 def\n","is_valid_url(self, url): 13 \"\"\" 14 Check if the URL is valid. 15 16\n","Parameters: 17 url (...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dbb4a62f-948d-49c5-8b21-c90788ad0202\n","Text: 8 term (str): The term or keyword associated with the image. 9\n","pbar (tqdm): The progress bar object. 10 11 Returns: 12 str: A message\n","indicating the status of the download. 13 \"\"\" 14 if not\n","self.is_valid_url(url): 15 pbar.update(1) 16 return f\"Invalid URL:\n","{url}\" 17 18 category_dir = os.path.join(self.download_dir, category)\n","19 if not os.path.ex...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c29007c9-3062-4fba-a71d-e22c7f63676f\n","Text: AI VIETNAM aivietnam.edu.vn 24 os.makedirs(term_dir) 25 26\n","filename = os.path.join(term_dir,\n","os.path.basename(urlparse(url).path)) 27 28\n","self.filename.add(filename) # Record the filename directory 29 30 try:\n","31 urllib.request.urlretrieve(url, filename) 32 pbar.update(1) 33\n","return f\"Downloaded: {url}\" 34 except Exception as e: 35\n","pbar.update(1) 3...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b6048121-dfc5-49c0-a176-06f6cae0b679\n","Text: 4 5 Returns: 6 None 7 \"\"\" 8 data = self.read_json() 9\n","download_tasks = [] 10 11 total_images = sum(len(urls) for terms in\n","data.values() for urls in terms.values()) 12 with\n","tqdm(total=total_images, desc=\"Downloading images\") as pbar: 13 with\n","concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as\n","executor: 14 for category, terms in...\n","----------------------------------------------------------------------------------------------------\n","Node ID: eef1dff6-aac9-4ed8-b55d-2601546f18d8\n","Text: AI VIETNAM aivietnam.edu.vn Đầu tiên, chúng ta cần hiểu rõ về cơ\n","chế đa luồng. Ở phần trước, chúng ta cũng đã áp dụng nó để tải các url\n","ảnh.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4c034def-fba1-4858-b7e8-b3567af0fb11\n","Text: Thông thường, sau khi chúng ta request cho 1 url, chúng ta sẽ\n","đợi phản hồi từ server, sau đó khi nhận được ảnh, chúng ta sẽ tiếp tục\n","request tiếp theo. Việc này khiến cho quá trình tải rất chậm chạp và\n","tốn rất nhiều thời gian. Trong khi đó nếu áp dụng multi-threading,\n","chúng ta sẽ liên tục gửi các request tới server, mỗi lần gửi sẽ là n\n","request l...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3796e122-5564-429a-b724-b9be6210715a\n","Text: AI VIETNAM aivietnam.edu.vn Hình 30: Lỗi request liên tục tới\n","server từ một địa chỉ IP Khi đã khai báo xong tất cả các phương thức\n","trong class ImageDownloader, thực hiện tạo đối tượng rồi gọi tới hàm\n","download_images để thực hiện tải ảnh. 1 downloader =\n","ImageDownloader(json_file=’image_urls.json’, download_dir=’Dataset’,\n","max_workers =4, delay=1) ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2b6b5a2b-c0de-4494-b5a7-809ad71751c7\n","Text: Sau đó nén file và lưu vào drive. 1 from google.colab import\n","drive 2 drive.mount(’/content/drive/’) 1 def\n","check_and_preprocess_images(image_dir): 2 \"\"\" 3 Check and preprocess\n","images in the specified directory. 4 5 Parameters: 6 image_dir (str):\n","The directory containing the images to be checked and preprocessed. 7\n","8 Returns: 32\n","----------------------------------------------------------------------------------------------------\n","Node ID: e84ba2b5-d863-43db-a8de-2f4677a08f5b\n","Text: AI VIETNAM aivietnam.edu.vn 9 None 10 \"\"\" 11 for root, _, files\n","in os.walk(image_dir): 12 for file in files: 13 file_path =\n","os.path.join(root, file) 14 try: 15 with Image.open(file_path) as img:\n","16 # Check if image is smaller than 50x50 pixels 17 if img.size[0] <\n","50 or img.size[1] < 50: 18 os.remove(file_path) 19 print(f\"Deleted\n","{file_path}: Ima...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 84423d1c-24e1-4e2d-b367-779b337cf2ca\n","Text: AI VIETNAM aivietnam.edu.vn Unzip file clean_dataset.zip: 1\n","!unzip Clean_Dataset Thực hiện phân chia, tổ chức lại thư mục như đã\n","đề cập ở trên: 1 import os 2 import shutil 3 from collections import\n","defaultdict 4 5 # Define the source and target directories 6\n","source_dir = \"Dataset\" 7 train_dir = \"data/train\" 8 test_dir =\n","\"data/test\" 9 10 # Create...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dc0d8292-6f5d-4f78-9252-85f9ab0fa695\n","Text: File name.txt sẽ được output ra sau khi khởi tạo đối tượng thuộc\n","lớp ImageDownloader và gọi tới hàm export_filename(). Nếu muốn tải\n","folder data về lưu trữ, chúng ta có thể zip folder data về drive: 34\n","----------------------------------------------------------------------------------------------------\n","Node ID: 833aba85-4b66-4935-b8dc-bccf2922bcb3\n","Text: AI VIETNAM aivietnam.edu.vn 1 !zip -r\n","/content/drive/MyDrive/data.zip data Kết quả cuối cùng chúng ta được\n","bộ dữ liệu ảnh với nhiều class như sau: Hình 32: Dataset chứa tất cả\n","các class được khai báo trong category 35\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e2a99f6-bcc1-4cb5-bfc6-be2ed7f8c44e\n","Text: Foundation of Prompt Engineering Team GenAIO Hoang-Bach Ngo\n","Minh-Hung An Ngày 6 tháng 3 năm 2024 Phần I: Tổng quan về Prompt\n","Engineering 1 LLM Settings Khi tương tác với các mô hình ngôn ngữ lớn,\n","bạn sẽ thường xuyên điều chỉnh một loạt các tham số và cài đặt. Việc\n","tinh chỉnh những tham số này là bước không thể thiếu để làm cho các\n","phản hồi trở n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 06c0452a-f5d2-49df-85f9-12b8e20bafb3\n","Text: Dưới đây là một số tham số cơ bản bạn thường gặp trong quá trình\n","sử dụng các mô hình LLM. Temperature- Như chúng ta có thể đã biết thì\n","Large Language Model (LLM) hoạt động theo cơ chế phân loại từ tiếp\n","theo, và như bao bài toán phân loại khác thì output đầu ra của LLM sẽ\n","áp dụng thêm một hàm softmax để tạo ra phân phối xác suất từ tiếp theo\n","sao ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 457f9ff0-3665-4676-9b25-accdbc5ebd91\n","Text: AI VIETNAM aivietnam.edu.vn Hình 1: Khi temperature được set\n","bằng 1.0, tức là không có một sự thay đổi nào so với hàm softmax bình\n","thường, lúc này sẽ là như apply một hàm softmax bình thường. Hình 2:\n","Khi temperature được đặt thành giá trị lớn hơn 1.0, nó sẽ \"làm mượt\"\n","lại phân bố xác suất, làm cho phân bố xác suất trở nên phẳng hơn. Điều\n","này có ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 93da62d3-7ea2-4593-ad84-74518d3bdef4\n","Text: 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3cc86ddf-d049-4543-993f-94795a91a3d5\n","Text: AI VIETNAM aivietnam.edu.vn Hình 3: Ngược lại, khi temperature\n","được đặt thành giá trị nhỏ hơn 1.0 (ví dụ: 0,5), nó sẽ \"làm sắc nét\"\n","phân bố xác suất. Điều này làm cho mô hình có nhiều khả năng chọn các\n","từ có xác suất cao hơn, dẫn đến kết quả đầu ra tập trung và mang tính\n","xác định hơn. Khi temperature bằng 0 (hình 4), phân bố xác suất sẽ\n","được đẩy...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 80d58b3e-c396-4dde-b918-a14009d4919d\n","Text: Stop Sequences- Đây là cách thiết lập một chuỗi dừng giúp ngăn\n","mô hình sinh thêm từ. Khi mô hình nhận diện một từ trong chuỗi dừng\n","này, nó sẽ ngừng việc tạo thêm nội dung. Xác định stop sequences là\n","một phương pháp hữu ích để kiểm soát chiều dài và bố cục của đầu ra mô\n","hình. Chẳng hạn, nếu muốn mô hình chỉ tạo ra danh sách tối đa 10 mục,\n","ta có t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e5ce231-1757-41f4-8104-f3f2b7322e07\n","Text: Để tạo ra nội dung đa dạng và sáng tạo hơn, chúng ta có thể tăng\n","mức độ phạt này lên. Ngược lại, nếu muốn mô hình tập trung hơn vào một\n","số ý tưởng nhất định, chúng ta có thể giảm mức phạt xuống. Để tổng kết\n","lại, sau đây là một số đề xuất của nhóm khi khởi tạo các tham số này:\n","3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4f915139-41e2-42df-bdef-781f7dfb40b9\n","Text: AI VIETNAM aivietnam.edu.vn • Temperature: Ban đầu khi nhìn vào\n","tham số này, chúng ta sẽ dễ bị lầm tưởng đây là tham số chạy từ 0 đến\n","1. Thực tế thì như giải thích ở trên thì ta có thể thấy là temperature\n","không hề là một biến chạy từ 0 đến 1. Vậy ta nên chọn tham số này như\n","thế nào để tối ưu nhất? – Chọn temperature = 0.0nếu chúng ta muốn kết\n","qu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c88e6172-efc3-4e07-b94f-8ea498bc468e\n","Text: – Tìm câu trả lời sáng tạo: Tăng giá trị Top P (0.6 - 0.9). •\n","Max Length – Điều chỉnh tùy theo nhu cầu của task: đối với phản hồi\n","ngắn, giới hạn số lượng token (ví dụ: 100-200 tokens); đối với bài\n","viết dài hơn, tăng số lượng token tối đa. • Stop Sequences – Đặt các\n","chuỗi cụ thể để kết thúc kết quả trả về: nếu bạn muốn mô hình chỉ tạo\n","ra một danh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 976b8e90-5efd-4434-9fd9-610fd2e3953c\n","Text: AI VIETNAM aivietnam.edu.vn Principle Prompt Principle for\n","Instructions 1 Nếu bạn muốn một câu trả lời ngắn gọn, không cần phải\n","lịch sự với mô hình LLM, không cần thêm những cụm từ như \"please\", \"if\n","you don’t mind\", \"thank you\", \"I would like to\", v.v. Ta nên đi thẳng\n","vào vấn đề 2 Nên đề cập đến đối tượng của câu trả lời ở trong prompt,\n","ví dụ: n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4728455f-fbd9-4c60-96a4-3782f94a8abf\n","Text: • Explain to me as if I’m a beginner in [field]. • Write the\n","[essay/text/paragraph] using simple English like you’re explain- ing\n","something to a 5-year-old. 6 Thêm “I’m going to tip $xxx“ (tôi sẽ bo\n","cho bạn $xxx) để kết quả cho ra tốt hơn 7 Triển khai prompt dựa trên\n","ví dụ (Dùng few-shot prompting). 8 Khi soạn prompt của bạn, bắt đầu\n","với “###Ins...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7d8ee1d6-009c-45bb-84aa-da1ee4e74079\n","Text: AI VIETNAM aivietnam.edu.vn 15 Để hỏi về một chủ đề hoặc ý tưởng\n","cụ thể hoặc bất kỳ thông tin nào và bạn muốn kiểm tra sự hiểu biết của\n","mình, bạn có thể sử dụng cụm sau: “Teach me any [theorem/topic/rule\n","name] and include a test at the end, and let me know if my answers are\n","correct after I respond, without providing the answers beforehand.”\n","(Hãy...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bb560931-1e8c-49a2-89b6-91386d9ac64c\n","Text: Bạn nên giữ nguyên phong cách viết ban đầu, đảm bảo rằng đoạn\n","văn trang trọng vẫn giữ được hình thức trang trọng.) 23 Khi bạn có một\n","prompt về lập trình phức tạp và có thể nằm trong các tệp khác nhau:\n","“From now and on whenever you generate code that spans more than one\n","file, generate a [programming language ] script that can be run to\n","automatica...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a9e69f87-c7da-40a6-8798-67006f48181a\n","Text: AI VIETNAM aivietnam.edu.vn 26 Để viết bất kỳ văn bản nào, chẳng\n","hạn như một bài luận hoặc đoạn văn, và bạn muốn văn phong tương tự như\n","mẫu được cung cấp, hãy bao gồm các prompt sau: • Use the same language\n","based on the provided paragraph[/title/text /es- say/answer]. Phần II:\n","Prompt Techniques 3 Zero-Shot Prompting Zero-Shot Prompting là một\n","ph...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1f81ed2e-03ae-45cc-96b4-2cea701822a4\n","Text: Cách tiếp cận này đã được chứng minh là cải thiện đáng kể khả\n","năng của mô hình trong việc thực hiện các nhiệm vụ chưa được nhìn thấy\n","(unseen tasks). Hiệu quả của việc tinh chỉnh prompt được thể hiện qua\n","sự cải thiện hiệu suất đáng kể so với các mô hình ngôn ngữ, lợi ích\n","càng trở nên rõ rệt khi số lượng nhiệm vụ tăng lên và khi được áp dụng\n","với L...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0ef314ee-999d-4c43-82ff-09051ff20ddd\n","Text: AI VIETNAM aivietnam.edu.vn 4 Few-Shot Prompting Khi zero-shot\n","learning không mang lại kết quả như mong đợi, việc đưa ra các mẫu ví\n","dụ cụ thể trong prompt sẽ là phương pháp khuyến khích được lựa chọn\n","tiếp theo, từ đó tiến tới kỹ thuật gợi ý với một số lượng nhỏ ví dụ.\n","Đây cũng chính là cơ sở cho kỹ thuật Few-Shot Prompting. Trong paper\n","\"Language...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 27747394-5957-4d8a-ac46-5ed0099e0d86\n","Text: • Không chỉ việc lựa chọn nhãn một cách chính xác mới quan\n","trọng, mà cả không gian nhãn và cách phân phối văn bản đầu vào từ\n","những ví dụ cũng đóng một vai trò quan trọng. • Cách thức chúng ta\n","trình bày dữ liệu cũng có ảnh hưởng đáng kể đến kết quả, thậm chí việc\n","sử dụng nhãn một cách ngẫu nhiên cũng tốt hơn là không sử dụng nhãn. •\n","Ngoài ra, việ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 204a8098-60b9-401d-b778-2c8090374b78\n","Text: AI VIETNAM aivietnam.edu.vn 5 Chain-of-Thought Prompting Phương\n","pháp \"Chain of Thought\" (COT) được giới thiệu này như một cách để cải\n","thiện hiệu suất của các mô hình ngôn ngữ lớn (LLMs) trong việc giải\n","quyết các bài toán đòi hỏi suy luận phức tạp. Phương pháp này dựa trên\n","việc kích thích mô hình tạo ra một chuỗi các bước suy nghĩ trung gian\n","tự n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7988e040-5fe1-4f9d-a4d9-0126b1e7476e\n","Text: Zero-shot COT thể hiện sự linh hoạt và khả năng áp dụng rộng rãi\n","trong các tác vụ suy luận khác nhau, từ toán học đến suy luận thông\n","thường, mở ra hướng mới trong việc khám phá và tận dụng tiềm năng sẵn\n","có của các LLMs. Trong bài báo \"Large Language Models are Zero-Shot\n","Reasoners\" Kojima et al., 2022 chỉ ra rằng chúng ta chỉ cần đơn giản\n","thêm cụ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e4a4568e-479e-47da-8cf6-d1dff14e344d\n","Text: AI VIETNAM aivietnam.edu.vn 7 Automatic Chain-of-Thought\n","Prompting Auto-CoT: Automatic Chain-of-Thought Prompting (Auto-COT)\n","được đề xuất trong paper \"Au- tomatic Chain of Thought Prompting in\n","Large Language Models\" Zhang et al., 2023a là một cách tiếp cận để tự\n","động tạo ra các chuỗi suy nghĩ trong các mô hình ngôn ngữ lớn (LLMs).\n","Auto-CoT tận d...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b76a8a37-5a78-4213-9a3f-9c15bb2a9000\n","Text: Phương pháp này giảm bớt sự cần thiết của việc tạo ví dụ minh\n","họa thủ công, mở ra khả năng áp dụng rộng rãi trong việc giải quyết\n","các bài toán suy luận phức tạp mà không cần tới sự can thiệp thủ công\n","từ người dùng. Auto-CoT hứa hẹn sẽ cải thiện hiệu suất và khả năng tự\n","học của LLMs, đặc biệt trong các tác vụ đòi hỏi suy luận nhiều bước.\n","10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 94e1262d-8f72-4c47-97d6-cb241ddef437\n","Text: AI VIETNAM aivietnam.edu.vn 8 Self-Consistency Self-Consistency\n","được đề xuất trong paper \"Self-Consistency Improves Chain of Thought\n","Reasoning in Language Models\" Wang et al., 2022 giúp cải thiện khả\n","năng suy luận của LLMs bằng cách sử dụng COT. Nó hoạt động bằng cách\n","tạo ra nhiều hướng suy nghĩ đa dạng và sau đó chọn câu trả lời phổ\n","biến nhất t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8409840f-3992-4a97-b3d2-da41d53500a3\n","Text: Điều này giúp tăng cường khả năng suy luận chính xác của mô\n","hình, đồng thời cung cấp cái nhìn sâu sắc và dễ hiểu về quá trình suy\n","nghĩ của mô hình, giúp người dùng có thể kiểm tra và hiểu rõ cách mô\n","hình đưa ra kết quả. Trong paper nhóm tác giả đề xuất thay thế cho\n","chiến lược giải mã tham lam (greedy decoding) thường được sử dụng\n","trong COT. Chiế...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 23f9c720-0756-43d7-aca6-7cc66678a9a0\n","Text: AI VIETNAM aivietnam.edu.vn Hình 7: Self-Consistency gồm 3 bước:\n","(1) Kỹ thuật prompt sử dụng COT, (2) Thay thế “greedy decode” trong\n","COT bằng cách lấy mẫu từ language model’s decoder để tạo ra một tập\n","hợp đa dạng các đường suy luận, (3) loại bỏ các đường suy luận và tổng\n","hợp bằng cách chọn câu trả lời nhất quán nhất trong câu trả lời cuối\n","cùng. ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5c7daa79-4f98-4783-ac6f-91c4e7908591\n","Text: Điều này giúp tăng cường khả năng suy luận của mô hình mà không\n","cần truy cập vào một cơ sở kiến thức có cấu trúc hay tinh chỉnh cụ thể\n","cho việc tích hợp kiến thức. Phương pháp này được chứng minh là hiệu\n","quả thông qua việc cải thiện hiệu suất trên nhiều bài toán suy luận\n","thông thường và đạt kết quả hàng đầu trên một số tác vụ. Sau đây là\n","chi tiế...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 58342f64-8308-4723-8cc7-ceae7d205e6c\n","Text: AI VIETNAM aivietnam.edu.vn q0 = q, q1 = [k1||q], . . . , qM =\n","[kM ||q], với [·||·] biểu thị sự kết hợp văn bản. Điểm tổng hợp cho\n","mỗi lựa chọn trả lời được tính dựa trên câu hỏi bổ sung hỗ trợ tốt\n","nhất lựa chọn đó dưới mô hình suy luận. Câu trả lời dự đoán cuối cùng\n","là lựa chọn được hỗ trợ nhiều nhất từ một trong các phát biểu kiến\n","thức. Mô hìn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c55696f4-4c52-421d-a021-a16fd4327bba\n","Text: • Làm thế nào để sinh ra các suy nghĩ tiềm năng từ mỗi trạng\n","thái? • Làm thế nào để đánh giá các trạng thái một cách có hệ thống? •\n","Sử dụng thuật toán tìm kiếm nào? 13\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e611197-1c9b-4482-8c4b-274931f93caa\n","Text: AI VIETNAM aivietnam.edu.vn Thought decomposition: khi các mẫu\n","COT có thể tạo ra suy nghĩ mà không cần phân rã rõ ràng, TOT tận dụng\n","đặc tính của vấn đề để thiết kế và phân rã các bước suy nghĩ trung\n","gian. Tùy thuộc vào từng vấn đề cụ thể, một suy nghĩ có thể là một vài\n","từ, một dòng của phương trình, hoặc một đoạn văn mô tả.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0d67d700-9609-429d-96df-36b17df084ec\n","Text: Điều quan trọng là suy nghĩ phải đủ nhỏ để có thể quản lý được,\n","nhưng cũng phải đủ \"lớn\" để có thể đánh giá tiến trình giải quyết vấn\n","đề một cách có ý nghĩa. Thought generator: với mục đích tạo ra các suy\n","nghĩ mới, khi có một trạng thái câys = [x, z1...zi] có hai cách để\n","sinh ra k ứng viên cho bước suy nghĩ tiếp theo: • Sample: Đầu tiên\n","chúng ta...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fa508846-db75-495f-a33f-a5274e690b39\n","Text: AI VIETNAM aivietnam.edu.vn đánh giá để chuyển đổi một heuristic\n","thành một giá trị. Sự suy luận đánh giá có thể thay đổi tùy thuộc vào\n","vấn đề và các bước suy nghĩ. • Bầu chọn các trạng thái: Đánh giá dựa\n","trên việc so sánh và bầu chọn giữa các trạng thái khác nhau, nơi\n","\"trạng thái tốt\" là trạng thái nhận được nhiều phiếu bầu nhất từ việc\n","so sánh ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ec11909f-366b-4eb0-9d10-1a67fcfe8464\n","Text: • Modularity: cũng như việc phân rã suy nghĩ, đánh giá và thuật\n","toán tìm kiếm có thể được thay đổi độc lập. • Adaptability: TOT có thể\n","thích ứng với các tính chất vấn đề khác nhau và khả năng của LM. •\n","Convenience: Không cần đào tạo thêm, chỉ cần một LM đã được đào tạo\n","sẵn. 11 Automatic Prompt Engineer (APE) Automatic Prompt Engineer là\n","một kĩ t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 852581e7-37ac-42f8-b942-faf633218582\n","Text: AI VIETNAM aivietnam.edu.vn Hình 9: Thuật toán của APE Hình 10:\n","Phương pháp APE, các bước được đánh số từ 1 đến 5 APE giúp tìm kiếm\n","những prompt cho ra kết quả tốt hơn khi sử dụng trong zero-shot CoT so\n","với prompt được định nghĩa bởi con người như \"Let’s think step by\n","step\" được đề xuất bởi Kojima et al., 2022. 16\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0017cb9a-f416-4b62-b9c7-50d5cc26645e\n","Text: AI VIETNAM aivietnam.edu.vn Hình 11: Prompt được tạo sinh bằng\n","phương pháp APE và prompt được định nghĩa bởi con người. Lợi ích của\n","việc sử dụng APE: • tự động hóa quá trình prompt engineering. • APE\n","đạt được kết quả ngang hoặc hơn con người trong một số task nhất định,\n","bao gồm zero-shot và few-shot learning. 12 Active-Prompt Một trong\n","những như...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 52c6c404-9408-49a7-88fa-5a7b99681cc9\n","Text: • Đầu tiên sử dụng mô hình LLM để gấn nhãn cho data train, có\n","thể có hoặc không sử dụng CoT với một số ví dụ ở bước này. • k câu trả\n","lời được tạo ra cho mỗi câu hỏi trong tập train. Sau đó ta sẽ sử dụng\n","một số phương pháp khác nhau để tính độ bất định cho từng câu hỏi. •\n","Những câu hỏi bất định nhất sẽ được lựa chọn để gấn nhãn thủ công bởi\n","con n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 028d2002-bc9e-4654-b79b-e5dcfeba0c7b\n","Text: AI VIETNAM aivietnam.edu.vn Hình 12: Các bước trong Active-\n","Prompting Các thang đo để tính độ bất định được sử dụng trong paper\n","bao gồm: • Disagreement: Ta có kết quả cho k câu hỏiA = a1, a2, ...ak.\n","Thang đo disagreement được tính bằng số đáp án trả lời riêng biệt chia\n","cho tổng số đáp án.\n","----------------------------------------------------------------------------------------------------\n","Node ID: f04ca931-b456-47bf-a32f-afaf3cf328fc\n","Text: • Entropy: Sử dụng hàm entropy để tính độ bất định. vời hàm\n","entropy được định nghĩa như sau: u = arg max i − kX j=1 Pθ(aj|qi)\n","lnPθ(aj|qi) (2) Trong đóPθ(aj|qi) là tần số xuất hiện của một câu trả\n","lời j nhất định trong tập tất cả các dự đoán. Entropy càng cao chứng\n","tỏ độ bất định càng lớn. • Variance: Sử dụng variance cho các câu hỏi\n","liên quan đế...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7a35c4e5-ac5b-43f6-980c-b2d9320d4e67\n","Text: AI VIETNAM aivietnam.edu.vn (LLMs) trong việc tạo ra các bản tóm\n","tắt phù hợp hơn với bản tóm tắt tham khảo mong muốn. Hình 13 minh họa\n","cách hoạt động của DSP.\n","----------------------------------------------------------------------------------------------------\n","Node ID: d9bbc5a1-253f-4322-b535-52b41c7b792f\n","Text: Hình 13: So sánh giữa DSP và Prompting truyền thống cho tác vụ\n","tóm tắt văn bản. DSP sử dụng các kích thích/chỉ dẫn hướng, trong\n","trường hợp này là các keyword được đánh dấu bằng màu cam, để các mô\n","hình LLM có thể sinh ra bản tóm tắt phù hợp với mong muốn. Trong paper\n","này, một mô hình language model nhỏ và có thể tune được (chẳng hạn như\n","T5) được ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4de3a180-09ca-427f-a1ab-1ad8a78bad51\n","Text: AI VIETNAM aivietnam.edu.vn 14 PAL: Program-aided Language\n","Models Điểm đặc biệt của PAL là việc giải quyết vấn đề được chuyển\n","giao cho trình thông dịch, như Python, giúp tận dụng sức mạnh tính\n","toán và độ chính xác cao của máy tính. Được đề xuất trong bài báo\n","\"PAL: Program-aided Language Models\" Gao et al., 2023, phương pháp này\n","tập trung vào việ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 907703c0-e75c-45ad-bd40-1291b26df76f\n","Text: Hình 15: Sơ đồ PAL: Đối với một câu hỏi suy luận toán học,\n","Chain-of-Thought tạo ra các bước suy luận trung gian dưới dạng văn bản\n","tự do. Ngược lại, PAL tạo ra các bước trung gian và mã Python. Điều\n","này chuyển vai trò thực thi các bước suy luận từ mô hình ngôn ngữ sang\n","trình thông dịch Python. Câu trả lời cuối cùng được thu bằng cách chạy\n","chuỗi s...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 14a5c504-a7d1-48a1-ad67-4dc0b2111446\n","Text: AI VIETNAM aivietnam.edu.vn bước giải quyết vấn đề một cách\n","logic và hiệu quả. Các bước trung gian và câu lệnh lập trình được mô\n","hình tạo ra sau đó được thực thi bởi trình thông dịch để thu được kết\n","quả cuối cùng. Điều này không chỉ tăng cường khả năng của mô hình\n","trong việc xử lý các vấn đề phức tạp mà còn mở rộng khả năng tương tác\n","của nó với ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0a334f72-3cfd-44e0-9e90-f9d3c515262a\n","Text: Cụ thể, trên các nhiệm vụ trả lời câu hỏi (HotpotQA) và xác minh\n","sự thật (Fever), ReAct giải quyết các vấn đề phổ biến về ảo giác và\n","lan truyền lỗi trong COT bằng cách tương tác với API Wikipedia đơn\n","giản, và tạo ra quỹ đạo giải quyết nhiệm vụ giống như con người dễ\n","hiểu hơn so với các tiêu chuẩn không có dấu vết suy luận. Thông qua\n","việc sử dụng...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e2f4654a-4ba3-43ae-919f-8953185d13a2\n","Text: AI VIETNAM aivietnam.edu.vn Hình 16: So sánh 4 phương pháp\n","prompting, standard, Chain-of-thought (CoT, Reason Only), Act- only và\n","ReAct (Reason+Act) 16 Reflexion Reflexion được đề xuất trong bài báo\n","\"Reflexion: Language Agents with Verbal Reinforcement Learning\" Shinn\n","et al., 2023, đánh dấu bước tiến trong việc cải thiện khả năng của các\n","languag...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0f793150-b20b-4cca-8376-c72224945e54\n","Text: Thay vì cập nhật trọng số, Reflexion tăng cường khả năng của\n","language agents thông qua phản hồi ngôn ngữ. Cụ thể, Reflexion agents\n","phản ánh về tín hiệu phản hồi công việc thông qua lời nói, sau đó duy\n","trì văn bản phản ánh của chính nó trong một bộ đệm theo dõi để thúc\n","đẩy quyết định tốt hơn trong các lần thử tiếp theo. Reflexion đủ linh\n","hoạt để ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e8f958e-6669-4120-824f-1c3f3348a127\n","Text: AI VIETNAM aivietnam.edu.vn Hình 17: Quá trình hoạt động của\n","Reflexion qua 3 tác vụ decision-making, programming và rea- soning.\n","Reflexion là một khái niệm đột phá trong lĩnh vực trí tuệ nhân tạo,\n","tạo ra một cách tiếp cận mới cho việc tăng cường khả năng tự học của\n","các language agents thông qua ba thành phần chính: Actor, Evaluator và\n","Self-Refle...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f23c254c-1909-46fb-8597-a2fa0fe60683\n","Text: Quy trình này không chỉ tối ưu hóa việc học từ kinh nghiệm mà\n","còn cho phép các đại diện nhanh chóng cải thiện quyết định của mình\n","thông qua việc sử dụng phản hồi có giá trị, tạo ra một cơ chế học tập\n","mạnh mẽ và linh hoạt. Hình 18: Diagram và Pseudo-Code của Reflexion 23\n","----------------------------------------------------------------------------------------------------\n","Node ID: dd6b6b31-8d6f-452f-81d6-b863ddfc374c\n","Text: AI VIETNAM aivietnam.edu.vn 17 Multimodal CoT Prompting Các mô\n","hình ngôn ngữ lớn đã thể hiện hiệu suất ấn tượng trong việc giải quyết\n","các bài toán suy luận phức tạp bằng cách sử dụng COT để tạo ra các\n","chuỗi suy luận trung gian như rationale (lý do) để suy luận ra câu trả\n","lời. Tuy nhiên, các nghiên cứu về COT trước đó đã tập trung vào mô\n","hình ngô...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b12cec8a-cf29-4f0c-9a8b-e713f3045cd3\n","Text: Hình 19: Multimodal-CoT bao gồm hai giai đoạn: (i) tạo lập lý do\n","và (ii) suy luận câu trả lời. Cả hai giai đoạn đều sử dụng cùng một\n","kiến trúc mô hình nhưng khác nhau về đầu vào và đầu ra. Trong giai\n","đoạn đầu tiên, cung cấp cho mô hình đầu vào ngôn ngữ và hình ảnh để\n","tạo ra lý do. Trong giai đoạn thứ hai, thêm đầu vào ngôn ngữ ban đầu\n","với lý do ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e915e8df-f89a-4c6e-92c3-756c3497d849\n","Text: AI VIETNAM aivietnam.edu.vn 18 Synthetic Prompting Bài báo\n","\"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for\n","Large Language Models\" Shao et al., 2023, khám phá ý tưởng sử dụng kỹ\n","thuật chain-of-thought (CoT) để hướng dẫn các mô hình ngôn ngữ lớn\n","(LLMs) giải quyết các nhiệm vụ suy luận khác nhau. Ý tưởng chính là\n","LLMs có thể tì...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 13729fb2-b947-4e59-8da8-0f832d6d2382\n","Text: Tuy nhiên, việc tạo ra những bước giải thích này một cách thủ\n","công là tốn thời gian và đắt đỏ. Bài báo giới thiệu một phương pháp\n","gọi là Synthetic Prompting, sử dụng một vài ví dụ thủ công để gợi ý\n","cho mô hình tự tạo ra thêm ví dụ. Phương pháp này bao gồm một quá\n","trình ngược để tạo ra các câu hỏi phù hợp với chuỗi suy nghĩ được lấy\n","mẫu, đảm bảo ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 72d58196-79f1-49ff-8906-073410a8e750\n","Text: AI VIETNAM aivietnam.edu.vn • Ở quá trình backward, câu hỏi sẽ\n","được LLM tạo ra dựa trên những câu hỏi mồi, cộng với một topic cho\n","trước, một độ phức tạp dự kiến và một chuỗi suy luận được tạo tự động.\n","Có một điểm cần lưu ý là chuỗi suy luận sẽ được tạo ra trước dựa trên\n","từ chủ đề và đô phức tạp dự kiến, sau đó mô hình LLM sẽ tạo ra câu hỏi\n","dựa t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 48914c0f-f862-4bc8-8ef7-6baf7fd3c2c2\n","Text: 35. 2022, 22199–22213. Zhang, Z., Zhang, A., Li, M., & Smola, A.\n","Automatic chain of thought prompting in large language models. In:\n","InThe eleventh international conference on learning representations\n","(iclr 2023). 2023.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 842f3a0a-c3be-4715-a1cb-a6a10b760c63\n","Text: 26\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6d95b105-f23d-4aa2-a92a-1c18a9bc19d4\n","Text: AI VIETNAM aivietnam.edu.vn Wang, X., Wei, J., Schuurmans, D.,\n","Le, Q., hsin Chi, E. H., & Zhou, D. (2022). Self-consistency improves\n","chain of thought reasoning in language models.ArXiv, abs/2203.11171.\n","https: //api.semanticscholar.org/CorpusID:247595263 Liu, J., Liu, A.,\n","Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., & Hajishirzi, H.\n","(202...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0cb32189-3919-4876-8467-3b66d9b7b489\n","Text: (2023a). Tree of thoughts: Deliberate problem solving with large\n","language models. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K.,\n","Pitis, S., Chan, H., & Ba, J. (2022). Large language models are human-\n","level prompt engineers. Diao, S., Wang, P., Lin, Y., & Zhang, T.\n","(2023). Active prompting with chain-of-thought for large language\n","models. Li, Z., ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ccfcb69c-6091-451c-93ca-38f54bd3e347\n","Text: (2023). Reflexion: Language agents with verbal reinforcement\n","learning. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., &\n","Smola, A. (2023b). Multimodal chain-of- thought reasoning in language\n","models. Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., & Chen, W.\n","(2023). Synthetic prompting: Generating chain-of-thought\n","demonstrations for large...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4734304c-ee62-4f60-88e5-7df6e69e9d9c\n","Text: AI VIET NAM – AI COURSE 2023 Xây dựng hệ thống hỏi-đáp mở với hệ\n","cơ sở dữ liệu vector Dinh-Thang Duong, Minh-Duc Bui và Quang-Vinh Dinh\n","PR-Team: Minh-Châu Phạm, Hoàng-Nguyên và Vũ Đăng-Nhã Nguyễn Ngày 13\n","tháng 2 năm 2024 Phần I: Giới thiệu Trong project này, chúng ta sẽ tập\n","trung vào việc phát triển một hệ thống end-to-end hỏi đáp tự động, với\n","k...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 07064990-e1a9-445d-b88d-4667d82b8880\n","Text: Cụ thể, ta sẽ xây dựng chương trình dựa vào dataset SQuAD2.0,\n","một bộ dữ liệu về đọc hiểu, vector database là FAISS và mô hình BERT\n","để thực hiện các nhiệm vụ cụ thể trong chương trình. Input và output\n","của chương trình như sau: • Input: Một câu hỏi. • Output: Câu trả lời\n","tương ứng. 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: a4a06e59-633e-4d53-829e-ea989267f397\n","Text: AI VIETNAM aivietnam.edu.vn Phần II: Nội dung Để xây dựng một\n","chương trình End-to-end Question Answering, chúng ta cần hoàn thiện\n","hai module chính bao gồm Retriever và Reader: Hình 1: Ảnh minh hoạt\n","tổng quát về một hệ thống End-to-end QA. Theo đó, nội dung của bài\n","viết sẽ trình bày chương trình cài đặt cho từng thành phần như sau: 1.\n","Dataset des...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7460059c-5667-4686-838b-c03e1cbc8ad5\n","Text: 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e7101ae-5c7d-4bf6-a5d4-1770e7400561\n","Text: AI VIETNAM aivietnam.edu.vn SQuAD 2.0 Train Total examples\n","130,319 Negative examples 43,498 Total articles 442 Articles with\n","negatives 285 Development Total examples 11,873 Negative examples\n","5,945 Total articles 35 Articles with negatives 35 Test Total examples\n","8,862 Negative examples 4,332 Total articles 28 Articles with\n","negatives 28 Bảng 1: Th...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7ee17c82-fa7c-4996-bf4e-78c385600bde\n","Text: Reader: DistilBERT Đầu tiên ta sẽ xây dựng model Reader hay\n","chính là model QA trong project này. (a) Install and import\n","bibraries:Đầu tiên ta sẽ install một số thư viện cần thiết mà Colab\n","chưa hỗ trợ. 1 !pip install -qq datasets==2.16.1 evaluate==0.4.1\n","transformers[sentencepiece ]==4.35.2 2 !pip install -qq\n","accelerate==0.26.1 3 !apt install git-...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 27058a96-759f-4fa6-baf6-4f93735357d2\n","Text: AI VIETNAM aivietnam.edu.vn 1 from huggingface_hub import\n","notebook_login 2 3 notebook_login() Cuối cùng ta sẽ import các thư\n","viện chính được sử dụng trong phần này: 1 import numpy as np 2 from\n","tqdm.auto import tqdm 3 import collections 4 5 import torch 6 7 from\n","datasets import load_dataset 8 from transformers import AutoTokenizer\n","9 from transfor...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e159f271-8c4e-45be-b9e7-43bac432a147\n","Text: Trong quá trình này, hàm trích xuất danh sách câu hỏi, mã hóa\n","thông tin đầu vào bằng tokenizer, và trích xuất offset_mapping và\n","sample_map để ánh xạ vị trí từ mã hóa về văn bản gốc. Hàm cũng xác\n","định vị trí bắt đầu và kết thúc của câu trả lời trong ngữ cảnh và thêm\n","thông tin về vị trí này vào biến inputs. 1 # Định nghĩa hàm\n","preprocess_training_e...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f2c5d506-f548-4d7f-9abc-be9290837a60\n","Text: AI VIETNAM aivietnam.edu.vn 7 8 # Tiến hành mã hóa thông tin đầu\n","vào sử dụng tokenizer 9 inputs = tokenizer( 10 questions, 11\n","examples[\"context\"], 12 max_length=MAX_LENGTH, 13\n","truncation=\"only_second\", 14 stride=STRIDE, 15\n","return_overflowing_tokens=True, 16 return_offsets_mapping=True, 17\n","padding=\"max_length\", 18 ) 19 20 # Trích xuất offset_mapp...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a09faa7c-8487-45fb-9df3-e8837e21b6ab\n","Text: AI VIETNAM aivietnam.edu.vn 66 start_positions.append(0) 67\n","end_positions.append(0) 68 else: 69 # Nếu không, gán vị trí bắt đầu và\n","kết thúc dựa trên 70 # vị trí của các mã thông tin 71 idx =\n","context_start 72 while idx <= context_end and offset[idx][0] <=\n","start_char: 73 idx += 1 74 start_positions.append(idx - 1) 75 76 idx =\n","context_end 77 while ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: be600a42-def1-4fa4-aaa3-5ccafc2b16be\n","Text: Sau đó xác định ví dụ tham chiếu cho từng dòng đầu vào và điều\n","chỉnh ánh xạ offset để loại bỏ các offset không phù hợp với\n","sequence_ids. Cuối cùng là thêm thông tin về ví dụ tham chiếu vào đầu\n","vào. 1 def preprocess_validation_examples(examples): 2 # Chuẩn bị danh\n","sách câu hỏi bằng cách 3 # loại bỏ khoảng trắng ở đầu và cuối mỗi câu\n","hỏi 4 questio...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 515e119a-89cf-4ef8-9c5d-0f7d0f51cb5a\n","Text: AI VIETNAM aivietnam.edu.vn 13 return_overflowing_tokens=True,\n","14 return_offsets_mapping=True, 15 padding=\"max_length\", 16 ) 17 18 #\n","Lấy ánh xạ để ánh xạ lại ví dụ tham chiếu cho từng dòng trong inputs\n","19 sample_map = inputs.pop(\"overflow_to_sample_mapping\") 20\n","example_ids = [] 21 22 # Xác định ví dụ tham chiếu cho mỗi dòng đầu\n","vào và 23 # điều ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4afc896b-7b3b-4db3-8ee3-cd644a40122c\n","Text: 12 len(raw_datasets[\"validation\"]), len(validation_dataset) (e)\n","Train model:Sau khi đã chuẩn bị xong dataset, ta sẽ tiến hành load\n","model từ HuggingFace để chuẩn bị cho quá trình training: 1 # Load\n","model 2 model =\n","AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME) Tiếp theo ta\n","sẽ định nghĩa một số parameter mà ta sẽ sử dụng để training mod...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7820b0b1-9646-4576-b1be-9ebbf212591e\n","Text: AI VIETNAM aivietnam.edu.vn 9 fp16=True, # Sử dụng kiểu dữ liệu\n","half-precision để tối ưu tài nguyên 10 push_to_hub=True, # Đẩy kết quả\n","huấn luyện lên HuggingFace Hub 11 ) Cuối cùng ta sẽ khởi tạo class\n","Trainer, đây là class chính để training model, ta sẽ không cần phải\n","định nghĩa hàm train, đưa input vào mode, tính toán loss, update\n","gradient nữa...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 77b2068c-4553-461a-b0db-5f0e3c193eb9\n","Text: AI VIETNAM aivietnam.edu.vn Hàm compute_metrics nhận các đầu vào\n","như start_logits, end_logits, features, và examples, và thực hiện các\n","bước sau để tính toán các độ đo và kết quả đánh giá mô hình hỏi đáp.\n","Trong quá trình tính toán, hàm này tạo một danh sách các câu trả lời\n","dự đoán dựa trên các logits được dự đoán bởi mô hình.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2bd38ecf-09d7-4d3b-b4b1-f4c1453d7dfe\n","Text: Điều này bao gồm việc xác định vị trí bắt đầu và kết thúc tốt\n","nhất cho các câu trả lời và đánh giá xem chúng có hợp lệ hay không dựa\n","trên độ dài tối đa cho câu trả lời. Cuối cùng, hàm tính toán các độ đo\n","và trả về kết quả đánh giá mô hình hỏi đáp dựa trên các câu trả lời dự\n","đoán và câu trả lời lý thuyết từ ví dụ. 1 N_BEST = 20 # Số lượng kết\n","quả...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f4cbc85e-3a34-4349-a7de-11f60424e6b3\n","Text: AI VIETNAM aivietnam.edu.vn 52 answer_dict = { 53 ’id’:\n","example_id, 54 ’prediction_text’: best_answer[’text’], 55\n","’no_answer_probability’: 1 - best_answer[’logit_score’] 56 } 57 else:\n","58 answer_dict = { 59 ’id’: example_id, 60 ’prediction_text’: ’’, 61\n","’no_answer_probability’: 1.0 62 } 63\n","predicted_answers.append(answer_dict) 64 65 # Tạo danh sá...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e074f260-f595-466c-975b-f4eceb084b12\n","Text: AI VIETNAM aivietnam.edu.vn lên HuggingFace, nếu muốn sử dụng\n","thì ta chỉ cần dùng class pipeline có sẵn của HuggingFace là đã có thể\n","load model và tiến hành inference: 1 # Use a pipeline as a high-level\n","helper 2 from transformers import pipeline 3 4 PIPELINE_NAME =\n","’question-answering’ 5 MODEL_NAME = ’thangduong0509/distilbert-\n","finetuned-squadv2’...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dd3a8b46-5da2-4152-a733-3336f2e0088d\n","Text: Các bạn có thể đọc thêm về Faiss tại đây. Hình 3: Source Tại\n","đây, chúng ta sẽ ứng dụng Faiss để làm module Retriever cho hệ thống\n","E2E QA của chúng ta. Với nhiệm vụ tìm kiếm các context phù hợp nhất\n","cho câu hỏi đầu vào, ta sẽ cài đặt Faiss theo cách thức như sau: (a)\n","Với bộ dữ liệu SQuAD2.0, ta sẽ xây dựng một database chứa thêm cột đại\n","diện cho ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d5883da0-f540-4e1a-a353-7929dbdc80df\n","Text: AI VIETNAM aivietnam.edu.vn Hình 4: Minh họa các bước xây dựng\n","một vector database với Faiss Để cài đặt Faiss phục vụ cho việc tìm\n","kiếm các văn bản context của các câu hỏi có nội dung giống với câu hỏi\n","đầu vào, ta thực hiện như sau: (a) Cài đặt và import các thư viện cần\n","thiết: 1 !pip install -qq transformers[sentencepiece]==4.35.2\n","datasets==2.1...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b368df7c-fd58-4ae8-b51a-70da184299f3\n","Text: Vì vậy, ta sẽ loại bỏ các trường hợp này ra khỏi bộ dữ liệu: 12\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8f21b36f-550c-4f2b-b1f0-3b95f71223eb\n","Text: AI VIETNAM aivietnam.edu.vn 1 raw_datasets =\n","raw_datasets.filter( 2 lambda x: len(x[’answers’][’text’]) > 0 3 ) (d)\n","Khởi tạo mô hình: Để tạo vector embedding cho các câu hỏi, ta sẽ sử\n","dụng mô hình pre-trained DistilBERT: 1 MODEL_NAME = \"distilbert-base-\n","uncased\" 2 tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) 3\n","model = AutoModel.from_pret...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2fa48241-b9f6-47a8-b4e6-e3fcf7a77683\n","Text: Đầu tiên, ta xây dựng hàm lấy final hidden state của token CLS:\n","1 def cls_pooling(model_output): 2 return\n","model_output.last_hidden_state[:, 0] Sau đó, xây dựng hàm đưa một văn\n","bản vào model để từ đó có thể gọi hàmcls_pooling(): 1 def\n","get_embeddings(text_list): 2 encoded_input = tokenizer( 3 text_list, 4\n","padding=True, 5 truncation=True, 6 return_...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1e449501-660e-45cd-97b8-5e73ef5f5c63\n","Text: AI VIETNAM aivietnam.edu.vn (f) Xây dựng vector database:Với hàm\n","tạo vector embedding đã triển khai, ta sẽ sử dụng nó để tạo một cột\n","trong bảng dữ liệu dùng để chứa kết quả lời gọi hàmget_embeddings()\n","với input là các câu hỏi của từng mẫu dữ liệu. Cụ thể, ta tạo cột mới\n","tên làquestion_embedding và lưu vector embedding của câu hỏi như sau:\n","1 EMBE...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 275bbe1d-7805-431f-9290-ae3142335e1a\n","Text: 14\n","----------------------------------------------------------------------------------------------------\n","Node ID: eeebb8b1-f4d9-4c60-8927-a5e9747c1aac\n","Text: AI VIETNAM aivietnam.edu.vn (g) Áp dụng mô hình hỏi-đáp để trả\n","lời câu hỏi:Như vậy, chúng ta đã có hai thành phần Retriever và\n","Reader.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4b137273-afb4-4b64-b750-34541539d4d2\n","Text: Chúng ta sẽ viết một đoạn code ngắn để thực hiện chương trình\n","End-to-End QA. Đầu tiên, khởi tạo mô hình hỏi-đáp đã huấn luyện: 1\n","from transformers import pipeline 2 3 PIPELINE_NAME = ’question-\n","answering’ 4 MODEL_NAME = ’thangduong0509/distilbert-finetuned-\n","squadv2’ 5 pipe = pipeline(PIPELINE_NAME, model=MODEL_NAME) Tận dụng\n","kết quả truy vấn vừa r...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f885ba72-8b95-4f5e-8411-339f17160173\n","Text: AI VIETNAM aivietnam.edu.vn Phần III: Câu hỏi trắc nghiệm 1. So\n","với QA, chương trình End-to-end QA có điểm gì khác biệt? (a) Mô hình\n","trích xuất câu hỏi tốt hơn. (b) Sử dụng kiến trúc transformer. (c) Có\n","sử dụng mô hình tìm kiếm context. (d) Tốc độ xử lý nhanh hơn.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 471d2977-e97c-446a-9e84-fb31dbc38c73\n","Text: 2. Tại sao mô hình Transformer được sử dụng phổ biến trong bài\n","toán Question Answering (QA)? (a) Do Transformer có khả năng tự học\n","đặc trưng từ văn bản tự nhiên. (b) Do Transformer chứa nhiều kiến thức\n","về dữ liệu. (c) Có sử dụng mô hình tìm kiếm context. (d) Do\n","Transformer có khả năng xử lý dữ liệu dạng sequence. 3. Trong QA, tại\n","sao phải sử dụn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6751ff25-3366-4dc6-bbbc-27ccb3af8bf0\n","Text: 5. Tham số stride có nghĩa là gì trong đoạn code sau: 1 inputs =\n","tokenizer( 2 question, # Danh sách các câu hỏi 3 context, # Nội dung\n","liên quan đến câu hỏi 4 max_length=MAX_LENGTH, # Độ dài tối đa cho đầu\n","ra mã hóa 5 truncation=\"only_second\", # Cắt bớt dữ liệu chỉ cho phần\n","thứ hai (context) 6 stride=STRIDE, 7 return_overflowing_tokens=True, #\n","Tr...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 785ba84b-efd3-478b-ab1f-d1a2dba3d0c8\n","Text: AI VIETNAM aivietnam.edu.vn 1 # Tạo đối tượng args là các tham\n","số cho quá trình huấn luyện 2 args = TrainingArguments( 3\n","output_dir=\"distilbert-finetuned-squadv2\", # Thư mục lưu trữ kết quả\n","huấn luyện 4 evaluation_strategy=\"no\", # Chế độ đánh giá không tự động\n","sau mỗi epoch 5 save_strategy=\"epoch\", # Lưu checkpoint sau mỗi epoch\n","6 learning_rate=...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 84cac954-aa6e-46e8-a0b2-684e5bc215e4\n","Text: (a) Nó cung cấp khả năng chuẩn hóa dữ liệu và ràng buộc tính\n","toàn vẹn tốt hơn. (b) Nó cho phép tìm kiếm và truy vấn dữ liệu dựa\n","trên nội dung một cách hiệu quả. (c) Nó cung cấp các cơ chế kiểm soát\n","giao dịch mạnh mẽ hơn. (d) Nó tăng cường khả năng truy vấn SQL cho\n","phân tích dữ liệu có cấu trúc. 9. Để tính sự tương đồng giữa hai\n","vector, độ đo nào...\n","----------------------------------------------------------------------------------------------------\n","Node ID: cbc301a3-750c-4be3-8c99-4c47e524608d\n","Text: AI VIETNAM aivietnam.edu.vn (c) [EOS] token (d) Final token 11.\n","Trong các vector database như Faiss, kỹ thuật nào thường được sử dụng\n","để tối ưu hiệu quả tìm kiếm trên dữ liệu đa chiều?\n","----------------------------------------------------------------------------------------------------\n","Node ID: cb8bc6bd-8728-4f9a-9022-b54cd5edc9e2\n","Text: (a) Linear Search (b) Indexing (c) Quantization (d) Encryption\n","12. Trong đoạn code dưới đây: 1 TOP_K = 5 2 scores, samples =\n","embeddings_dataset.get_nearest_examples( 3 EMBEDDING_COLUMN,\n","input_quest_embedding, k=TOP_K 4 ) Biến TOP_K còn được hiểu là? (a) Số\n","lượng cluster (b) Số lượng kết quả trả về (c) Số epochs (d) Số chiều\n","trong không gian embe...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 13ea9023-64f0-4c90-b98b-b62b334bd4ab\n","Text: AI VIET NAM – COURSE 2023 Foundation of Prompt Engineering Ngày\n","19 tháng 3 năm 2024 Phần I: Tổng quan về RAG Phần II: Retrieval\n","Augmented Generation (RAG) Trong bối cảnh các mô hình ngôn ngữ lớn\n","(LLM) phát triển mạnh mẽ, sự xuất hiện của các mô hình GPT (OpenAI),\n","LLama (Meta), Gemini (Google) đã thể hiện khả năng ấn tượng trong việc\n","sinh ngôn ng...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8ee53ead-5464-46d3-b681-4258637d369f\n","Text: Trong bài báo của Lewis et al. (2021), khi lần đầu đề cấp đến hệ\n","thống RAG, Lewis đã sử dụng một mô hình seq2seq huấn luyện trước ... 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4f932ad3-38a1-4a87-b64c-b72d34555de5\n","Text: AI VIETNAM aivietnam.edu.vn Hình 1: Giải pháp RAG cung cấp bởi\n","Lewis (2020). Giải pháp kết hợp một mô hình truy vấn được huấn luyện\n","trước (Query Encoder + Document Index) và một mô hình seq2seq huấn\n","luyện trước (Generator). Với đầu vào là truy vấn x, Maximum Inner\n","Product Search (MIPS) được sử dụng để tìm ra top-k tài liệu liên quan.\n","Tài liệu tì...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 95901bc1-4a10-413b-b757-4e72ef8b2fb8\n","Text: Hình 2: Hệ thống RAG cơ bản với các cấu phần: Nguồn thông tin\n","tra cứu (Knowledge Base), Truy vấn (Retriever), Mô hình sinh\n","(Generator), Truy vấn đầu vào và Kết quả đầu ra. Hệ thống RAG cho phép\n","tìm kiếm và nhận về các tài liệu liên quan đến câu hỏi truy vấn của\n","người dùng. Các tài liệu này được sử dụng như nguồn thông tin kết hợp\n","với câu hỏi tru...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 83582517-2237-4b6b-8ab4-af00678742f1\n","Text: AI VIETNAM aivietnam.edu.vn Hình 3: Ví dụ chi tiết hoạt động của\n","hệ thống RAG. Nguồn: Gao et al. (2024) Các thành phần trong ví dụ có\n","thể giải thích như sau: • Input: Câu hỏi đầu vào, thường được cung\n","cấp/yêu cầu bởi người dùng nhằm truy vấn và tương tác với mô hình ngôn\n","ngữ. • Indexing: Trong hệ thống RAG, để xây dựng Knowledge Base, các\n","văn bả...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 28df7404-207f-4f76-9d64-b26f5d52767f\n","Text: Theo đó, các hệ 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 963c3e44-b063-4a33-ab18-1cfe55b18626\n","Text: AI VIETNAM aivietnam.edu.vn thống RAG có thể được phân loại theo\n","các bước phát triển của mô hình từ RAG cơ bản (Naive) đến RAG nâng cao\n","(Advanced) và Mô-đun RAG (Modular). Các mô hình có sự cập nhật và giải\n","quyết được các giới hạn của mô hình trước về các mặt mức độ thể hiện\n","(performance), thời gian và hiệu quả (cost and efficiency). Hình 4:\n","Phâ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9cffd861-fc7d-402d-8da8-3637b937f8a9\n","Text: Nguồn: Gao et al. (2024) (a) RAG cơ bảnbao gồm các bước thực\n","hiện truyền thống từ: xử lý dữ liệu nguồn (indexing), truy vấn\n","(retrieval) và sinh câu trả lời (generation). RAG cơ bản tồn tại các\n","giới hạn như chỉ lệ chính xác chưa cao đén từ việc truy vấn thiếu\n","chính xác các phân đoạn tài liệu (chunks) liên quan. Điều này cũng là\n","nguyên nhân khiến ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 698e439f-bea9-41be-992e-ab22082fcb9d\n","Text: AI VIETNAM aivietnam.edu.vn (c) Mô-đun RAG:Một cách hệ thống,\n","giải pháp RAG bao gồm nhiều cấu phần, mỗi cấu phần thực hiện các chức\n","năng khác nhau. Mô-đun RAG được thiết kế với các mô đun chức năng nhằm\n","cải thiện chất lượng của các cấu phần thuộc hệ thống RAG. Một cách\n","tổng quan, RAG cơ bản là một giải pháp thuộc RAG nâng cao, RAG nâng\n","cao là mộ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bc2a6700-f64c-4422-bf55-406b75f9310e\n","Text: Nhằm tăng tính linh hoạt của hệ thống RAG, một số kỹ thuật có\n","thể áp dụng như: • Hybrid Search Exploration:Là giải pháp kết hợp hai\n","giải pháp tìm kiếm là tìm kiếm với từ khoá (keyword) và tìm kiếm theo\n","ngữ nghĩa (semantic). Giải pháp này tối ưu được kết quả chính xác từ\n","việc tìm theo từ khoá và việc linh hoạt tìm kiếm với các từ cùng ngữ\n","cảnh. K...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 23ab9b88-91c3-4852-8fa8-3a38e4bd4968\n","Text: Sau đó, cấp lớn chứa đoạn tài liệu dài hơn, bao chùm ngữ cảnh\n","tốt hơn (chứa đoạn nhỏ) được trả về như kết quả tìm kiếm. Giải pháp\n","cho phép tìm kiếm chính xác hơn mà vẫn giữ được ngữ cảnh của tài liệu.\n","• Sub-Queries: Trong bối cảnh thực tế, rất nhiều loại truy vấn được\n","thực hiện. Nhằm tối ưu việc truy vấn, các giải pháp về truy vấn được\n","đề xuất, ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: caa26890-7579-45f2-be88-3c53a974df0a\n","Text: AI VIETNAM aivietnam.edu.vn Hình 5: Các nhiệm vụ trong công tác\n","Truy vấn - Retrieval 4.1.1 Xử lý dữ liệu Các mô hình RAG tốt thường\n","thích ứng với đa dạng các định dạng dữ liệu đầu vào. Tuy nhiên, một số\n","bài toán cho dữ liệu riêng cần được thiết kế công cụ đọc và bóc tách\n","dữ liệu hiệu quả.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 54e4dc3a-ad25-4ce3-b74f-c3c847e7640d\n","Text: Về phía các mô hình ngôn ngữ lớn (LLM) ở thời điểm hiện tại đều\n","gặp giới hạn về độ dài ngữ cảnh (context) văn bản mà LLM có thể xử lý.\n","Ví dụ: giới hạn 128k với GPT4 phiên bản mới nhất, 16k với GPT3.5, 32k\n","với Gemini, 32k Llama-2, 8k Mixtral 8x7b,... Ngoài ra, việc đưa lượng\n","lớn dữ liệu vào ngữ cảnh cho LLM được chứng minh là không hiệu quả với\n","m...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 337dbd65-2f02-4ecc-9c61-a474dc4f1211\n","Text: AI VIETNAM aivietnam.edu.vn cảnh. Độ dài của các phân đoạn có\n","thể linh hoạt ấn định cho phù hợp với nội dung ngữ nghĩa. Một số giải\n","pháp có thể kể đến như sau: • Sentence Window:là giải pháp phân chia\n","theo câu kết hợp cửa sổ mở rộng. Dữ liệu được ngữ nghĩa hoá (word2vec)\n","là đơn vị câu, tuy nhiên dữ liệu trả về là dữ liệu mở rộng cho câu lân\n","cận....\n","----------------------------------------------------------------------------------------------------\n","Node ID: f7d20ac7-348b-4aa4-a475-09b260a32ce3\n","Text: Các phương pháp đơn giản sẽ chuyển đổi cấu trúc bảng thành các\n","cấu trúc dễ hiểu hơn với máy như markdown hoặc html. 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: ab0dea18-8af3-4914-ad79-162e1928a191\n","Text: AI VIETNAM aivietnam.edu.vn Hình 8: Xử lý thông tin dạng bảng\n","với phép biến đổi markdown và html. Thông tin trích xuất trực tiếp với\n","Raw-text cho thấy cấu trúc bảng không rõ ràng. Cấu trúc bảng của\n","markdown và html cho phép các LLM dễ hiểu hơn nội dung trong bảng. Dữ\n","liệu có cấu trúc:Một số giải pháp coi dữ liệu dạng bảng như một nguồn\n","dữ liệu c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 571dc11f-07b5-4bb7-8bce-0d98bfb37d1c\n","Text: (2024) đề cập giải pháp có tên gọi Chain of Table, trong đó\n","phương pháp thiết lập một chuỗi các lập luận với LLM để thực thi các\n","truy vấn nhằm đạt được kết quả liên quan nhất với câu hỏi đầu vào.\n","Hình 9: So sánh giải pháp truy vấn thông tin bảng biểu (a) generic\n","reasoning, (b) program-aided reasoning, and (c) CHAIN-OF-TABLE..\n","Knowledge Graph: Bê...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8549e88c-473e-4d5e-9b46-d7c250c42e8b\n","Text: AI VIETNAM aivietnam.edu.vn Knowledge Graph được cân nhắc như\n","một giải pháp hiệu quả để quản lý kiến thức. Bởi sự phát triển mối\n","quan hệ theo chiều ngang, việc thiết lập các lập luận (reasoning) được\n","thực hiện dễ dàng hơn. Tuy nhiên, các mô hình Knowledge Graph thường\n","tốn công sức để xây dựng hơn các kiến trúc dữ liệu khác. 4.1.2\n","Embedding Model...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 620af339-c052-478b-ba07-da598bfe0d7a\n","Text: 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1df466a5-9896-4d11-a525-a23921248e08\n","Text: AI VIETNAM aivietnam.edu.vn Dựa trên kết quả đánh giá, một bảng\n","xếp hạng các mô hình embedding đã được xây dựng dựa vào các tiêu chí.\n","Hình 12: Bảng xếp hạng các mô hình embeddings dựa trên kết quả đánh\n","giá của Muennighoff et al. (2023) Hiệuchỉnh(fine-\n","tuning)môhìnhembedding: TrongcácgiảiphápRAG,hiệuchỉnhembedding là một\n","cách được cân nhắc để cải ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bde87291-967f-481a-a0a6-3ccd9c8368ee\n","Text: Mục tiêu là truy vấn được các thông tin liên quan nhất với câu\n","hỏi đầu vào. Một số giải pháp có thể đề cập đến như: 10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1c3fe1d9-85b9-4364-b361-56f57051135b\n","Text: AI VIETNAM aivietnam.edu.vn • Viết lại câu hỏi:Trong quá trình\n","hỏi đáp, việc làm rõ câu hỏi đầu vào giúp cho việc tổ chức truy vấn\n","hiệu quả. Việc sử dụng một LLM có hướng dẫn (instruction) để viết lại\n","câu hỏi đầu vào của người dùng là giải pháp phù hợp với những câu hỏi\n","quá đơn giản hoặc quá phức tạp. • Hypothetical Document Embedding:Tạo\n","ra câu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e4d08869-4dc8-47b1-bfc5-0a909f3fcca3\n","Text: Bộ lọc tìm kiếm giúp thu gọn phạm vi tìm kiếm, nâng cao chất\n","lượng truy vấn. 4.2 Mô hình sinh ngôn ngữ (Generation) Generation là\n","công đoạn cuối trong chu trình RAG với nhiệm vụ chuyển đổi thông tin\n","nhận được từ bối cảnh thành câu trả lời cho câu hỏi đầu vào của người\n","dùng thông qua một mô hình ngôn ngữ lớn. Quá trình sinh câu trả lời\n","cần đảm bả...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 87bfb093-45c3-4ef6-8308-1afb2cf2cad5\n","Text: AI VIETNAM aivietnam.edu.vn Hình 13: Kojima et al. (2023) sử\n","dụng Zero-Shot-CoT để cải thiện chất lượng câu trả lời của LLM • Hiệu\n","chỉnh mô hình ngôn ngữ lớn (Fine-tuning LLM):Công tác hiệu chỉnh mô\n","hình ngôn ngữ lớn thực hiện việc huấn luyện lại một phần tham số của\n","mô hình LLM gốc. Các dữ liệu huấn luyện được chuẩn bị trước từ nguồn\n","dữ liệu ri...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8c0b5f70-3e99-4643-a343-8aca0b08e1f5\n","Text: Công tác huấn luyện sẽ ghi đè một phần tham số trong bộ tham số\n","gốc. Giải pháp Fine-tuning được đánh giá là giải pháp hiệu quả cho\n","chất lượng kiểm soát tốt khi lượng dữ liệu huấn luyện đầy đủ, phủ khắp\n","các trường hợp hỏi đáp. Điểm trừ của giải pháp là giải pháp yêu cầu\n","nhiều công sức cho việc chuẩn bị dữ liệu, nhiều tài nguyên cho việc\n","huấn luyê...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 46dd0958-0520-499b-a5ec-34d0c359d272\n","Text: AI VIETNAM aivietnam.edu.vn Hình 14: Gao et al. (2024) đã hệ\n","thống các giải pháp đánh giá hệ thống RAG và danh sách các công cụ sử\n","dụng để đánh giá. Việc tự động hoá công tác đánh giá các hệ thống RAG\n","ngày càng phổ biến với sự xuất hiện của các công cụ như RAGAS, ARES,\n","TruLens. Phần III: Advanced RAG Ở phần này chúng ta sẽ cùng tìm hiểu\n","về vấn đ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ba22df1b-409b-4f81-b460-bedd5459d747\n","Text: Tuy nhiên, sự phổ biến và tính ứng dụng rộng rãi 13\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1793e37b-702b-4803-aa38-a8f72414fbbf\n","Text: AI VIETNAM aivietnam.edu.vn của RAG cũng dẫn đến một loạt thách\n","thức cần giải quyết. Vấn đề chính mà RAG đối mặt là việc tối ưu hóa\n","hiệu suất của nó, cụ thể là làm thế nào để quá trình truy xuất thông\n","tin diễn ra nhanh chóng hơn và kết quả thu được chính xác hơn.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4fc823a5-9b49-4243-830e-49ff2c59a845\n","Text: Bây giờ chúng ta sẽ tìm hiểu một số lý do mà một hệ thống Naive\n","RAG sẽ gặp phải làm ảnh hưởng tới hiệu suất của nó. Hình 15: Một số\n","vấn đề của Naive RAG Với hình trên chúng ta thấy rằng cả ba quá trình\n","Indexing, Retrieval và Generation đều gặp phải những vấn đề riêng của\n","chúng. Những vấn đề này có thể là độc lập hoặc liên quan với nhau để\n","\"cùng 14\n","----------------------------------------------------------------------------------------------------\n","Node ID: e928cf86-4419-4e02-90c1-95b876ab2f6a\n","Text: AI VIETNAM aivietnam.edu.vn nhau\" làm giảm hiệu suất của hệ\n","thống RAG. Với quá trình Indexing: • Quá trình Indexing còn chưa hoàn\n","thiện, vì nó chưa xử lý hiệu quả thông tin hữu ích trong hình ảnh,\n","biểu đồ và bảng biểu trong các tệp dữ liệu không cấu trúc như PDF. •\n","Quá trình chunking sử dụng chiến lược “one-size-fits-all” thay vì chọn\n","lựa các ch...\n","----------------------------------------------------------------------------------------------------\n","Node ID: cc11b64f-4f7c-4276-a61a-dd6a2d7eb6df\n","Text: Hơn nữa, nó không xem xét các chi tiết quan trọng, như các tiêu\n","đề hiện có trong văn bản. • Cấu trúc indexing chưa được tối ưu hóa đủ\n","mức, dẫn đến chức năng truy xuất chưa hiệu quả. • Khả năng biểu diễn\n","ngữ nghĩa của embedding model chưa đủ mạnh. Đối với quá trình\n","Retrieval • Mức độ liên quan của các ngữ cảnh được ghi nhớ không đủ và\n","độ chính xá...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 750ee97e-fed6-4d69-a85f-13d498e88ce2\n","Text: AI VIETNAM aivietnam.edu.vn loạt hướng dẫn cho máy đọc hoặc máy\n","in PDF biết cách hiển thị các ký tự trên màn hình hoặc giấy. Điều này\n","trái ngược với các định dạng tệp như HTML và docx, sử dụng các thẻ như\n","<p>, <w:p>, <table>, và <w:tbl> để tổ chức các cấu trúc logic khác\n","nhau. Thách thức trong việc phân tích cú pháp tài liệu PDF nằm ở việc\n","chính...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a8b8130c-3691-4e39-bb38-fe1c1103846b\n","Text: 16\n","----------------------------------------------------------------------------------------------------\n","Node ID: 849efe35-9da1-4f51-a306-b3db1357588d\n","Text: AI VIETNAM aivietnam.edu.vn Hình 17: Hình minh họa cho việc kết\n","hợp nhiều Deep learning Model cho quá trình parse PDF 7 Re-ranking Re-\n","ranking đóng vai trò quan trọng trong RAG. Trong naive RAG, một số\n","lượng lớn ngữ cảnh có thể được truy vấn, nhưng không phải tất cả đều\n","cần thiết liên quan đến câu hỏi. Re-ranking cho phép sắp xếp lại và\n","lọc các t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 16a19c0c-34e2-4ef0-b588-7bd78cc560f5\n","Text: Bằng cách hiểu sâu toàn bộ tài liệu và truy vấn, có thể nắm bắt\n","thông tin ngữ nghĩa một cách toàn diện hơn. 8 Exploring Semantic\n","Chunking 9 Exploring Query Rewriting Phần VI: Research in RAG 9.1\n","FLARE https://arxiv.org/pdf/2305.06983.pdf 17\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3cb14686-f2b4-4ee1-ae16-5de81b7d3b0a\n","Text: AI VIETNAM aivietnam.edu.vn Trong bài báo \"Active Retrieval\n","Augmented Generation\" Jiang et al., 2023, các tác giả giới thiệu\n","phương pháp mới có tên FLARE, viết tắt của Forward-Looking Active\n","Retrieval Augmented Generation. Phương pháp này cho phép các mô hình\n","ngôn ngữ lớn tích cực lựa chọn thời điểm và loại thông tin cần truy\n","vấn trong suốt quá ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1ca9a28e-7195-41d4-82aa-9db8c128f090\n","Text: FLARE hoạt động bằng cách tạo ra các câu tạm thời, và nếu nhận\n","thấy có từ nào trong câu đó không chắc chắn, nó sẽ tìm kiếm thông tin\n","liên quan để cải thiện câu tiếp theo. Điều này tiếp tục diễn ra cho\n","đến khi hoàn thành văn bản. Một ưu điểm đáng chú ý là FLARE có thể áp\n","dụng cho bất kỳ mô hình ngôn ngữ nào mà không cần phải huấn luyện lại\n","từ đầu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9118b5c3-24a6-4575-9e88-b5cf8a8e942e\n","Text: AI VIETNAM aivietnam.edu.vn FLAREdirect Trong phương\n","phápFLAREinstruct, có lúc các câu truy vấn được tạo ra không chính\n","xác, làm giảm độ tin cậy của quá trình truy vấn. Để giải quyết vấn đề\n","này, bài báo đề xuất một cách tiếp cận truy vấn chủ động khác, dựa vào\n","việc xem xét câu tiếp theo để quyết định thời điểm truy vấn. Cụ thể,\n","mô hình ngôn ngữ ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: eb11bab8-5801-4eb0-a14f-3b1377a0a832\n","Text: Khác với các mô hình RAG truyền thống có thể trích xuất thông\n","tin không liên quan, Self-RAG sử dụng các mã token đặc biệt gọi là\n","token phản ánh để làm cho mô hình có thể kiểm soát và thích nghi với\n","nhiều nhiệm vụ khác nhau. Các tác giả chứng minh rằng Self-RAG vượt\n","trội hơn các mô hình LLM hiện tại và các mô hình tăng cường tìm kiếm\n","trong các nh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 29a62177-04a2-4263-a490-15075c526841\n","Text: AI VIETNAM aivietnam.edu.vn Trong nghiên cứu này, các tác giả\n","huấn luyện mô hình LLM để tự sinh ra 4 loại token phải ứng (reflection\n","token) để hỗ trợ cho quá trình trả lời, bao gồm: • Retrieve: Cho mô\n","hình biết có nên truy vấn/tiếp tục truy vấn hay không. • IsRel: Đánh\n","giá xem kết quả truy vấn có cung cấp thông tin hữu dụng cho quá trình\n","trả lời...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 383b0315-c666-47f2-a8ad-7cd3d67db9b5\n","Text: 20\n","----------------------------------------------------------------------------------------------------\n","Node ID: 68246cef-dd3b-4e61-8b09-ed60aba0b310\n","Text: AI VIETNAM aivietnam.edu.vn 9.3 SELF-DISCOVER: Large Language\n","Models Self-Compose Reasoning Struc- tures\n","https://arxiv.org/pdf/2402.03620.pdf 52 y Trong các nghiên cứu, các\n","phương pháp prompting khác nhau thường được đề xuất để giải quyết một\n","vấn đề hết sức cụ thể, chẳng hạn như giải toán, suy luận theo từng\n","bước, hay suy luận số học. Tuy nhiên ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 825817d9-dd25-4e80-89c0-5f0d3933c3fa\n","Text: Phương pháp này được chia thành 2 gai đoạn. Ở giai đoạn đầu\n","tiên, phương pháp self-discover sẽ tìm ra những cấu trúc prompt suy\n","luận phù hợp nhất cho từng task. Sau đó các cấu trúc này sẽ được\n","format về dạng JSON, bới cấu trúc này được chứng minh là tăng cường\n","khả năng suy luận của mô hình Zhou et al., 2023. Bước này có mục tiêu\n","là tìm những cấu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ec31e595-8cc2-4c18-a1ec-e222c1cfa9e9\n","Text: AI VIETNAM aivietnam.edu.vn Trong các ứng dụng RAG thông thường,\n","thông tin ngữ cảnh truy vấn sẽ được đưa vào mô hình để sinh ra kết\n","quả, bất kết rằng thông tin đó có liên quan hay không. Điều này làm\n","cho kết quả sinh ra bởi mô hình trở nên nhiễu và làm chậm tốc độ tạo\n","sinh đi rất nhiều. Vì vậy nghiên cứu \"Corrective Retrieval Augmented\n","Generatio...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f396951f-94f5-4133-9400-37f1e475c076\n","Text: Cụ thể, phương pháp này hoạt động như sau. Cho một câu query và\n","nhiều văn bản được truy vấn. Một mô hình đánh giá gọn nhẹ sẽ được sử\n","dụng để đánh giá độ phù hợp của các văn bản với câu query đầu vào, quy\n","thành 3 mức độ: Đúng, Không đúng và không rõ ràng, tương ứng với những\n","hành động khác nhau. Nếu những văn bản truy vấn được là Đúng, những\n","văn ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a5b598c7-3cad-4c82-b420-ebe9f9642b00\n","Text: AI VIETNAM aivietnam.edu.vn quan sẽ được loại bỏ đi, trong khi\n","những dải có liên quan sẽ được giữ lại. Khi những văn bản truy vấn là\n","Không Đúng, quá trình tìm kiếm trên web sẽ được bắt đầu. Cụ thể câu\n","đầu vào sẽ được viết lại thành những keyword để mô phỏng quá trình tìm\n","kiếm trên web. Sau đó quá trình chắt lọc thông tin tương tự như bước\n","trên s...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1bd87b3e-5070-499c-b20a-845418babc7f\n","Text: Nhìn tổng quan, WikiChat bao gồm 2 giai đoạn chính, giai đoạn\n","đầu là thu thập thông tin dựa trên đoạn chat với người dùng, và giai\n","đoạn hai đưa ra câu trả lời dựa trên thông tin thu thập được. 5 bước\n","đầu của phương pháp này thuộc về giai đoạn 1, và 2 bước sau thuộc về\n","giai đoạn 2, sau đây chúng ta đi chi tiết từng giai đoạn. Giai đoạn 1:\n","Thu thậ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9b9e2152-d271-40d7-8221-60e2e95e07a7\n","Text: AI VIETNAM aivietnam.edu.vn Giai đoạn 2: Tạo ra câu trả lờiBước\n","tiếp theo là dùng những thông tin đã thu thập được tạo ra câu trả lời\n","chính xác. Nghiên cứu này chỉ ra rằng việc trực tiếp tạo ra câu trả\n","lời sẽ gây khó khăn trong việc duy trì tính hội thoại cho câu trả lời\n","của mô hình. Vì vậy, giai đoạn này sẽ chia thành 2 bước nhỏ. • Bước 6:\n","Wiki...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 082159ac-61fa-432c-ba44-d7fe187bd6bf\n","Text: (2021). Retrieval-augmented generation for knowledge-intensive\n","nlp tasks. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai,\n","Y., Sun, J., Guo, Q., Wang, M., & Wang, H. (2024). Retrieval-augmented\n","generation for large language models: A survey. Gao, L., Ma, X., Lin,\n","J., & Callan, J. (2022). Precise zero-shot dense retrieval without\n","relev...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5d3604a4-4c18-4ca0-8e8e-7365e53d452e\n","Text: AI VIETNAM aivietnam.edu.vn Kamradt, G. (2024).The 5 levels of\n","text splitting for retrieval. https://youtu.be/8OJC21T2SL4 Chen, T.,\n","Wang, H., Chen, S., Yu, W., Ma, K., Zhao, X., Zhang, H., & Yu, D.\n","(2023). Dense x retrieval: What retrieval granularity should we use?\n","Wang, Z., Zhang, H., Li, C.-L., Eisenschlos, J. M., Perot, V., Wang,\n","Z., Miculic...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ca00f925-45fd-4f76-8c74-ab911065b2b7\n","Text: (2023). Active retrieval augmented generation. Schick, T.,\n","Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Zettlemoyer, L.,\n","Cancedda, N., & Scialom, T. Toolformer: Language models can teach\n","themselves to use tools. In: 2023.\n","----------------------------------------------------------------------------------------------------\n","Node ID: eeb1a7b0-4428-4a6e-8c3b-666e129ca293\n","Text: Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le, Q.\n","V., Chi, E. H., Zhou, D., Mishra, S., & Zheng, H. S. (2024). Self-\n","discover: Large language models self-compose reasoning structures.\n","Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., &\n","Ba, J. (2023). Large language models are human-level prompt engineers.\n","Yan, S.-Q....\n","----------------------------------------------------------------------------------------------------\n","Node ID: 77cf48d9-d85f-485c-9336-80a7c8bb7345\n","Text: Switch-case Alternatives in Python Dinh-Tiem Nguyen và Quang-\n","Vinh Dinh 1 Switch Case Switch case là một cấu trúc điều khiển trong\n","lập trình giúp kiểm tra giá trị của một biến, biểu thức và thực hiện\n","các hành động khác nhau tùy thuộc vào giá trị đó. Để đơn giản thì hãy\n","tưởng tượng bạn có một chiếc hộp điều khiển với nhiều nút bấm.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 06e0d205-4999-4516-97d9-b64014731328\n","Text: Mỗi nút bấm trên chiếc hộp này sẽ làm một việc khác nhau. Khi\n","bạn nhấn vào một nút, chiếc hộp sẽ biết cần phải làm gì tiếp theo. Đó\n","chính là cách mà switch case hoạt động. Trong biểu đồ trên thể hiện\n","các thành phần và cách switch case hoạt động, trong đó: • Switch\n","Expression (Biểu thức kiểm tra): Chúng ta bắt đầu với việc kiểm tra\n","biểu thức. Ví ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6e34afae-8f7b-4419-bf83-88cdb64b73c8\n","Text: AIO2024 aivietnam.edu.vn • Case 2: Nếu biểu thức không khớp với\n","trường hợp đầu tiên, chúng ta kiểm tra trường hợp thứ hai. Nếu biểu\n","thức khớp với trường hợp này, chúng ta thực hiện \"Statement 2\"và sau\n","đó kết thúc. Ví dụ: Case bạn muốn ăn chuối thì thực hiện Statement 1:\n","Lấy trên bàn ăn. • Case n: Tiếp tục kiểm tra các trường hợp khác cho\n","đến khi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a7e97ad6-d1da-4c02-8f8f-63599d71065e\n","Text: Ta sẽ viết chương trình nhập đầu vào là thời gian để xem hoạt\n","động của Tom. Input Output 5 AM Wake up 6 AM Yoga 7 AM Work Giá trị\n","khác Do something else 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 69f3dc0a-9e23-419c-9bf0-1d5a6ae97066\n","Text: AIO2024 aivietnam.edu.vn 2.1 If-elif-else Cách phổ biến nhất để\n","thay thế switch case trong python là sử dụng cấu trúc if-elif-else.\n","Cấu trúc này bắt đầu bằng câu lệnh if và điều kiện condition, nếu điều\n","kiện này đúng khối lệnh trong if(statement) sẽ được thực hiện. Nếu\n","điều kiện sai sẽ chuyển sang câu lệnh elif và tiếp tục kiểm tra điều\n","kiện. Nế...\n","----------------------------------------------------------------------------------------------------\n","Node ID: cda81e59-dc51-4b47-9696-253294dd7d90\n","Text: Sau đây là cách sử dụng dictionary để xây dựng chương trình theo\n","dõi hoạt động của Tom. 1 todo = {\"5 AM\": \" Wake up\", 2 \"6 AM\": \" Yoga\n","\", 3 \"7 AM\": \" Work \"} 4 time = input (\" Input time : \") 5 todo . get\n","(time , \"Do something else \") Đầu tiên chúng ta tạo một từ điển todo\n","với các cặp khóa-giá trị. Mỗi khóa là một thời gian cụ thể (ví dụ: \"5\n","AM\"...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 694d42f2-845e-4f37-bca6-7ddc7b5d1417\n","Text: AIO2024 aivietnam.edu.vn up\", \"Yoga\", \"Work\"). Tiếp theo chương\n","trình yêu cầu người dùng nhập thời gian.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5b25e8d3-8b1a-4ab6-98b0-ff0f163652c5\n","Text: Cuối cùng chúng ta sử dụng phương thức get của từ điển để tìm\n","công việc tương ứng với thời gian nhập vào. Nếu thời gian nhập vào có\n","trong từ điển, phương thức get sẽ trả về công việc tương ứng. Nếu thời\n","gian nhập vào không có trong từ điển, phương thức get sẽ trả về giá\n","trị mặc định là \"Do something else\". 2.3 Mactch case Cấu trúc match\n","case tro...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1a262eb1-1d1f-4df2-aba7-7afce400cf51\n","Text: Text-to-Video Generation Guide Hoang-Bach Ngo Minh-Hung An Ngày\n","28 tháng 2 năm 2024 Phần I: Giới thiệu tổng quan Trong những năm gần\n","đây, sự phát triển không ngừng của công nghệ đã mở đường cho một bước\n","đột phá đáng kinh ngạc trong lĩnh vực tạo sinh nội dung: khả năng\n","chuyển đổi văn bản thành video, một tiến bộ mới nhất trong chuỗi các\n","thành tựu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 27238f57-bf53-41d5-a382-58c6d9fd25f7\n","Text: Mô hình Sora, phát triển bởi OpenAI, hay Genie, phát triển bởi\n","Google, là những minh chứng nổi bật cho tiến trình này, đánh dấu một\n","cột mốc quan trọng trong việc mô hình hóa và tạo sinh video dựa trên\n","văn bản, giúp biến những mô tả văn bản thành video sinh động với mạch\n","lạc thời gian và không gian cao. Lý do phía sau việc nghiên cứu và\n","phát triể...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2b7972d9-a878-4ddf-acad-4b379ff55a37\n","Text: AI VIETNAM aivietnam.edu.vn Phần II: Các thách thức cho bài toán\n","text-to-video Trước hết, chúng ta hãy cùng nhau khám phá những thách\n","thức lớn mà công nghệ tạo sinh video phải đối mặt. Những thách thức\n","này chủ yếu liên quan đến việc giữ cho video có sự nhất quán về không\n","gian và liên kết mạch lạc theo thời gian, đồng thời cũng cần phải quan\n","tâm ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8a2af15c-2f6a-4ab7-b463-90835f75a846\n","Text: Mỗi khung hình trong video được tạo ra đều phải liến kết đến tất\n","cả các khung hình trước đó, đến từng chi tiết để đảm bảo độ chính xác\n","về không gian và quá trình chuyển đổi giữa các khung hình phải được mô\n","hình hóa cẩn thận để duy trì dòng thời gian. Quá trình này yêu cầu tài\n","nguyên tính toán khổng lồ [8] [11], đặc biệt đối với các video có độ\n","p...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e935dc11-c8ba-4bda-abb8-830e9abd422c\n","Text: AI VIETNAM aivietnam.edu.vn thuật học chuyển giao từ các nhiệm\n","vụ có liên quan. Mặc dù đã có nhiều cố gắng, việc vượt qua những trở\n","ngại liên quan đến việc tiếp cận dữ liệu vẫn là một bước tiến quan\n","trọng để phát triển công nghệ chuyển đổi văn bản thành video. Nhìn\n","chung đó là những thách thức chính cho bài toán tạo sinh video từ văn\n","bản. Ở phần...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6bf4a6ae-9396-4ca7-9190-21907b47b5d5\n","Text: 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1558bce9-aff7-4c94-9ebb-c5b7cb468a54\n","Text: AI VIETNAM aivietnam.edu.vn 3.2 WebVid WebVid\n","(https://maxbain.com/webvid-dataset)là tập dữ liệu được thu thập từ\n","web, bao gồm 2.5 triệu cặp video-text, vượt trội hơn hẳn so với các\n","tập dữ liệu trước đó. Quá trình thu thập dữ liệu này tuân thủ một\n","phương pháp tương tự như Google Conceptual Captions, với việc nhận\n","thấy một phần đáng kể các hình ả...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c8205040-be66-4fa6-bd73-ce38c4020543\n","Text: Sự vượt trội của CelebV-Text so với các tập dữ liệu khác được\n","thể hiện qua phân tích thống kê toàn diện về video, văn bản mô tả và\n","mối liên quan text-video. 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: 62e2df76-0800-4af2-b8f3-368e69d9f5c9\n","Text: AI VIETNAM aivietnam.edu.vn Hình 3: CelebV-Text bao gồm (a)\n","70,000 mẫu video và (b) 1,400,000 văn bản mô tả. Mỗi mẫu video được\n","ghi chú với hình dáng tổng quát, hình dáng chi tiết, điều kiện ánh\n","sáng, hành động, cảm xúc, và hướng ánh sáng. Phần IV: Các hướng tiếp\n","cận chính Khi nói đến việc tạo video từ văn bản, có hai phương pháp\n","tiếp cận chính ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fdc283f9-5a30-4075-bcec-3120a3a27e35\n","Text: Trong VideoGPT, có một số thành phần chính được đưa ra, mỗi\n","thành phần đều đóng vai trò quan trọng 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: e4b0e585-36b7-4f5a-966c-37ca8322b224\n","Text: AI VIETNAM aivietnam.edu.vn trong việc tạo ra các video chất\n","lượng cao và đa dạng từ văn bản hoặc các dạng đầu vào khác. Một số\n","thành phần chính của VideoGPT được thể hiện trong Hình 7 bao gồm: Hình\n","4: Kiến trúc của VideoGPT bao gồm 2 thành phần: khối VQ-VAE và khối\n","GPT • VQ-VAE (Vector Quantized Variational AutoEncoder): được đề xuất\n","bởi [4], l...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b7e3dc88-ba99-49c2-87d4-80b9f18f44ee\n","Text: Tóm lại, videoGPT giới thiệu một kiến trúc đơn giản và hiệu quả\n","cho bài toán tạo sinh video, sử dụng 2 thành phần chính là khối VQ-VAE\n","có chức năng học không gian latent rời rạc của các video và khối kiến\n","trúc tương tự như GPT để mô hình hóa và tạo sinh chuỗi. Sự đơn giản\n","của phương pháp này cùng với tính hiệu quả của nó mang đến một hướng\n","nghiê...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fbdd3ce4-70f1-4396-a90d-5a63852209e2\n","Text: AI VIETNAM aivietnam.edu.vn 4.2 Make-A-Video Được giới thiệu\n","trong [7], đây là một phương pháp tạo video từ văn bản mà không cần dữ\n","liệu video đi kèm văn bản mô tả, từ đó giải quyết vấn đề về thiếu hụt\n","data trong tác vụ tạo sinh video. Phương pháp này mở rộng từ mô hình\n","tạo hình ảnh từ văn bản mô tả (Text to Image-T2I) bằng cách thêm vào\n","các mô-...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 746f29e0-0b31-4d27-9388-7230b90cfe5d\n","Text: Ngoài ra, spatiotemporal decoder network tạo ra những khung hình\n","RGB độ phân giải thấp ban đầu. Các khung hình này sau đó được cải\n","thiện bởi interpolation network và các super-resolution networks đảm\n","bảo sự nhất quán giữa các khung hình để tránh flickering artifacts.\n","Super-resolution được sử dụng do hạn chế về bộ nhớ và khả năng tính\n","toán cùng v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 127f4496-938c-4c76-88a2-ec5e115326f5\n","Text: AI VIETNAM aivietnam.edu.vn Hình 7: Kiến trúc và cơ chế khởi tạo\n","của Pseudo-3D convolutional và attention layers, cho phép chuyển đổi\n","mô hình T2I đã được huấn luyện trước sang chiều không gian thời gian.\n","– Pseudo-3D convolutional layers: Việc sử dụng Pseudo-3D convolutional\n","layers nhằm nâng cao 2D convolutional network cho temporal learning mà\n","k...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 467a88f1-608d-4af7-9490-ba4d811c53e0\n","Text: Cách tiếp cận này giúp khả thi về quản lý tài nguyên bộ nhớ và\n","tính toán, đồng thời giải quyết thách thức tích hợp thời gian vào tạo\n","ảnh và video. • Spatiotemporal network còn có thêm một thành phần quan\n","trọng cho việc tạo video làframe interpolation networknhằm cải thiện\n","tốc độ tạo khung hình. Network có thể tăng số lượng khung hình của\n","video đ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bd30f40c-8100-4800-8fe3-32471f49c638\n","Text: AI VIETNAM aivietnam.edu.vn 4.3 Phenaki Hình 8: Kiến trúc chung\n","của mô hình Phenaki Mô hình Phenaki, được trình bày trong công trình\n","\"Phenaki: Variable Length Video Generation From Open Domain Textual\n","Description\" [8], mở ra một hướng mới trong việc tạo video từ văn bản.\n","Được thiết kế để tạo ra những đoạn video chân thực từ các mô tả văn\n","bản, Ph...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8cec229c-7859-49a8-a7bb-dda440b8718c\n","Text: Đó là việc tối ưu hóa hiệu suất tính toán, đối mặt với tình\n","trạng thiếu hụt nguồn dữ liệu văn bản-video chất lượng cao và duy trì\n","sự nhất quán trong không gian cũng như tính liên kết về thời gian cho\n","các video dài. Bằng cách biến đổi các đoạn văn bản thành những câu\n","chuyện video liền mạch, Phenaki không chỉ đặt ra một tiêu chuẩn mới mà\n","còn cho t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 98901af3-f4a7-4208-8804-89eba8c03294\n","Text: AI VIETNAM aivietnam.edu.vn sinh video và nén video trong các\n","chiều thời gian và không gian, trong khi vẫn có tính hồi quy theo thời\n","gian. Khối Encoder: Hình 9: Kiến trúc của khối encoder Khối encoder\n","chuyển chuỗi video thành các token biểu diễn nhỏ gọn phù hợp để xử lý\n","bởi các lớp transformers của mô hình.Quy trình bắt đầu với một chuỗi\n","video b...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7b4bdd14-74ad-4478-be11-b6dd5ab4f299\n","Text: Quá trình nén được thực hiện bằng cách trích xuất các patches\n","không chồng lấn từ khung hình đầu tiên và các khung hình video tiếp\n","theo, sau đó được làm phẳng và chiếu vào không gian chiều thấp hơn.\n","Các chiều không gian và thời gian của các patches này được sắp xếp lại\n","thành định dạng tensor phân biệt rõ ràng giữa chiều không gian và\n","chiều thời g...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 654440e5-4b47-41ef-9910-5461ec580380\n","Text: AI VIETNAM aivietnam.edu.vn các lớp transformer bổ sung trên\n","chiều thời gian sử dụng causal attention. Các lớp này có nhiệm vụ học\n","thông tin về thời gian. Điều này đảm bảo rằng mỗi token không gian chỉ\n","tương tác với các token không gian từ các khung hình trước, cho phép\n","khung hình đầu tiên được mã hóa độc lập. Thiết kế này tạo điều kiện\n","cho việc...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 08d7d763-64ce-4e58-b57c-498fc89c791b\n","Text: Tương tự như trong kiến trúc VQ-VAE. Khối Decoder: Khối decoder\n","của C-ViViT đơn giản là một khối encoder được lật ngược. Đầu tiên\n","token được chuyển đổi thành các embedding. Sau đó là temporal\n","transformer, tiếp theo là spatial transformer. Đầu ra sau đó được áp\n","dụng một phép chiếu tuyến tính đơn để ánh xạ các token trở lại không\n","gian pixel. 4.3.2...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3d087715-9c81-4fbc-b147-4844eb10d8ec\n","Text: AI VIETNAM aivietnam.edu.vn 4.3.3 Tổng kết Phenaki được giới\n","thiệu là một mô hình có khả năng tạo ra video có độ dài biến thiên dựa\n","trên chuỗi prompt văn bản từ các chủ đề mở. Nó sử dụng C-ViViT làm bộ\n","mã hóa video, một mô hình mới cung cấp khả năng nén không gian-thời\n","gian hiệu quả trong khi vẫn duy trì tính tự hồi quy theo thời gian.\n","Phenaki c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f4c50078-dc40-4fb1-8bd8-7e5dc93d0e5e\n","Text: Một sự thay đổi lớn so với khung hình trước có thể gây ra sai số\n","lớn. Điều này khiến các mô hình ít có xu hướng khám phá mối liên hệ\n","dài hạn vì đơn giản sao chép khung hình trước đó giống như một lối\n","tắt. Trong quá trình huấn luyện, nhằm đạt được được sự khớp chính xác\n","giữa văn bản mô tả và hình ảnh. CogVideo thiết lập trước một dãy các\n","frame-ra...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 620be051-73f1-4ea7-881b-ad010859b571\n","Text: AI VIETNAM aivietnam.edu.vn Hình 12: Multi-frame-rate\n","hierarchical generation framework in CogVideo Quá trình Multi-frame-\n","rate hierarchical generation của CogVideo là một quá trình đệ quy gồm\n","hai giai đoạn. Đầu tiên là sinh tuần tự khung hình chủ chốt dựa trên\n","frame-rate thấp và văn bản mô tả; thứ hai là nội suy đệ quy các khung\n","hình dựa trên vă...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e93faea-e57f-410a-b7ba-562926d258bb\n","Text: 13\n","----------------------------------------------------------------------------------------------------\n","Node ID: e0e7875d-e2e3-4d6c-aa07-776a1cf17ac1\n","Text: AI VIETNAM aivietnam.edu.vn Hình 13: Dual-channel attention\n","block Để giảm thiểu áp lực về thời gian và bộ nhớ trong quá trình huấn\n","luyện và suy luận, CogVideo áp dụng Swin Attention, mở rộng nó cho các\n","tình huống auto-regressive và temporal scenario bằng cách sử dụng\n","auto-regressive attention mask trong shifted windows. Phát hiện thú vị\n","là Swin ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 07e01aeb-09af-4215-90c9-907f4d841043\n","Text: Các nghiên cứu tiên tiến trong lĩnh vực này đang khám phá cách\n","thức áp dụng các nguyên tắc diffusion để tạo ra video không chỉ có\n","chất lượng hình ảnh sắc nét mà còn đảm bảo tính liên tục và mượt mà về\n","mặt chuyển động. Dưới đây là một số công trình nổi bật đã đưa ra cách\n","tiếp cận sử dụng mô hình diffusion cho việc sinh video, mở ra những\n","khả năng...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0b367dc1-aadc-4868-acd3-080ef5de9425\n","Text: AI VIETNAM aivietnam.edu.vn 5.1 Video Diffusion Models Đây là\n","một trong những công trình nghiên cứu đầu tiến tiếp cận theo hướng sử\n","dụng mô hình diffusion để tạo sinh video. Video Diffusion Models [2]\n","là sự mở rộng tự nhiên của kiến trúc image diffusion và cho phép đào\n","tạo chung từ dữ liệu ảnh và video. Để tạo ra video dài và độ phân giải\n","cao hơ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e119cdbe-3702-4f4d-a1c7-9e03e2f86e0f\n","Text: 5.2 MagicVideo MagicVideo [11] có thể tạo ra các đoạn video mượt\n","mà phù hợp với văn bản mô tả đã cho. Nhờ vào kiến trúc 3D U-Net và\n","hiệu quả cùng với việc mô hình hóa phân phối video trong không gian\n","low- dimensional. MagicVideo có thể tổng hợp video 256×256 spatial\n","resolution trên một card GPU, giảm đến 64 lần lượng tính toán so với\n","Video Diffu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 23ea7d81-7095-48c1-aae8-5738ce78f78d\n","Text: AI VIETNAM aivietnam.edu.vn Hình 16: (a) data flow cho cả quá\n","trình training và inference: trong quá trình training, timestep t sẽ\n","được chọn mẫu ngẫu nhiên từ [0, T] và các khung hình video đầu vào bị\n","làm nhiễu qua quá trình lan truyền, U-Net được sử dụng để học cách tái\n","tạo các khung hình video. Gaussian noise được chọn mẫu ngẫu nhiên\n","trong suy...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 45dd9014-8551-4ad4-ae29-5556ff679370\n","Text: Để cải thiện chất lượng hình ảnh mà không tăng tính toán, tác 16\n","----------------------------------------------------------------------------------------------------\n","Node ID: a7e9ea2d-986b-447d-8a1c-acfe87b74175\n","Text: AI VIETNAM aivietnam.edu.vn giả đã giữ kích thước thấp cho\n","latent features và thêm vào decoder hai khối temporal directed\n","attention layers, tạo nên VideoVAE decoder giúp hiệu quả trong việc\n","giảm thiểu dithering. 5.3 Tune-A-Video \"Tune-A-Video: One-Shot Tuning\n","of Image Diffusion Models for Text-to-Video Generation\" [9] là paper\n","giới thiệu một phư...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 46490be8-a8e6-4cc2-a90b-2b94f52974a3\n","Text: Cụ thể với một video cho trước, nhiệm vụ của model là fine-tune\n","cụ thể trên video đó, để từ đó có thể edit video đó dựa theo yêu cầu\n","từ text prompt của user. Điều này giúp giảm chi phí tính toán, khi\n","thay vì phải train trên một tập dataset về video lớn, với chi phí về\n","gán nhãn và về phần cứng khổng lồ, ta có thể tận dụng các mô hình\n","Text-to-imag...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d44eefb8-fc70-4194-8199-4d81903ae411\n","Text: AI VIETNAM aivietnam.edu.vn Paper này sử dụng Latent Diffusion\n","Model (LDM, [6]) đã được pretrained như một mô hình text-to- image.\n","Trong đó LDM sử dụng kiến trúc U-Net, sử dụng nhiều khối Convolution\n","2D và khối transformer, với từng khối transformer sử dụng các layer\n","self-attention, cross-attention và feed-forward. Để sử dụng các khối\n","này cho tá...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 37026502-cd0d-41c8-8c3c-75aa89295279\n","Text: Hiệu quả của phương pháp này được chứng minh thông qua các thử\n","nghiệm rộng rãi trên nhiều ứng dụng, cho thấy khả năng ấn tượng của\n","nó. 6 Diffusion Models with Transformer Đây là một phương pháp kết hợp\n","những điểm mạnh của cả hai cách làm trước đây. Bằng cách dùng mô hình\n","diffusion cùng với kiến trúc transformer - thay vì dùng kiến trúc UNet\n","thôn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 868efcf5-c89a-45db-b2b6-8ccafef106d9\n","Text: AI VIETNAM aivietnam.edu.vn mở rộng của các Diffusion\n","Transformers (DiTs) thông qua phân tích độ phức tạp của quá trình lan\n","truyền xuôi, được đo bằng Gflops. Nghiên cứu đã tìm thấy một xu hướng\n","nhất quán, khi mô hình DiTs có Gflops cao hơn, đạt được thông qua việc\n","tăng độ sâu/rộng của transformer hoặc số lượng token đầu vào, thể hiện\n","điểm Freche...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fe19d794-0e93-4cf6-b565-fcdcd128bfec\n","Text: Đầu vào của khối DiT là biểu diễn trong không gian latentz của\n","một tấm ảnh (với một tấm ảnh kích thước256 ×256 ×3, z có kích thước32\n","×32 ×4). Sau đóz sẽ được chia thành từng patches ảnh nhỏ, sau đó trải\n","phẳng để tạo thành một chuỗiT các token hình ảnh, với mỗi token có\n","chiều làd. 6.1 Diffusion Transformer Bây giờ chúng ta sẽ tìm hiểu tổng\n","quan v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: eda320ea-90d6-4d87-a655-cb9fa77d435b\n","Text: AI VIETNAM aivietnam.edu.vn Hình 22: DiT Block. DiT block\n","design- Với DiT block tác giả đã có một số những thay đổi nhỏ so với\n","ViT block chuẩn thông thường: • In-context conditioning: DiT thêm các\n","vector embedding của timestep và class labels như là hai token bổ sung\n","trong chuỗi đầu vào, xử lý chúng không khác gì so với các token ảnh.\n","Điều này t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7c816b26-0a50-4e78-ad8d-d6c83bebf264\n","Text: DiT áp dụng chiến lược này trong khối adaLN của mô hình DiT,\n","đồng thời hồi quy thêm các tham số tỷ lệ để tối ưu hóa hiệu suất trước\n","khi thực hiện kết nối phần dư. Phần V: Mô hình Sora Sora là một mô\n","hình AI có khả năng tạo ra các video lên đến một phút, với chất lượng\n","hình ảnh cao và nội dung bám sát theo yêu cầu của người dùng. Mô hình\n","hiện đan...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 374f9ac1-ab3d-460d-8559-60bc753fbdae\n","Text: AI VIETNAM aivietnam.edu.vn để đánh giá về các rủi ro tiềm ẩn và\n","cũng được cung cấp cho các nghệ sĩ, nhà thiết kế và nhà làm phim để\n","thu thập phản hồi nhằm cải thiện mô hình cho nhu cầu sáng tạo. Sora có\n","thể tạo ra các cảnh phức tạp với nhiều nhân vật, các loại chuyển động\n","cụ thể, và chi tiết chính xác về đối tượng và phông nền. Mặc dù có khả\n","nă...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e6ccfc28-5214-46f7-8929-3708a8526d2c\n","Text: Biểu diễn dựa trên patches của chúng cho phép Sora huấn luyện\n","trên video và hình ảnh với độ phân giải, thời lượng và tỷ lệ khung\n","hình đa dạng. Tại thời điểm suy luận, 21\n","----------------------------------------------------------------------------------------------------\n","Node ID: f4f9dfba-916b-4b19-b032-d4b5237376fb\n","Text: AI VIETNAM aivietnam.edu.vn chúng ta có thể kiểm soát kích thước\n","của video được sinh ra bằng cách sắp xếp các patches được khởi tạo\n","ngẫu nhiên trong một lưới có kích thước phù hợp. Mở rộng quy mô mạng\n","transformer cho việc tạo sinh video- Như chúng ta đã biết diffusion\n","model là một loại mô hình sinh mô phỏng quá trình xoá mờ và tái tạo dữ\n","liệu, t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4c3c7e77-1b7c-46ea-a86d-516615d8c473\n","Text: Quá trình này thường được hỗ trợ bởi thông tin điều kiện, như\n","văn bản mô tả, để hướng dẫn quá trình tái tạo dữ liệu theo mong muốn.\n","Trong bối cảnh của Sora, diffusion model được áp dụng để dự đoán và\n","tái tạo các clean patches từ các noisy patches, với sự hỗ trợ của\n","thông tin điều kiện như văn bản mô tả. Đặc biệt, Sora kết hợp cấu trúc\n","của diffus...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 530f7e6f-37a9-4ba8-baa8-5624b11c820d\n","Text: AI VIETNAM aivietnam.edu.vn nhưng tiềm năng để phát triển thành\n","một hệ thống mô phỏng thực tế ảo, nơi vật thể và con người có thể\n","tương tác một cách tự nhiên, là điều không thể phủ nhận. SORA không\n","chỉ mở ra cánh cửa cho những cải tiến công nghệ tiếp theo mà còn hứa\n","hẹn sẽ mang lại những ứng dụng sáng tạo và cách mạng trong các ngành\n","như metaver...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f7d73b09-b63d-468f-8534-3b47c4175fb7\n","Text: Điều này không chỉ làm giảm bớt gánh nặng về dữ liệu mà còn tạo\n","điều kiện cho việc áp dụng rộng rãi. Hơn nữa, latent action space mà\n","Genie học được mở ra khả năng đào tạo các đại lý để mô phỏng hành vi\n","từ video chưa từng thấy, tiến một bước dài hướng tới mục tiêu phát\n","triển các generalist agents cho tương lai. Genie bao gồm ba thành phần\n","chính: ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6cfa286c-2599-42fa-93cf-210e0f73d97a\n","Text: AI VIETNAM aivietnam.edu.vn Hình 23: Kiến trúc khôi ST-\n","Transformer pháp này giúp cân đối hiệu quả giữa khả năng xử lý của mô\n","hình và các giới hạn về khả năng tính toán. Kiến trúc của khối này\n","được minh họa trong Hình 23. Khác biệt so với các khối transformer\n","thông thường, nơi mỗi phần tử dữ liệu (token) quan tâm đến mọi phần tử\n","khác, ST-transfor...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e2faac0-1624-4587-9757-f5820e2db95a\n","Text: Để giải quyết vấn đề này, mô hình sử dụng khối LAM, giúp máy học\n","được các hành động ẩn mà không cần dữ liệu được gán nhãn cụ thể. Khối\n","LAM hoạt động bằng cách xem xét một loạt các khung hình trước và sau\n","đó dự đoán chuỗi hành động ẩn có thể dẫn đến khung hình tiếp theo. Một\n","bộ giải mã sau đó sử dụng thông tin này cùng với chuỗi khung hình để\n","dự ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7ee4401c-b053-4ed0-88ed-315171457bc7\n","Text: AI VIETNAM aivietnam.edu.vn Video TokenizerKhối này có nhiệm vụ\n","nén các video thành những token rời rạc, với mục đích để giảm chiều và\n","tăng chất lượng tạo sinh video. Một lần nữa kiến trúc VQ-VAE lại được\n","sử dụng cho thành phần này.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 84488a45-4014-4d4d-9a50-886050fa2821\n","Text: Khác với những công trình nghiên cứu khác chỉ tập trung vào việc\n","nén trong chiều không gian, nghiên cứu này sử dụng khối ST-transformer\n","trong cả encoder và decoder để có thể đảm bảo sự liên kết trong chiều\n","thời gian. Khối Dynamics Model Đây là một khối decoder transformer\n","tương tự như trong nghiên cứu MaskGIT [1]. Ở mỗi bước, khối này nhận\n","đầu v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 43fe5a76-cca4-4432-ba51-ade2040bcd8a\n","Text: AI VIETNAM aivietnam.edu.vn nhiều lĩnh vực khác nhau như điện\n","ảnh, làm game, thực tế ảo, v.v. Hơn hết, cả 2 mô hình đều cho thấy\n","việc huấn luyện các mô hình deep learning trên tập data lớn về video\n","có thể giúp các mô hình trí tuệ nhân tạo học được cách vận hành của\n","thế giới vật chất, mở ra hướng mới cho việc phát triển AI có khả năng\n","tương tác v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 598f2494-9050-4a8a-a6be-3e27e9bb3edb\n","Text: [10] Wilson Yan et al. VideoGPT: Video Generation using VQ-VAE\n","and Transformers. 2021. arXiv: 2104.10157 [cs.CV]. [11] Wang W. Yan H.\n","Lv W. Zhu Y. Feng J. Zhou D. “MagicVideo: Efficient Video Generation\n","With Latent Diffusion Models”. In:arXiv preprint\n","arXiv:2211.11018(2022).\n","----------------------------------------------------------------------------------------------------\n","Node ID: 53672d0f-a496-472d-87f2-fbb1b13bd357\n","Text: - Hết - 26\n","----------------------------------------------------------------------------------------------------\n","Node ID: dda93eed-3a75-4f83-abdf-7734ee210d6b\n","Text: AI VIET NAM – AI COURSE 2024 Tutorial: Phát hiện đối tượng trong\n","ảnh với YOLOv10 Dinh-Thang Duong, Nguyen-Thuan Duong, Minh-Duc Bui và\n","Quang-Vinh Dinh Ngày 30 tháng 5 năm 2024 I. Giới thiệu Object\n","Detection (Tạm dịch: Phát hiện đối tượng)là một bài toán cổ điển thuộc\n","lĩnh vực Computer Vision. Mục tiêu của bài toán này là tự động xác\n","định vị trí ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 279781dd-1648-43fa-a989-3a9ab14798bc\n","Text: Tính tới thời điểm hiện tại, đã có rất nhiều phương pháp được\n","phát triển nhằm giải quyết hiệu quả bài toán này. Trong đó, các phương\n","pháp thuộc họ YOLO (You Only Look Once) thu hút được sự chú ý rất lớn\n","từ cộng đồng nghiên cứu bởi độ chính xác và tốc độ thực thi mà loại mô\n","hình này mang lại. Hình 1: Logo của mô hình YOLO. Ảnh: link. Thời gian\n","vừ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4c494b49-2501-4ed9-9353-d7aeee5d39df\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Hình 2: Hiệu suất của\n","YOLOv10 khi so sánh với các mô hình khác. Trên tập dữ liệu COCO,\n","YOLOv10 đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số\n","lượng tham số mô hình (Number of parameters) trong khi vẫn giữ được độ\n","chính xác (COCO AP) cao. Ảnh: [10].\n","----------------------------------------------------------------------------------------------------\n","Node ID: ef81f7eb-c877-48bb-bdd2-ed4bca3aee18\n","Text: Trong bài viết này, chúng ta sẽ cùng nhau tìm hiểu về YOLOv10 và\n","cách sử dụng mô hình này. Thông qua đó, nhóm cũng sẽ trình bày sơ lược\n","về bài toán Object Detection cũng như tóm tắt ngắn gọn các phiên bản\n","YOLO trước đó để bạn đọc có một cái nhìn tổng quan hơn về nội dung\n","này. Theo đó, bài viết được bố cục như sau: - Phần I:Giới thiệu về nội\n","dung...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 901ad827-12ef-431d-978c-a0fc246f064c\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn II. Bài toán Object\n","Detection và các phiên bản YOLO đời trước II.I. Bài toán Object\n","Detection Trong Computer Vision, bài toán Object Detection hướng đến\n","xây dựng một chương trình có thể tự động xác định vị trí và nhận diện\n","tên (class) của các vật thể trong một bức ảnh. Tổng hợp hai thông tin\n","đầu ra này còn đ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 11983ef3-3aef-4c97-8237-fe69cd77d2bd\n","Text: 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 25a7ccfd-ffda-4e9c-bb70-60c882ef3950\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Ở phần sau, chúng ta sẽ\n","tập trung điểm qua các phiên bản YOLO (từ v1 đến v9). II.II. YOLOv1\n","YOLOv1 [1] là mô hình one-stage (hoặc single-stage) real-time object\n","detection được giới thiệu vào năm 2016. Hình 4: Kiến trúc mô hình\n","YOLOv1 với 24 lớp conv và 2 lớp mlp. Ảnh: [1]. - Điểm mới: YOLOv1 sử\n","dụng một mạng...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 208a4d80-1089-44ec-bbf5-b9cf41071646\n","Text: 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: 292b276e-1ae1-4070-903d-8498d21239ff\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn - Điểm mới: Sử dụng anchor\n","boxes, mạng Darknet-19, và tăng training data để tăng độ chính xác. -\n","Ưu điểm: Tăng độ chính xác và khả năng nhận diện nhiều object trong 1\n","cell. - Nhược điểm: Phức tạp hơn, cần nhiều tài nguyên tính toán, và\n","khó detect các object nhỏ. II.IV.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 45fbd727-4ebd-40ca-b4ab-a97b8de27f08\n","Text: YOLOv3 YOLOv3 [3] ra mắt năm 2018, tiếp tục cải tiến từ YOLOv2.\n","Hình 6: Kiến trúc mô hình YOLOv3. Ảnh: [3]. - Điểm mới: Sử dụng mạng\n","Darknet-53 và detect object ở ba cấp độ khác nhau (multi-scale\n","detection) để cải thiện độ chính xác. - Ưu điểm: Độ chính xác cao hơn,\n","khả năng phát hiện object nhỏ tốt hơn. - Nhược điểm: Tốc độ chậm hơn\n","so với các ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6940c4eb-fd6c-43d2-9487-8e9b21fb59b0\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn II.VI. YOLOv5 YOLOv5 [5],\n","không phải do tác giả gốc phát triển, nhưng được cộng đồng sử dụng\n","rộng rãi từ năm 2020. - Điểm mới: Tập trung vào tối ưu hóa và dễ dàng\n","sử dụng với các framework như PyTorch. Sử dụng CSPNet làm backbone và\n","PANet để fusion giúp cải thiện độ chính xác của mô hình. - Ưu điểm: Dễ\n","dàng ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dc6bc880-9970-4ce5-8f71-4abedbcf11a4\n","Text: YOLOv8 YOLOv8 [8] được giới thiệu vào năm 2023 bởi Ultralytics.\n","Mô hình này cải thiện độ chính xác và tốc độ so với YOLOv7 và giới\n","thiệu nhiều tính năng mới như anchor-free detection. 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: b35d6592-c298-4984-a6e5-4ba650b11193\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Hình 7: So sánh 2 phương\n","pháp hand-crafted anchor (trên) và anchor-free (dưới). Ảnh: [11]. -\n","Điểm mới: Sử dụng anchor-free detection, giúp đơn giản hóa kiến trúc\n","mô hình và cải thiện hiệu suất. - Ưu điểm: + Độ chính xác cao hơn:\n","YOLOv8 đạt mAP 50.2% trên bộ dữ liệu COCO, cao hơn so với YOLOv7. + Dễ\n","sử dụng: ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4617c9d7-3fb6-4cde-9770-e893d916956a\n","Text: YOLOv9 YOLOv9 [9] được giới thiệu vào năm 2024 bởi Chien-Yao\n","Wang, I-Hau Yeh, và Hong-Yuan Mark Liao. Mô hình này cải thiện độ\n","chính xác và tốc độ so với YOLOv8 và giới thiệu nhiều kỹ thuật mới như\n","Programmable Gradient Information (PGI) và Generalized Efficient Layer\n","Aggregation Network (GELAN). 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8ac5705f-fa66-4719-b25c-febd0805f330\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Hình 8: PGI và các kiến\n","trúc tương tự. Ảnh: [9]. - Điểm mới: YOLOv9 sử dụng PGI và GELAN để\n","cải thiện độ chính xác và hiệu suất của mô hình. - Ưu điểm: + Kiến\n","trúc tiên tiến: Sử dụng PGI và GELAN giúp mô hình duy trì thông tin\n","quan trọng và tối ưu hóa quá trình huấn luyện, làm cho YOLOv9 trở nên\n","mạnh mẽ và l...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c3eee2bb-b15f-41d4-8450-21511b549a7b\n","Text: 8\n","----------------------------------------------------------------------------------------------------\n","Node ID: 07d77b8e-2149-45f9-8698-d0c4e84c7d06\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn III. YOLOv10: Real-Time\n","End-to-End Object Detection Ao Wang và các cộng sự đã đặt nghi vấn về\n","sự tối ưu trong việc phụ thuộc vào kỹ thuật hậu xử lý Non-maximum\n","Suppresion (NMS) và cách thiết kế mô hình của các phiên bản YOLO trước\n","đó. Với các hạn chế quan sát được từ hai điều trên và mục tiêu xây\n","dựng một mô...\n","----------------------------------------------------------------------------------------------------\n","Node ID: af1674ce-0599-431b-8c16-03eb3dd89542\n","Text: Tuy vậy, chiến lược này lại dẫn đến hiệu suất mô hình không được\n","tốt. Hình 10: Minh họa chiến lược one-to-one và one-to-many label\n","assignemnts. 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: e8be21eb-41ff-4d26-9db7-2c65ca0a4c2a\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Để khắc phục trình trạng\n","của hai cách nêu trên, YOLOv10 cài đặt một chiến lược huấn luyện mới\n","là sự kết hợp của one-to-one và one-to-many, mang tên Dual label\n","assignments. Chiến lược này được minh họa theo như hình sau: Hình 11:\n","Minh họa chiến lược Dual label assignments. Ảnh: [10].\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2bd58ac1-4921-4e43-bc07-efbb2f728ac3\n","Text: Về cơ bản, trong quá trình huấn luyện, tác giả sử dụng thông tin\n","của cả hai chiến lược. Đến quá trình inference, nhánh one-to-many sẽ\n","được bỏ đi để tránh việc sử dụng NMS. Về cách bắt cặp ground-truth và\n","bounding box dự đoán, cả hai chiến lược đều sử dụng chung một độ đo là\n","Consistent Matching Metric. 2. Holistic Efficiency-Accuracy Driven\n","Model...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 54011f35-90b8-4229-a5a1-ed29815fdc62\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Hình 12: Minh họa về phép\n","depth-wise convolution và point-wise convolution để thay thế phép\n","convolution thông thường. Ảnh: link. - Rank-guided block design: Quan\n","sát YOLOv8, tác giả nhận thấy toàn bộ các stage trong kiến trúc đều sử\n","dụng chung một building block. Tuy nhiên, thông qua việc tính\n","intrinsic rank...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 489b82ed-8207-4098-8d00-048f30ed58b1\n","Text: 11\n","----------------------------------------------------------------------------------------------------\n","Node ID: d70be916-5e20-4317-84cd-60a8c28dfdac\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Về độ chính xác, dựa trên\n","ý tưởng liên quan đến receptive field và phép self-attention, nhóm tác\n","giả thực hiện hiệu chỉnh các nội dung sau: - Large-kernel\n","convolution:Ở các phiên bản YOLOv10 kích thước nhỏ, nhóm tác giả thực\n","hiện tăng kích thước kernel từ 3x3 lên 7x7 của phép depth-wise\n","convolution trong CIB...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fb305f72-f252-4a6b-b25f-5cdd9f5a76b6\n","Text: 12\n","----------------------------------------------------------------------------------------------------\n","Node ID: 38053bf1-3d15-4c38-8492-1b31d984aa21\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn IV. Cài đặt chương trình\n","và đánh giá Trong phần này, nhóm sẽ trình bày cách cài đặt, sử dụng và\n","huấn luyện YOLOv10 trên bộ dữ liệu mới. Đồng thời, nhóm cũng thực hiện\n","một thực nghiệm nhỏ nhằm so sánh hiệu suất của YOLOv10 so với hai\n","phiên bản gần nhất là YOLOv8 và YOLOv9. Môi trường lập trình nhóm sử\n","dụng là...\n","----------------------------------------------------------------------------------------------------\n","Node ID: cb14043e-6fab-4337-b23f-fc708fd6cfcc\n","Text: com /THU - MIG / yolov10 . git 2 %cd yolov10 3 ! pip install -q\n","-r requirements . txt 4 ! pip install -e .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9035396f-a710-49ba-ab45-98ab3792906d\n","Text: 2. Tải trọng số của pre-trained models:Để sử dụng được pre-\n","trained models, chúng ta cần tải về file trọng số (file .pt). Các bạn\n","chạy đoạn code sau để tải về file trọng số phiên bản YOLOv10n: 1 !\n","wget https :// github . com /THU - MIG / yolov10 / releases / download\n","/v1 .1/ yolov10n .pt 3. Khởi tạo mô hình:Để khởi tạo mô hình với trọng\n","số vừa tả...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b9b2afb7-eba4-47cd-b1b3-e5ef6d991513\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn 5. Dự đoán:Để chạy dự đoán\n","cho ảnh đã tải về, các bạn truyền đường dẫn ảnh vào mô hình như đoạn\n","code sau: 1 image_path = \"./ images / HCMC_Street . jpg \" 2 result =\n","model ( source = image_path ) [0] Hình 15: Ảnh cần dự đoán. 6. Lưu kết\n","quả dự đoán:Để lưu lại ảnh đã được dự đoán, các bạn chạy đoạn code\n","sau: 1...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d6528311-1816-4b02-a5a9-27620885670c\n","Text: png \") Hình 16: Kết quả dự đoạn của mô hình YOLOv10 phiên\n","bảnnano (yolov10n.pt). 7. Dự đoán youtube video:Để dự đoán với input\n","là youtube video, các bạn chỉ cần thay thế image_path bằng đường dẫn\n","youtube video như đoạn code sau: 14\n","----------------------------------------------------------------------------------------------------\n","Node ID: a48a9321-4399-469e-bdec-c1d770b506ac\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn 1 youtube_video_path = \"\n","https :// youtu .be/ wqPSsu7XQ74 \" 2 video_result = model ( source =\n","youtube_video_path ) Kết quả dự đoán sẽ là một video được lưu dưới\n","dạng .avi trong thư mục:/content/yolov10 /runs/detect/predict IV.II.\n","Huấn luyện YOLOv10 trên tập dữ liệu mới Trong phần này, chúng ta sẽ\n","thực hiện h...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 947f6f10-d4db-4521-a51f-6d00c4237ffa\n","Text: Các bước thực hiện như sau: 1. Tải bộ dữ liệu:Chúng ta sẽ giải\n","quyết bài toán phát hiện các loại lá được phân biệt theo trình trạng\n","bệnh của chúng. Bộ dữ liệu được sử dụng trong bài toán này là PlantDoc\n","[12]. Để dễ hình dung, các bạn có thể quan sát ảnh minh họa sau: Hình\n","17: Một vài mẫu dữ liệu trong bộ dữ liệu về bệnh của lá. Để tải bộ dữ\n","liệu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e76f6ae3-b86e-4564-b674-5781441678f1\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn 1 ! mkdir datasets 2 !\n","unzip -q \"/ content / PlantDocv4 . zip \" -d \"/ content / datasets /\n","PlantDocv4 /\" 3 !rm / content / PlantDocv4 . zip Quan sát thư mục giải\n","nén, có thể thấy bộ dữ liệu này đã được gán nhãn và đưa vào format cấu\n","trúc dữ liệu training theo yêu cầu của YOLO. Vì vậy, chúng ta sẽ không\n","cần t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b4f60bc0-e6a9-4434-8937-d5b8f94b8e98\n","Text: com /THU - MIG / yolov10 . git 2 %cd yolov10 3 ! pip install -q\n","-r requirements . txt 4 ! pip install -e .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 38b52c72-4c0b-4380-880c-5aeb2d6ede99\n","Text: 3. Khởi tạo mô hình YOLOv10: Chúng ta sẽ khởi tạo mộ hình\n","YOLOv10 với phiên bản nano (n) từ trọng số đã được huấn luyện trên bộ\n","dữ liệu COCO. Để tải trọng số yolov10n.pt, các bạn chạy đoạn code sau:\n","1 ! wget https :// github . com /THU - MIG / yolov10 / releases /\n","download /v1 .1/ yolov10n .pt Sau đó, để khởi tạo mô hình từ trọng số\n","đã tải về, c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 22ad65fc-2b6c-4c9e-96a5-9a725518630c\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn Hình 18: Quá trình huấn\n","luyện mô mình YOLOv10 trên tập dữ liệu PlantDoc. 5. Đánh giá mô\n","hình:Để thực hiện đánh giá mô hình trên tập test, các bạn chạy đoạn\n","code sau: 1 model = YOLOv10 (\"./ runs / detect / train / weights /\n","best .pt\") 2 3 model . val ( data =\" ../ datasets / PlantDocv4 / data\n",". yaml \", 4 imgs...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 97b617af-76fc-4d10-84c2-870f6410e5b2\n","Text: 17\n","----------------------------------------------------------------------------------------------------\n","Node ID: dc28d8af-8c7b-4735-87ef-54421ddd568d\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn IV.III. Đánh giá Nhóm thực\n","hiện đánh giá mô hình YOLO qua các phiên bản v8, v9 và v10.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 52da908f-a773-4576-bd8e-1e24808861a4\n","Text: Bằng cách lựa chọn cả 3 phiên bản có cùng số lượng tham số\n","khoảng 25M tương ứng là YOLOv8-M, YOLOv9-C và YOLOv10-L. Các thử\n","nghiệm được thực hiện trên cùng một thiết bị, python version, random\n","seed và một số hyperparameter như: batch_size=16, image_size=640,...\n","Sau khi quá trình huấn luyện kết thúc, thực hiện đánh giá trên tập\n","test và ghi lại kế...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 99ecb755-328a-4c2d-8039-235313f4692c\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn V. Trích dẫn [1] Redmon,\n","J., Divvala, S., Girshick, R., & Farhadi, A. (2015). You Only Look\n","Once: Unified, Real-Time Object Detection. ArXiv. /abs/1506.02640 [2]\n","Redmon, J., & Farhadi, A. (2016). YOLO9000: Better, Faster, Stronger.\n","ArXiv. /abs/1612.08242 [3] Redmon, J., & Farhadi, A. (2018). YOLOv3:\n","An Incre...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fdb22be8-41aa-440f-807d-ea20bbc4ed89\n","Text: /abs/2004.10934 [5] Jocher, G. (2020). YOLOv5 by Ultralytics.\n","Zenodo. /record/3908559 [6] Li, C., Li, L., Jiang, H., Weng, K., Geng,\n","Y., & Wei, X. (2022). YOLOv6: A Single-Stage Object Detection\n","Framework for Industrial Applications. ArXiv. /abs/2209.02976 [7]\n","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M.\n","----------------------------------------------------------------------------------------------------\n","Node ID: fc564cc2-58ce-46fa-abd0-1b0ed0947e37\n","Text: (2022). YOLOv7: Trainable bag-of-freebies sets new state-of-the-\n","art for real-time object detectors. ArXiv. /abs/2207.02696 [8] Jocher,\n","G., Stoken, A., Borovec, J., Christopher, S. T. A. N., Laughing, L.\n","C., & Ultralytics. (2023). YOLOv8 by Ultralytics. GitHub.\n","/ultralytics/ultralytics [9] Wang, C., Yeh, I., & Liao, H. (2024).\n","YOLOv9: Learning Wh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: af691293-5b32-42c1-92b9-cfb92ba4e1a3\n","Text: (2019). FreeAnchor: Learning to Match Anchors for Visual Object\n","Detection. ArXiv. /abs/1909.02466 [12] Singh, Davinder and Jain, Naman\n","and Jain, Pranjali and Kayal, Pratik and Kumawat, Sudhakar and Batra,\n","Nipun (2020). PlantDoc: A Dataset for Visual Plant Disease Detection.\n","https://doi.org/10.1145/3371158.3371196 [13] Liu, W., Anguelov, D.,\n","Erha...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a2925e1e-4624-457b-8424-a3730913d1cc\n","Text: /abs/2005.12872 19\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9fc52c04-31a3-41ae-86d1-c72b511f5372\n","Text: AI VIETNAM (AIO2024) aivietnam.edu.vn [17] Caron, M., Touvron,\n","H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A.\n","(2021). Emerging Properties in Self-Supervised Vision Transformers.\n","ArXiv. /abs/2104.14294 [18]\n","Wang,J.,Song,L.,Li,Z.,Sun,H.,Sun,J.,&Zheng,N.(2020).End-to-\n","EndObjectDetection with Fully Convolutional Network. ArXiv.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0f53486d-d6ef-4d36-92f7-930641c0d0ae\n","Text: /abs/2012.03544 - Hết - 20\n","----------------------------------------------------------------------------------------------------\n","Node ID: e3b60f49-3984-4ca9-bd83-d64c4dd1ae85\n","Text: AI VIET NAM – AIO COURSE 2023 Exercise: Image Inpainting using\n","Denoising Diffusion Probabilistic Model Quoc-Thai Nguyen và Quang-Vinh\n","Dinh PR-Team: Đăng-Nhã Nguyễn, Minh-Châu Phạm và Hoàng-Nguyên Vũ Ngày\n","25 tháng 3 năm 2024 Phần I. Giới thiệu Hình 1: Ví dụ minh hoạ Image\n","Inpainting sử dụng mô hình Denoising Diffusion Probabilistic Model.\n","Diffusi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2909fd35-3604-45ac-b94b-e46f2cdaf577\n","Text: 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: bf209b92-64a6-4013-baee-99e64921b208\n","Text: AI VIETNAM aivietnam.edu.vn (a) Forward Diffusion Process (FDP):\n","Từ một điểm dữ liệu đầu vàox0 thuộc một phân phối biết trước x0 ∼\n","q(x), FDP sẽ thêm từ từ lần lượt theo thời gian một lượng nhỏ nhiễu\n","được lấy mẫu từ phân phối Gaussϵ ∼ N(µ, σ2) tạo ra các mẫu chứa\n","nhiễux1, x2, ..., xT , vớiT (steps) số bước thêm nhiễu vào. (b)\n","Reverse Diffusion Pr...\n","----------------------------------------------------------------------------------------------------\n","Node ID: cf986063-070b-430a-a5f4-80f373da6be5\n","Text: 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 641c2871-4213-49c2-a4a1-de65a5b2e632\n","Text: AI VIETNAM aivietnam.edu.vn Phần II. Image Inpainting using De-\n","noising Diffusion Probabilistic Model Trong phần này chúng ta sẽ huấn\n","luyện mô hình Denoising Diffusion Probabilistic Model (DDP) (paper) để\n","giải quyết bài toán Image Inpainting dựa vào bộ dữ liệu CelebA Nội\n","dung thực nghiệm bao gồm 5 phần: (a) Data Preparing: Chuẩn bị dữ liệu\n","Celeb...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e838b157-d03d-4137-b2bf-1151ef8f745a\n","Text: data import Dataset 9 10 file_names = os. listdir (’./\n","img_align_celeba ’) 11 img_paths = [’./ img_align_celeba /’ +\n","file_name for file_name in file_names ] 12 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: d761b9ee-0ede-44e3-b5de-55cb332ea91d\n","Text: AI VIETNAM aivietnam.edu.vn 13 # train : valid split 14\n","num_train = 150000 15 train_imgpaths = img_paths [: num_train ] 16\n","val_imgpaths = img_paths [ num_train :] 17 18 # generate mask image 19\n","def bbox2mask ( img_shape , bbox , dtype =’uint8 ’): 20 \"\"\" 21\n","Generate mask in ndarray from bbox . 22 bbox ( tuple [ int ]):\n","Configuration tuple , (top ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 70bdb023-a6f5-4c75-96db-2ea08bace5b5\n","Text: tfs = transforms . Compose ([ 37 transforms . Resize ((\n","image_size [0] , image_size [1]) ), 38 transforms .\n","----------------------------------------------------------------------------------------------------\n","Node ID: f01fc2a7-ae1a-40a2-8b03-0b907914ebb6\n","Text: ToTensor () , 39 transforms . Normalize ( mean =[0.5 , 0.5 ,\n","0.5] , std =[0.5 ,0.5 , 0.5]) 40 ]) 41 self . mask_mode = mask_mode 42\n","self . image_size = image_size 43 44 def __getitem__ (self , index ):\n","45 img_path = self . img_paths [ index ] 46 img = Image . open (\n","img_path ). convert (’RGB ’) 47 img = self . tfs ( img ) 48 mask =\n","self . get_ma...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 49d2c729-d0fe-4d80-9398-9d8445836959\n","Text: AI VIETNAM aivietnam.edu.vn 2. Model Trong phần này chúng ta sẽ\n","xây dựng mô hình cho quá trình RDP. Mô hình UNet sẽ được sử dụng làm\n","mô hình khử nhiễu qua mỗi bước thời gian. Quá trình FDP sẽ thêm lần\n","lượt theo bước thời gian T nhiễu Gauss vào ảnh đầu vào, vì vậy ở bước\n","RDP chúng ta sẽ bổ sung thêm không gian embedding của bước thời gian\n","vào mô ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2e915f95-5d34-40c0-96ad-a3dfe4a7d35f\n","Text: 1 from torch import nn 2 import math 3 4 5 class Block (nn.\n","Module ): 6 def __init__ (self , in_ch , out_ch , time_emb_dim , up=\n","False ): 7 super (). __init__ () 8 self . time_mlp = nn. Linear (\n","time_emb_dim , out_ch ) 9 if up: 10 self . conv1 = nn. Conv2d (2*\n","in_ch , out_ch , 3, padding =1) 11 self . transform = nn.\n","ConvTranspose2d ( out_ch , o...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 629c83db-e1f6-447d-8143-bbe9ad911428\n","Text: relu ( self . conv1 (x))) 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3d8e5094-d97b-47cb-8078-fcf79b8b5c87\n","Text: AI VIETNAM aivietnam.edu.vn 23 # Time embedding 24 time_emb =\n","self . relu ( self . time_mlp (t)) 25 # Extend last 2 dimensions 26\n","time_emb = time_emb [(... , ) + (None , ) * 2] 27 # Add time channel\n","28 h = h + time_emb 29 # Second Conv 30 h = self . bnorm2 ( self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 470ac54c-b50d-498f-a9c8-d7ed6feed624\n","Text: relu ( self . conv2 (h))) 31 # Down or Upsample 32 return self .\n","transform (h) 33 34 class SinusoidalPositionEmbeddings (nn. Module ):\n","35 def __init__ (self , dim ): 36 super ().\n","----------------------------------------------------------------------------------------------------\n","Node ID: 566fe10d-4eca-4050-8577-0b8b72c2274b\n","Text: __init__ () 37 self . dim = dim 38 39 def forward (self , time\n","): 40 device = time . device 41 half_dim = self . dim // 2 42\n","embeddings = math . log (10000) / ( half_dim - 1) 43 embeddings =\n","torch . exp ( torch . arange ( half_dim , device = device ) * -\n","embeddings ) 44 embeddings = time [: , None ] * embeddings [None , :]\n","45 embeddings = torch ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 834b532d-37e4-422c-b734-b5b903980f60\n","Text: AI VIETNAM aivietnam.edu.vn 83 84 def forward (self , x,\n","timestep ): 85 # Embedd time 86 t = self . time_mlp ( timestep ) 87 #\n","Initial conv 88 x = self . conv0 (x) 89 # Unet 90 residual_inputs = []\n","91 for down in self . downs : 92 x = down (x, t) 93 residual_inputs .\n","append (x) 94 for up in self . ups : 95 residual_x = residual_inputs .\n","pop () 9...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e6fc5350-fde8-4108-a56b-c1edb52bafbf\n","Text: output (x) 2.2. Improved UNet Model Trong phần này chúng ta sẽ\n","xây dựng mô hình UNet kết hợp với cơ chế Attention và Adaptive Group\n","Normalization. Kết trúc mô hình UNet sử dụng kết hợp với cơ chế\n","Attention được mô tả như sau: Hình 5: Mô hình UNet kết hợp cơ chế\n","Attention trong DDP. 1 # Reference : https :// github . com / openai /\n","guided - diffu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9e4b4d55-d782-4f2b-91dd-0c494e48aa33\n","Text: AI VIETNAM aivietnam.edu.vn 18 Zero out the parameters of a\n","module and return it. 19 \"\"\" 20 for p in module . parameters (): 21 p.\n","detach (). zero_ () 22 return module 23 24 25 def scale_module (\n","module , scale ): 26 \"\"\" 27 Scale the parameters of a module and\n","return it. 28 \"\"\" 29 for p in module . parameters (): 30 p. detach ().\n","mul_ ( scale ) ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bbe957ec-cda5-40f3-bb88-52c5445c3586\n","Text: Module for normalization . 47 \"\"\" 48 return GroupNorm32 (32 ,\n","channels ) 49 50 51 52 def checkpoint (func , inputs , params , flag\n","): 53 \"\"\" 54 Evaluate a function without caching intermediate\n","activations , allowing for 55 reduced memory at the expense of extra\n","compute in the backward pass . 56 57 : param func : the function to\n","evaluate .\n","----------------------------------------------------------------------------------------------------\n","Node ID: f1eb8597-cc5f-4c11-99eb-42577d139cc1\n","Text: 58 : param inputs : the argument sequence to pass to ‘func ‘. 59\n",": param params : a sequence of parameters ‘func ‘ depends on but does\n","not 60 explicitly take as arguments . 61 : param flag : if False ,\n","disable gradient checkpointing . 62 \"\"\" 63 if flag : 64 args = tuple (\n","inputs ) + tuple ( params ) 65 return CheckpointFunction . apply (func\n",", l...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ddc963e8-a2ec-4820-972a-dc9c0cf0adc0\n","Text: AI VIETNAM aivietnam.edu.vn 78 return output_tensors 79 80\n","@staticmethod 81 def backward (ctx , * output_grads ): 82 ctx .\n","input_tensors = [x. detach (). requires_grad_ ( True ) for x in ctx .\n","input_tensors ] 83 with torch . enable_grad (): 84 # Fixes a bug where\n","the first op in run_function modifies the 85 # Tensor storage in place\n",", which is n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: eb26e6e9-53aa-45e1-bb57-49a2c8b3ccce\n","Text: 126 : param dim : the dimension of the output . 127 : param\n","max_period : controls the minimum frequency of the embeddings . 128 :\n","return : an [N x dim ] Tensor of positional embeddings .\n","----------------------------------------------------------------------------------------------------\n","Node ID: e403628e-8111-469c-b5b9-9e4ebeaf4c81\n","Text: 129 \"\"\" 130 half = dim // 2 131 freqs = torch . exp ( 132 -math\n",". log ( max_period ) * torch . arange ( start =0 , end =half , dtype =\n","torch . float32 ) / half 133 ).to( device = gammas . device ) 134 args\n","= gammas [: , None ]. float () * freqs [ None ] 135 embedding = torch\n",". cat ([ torch . cos ( args ), torch . sin ( args )], dim = -1) 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: c51820f5-b9bc-47d5-bfb0-69e695473240\n","Text: AI VIETNAM aivietnam.edu.vn 136 if dim % 2: 137 embedding =\n","torch . cat ([ embedding , torch . zeros_like ( embedding [: , :1]) ],\n","dim = -1) 138 return embedding 1 import math 2 import torch 3 import\n","torch .nn as nn 4 import torch .nn. functional as F 5 6 from abc\n","import abstractmethod 7 8 class SiLU (nn. Module ): 9 def forward\n","(self , x): 10 r...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 93aed176-a4e7-4b6e-9a73-c530d60fcd43\n","Text: Conv2d ( self . channels , self . out_channel , 3, padding =1)\n","52 53 def forward (self , x): 54 assert x. shape [1] == self .\n","channels 55 x = F. interpolate (x, scale_factor =2 , mode =\" nearest\n","\") 56 if self . use_conv : 57 x = self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3c625207-e6be-4c93-9773-20a0effe6212\n","Text: conv (x) 10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 835a6753-73c6-4ffd-b3c8-c2b367ff8fef\n","Text: AI VIETNAM aivietnam.edu.vn 58 return x 59 60 class Downsample\n","(nn. Module ): 61 \"\"\" 62 A downsampling layer with an optional\n","convolution . 63 : param channels : channels in the inputs and outputs\n",". 64 : param use_conv : a bool determining if a convolution is applied\n",". 65 \"\"\" 66 67 def __init__ (self , channels , use_conv , out_channel\n","= None ):...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3764a7a0-5e4d-49e6-9e6b-86641e9d245a\n","Text: __init__ () 69 self . channels = channels 70 self . out_channel\n","= out_channel or channels 71 self . use_conv = use_conv 72 stride = 2\n","73 if use_conv : 74 self .op = nn. Conv2d ( 75 self . channels , self\n",". out_channel , 3, stride = stride , padding =1 76 ) 77 else : 78\n","assert self . channels == self . out_channel 79 self .op = nn.\n","AvgPool2d ( ke...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 91c41a95-5b96-4ea3-9ec2-9f7a5dadc623\n","Text: 98 : param down : if True , use this block for downsampling . 99\n","\"\"\" 100 101 def __init__ ( 102 self , 103 channels , 104 emb_channels\n",", 105 dropout , 106 out_channel =None , 107 use_conv = False , 108\n","use_scale_shift_norm = False , 109 use_checkpoint = False , 110 up=\n","False , 111 down = False , 112 ): 113 super (). __init__ () 114 self .\n","channe...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d9419679-a90a-4494-82b2-95dd28ddfcba\n","Text: AI VIETNAM aivietnam.edu.vn 118 self . use_conv = use_conv 119\n","self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: dafdfcc8-43e4-40b1-aad2-bcf7fb450d1e\n","Text: use_checkpoint = use_checkpoint 120 self . use_scale_shift_norm\n","= use_scale_shift_norm 121 122 self . in_layers = nn. Sequential ( 123\n","normalization ( channels ), 124 SiLU () , 125 nn. Conv2d ( channels ,\n","self . out_channel , 3, padding =1) , 126 ) 127 128 self . updown = up\n","or down 129 130 if up: 131 self . h_upd = Upsample ( channels , False\n",")...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a252d5f8-877e-4039-ac14-b054b49672c6\n","Text: out_channel , self . out_channel , 3, padding =1) 152 ), 153 )\n","154 155 if self . out_channel == channels : 156 self . skip_connection\n","= nn. Identity () 157 elif use_conv : 158 self . skip_connection = nn.\n","Conv2d ( 159 channels , self . out_channel , 3, padding =1 160 ) 161\n","else : 162 self . skip_connection = nn. Conv2d ( channels , self .\n","out_ch...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 28049959-c6bb-4ebd-a02e-8693c97d575f\n","Text: in_layers [: -1] , self . in_layers [ -1] 12\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4fbc1fef-67f9-4a5b-8400-0876db235272\n","Text: AI VIETNAM aivietnam.edu.vn 178 h = in_rest (x) 179 h = self .\n","h_upd (h) 180 x = self . x_upd (x) 181 h = in_conv (h) 182 else : 183\n","h = self . in_layers (x) 184 emb_out = self . emb_layers ( emb ). type\n","(h. dtype ) 185 while len ( emb_out . shape ) < len (h. shape ): 186\n","emb_out = emb_out [... , None ] 187 if self . use_scale_shift_norm :\n","188 o...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b7748911-4627-4ca2-92bb-25c3f41527f8\n","Text: 201 https :// github . com / hojonathanho / diffusion / blob /1\n","e0dceb3b3495bbe19116a5e1b3596cd0706c543 / diffusion_tf / models / unet\n",".py# L66 . 202 \"\"\" 203 204 def __init__ ( 205 self , 206 channels ,\n","207 num_heads =1 , 208 num_head_channels =-1, 209 use_checkpoint =\n","False , 210 use_new_attention_order = False , 211 ): 212 super ().\n","__init__ (...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8de42cdc-ae52-4d2e-9a00-93308fb858d7\n","Text: proj_out = zero_module (nn. Conv1d ( channels , channels , 1))\n","232 233 def forward (self , x): 234 return checkpoint ( self .\n","_forward , (x ,) , self . parameters () , True ) 235 13\n","----------------------------------------------------------------------------------------------------\n","Node ID: 78c398f8-909d-43da-8ee9-586a3395c271\n","Text: AI VIETNAM aivietnam.edu.vn 236 def _forward (self , x): 237 b,\n","c, * spatial = x. shape 238 x = x. reshape (b, c, -1) 239 qkv = self .\n","qkv ( self . norm (x)) 240 h = self . attention ( qkv ) 241 h = self .\n","proj_out (h) 242 return (x + h). reshape (b, c, * spatial ) 243 244\n","245 class QKVAttentionLegacy (nn. Module ): 246 \"\"\" 247 A module which\n","pe...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4dfee0b5-470f-41f4-a5c6-bec1babaf4e5\n","Text: type ( weight .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2301a5d5-d5f2-4af1-b79f-7b2c55b863ef\n","Text: dtype ) 269 a = torch . einsum (\"bts ,bcs -> bct \", weight , v)\n","270 return a. reshape (bs , -1, length ) 271 272 @staticmethod 273 def\n","count_flops ( model , _x , y): 274 return count_flops_attn ( model ,\n","_x , y) 275 276 277 class QKVAttention (nn. Module ): 278 \"\"\" 279 A\n","module which performs QKV attention and splits in a different order .\n","280 \"...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4ca75196-5acb-4c4d-930e-70c062b30139\n","Text: AI VIETNAM aivietnam.edu.vn 295 q, k, v = qkv . chunk (3 , dim\n","=1) 296 scale = 1 / math . sqrt ( math . sqrt (ch)) 297 weight = torch\n",". einsum ( 298 \"bct ,bcs -> bts \", 299 (q * scale ). view (bs * self .\n","n_heads , ch , length ), 300 (k * scale ). view (bs * self . n_heads ,\n","ch , length ), 301 ) # More stable with f16 than dividing afterwards\n","30...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 879ac8db-1a7c-4873-8e35-7cbaeb007e22\n","Text: type ( weight . dtype ) 303 a = torch . einsum (\"bts ,bcs -> bct\n","\", weight , v. reshape (bs * self . n_heads , ch , length )) 304\n","return a. reshape (bs , -1, length ) 305 306 @staticmethod 307 def\n","count_flops ( model , _x , y): 308 return count_flops_attn ( model ,\n","_x , y) 309 310 class UNet (nn. Module ): 311 \"\"\" 312 The full UNet\n","model with at...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 301c6e4f-73d8-492f-9cc7-467f53f71465\n","Text: 319 For example , if this contains 4, then at 4x downsampling ,\n","attention 320 will be used . 321 : param dropout : the dropout\n","probability . 322 : param channel_mults : channel multiplier for each\n","level of the UNet . 323 : param conv_resample : if True , use learned\n","convolutions for upsampling and 324 downsampling . 325 : param\n","use_checkpoint : ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4f483663-75d1-4b5a-969f-93401b57cb93\n","Text: AI VIETNAM aivietnam.edu.vn 353 use_scale_shift_norm =True , 354\n","resblock_updown =True , 355 use_new_attention_order = False , 356 ):\n","357 358 super (). __init__ () 359 360 if num_heads_upsample == -1: 361\n","num_heads_upsample = num_heads 362 363 self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: be1a5480-31e3-4a2d-ac31-bf5461e19e10\n","Text: image_size = image_size 364 self . in_channel = in_channel 365\n","self . inner_channel = inner_channel 366 self . out_channel =\n","out_channel 367 self . res_blocks = res_blocks 368 self . attn_res =\n","attn_res 369 self . dropout = dropout 370 self . channel_mults =\n","channel_mults 371 self . conv_resample = conv_resample 372 self .\n","use_checkpoint = use_c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d44f2382-1644-4998-b6a6-8e3e246cdc80\n","Text: num_heads = num_heads 375 self . num_head_channels =\n","num_head_channels 376 self . num_heads_upsample = num_heads_upsample\n","377 378 cond_embed_dim = inner_channel * 4 379 self . cond_embed = nn.\n","Sequential ( 380 nn. Linear ( inner_channel , cond_embed_dim ), 381\n","SiLU () , 382 nn. Linear ( cond_embed_dim , cond_embed_dim ), 383 )\n","384 385 ch = input...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 52034553-3376-4566-ba02-d744fb8c5707\n","Text: AI VIETNAM aivietnam.edu.vn 413 ) 414 ) 415 self . input_blocks\n",". append ( EmbedSequential (* layers )) 416 self . _feature_size += ch\n","417 input_block_chans . append (ch) 418 if level != len (\n","channel_mults ) - 1: 419 out_ch = ch 420 self . input_blocks . append\n","( 421 EmbedSequential ( 422 ResBlock ( 423 ch , 424 cond_embed_dim ,\n","425 dropout , 4...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ceef5d4e-79b0-4f02-b742-20ea321edc46\n","Text: output_blocks = nn. ModuleList ([]) 468 for level , mult in list\n","( enumerate ( channel_mults )) [:: -1]: 469 for i in range (\n","res_blocks + 1): 470 ich = input_block_chans . pop () 471 layers = [\n","472 ResBlock ( 17\n","----------------------------------------------------------------------------------------------------\n","Node ID: 497e0a6e-f1d8-49d6-8700-936c751f1457\n","Text: AI VIETNAM aivietnam.edu.vn 473 ch + ich , 474 cond_embed_dim ,\n","475 dropout , 476 out_channel = int ( inner_channel * mult ), 477\n","use_checkpoint = use_checkpoint , 478 use_scale_shift_norm =\n","use_scale_shift_norm , 479 ) 480 ] 481 ch = int ( inner_channel * mult\n",") 482 if ds in attn_res : 483 layers . append ( 484 AttentionBlock (\n","485 ch , 486 use...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a34da7d1-52cd-460c-add2-2e99086efc98\n","Text: Sequential ( 512 normalization (ch), 513 SiLU () , 514\n","zero_module (nn. Conv2d ( input_ch , out_channel , 3, padding =1) ),\n","515 ) 516 517 def forward (self , x, gammas ): 518 \"\"\" 519 Apply the\n","model to an input batch . 520 : param x: an [N x 2 x ...] Tensor of\n","inputs (B&W) 521 : param gammas : a 1-D batch of gammas . 522 : return\n",": an [N x C x ....\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7bb972ae-0651-4ce0-a403-faeea48d7d1b\n","Text: AI VIETNAM aivietnam.edu.vn 533 for module in self .\n","output_blocks : 534 h = torch .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 92757174-83d9-4c04-9a1c-e9087a3857dd\n","Text: cat ([h, hs. pop ()], dim =1) 535 h = module (h, emb ) 536 h =\n","h. type (x. dtype ) 537 return self . out (h) 19\n","----------------------------------------------------------------------------------------------------\n","Node ID: 261a1931-bf2e-4d38-a5f8-3d12612a5f9b\n","Text: AI VIETNAM aivietnam.edu.vn 2.3. Gaussian Diffusion Model Trong\n","phần này chúng ta định nghĩa mô hình Diffusion Model đầy đủ với 2\n","bước: FDP ước lượng bước thời gianβ dựa vào hàm linear, các giá trị\n","xác suất để tínhq(xt|x0) và q(xt−1|xt, x0), thuật toán huấn luyện và\n","lấy mẫu. 1 from tqdm import tqdm 2 from functools import partial 3 4\n","def make_be...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ad98de0b-bf7f-4422-ae43-3e0bc6f8857d\n","Text: linspace ( 7 linear_start , linear_end , n_timestep , dtype =np.\n","float64 8 ) 9 else : 10 raise NotImplementedError ( schedule ) 11\n","return betas 12 13 def get_index_from_list (vals , t, x_shape =(1 ,1\n",",1 ,1) ): 14 \"\"\" 15 Returns a specific index t of a passed list of\n","values vals 16 while considering the batch dimension . 17 \"\"\" 18\n","batch_size , *_...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 31d63e86-e953-4c0e-91b6-b007be62320e\n","Text: gather (-1, t) 20 return out . reshape ( batch_size , *((1 ,) *\n","( len ( x_shape ) - 1))).to( device ) 21 22 class\n","InpaintingGaussianDiffusion (nn. Module ): 23 def __init__ (self ,\n","unet_config , beta_schedule , ** kwargs ): 24 super (\n","InpaintingGaussianDiffusion , self ). __init__ (** kwargs ) 25 self .\n","denoise_fn = UNet (** unet_config ) 26 sel...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bff3e614-b2e4-457f-b591-19290522c6f6\n","Text: AI VIETNAM aivietnam.edu.vn 51 self . loss_fn = loss_fn 52 53\n","def predict_start_from_noise (self , y_t , t, noise ): 54 return ( 55\n","get_index_from_list ( self . sqrt_recip_gammas , t, y_t .\n","----------------------------------------------------------------------------------------------------\n","Node ID: eb25d127-6489-4617-bf97-c1212279e632\n","Text: shape ) * y_t - 56 get_index_from_list ( self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: ba21abcf-0c4e-4a27-9cb6-53aca26762d8\n","Text: sqrt_recipm1_gammas , t, y_t . shape ) * noise 57 ) 58 59 def\n","q_posterior (self , y_0_hat , y_t , t): 60 \"\"\" 61 Compute the mean and\n","variance of the diffusion posterior : 62 63 q(x_{t -1} | x_t , x_0 )\n","64 65 \"\"\" 66 posterior_mean = ( 67 get_index_from_list ( self .\n","posterior_mean_coef1 , t, y_t . shape ) * y_0_hat + 68\n","get_index_from_list ( self...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6d638243-6039-4a0d-9346-9af43a28ba22\n","Text: AI VIETNAM aivietnam.edu.vn 109 for i in reversed ( range (0 ,\n","self . num_timesteps )): 110 t = torch . full ((b ,) , i, device =\n","y_cond . device , dtype = torch . long ) 111 y_t = self . p_sample\n","(y_t , t, y_cond = y_cond ) 112 if mask is not None : 113 y_t = y_0\n","*(1. - mask ) + mask * y_t 114 if i % sample_inter == 0: 115 ret_arr =\n","torch . cat...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8137b69c-1dd1-4e27-9d9c-ce5e82bad2f0\n","Text: randint (1 , self . num_timesteps , (b ,) , device = y_0 .\n","device ). long () 122 gamma_t1 = get_index_from_list ( self . gammas ,\n","t -1 , x_shape =(1 , 1)) 123 sqrt_gamma_t2 = get_index_from_list (\n","self . gammas , t, x_shape =(1 , 1)) 124 sample_gammas = (\n","sqrt_gamma_t2 - gamma_t1 ) * torch . rand ((b, 1) , device = y_0 .\n","device ) + gamma_t1 125 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 26245526-84ec-47a8-b59d-340fa599860c\n","Text: optim . Adam ( list ( filter ( 7 lambda p: p. requires_grad ,\n","self . model .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 63d11a74-587b-4e1b-9657-292b14dc3451\n","Text: parameters () 8 )), ** optimizers ) 9 self . model . set_loss (\n","mse_loss ) 10 self . model . set_new_noise_schedule ( device ) 22\n","----------------------------------------------------------------------------------------------------\n","Node ID: 74d74910-48f9-41bd-920f-f7e589a240f5\n","Text: AI VIETNAM aivietnam.edu.vn 11 self . sample_num = sample_num 12\n","self . train_loader = train_loader 13 self . val_loader = val_loader\n","14 self . device = device 15 self . epochs = epochs 16 self .\n","save_model = save_model + \"/ best_model . pth \" 17 18 def train_step (\n","self ): 19 self . model . train () 20 losses = [] 21 for batch in tqdm\n","( self . ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dae83db0-d2c4-4fb9-a47c-1f15f297a13d\n","Text: append ( loss . item ()) 33 self . optimizer . step () 34 return\n","sum ( losses )/ len ( losses ) 35 36 def val_step ( self ): 37 self .\n","model . eval () 38 losses , metrics = [] , [] 39 with torch . no_grad\n","(): 40 for batch in tqdm ( self . val_loader ): 41 gt_image = batch [’\n","gt_image ’]. to( self . device ) 42 cond_image = batch [’ cond_image\n","’]...\n","----------------------------------------------------------------------------------------------------\n","Node ID: be93b0bf-7ced-403e-8d69-32c1761af621\n","Text: model .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 748feedd-f13b-41db-8779-d3b6819c9cfa\n","Text: state_dict () , self . save_model ) 63 # Print loss , acc end\n","epoch 64 print (\"-\" * 59) 65 print ( 66 \"| End of epoch {:3 d} | Time\n",": {:5.2 f}s | Train Loss {:8.3 f} \" 67 \"| Valid Loss {:8.3 f} | Valid\n","MAE {:8.3 f} \". format ( 68 epoch +1 , time . time () -\n","epoch_start_time , train_loss , val_loss , val_mae 23\n","----------------------------------------------------------------------------------------------------\n","Node ID: 444703bb-1bc4-4cd3-875e-2f0223c9e37e\n","Text: AI VIETNAM aivietnam.edu.vn 69 ) 70 ) 71 print (\"-\" * 59) 72\n","self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: b03b35d5-e989-4148-884a-f91f4fad6449\n","Text: model . load_state_dict ( torch . load ( self . save_model )) 73\n","74 epochs = 200 # 5 75 sample_num = 8 76 save_model = ’./ save_model ’\n","77 optimizers = { \"lr\": 5e -5 , \" weight_decay \": 0} 78 device = \"\n","cuda \" if torch . cuda . is_available () else \" cpu \" 79 80\n","unet_config = { 81 \" in_channel \": 6, 82 \" out_channel \": 3, 83 \"\n","inner_channel \": 6...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a084e72c-5310-4afb-86fc-f9c46c6ee0e3\n","Text: AI VIETNAM aivietnam.edu.vn 22 23 output , visuals = inference (\n","inpainting_model , test_sample ) 24 25 # show result 26 def\n","show_tensor_image ( image , show = True ): 27 reverse_transforms =\n","transforms . Compose ([ 28 transforms .\n","----------------------------------------------------------------------------------------------------\n","Node ID: a78b9d47-f965-4ae2-88b2-98444a51ef0d\n","Text: Lambda ( lambda t: (t + 1) / 2) , 29 transforms . Lambda (\n","lambda t: t. permute (1 , 2, 0)), # CHW to HWC 30 transforms . Lambda\n","( lambda t: t * 255.) , 31 transforms . Lambda ( lambda t: t. numpy\n","(). astype (np. uint8 )), 32 transforms . ToPILImage () , 33 ]) 34 35\n","# Take first image of batch 36 if len ( image . shape ) == 4: 37 image\n","= image [...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 959a6fb3-8484-498f-8b62-10d554389ab7\n","Text: AI VIETNAM aivietnam.edu.vn Phần 4. Câu hỏi trắc nghiệm Câu hỏi\n","1Mục tiêu của Forward Diffusion Process trong mô hình Diffusion là gì?\n","a) Thêm một lượng nhỏ nhiễu được lấy mẫu từ phân phối Gauss vào giá\n","trị inputx0 lần lượt theo bước nhảy T với lịch trình phương saiβ1,\n","..., βT b) Khử nhiễu trongxT để khôi phục giá trị đầu vào c) Cả 2 đáp\n","án trên...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b1f42ec9-bfaa-4b92-ace4-864ff150b660\n","Text: a) Mask các vị trí điểm ảnh trung tâm theo hình chữ nhật b) Mask\n","các vị trí điểm ảnh ở góc trái trên theo hình chữ nhật c) Mask các vị\n","trí điểm ảnh ở góc trái dưới theo hình chữ nhật d) Mask các vị trí\n","điểm ảnh trung tâm theo hình tròn Câu hỏi 5Dựa vào bài báo cáo nghiên\n","cứu \"Denoising Diffusion Probabilistic Model\", công thức tính giá trị\n","xác s...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b2edda89-e50e-4fc5-b032-53248c308536\n","Text: AI VIETNAM aivietnam.edu.vn d) eβt = 1 1−¯αt βt Câu hỏi 7Dựa vào\n","phần thực nghiệm 2, phương pháp embedding cho bước thời gian được sử\n","dụng khi xây dựng mô hình Basic UNet là? a) ALiBi b) Rotary Embedding\n","c) Conditional Positional Embedding d) Sinusoidal Positional Embedding\n","Câu hỏi 8Dựa vào phần thực nghiệm 2, bộ dữ liệu huấn luyện mô hình nào\n","đ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: eb45a06b-61b9-4204-8411-ac8b8d8f6676\n","Text: a) BCELoss b) MSELoss c) L1Loss d) CTCLoss Câu hỏi 10Dựa vào\n","phần thực nghiệm 2, độ đo đánh giá mô hình nào sau đây không được sử\n","dụng? a) F1 b) Recall c) Precision d) MAE - Hết - 27\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1128fad2-d481-433f-81fa-2a1b7d2e5cf6\n","Text: AI VIET NAM – 2024 Graph Neural Network - Exercise Minh-Duc Bui\n","và Vinh Dinh Nguyen Ngày 28 tháng 4 năm 2024 Phần I: Giới thiệu Graph\n","Neural Network - GNN là một nerual network sử dụng để xử lý dữ liệu\n","được biểu diễn dưới dạng graph. Trong graph, các đỉnh (node) biểu diễn\n","các thực thể, và các cạnh (edge) thể hiện các mối quan hệ giữa các\n","node. G...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c8d2b2ef-02e0-402f-9c1f-595f736f5659\n","Text: 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: a7b63c77-b9da-4ca3-9626-207a03d8b9e2\n","Text: AI VIETNAM aivietnam.edu.vn Phần II: Nội dung 1. Ý tưởng chung\n","GNN hoạt động bằng cách tổng hợp thông tin từ các node có liên quan\n","với nhau (có edge), trong các dataset dành riêng cho GNN (ví dụ Cora,\n","Citeseer, Pubmed,...), các edge được đã được định nghĩa sẵn, điều này\n","gây ra khó khăn khi sử dụng GNN vào các bài toán khác (ví dụ image\n","classific...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 134c984f-61c5-424d-8a2c-6f766a0c9d49\n","Text: • Edge là gì? • Ta có thể thêm hàm loss phụ trợ không? Ba câu\n","hỏi trên sẽ quyết định việc ta có thể sử dụng GNN vào một bài toán bất\n","kì được hay không. Tuy nhiên, để GNN có thể cải thiện performance của\n","model hiện tại ta cần phải trả lời câu hỏi quan trọng sau: • GNN sẽ\n","hoạt động như thế nào? Ta cần phải giải thích rõ cách GNN sẽ hoạt động\n","trước...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e8626dd3-477f-45ce-976f-4a30cce6d3d4\n","Text: AI VIETNAM aivietnam.edu.vn đó giúp model học tốt các sample khó\n","và performance tổng quát của model sẽ được cải thiện. Ta sẽ tăng độ\n","khó của dataset trong bài toán này để thấy sự hiệu quả của GNN.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2def5e1b-1988-4ebf-abb1-a23b2ad08f05\n","Text: 3. Dataset PACS PACS là một tập dữ liệu hình ảnh về domain\n","generalization. PACS bao gồm 4 domain: photo (1.670 ảnh), art painting\n","(2.048 ảnh), cartoon (2.344 ảnh) và sketch (3.929 ảnh). Mỗi domain bao\n","gồm 7 class. Điểm khó của PACS là việc sẽ có nhiều domain cho cùng 1\n","class, ví dụ trong class person sẽ có 4 domain (cột số 2 trong hình\n","1): • Ảnh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1726e541-475a-451c-b6b9-b2d0ef62934d\n","Text: AI VIETNAM aivietnam.edu.vn 7 transforms . CenterCrop (224) , 8\n","transforms .\n","----------------------------------------------------------------------------------------------------\n","Node ID: f4acd309-6df8-4c64-9fe8-5ef8fcfb84a6\n","Text: ToTensor () , 9 transforms . Normalize ( means , stds ), 10 ] 11\n",") 12 13 # Clone github repository with data 14 if not os. path . isdir\n","(\"./ Homework3 - PACS \"): 15 ! git clone https :// github . com /\n","MachineLearning2020 / Homework3 - PACS 16 17 # Define datasets root 18\n","DIR_PHOTO = \" Homework3 - PACS / PACS / photo \" 19 DIR_ART = \"\n","Homework3 -...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1676f265-4c57-487d-b1a7-9dc551b8295d\n","Text: data . ConcatDataset ([ test_dataset , art_dataset ]) 11 12 #\n","Create Dataloaders 13 trainloader = DataLoader ( 14 train_dataset ,\n","batch_size =128 , shuffle =True , num_workers =4 , drop_last = True 15\n",") 16 testloader = DataLoader ( test_datasets , batch_size =128 ,\n","shuffle = False , num_workers =4) 4. Áp dụng GNN cho bài toán image\n","classificatio...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1f33d0eb-3435-4c50-89d2-28068106f50d\n","Text: AI VIETNAM aivietnam.edu.vn edge với nhau hay không, nên ta mới\n","cần edge network làm nhiệm vụ edge prediction. 1 class GCN (nn. Module\n","): 2 def __init__ (self , in_features , edge_features , out_feature ,\n","device , ratio =(1 ,) ): 3 super (GCN , self ). __init__ () 4 5 self .\n","edge_net = EdgeNet ( 6 in_features = in_features , 7 num_features =\n","edg...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c8f8bb13-8d95-4153-91eb-c66bd96781bb\n","Text: unsqueeze ( -1). repeat (1 , 1, num_sample ) 25 label_j =\n","label_i . transpose (1 , 2) 26 edge = torch .eq( label_i , label_j ).\n","float () 27 target_edge_mask = ( 28 torch .eq( label_i , self .\n","mask_val ) + torch .eq( label_j , self . mask_val ) 29 ).\n","----------------------------------------------------------------------------------------------------\n","Node ID: ec340e5c-f420-4688-81d3-6a10e11be5a7\n","Text: type ( torch . bool ) 30 source_edge_mask = ~ target_edge_mask\n","31 init_edge = edge * source_edge_mask . float () 32 return init_edge\n","[0] , source_edge_mask 33 34 def forward (self , init_node_feat ): 35\n","# compute normalized and not normalized affinity matrix 36 edge_feat ,\n","edge_sim = self . edge_net ( init_node_feat ) 37 # compute node\n","features ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dbaa43a2-76f8-4c19-a66f-1437f5746de5\n","Text: AI VIETNAM aivietnam.edu.vn 5 self . device = device 6 # define\n","layers 7 layer_list = OrderedDict () 8 for l in range ( len (\n","num_features_list )): 9 layer_list [\" conv {}\". format (l)] = nn.\n","Conv2d ( 10 in_channels = num_features_list [l - 1] if l > 0 else\n","in_features , 11 out_channels = num_features_list [l], 12 kernel_size\n","=1 , 13 bias = Fals...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a8e84fc8-8d2d-4c3f-b14e-6c1035929be9\n","Text: squeeze (0) .to( self . device ) 36 ) # (bs , bs) 37 # normalize\n","affinity matrix 38 force_edge_feat = ( 39 torch . eye ( num_data ).\n","----------------------------------------------------------------------------------------------------\n","Node ID: 11808c85-0a0e-418e-acb9-dc3eda1a28e5\n","Text: unsqueeze (0) . repeat ( num_tasks , 1, 1).to( self . device )\n","40 ) # (1 , bs , bs) 41 edge_feat = sim_val + force_edge_feat # (bs ,\n","bs) 42 edge_feat = edge_feat + 1e -6 # add small value to avoid nan 43\n","edge_feat = edge_feat / torch . sum ( edge_feat , dim =1) . unsqueeze\n","(1) # normalize 44 return edge_feat , sim_val # (bs , bs), (bs , bs)\n","Lưu ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 98d51df0-e355-4d26-8e20-a2543002406d\n","Text: AI VIETNAM aivietnam.edu.vn 3 super ( NodeNet , self ). __init__\n","() 4 num_features_list = [ num_features * r for r in ratio ] 5 self .\n","device = device 6 # define layers 7 layer_list = OrderedDict () 8 for\n","l in range ( len ( num_features_list )): 9 layer_list [\" conv {}\".\n","format (l)] = nn. Conv2d ( 10 in_channels = num_features_list [l - 1]\n","if l ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 70c6f795-4899-4c57-9b97-00564f3f4acd\n","Text: LeakyReLU () 20 self . network = nn. Sequential ( layer_list\n",").to( device ) 21 22 def forward (self , node_feat , edge_feat ): 23\n","\"\"\" node_feat : (bs , dim ), edge_feat : (bs , bs) \"\"\" 24 node_feat =\n","node_feat . unsqueeze ( dim =0) # (1 , bs , dim ) 25 num_tasks =\n","node_feat . size (0) # 1 26 num_data = node_feat . size (1) # bs 27 #\n","get eye matr...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e682a9d4-9ba3-4731-8358-b028cab04f8a\n","Text: unsqueeze (0) . repeat ( num_tasks , 1, 1).to( 29 self . device\n","30 ) # (1 , bs , bs) 31 # set diagonal as zero and normalize 32\n","edge_feat = F. normalize ( edge_feat * diag_mask , p=1 , dim = -1) #\n","(bs , bs) 33 # compute attention and aggregate 34 aggr_feat = torch .\n","bmm ( edge_feat . squeeze (1) , node_feat ) # (bs , dim ) 35 node_feat\n","= torch ....\n","----------------------------------------------------------------------------------------------------\n","Node ID: 54a269a9-340a-4e31-9882-1e533e5917d2\n","Text: AI VIETNAM aivietnam.edu.vn 6 self . backbone =\n","mobilenet_v3_small ( pretrained = True ) 7 self . backbone .\n","classifier = nn.\n","----------------------------------------------------------------------------------------------------\n","Node ID: fecd9aeb-03ba-4f57-9a9e-16ef17130272\n","Text: Sequential () 8 9 self . gcn = GCN ( 10 in_features =576 , 11\n","edge_features =576 , 12 out_feature = num_classes , 13 device =\" cuda\n","\", 14 ratio =(1 ,) , 15 ) 16 17 def forward (self , x): 18 x = self .\n","backbone (x) 19 x, edge_sim = self . gcn (x) 20 return x, edge_sim Để\n","dễ dàng hình dung hơn nữa thì ta hãy nhìn vào model bình thường không\n","có GN...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e217003-9db6-4885-bf5a-830d1389f15e\n","Text: CrossEntropyLoss () 2 criterion_edge = nn. BCELoss () 3 ... 4\n","for i, ( inputs , labels ) in enumerate ( trainloader , 0): 8\n","----------------------------------------------------------------------------------------------------\n","Node ID: d607d1f7-cdf8-41e5-88c5-0be3534f23c6\n","Text: AI VIETNAM aivietnam.edu.vn 5 # Move inputs and labels to the\n","device 6 inputs , labels = inputs .to( device ), labels .to( device )\n","7 8 # Zero the parameter gradients 9 optimizer . zero_grad () 10 11 #\n","Forward pass 12 outputs , edge_sim = model ( inputs ) 13 14 # Cls loss\n","15 loss_cls = criterion ( outputs , labels ) 16 17 # Edge loss 18\n","edge_gt ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 77bafcc0-37c2-4b48-abbb-c34859a64010\n","Text: backward () 30 optimizer . step () 31 ... Lưu ý: file pdf này\n","chỉ cung cấp một số đoạn code chính để tránh người đọc phân tâm. Tất\n","cả các phần code còn lại như một bài toán image classification bình\n","thường, full code sẽ nằm ở file notebook. 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: 96af4e4e-67ab-4793-8fdd-4a4a11b854bb\n","Text: AI VIETNAM aivietnam.edu.vn Phần III: Câu hỏi trắc nghiệm 1. Đâu\n","là lưu ý khi áp dụng GNN vào các bài toán bất kì? (a) Label là gì? (b)\n","Độ lớn dataset (c) Độ phức tạp của model (d) Edge là gì? 2. Đâu là lưu\n","ý khi áp dụng GNN vào các bài toán bất kì? (a) Label là gì? (b) Node\n","là gì? (c) Số lượng param (d) Loss là gì? 3. Đâu là lưu ý khi áp dụng\n","G...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0507143a-d121-4d13-8ef2-765788ce8e1f\n","Text: 4. Sau khi chắc chắn GNN có thể sử dụng, điều gì ta cần cân nhắc\n","tiếp theo là phù hợp nhất? (a) Không cần gì (b) Độ lớn của model sau\n","khi thêm GNN (c) Hiểu rõ cách GNN tổng hợp thông tin (d) Tốc độ của\n","model sau khi thêm GNN 5. Nếu input của GNN có shape [batch_size,\n","dim_1] và output có shape [batch_size, dim_2] thì nhận định nào sau\n","đây làSAI: ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 98d426f1-6762-438a-8fd9-d3e0af76bcce\n","Text: AI VIETNAM aivietnam.edu.vn (d) Gradient descent 8. Đâu là điểm\n","yếu của GNN?\n","----------------------------------------------------------------------------------------------------\n","Node ID: 33084eda-bf9e-4c64-b3c6-2eadb2e188f3\n","Text: (a) Tốc độ train model chậm (b) Tốc độ inference nhanh (c) Số\n","lượng param cực lớn (d) Không có điểm yếu nào 9. Đâu là điểm yếu của\n","GNN? (a) Cần train model theo 2 stage (b) Cần inference model theo 2\n","stage (c) Tốc độ inference chậm (d) Tốc độ train model nhanh 10. Đâu\n","là những yếu cầu của GNN? (a) Batch size bất kì, data bất kì (b) Batch\n","size = ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ea505e7d-0659-4000-8072-c9f1cf79480b\n","Text: AI VIET NAM – AIO COURSE 2023 Exercise: Multimodal Large\n","Language Models Quoc-Thai Nguyen và Duong-Thuan Nguyen Ngày 23 tháng 4\n","năm 2024 Phần I. Giới thiệu Hình 1: Giới thiệu các thành phần cơ bản\n","trong mô hình MLLMs. Hình 2: Ví dụ về sinh ảnh dựa vào văn bản sử dụng\n","mô hình MLLMs.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 56ed1439-82f2-4410-8d21-838e2dde6cb2\n","Text: 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: 01f301e0-bf37-4053-b4ab-158d2f64aa9c\n","Text: AI VIETNAM aivietnam.edu.vn MLLMs - Multimodal Large Language\n","Modelsngày càng được phát triển rộng rãi với mục tiêu xây dựng một mô\n","hình ngôn ngữ lớn có thể xử lý cho các kiểu dữ liệu khác nhau như: văn\n","bản, hình ảnh, âm thanh, video. Với các mô hình MLLMs đầu vào có thể\n","là văn bản, hình ảnh, video, âm thanh hoặc kết hợp của các kiểu dữ\n","liệu trê...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 11ce0d5e-cb59-48b5-94e1-bd2265c27b93\n","Text: Hình 3: Một số mô hình MLLMs điển hình hiện nay. Trong phần tiếp\n","theo, chúng ta sẽ huấn luyện mô hình cơ bản BLIP-2 trên bài toán VQA -\n","Visual Question Answering. 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 485d09ce-d8ef-4368-ab4e-7636f202401f\n","Text: AI VIETNAM aivietnam.edu.vn Phần II. Visual Question Answering\n","us- ing BLIP-2 Hình 4: Kiến trúc mô hình BLIP-2. Kiến trúc mô hình\n","BLIP-2 được mô tả trong Hình 4. Đầu vào mô hình là câu hỏi và hình\n","ảnh. Đầu ra của mô hình là câu trả lời dựa vào câu hỏi và ngữ cảnh là\n","hình ảnh. Mô hình BLIP-2 bao gồm các thành phần: • Image Encoder: CLIP\n","ViT. • In...\n","----------------------------------------------------------------------------------------------------\n","Node ID: aae070f6-039d-4a11-8a14-b9d421c171d2\n","Text: Build Dataset 1 # install libs 2 ! pip install -q peft\n","transformers bitsandbytes datasets 3 4 import os 5 import torch 6 from\n","PIL import Image 7 8 class VQADataset ( torch . utils .\n","----------------------------------------------------------------------------------------------------\n","Node ID: f6999bd6-dd91-4b83-960c-1da5e2447934\n","Text: data . Dataset ): 9 \"\"\" VQA (v2) dataset . \"\"\" 10 11 def\n","__init__ (self , dataset , processor , data_path ): 12 self . dataset\n","= dataset 13 self . processor = processor 14 self . data_path =\n","data_path 15 16 def __len__ ( self ): 17 return len ( self . dataset )\n","3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4f4c23bb-1b67-49e1-b485-56ef28b5b856\n","Text: AI VIETNAM aivietnam.edu.vn 18 19 def __getitem__ (self , idx ):\n","20 # get image + text 21 question = self . dataset [ idx ][ ’ question\n","’] 22 answer = self . dataset [ idx ][ ’ answer ’] 23 image_id = self\n",". dataset [ idx ][ ’pid ’] 24 image_path = os. path . join ( self .\n","data_path , f\" train_fill_in_blank / train_fill_in_blank /{ image_id\n","}/ i...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9c16c6c3-d98c-4674-bf7b-d6eac49890b1\n","Text: open ( image_path ). convert (\" RGB \") 26 text = question 27 28\n","encoding = self . processor ( image , text , padding =\" max_length \",\n","truncation =True , return_tensors =\"pt\") 29 labels = self . processor\n",". tokenizer . encode ( 30 answer , max_length = 8, pad_to_max_length\n","=True , return_tensors =’pt ’ 31 ) 32 encoding [\" labels \"] = labels\n","33 fo...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b3f680c3-bce7-48ea-aeee-cae4519175c1\n","Text: cuda . is_available () else \" cpu \") 18 model .to( device ) 19\n","model . print_trainable_parameters () 2.3. Create Dataloader 1 from\n","datasets import load_dataset 2 from torch . utils . data import\n","DataLoader 3 4 data_path = ’./ IconDomainVQAData ’ 5 ds = load_dataset\n","(\" json \", data_files =f\"{ data_path }/ train . jsonl \") 6 7\n","train_dataset = VQAD...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f375b0d3-5793-4769-a43f-8e7ce3ca3d78\n","Text: AI VIETNAM aivietnam.edu.vn 17 18 batch_size = 8 19\n","train_dataloader = DataLoader ( train_dataset , batch_size =\n","batch_size , shuffle =True , pin_memory = True ) 20 valid_dataloader =\n","DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False\n",", pin_memory = True ) 2.4. Train 1 from tqdm import tqdm 2 3 optimizer\n","= torch . optim . Ada...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b23c35b1-2e0c-4800-ad08-15029c608d0b\n","Text: amp . GradScaler () 9 10 for epoch in range ( num_epochs ): 11\n","epoch_loss = [] 12 model . train () 13 for idx , batch in zip ( tqdm (\n","range ( len ( train_dataloader )), desc =f’ Training batch : { epoch\n","+1} ’), train_dataloader ): 14 input_ids = batch . pop (’ input_ids\n","’).to( device ) 15 pixel_values = batch . pop (’ pixel_values ’).to(\n","device ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2152571d-17bb-4554-ac09-4f9ae830b936\n","Text: update () 33 34 model . eval () 35 valid_loss = [] 36 for idx ,\n","batch in zip ( tqdm ( range ( len ( valid_dataloader )), desc =f’\n","Validating batch : { epoch +1} ’), valid_dataloader ): 37 input_ids =\n","batch . pop (’ input_ids ’).to( device ) 38 pixel_values = batch . pop\n","(’ pixel_values ’).to( device ) 39 attention_masked = batch . pop (’\n","attenti...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c0ed2534-9b9f-4c67-b8e1-2cf3b6fc0c75\n","Text: AI VIETNAM aivietnam.edu.vn 49 50 loss = outputs . loss 51\n","valid_loss . append ( loss . item ()) 52 print (\" Epoch : {} -\n","Training loss : {} - Eval Loss : {} - LR: {}\". format ( epoch +1 , sum\n","( epoch_loss )/ len ( epoch_loss ), sum ( valid_loss )/ len (\n","valid_loss ), optimizer . param_groups [0][ \"lr\"])) 53 scheduler .\n","step () 54 avg_loss = sum...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 76fbea63-166e-4e19-b88b-5c89367504ec\n","Text: Một ví dụ sử dụng đánh giá mô hình được mô tả như Hình 5. Hình\n","5: Hình ảnh ví dụ với câu hỏi: \"How many diamonds are there?\". 1\n","import json 2 processor = BlipProcessor . from_pretrained (\"./\n","save_model \") 3 model = BlipForQuestionAnswering . from_pretrained\n","(\"./ save_model \").to( device ) 4 test_data_dir = f\"{ data_path }/\n","test_data / test_data ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 088f469e-6291-43d2-b212-9ad559bb7aa1\n","Text: AI VIETNAM aivietnam.edu.vn 9 json_path = os. path . join (\n","sample_path , \" data . json \") 10 with open ( json_path , \"r\") as\n","json_file : 11 data = json . load ( json_file ) 12 question = data [\"\n","question \"] # How many diamonds are there ? 13 image_id = data [\"id\"]\n","14 15 # load image 16 image_path = os. path . join ( test_data_dir ,\n","f\"{ image_id...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 766bdc27-50cb-4f68-8837-cc08c77818f2\n","Text: open ( image_path ). convert (\" RGB \") 18 19 encoding =\n","processor ( image , question , return_tensors =\"pt\").to( device ,\n","torch . float16 ) 20 21 out = model . generate (** encoding ) 22\n","generated_text = processor . decode ( out [0] , skip_special_tokens =\n","True ) 23 24 generated_text 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: 839f4af2-cfd7-4099-8250-fe009f349490\n","Text: AI VIETNAM aivietnam.edu.vn Phần 4. Câu hỏi trắc nghiệm Câu hỏi\n","1Mô hình nào sau đây không phải là mô hình ngôn ngữ lớn (LLMs)? a)\n","Vicuna b) LLaMA c) PaLM d) ResNet Câu hỏi 2Thành phần nào sau đây\n","không có trong các thành phần chính của mô hình MLLMs? a) Contrastive\n","Search b) Modality Encoder c) LLMs d) Input Projector Câu hỏi 3Mô hình\n","được sử d...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e293a53-d1c3-46f8-8165-e505bf98e6ad\n","Text: a) Flan-T5 b) Vicuna c) GPT3 d) PaLM Câu hỏi 7Mô hình BLIP-2\n","không xử lý được bài toán nào sau đây? a) Image Captioning b) Prompted\n","Image Captioning 8\n","----------------------------------------------------------------------------------------------------\n","Node ID: 281f6d94-6eea-4539-a516-a7c7c599b2fd\n","Text: AI VIETNAM aivietnam.edu.vn c) Visual Question Answering d)\n","Audio Generation Câu hỏi 8Mô hình nào sau đây có thể encode cho hình\n","ảnh, văn bản, video? a) CLIP b) ViT c) Eva-CLIP ViT d) ImageBlind Câu\n","hỏi 9Mô hình NExT-GPT sử dụng mô hình nào cho bước Output Generator?\n","----------------------------------------------------------------------------------------------------\n","Node ID: e1488713-f99e-4bdf-8ca4-36ba5df09c97\n","Text: a) MLP b) Linear Projector c) Tiny Transformer d) Cross-\n","Attention Câu hỏi 10Mô hình NExT-GPT sử dụng LLM nào? a) Flan-T5 b)\n","Vicuna c) GPT3 d) PaLM - Hết - 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9affe869-282b-41a9-9fc0-e3005b7ef8c2\n","Text: AI VIET NAM – COURSE 2024 Python OOP – Exercise Ngày 15 tháng 6\n","năm 2024 I. Câu hỏi tự luận 1. Viết class và cài phương thức softmax.\n","Trong pytorch, torch.nn.Module là lớp cơ bản để từ đó xây dựng lên các\n","mô hình hoặc các phương thức kích hoạt (activation funtion) như\n","sigmoid, softmax,... Trong phần này, chúng ta xây dựng class Softmax\n","và softma...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4b6e1f35-eb26-4b06-9334-9ea4c9491066\n","Text: Tensor ([1 , 2, 3]) 9 softmax_stable = softmax_stable () 10\n","output = softmax_stable ( data ) 11 output 12 >> tensor ([0.0900 ,\n","0.2447 , 0.6652]) 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0482f2f9-0c08-47ea-9750-29475b5d84cf\n","Text: AI VIETNAM aivietnam.edu.vn 2. Một Ward (phường) gồm có name\n","(string) và danh sách của mọi người trong Ward. Một người person có\n","thể là student, doctor, hoặc teacher. Một student gồm có name, yob\n","(int) (năm sinh), và grade (string). Một teacher gồm có name, yob, và\n","subject (string). Một doctor gồm có name, yob, và specialist (string).\n","Lưu ý cần ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bb49d7a7-71dc-4c0b-aa36-1b1bdaaeb155\n","Text: (a) Cài đặt các class Student, Doctor, và Teacher theo mô tả\n","trên. Thực hiện phương thức describe() method để in ra tất cả thông\n","tin của các object. (b) Viết add_person(person) method trong Ward\n","class để add thêm một người mới với nghề nghiệp bất kỳ (student,\n","teacher, doctor) vào danh sách người của ward. Tạo ra một ward object,\n","và thêm vào 1 st...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 60d4793f-ae4c-4155-a36a-3eca10c1d0ef\n","Text: add_person ( teacher1 ) 26 ward1 . add_person ( teacher2 ) 27\n","ward1 . add_person ( doctor1 ) 28 ward1 . add_person ( doctor2 ) 29\n","ward1 . describe () 30 31 # output 32 >> Ward Name : Ward1 33 Student\n","- Name : studentA - YoB : 2010 - Grade : 7 34 Teacher - Name :\n","teacherA - YoB : 1969 - Subject : Math 35 Teacher - Name : teacherB -\n","YoB : 1995 - S...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 14f4d95a-5b2e-4ea0-8ecd-af9cfe38de9f\n","Text: AI VIETNAM aivietnam.edu.vn 40 print (f\"\\ nNumber of doctors : {\n","ward1 . count_doctor ()}\") 41 42 # output 43 >> Number of doctors : 2\n","44 45 # 2(d) 46 print (\"\\ nAfter sorting Age of Ward1 people \") 47\n","ward1 .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e0bb719-c967-4a3d-bca9-a4ca77767c6e\n","Text: sort_age () 48 ward1 . describe () 49 50 # output 51 >> After\n","sorting Age of Ward1 people 52 Ward Name : Ward1 53 Student - Name :\n","studentA - YoB : 2010 - Grade : 7 54 Teacher - Name : teacherB - YoB :\n","1995 - Subject : History 55 Doctor - Name : doctorB - YoB : 1975 -\n","Specialist : Cardiologists 56 Teacher - Name : teacherA - YoB : 1969 -\n","Subject...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5c277809-eeca-4fba-b8d5-8342df95e57c\n","Text: AI VIETNAM aivietnam.edu.vn Hình 2: Stack 3. Thực hiện xây dựng\n","class Stack với các phương thức (method) sau đây • initialization\n","method nhận một input \"capacity\": dùng để khởi tạo stack với capacity\n","là số lượng element mà stack có thể chứa • .is_empty(): kiểm tra stack\n","có đang rỗng • .is_full(): kiểm tra stack đã full chưa • .pop(): loại\n","bỏ top...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 45660b82-4cf4-4727-9e0e-76833a3ad65e\n","Text: push (2) 6 7 print ( stack1 . is_full ()) 8 >> False 9 10 print\n","( stack1 . top ()) 11 >>2 12 13 print ( stack1 . pop ()) 14 >> 2 15 16\n","print ( stack1 . top ()) 17 >> 1 18 19 print ( stack1 . pop ()) 20 >>\n","1 21 22 print ( stack1 . is_empty ()) 23 >> True 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9895a3f6-482e-4399-9452-de610c6ebb38\n","Text: AI VIETNAM aivietnam.edu.vn Hình 3: Queue 4. Thực hiện xây dựng\n","class Queue với các chức năng (method) sau đây • initialization method\n","nhận một input \"capacity\": dùng để khởi tạo queue với capacity là số\n","lượng element mà queue có thể chứa • .is_empty(): kiểm tra queue có\n","đang rỗng • .is_full(): kiểm tra queue đã full chưa • .dequeue(): loại\n","bỏ f...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5f0b422b-ef4c-429a-8b29-8d831d0b04ed\n","Text: enqueue (2) 6 7 print ( queue1 . is_full ()) 8 >> False 9 10\n","print ( queue1 . front ()) 11 >> 1 12 13 print ( queue1 . dequeue ())\n","14 >> 1 15 16 print ( queue1 . front ()) 17 >> 2 18 19 print ( queue1\n",". dequeue ()) 20 >> 2 21 22 print ( queue1 . is_empty ()) 23 >> True 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: ba9efb67-c8ca-48d5-bac0-c9ec4ea8577b\n","Text: AI VIETNAM aivietnam.edu.vn II. Câu hỏi trắc nghiệm Câu hỏi 1:\n","Kết quả của đoạn code dưới đây là bao nhiêu. 1 import torch 2 import\n","torch .nn as nn 3 4 data = torch . Tensor ([1 , 2, 3]) 5\n","softmax_function = nn. Softmax ( dim =0) 6 output = softmax_function (\n","data ) 7 assert round ( output [0]. item () , 2) == 0.09 8 output a)\n","[0.0900, 0.2747, 0...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c462a131-764a-4625-adaa-6ea70fbb6fcf\n","Text: Module ): 5 def __init__ ( self ): 6 super (). __init__ () 7 8\n","def forward (self , x): 9 ### Your Code Here 10 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e9cf1f0-91c0-47d1-8367-db853152e8c0\n","Text: AI VIETNAM aivietnam.edu.vn 11 ### End Code Here 12 13 data =\n","torch . Tensor ([1 , 2, 300000000]) 14 my_softmax = MySoftmax () 15\n","output = my_softmax ( data ) 16 assert round ( output [0]. item () ,\n","2) == 0.0 17 output a) [0.0900, 0.2747, 0.6652] b) [0.7054, 0.0351,\n","0.2595] c) [0., 0., nan] d) [0.7054, 0.0351, 0.009] Câu hỏi 4: Hoàn\n","thành đoạn c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b383dd65-c57a-4bd7-a29d-17814160d98c\n","Text: Một student gồm có name (string), yob (int) (năm sinh), và grade\n","(string). Các bạn thực hiện viết class Student theo mô tả trên (Các\n","bạn sẽ viết thêm describe() method để print ra tất cả thông tin của\n","object) và kết quả đầu ra là gì? Chọn đáp án đúng nhất bên dưới. 1\n","from abc import ABC , abstractmethod 2 3 class Person ( ABC ): 4 def\n","__init__ (...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7b2e23d5-c454-44d4-a99c-343a7a3aad47\n","Text: AI VIETNAM aivietnam.edu.vn 12 def describe ( self ): 13 pass 14\n","15 16 class Student ( Person ): 17 def __init__ (self , name :str ,\n","yob :int , grade : str ): 18 ### Your Code Here 19 20 ### End Code\n","Here 21 22 def describe ( self ): 23 ### Your Code Here 24 25 ### End\n","Code Here 26 27 student1 = Student ( name =\" studentZ2023 \", yob =2011\n",", grad...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9f07641b-efa1-48c4-b0bc-972f4044d9a7\n","Text: 1 from abc import ABC , abstractmethod 2 3 class Person ( ABC ):\n","4 def __init__ (self , name :str , yob : int ): 5 self . _name = name\n","6 self . _yob = yob 7 8 def get_yob ( self ): 9 return self . _yob 10\n","11 @abstractmethod 12 def describe ( self ): 13 pass 14 15 16 class\n","Teacher ( Person ): 17 def __init__ (self , name :str , yob :int ,\n","subject...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 254df5b1-5e72-4b58-bf38-24cc816012a9\n","Text: AI VIETNAM aivietnam.edu.vn a) Teacher - Name: 1991 - YoB:\n","teacherZ2023 - Subject: History b) Teacher - Name: teacherZ2023 - YoB:\n","1991 - Subject: History c) Teacher - Name: History - YoB: teacherZ2023\n","- Subject: 1991 d) Tất cả đều sai Câu hỏi 7: Một người (person) có thể\n","là student, doctor, hoặc teacher. Một doctor gồm có name (string), yob\n","(str...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6cada8fd-43d5-40f6-aeff-6ce370603577\n","Text: 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8e85ac51-9186-4a18-8064-1bee0effabc9\n","Text: AI VIETNAM aivietnam.edu.vn 1 class Ward : 2 def __init__ (self\n",", name : str ): 3 self . __name = name 4 self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0ba3bc51-9ea3-43c8-be8e-02e08483afed\n","Text: __listPeople = list () 5 6 def add_person (self , person :\n","Person ): 7 self . __listPeople . append ( person ) 8 9 def describe (\n","self ): 10 print (f\" Ward Name : { self . __name }\") 11 for p in self\n",". __listPeople : 12 p. describe () 13 14 def count_doctor ( self ): 15\n","### Your Code Here 16 17 ### End Code Here 18 19 student1 = Student (\n","name =...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1fa65334-500a-4f68-9ea8-8797ae52cbad\n","Text: .push(value) add thêm value vào trong stack. Kết quả đầu ra là\n","gì? 1 class MyStack : 2 def __init__ (self , capacity ): 3 self .\n","__capacity = capacity 4 self . __stack = [] 5 6 def is_full ( self ):\n","7 return len ( self . __stack ) == self . __capacity 8 9 def push\n","(self , value ): 10 ### Your Code Here 11 12 ### End Code Here 13 14\n","stack1 = MySt...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 44983db3-f8b5-429c-97de-c0691b07153b\n","Text: AI VIETNAM aivietnam.edu.vn 16 assert stack1 . is_full () ==\n","False 17 stack1 . push (2) 18 print ( stack1 . is_full ()) a) True b)\n","False c) None d) Raise an error Câu hỏi 10: Thực hiện xây dựng class\n","Stack với các chức năng (method) sau đây: initialization method nhận\n","một input \"capacity\", dùng để khởi tạo stack với capacity là số lượng\n","element ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7d503e03-a685-4795-b899-25660a8dcbb9\n","Text: 1 class MyStack : 2 def __init__ (self , capacity ): 3 self .\n","__capacity = capacity 4 self . __stack = [] 5 6 def is_full ( self ):\n","7 return len ( self . __stack ) == self . __capacity 8 9 def push\n","(self , value ): 10 ### Your Code Here 11 12 ### End Code Here 13 14\n","def top ( self ): 15 ### Your Code Here 16 17 # End Code Here 18 19\n","stack1 = MyS...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 04b6fe44-6025-4a99-9d10-19d10f3d9888\n","Text: AI VIETNAM aivietnam.edu.vn 5 6 def is_full ( self ): 7 return\n","len ( self . __queue ) == self . __capacity 8 9 def enqueue (self ,\n","value ): 10 ### Your Code Here 11 12 ### End Code Here 13 14 queue1 =\n","MyQueue ( capacity =5) 15 queue1 . enqueue (1) 16 assert queue1 .\n","is_full () == False 17 queue1 . enqueue (2) 18 print ( queue1 .\n","is_full ()) a) F...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6468651a-1ec3-42cc-a46c-4ebd8c4cd9a7\n","Text: 1 class MyQueue : 2 def __init__ (self , capacity ): 3 self .\n","__capacity = capacity 4 self . __queue = [] 5 6 def isEmpty ( self ):\n","7 return len ( self . __queue ) == 0 8 9 def is_full ( self ): 10\n","return len ( self . __queue ) == self . __capacity 11 12 def dequeue (\n","self ): 13 14 def enqueue (self , value ): 15 16 def front ( self ):\n","17 ### Yo...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c11bb363-4757-42d1-8a4d-e1c43765c3c3\n","Text: AI VIET NAM – COURSE 2024 Probability Exercise (Naive Bayes\n","Classifier) Ngày 20 tháng 7 năm 2024 Giả sửX có các đặc trưng thuộc\n","tính độc lập với nhaux1, x2, ..., xn, để phân loạiX vào lớp một trong\n","các lớpC = c1, c2, ..., cm, dựa vào công thức Bayes ta có: P(c|X) =\n","P(X|c).P(c) P(X) Dựa vào ước lượng tối đa xác suất hậu nghiệm (MAP -\n","Maximum A Po...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7dcbeff5-61f7-455a-b099-74aaac235cfd\n","Text: AI VIETNAM aivietnam.edu.vn Câu hỏi 1: Xác suất xảy ra sự kiện\n","\"Play Tennis\"=\"Yes\" và sự kiện \"Play Tennis\"=\"No\" lần lượt là: a)\n","P(\"Play Tennis\" = \"Yes\") = 6/10, P(\"Play Tennis\" = \"No\") = 4/10 b)\n","P(\"Play Tennis\" = \"Yes\") = 4/10, P(\"Play Tennis\" = \"No\") = 6/10 c)\n","P(\"Play Tennis\" = \"Yes\") = 6/10, P(\"Play Tennis\" = \"No\") = 6/10 d)\n","P(\"Play Tennis\" =...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3009281c-aac3-40b6-ba38-8a8340bae807\n","Text: AI VIETNAM aivietnam.edu.vn 2.\n","----------------------------------------------------------------------------------------------------\n","Node ID: aec05a37-8c7a-49a6-a124-32f36d6ff550\n","Text: MULTI-LABEL CLASSIFICATION - TRAFFIC DATACho tập dữ liệu huấn\n","luyện mô hình phân loại Naive Bayes gồm các thuộc tính \"Day\",\n","\"Season\", \"Fog\", \"Rain\". Day Season Fog Rain Class Weekday Spring None\n","None On Time Weekday Winter None Slight On Time Weekday Winter None\n","None On Time Holiday Winter High Slight Late Saturday Summer Normal\n","None On Time Wee...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 733d7e62-9166-4cac-86bb-8f1a76877f73\n","Text: AI VIETNAM aivietnam.edu.vn (C) P(\"Class\" = \"On Time\" | X)∝\n","0.0026 (D) P(\"Class\" = \"On Time\" | X)∝ 0.0000 Câu hỏi 7: Xác suất xảy\n","ra sự kiện \"Class\"=\"Late\" khi sự kiện X xảy ra là: (A) P(\"Class\" =\n","\"Late\" | X)∝ 0.0222 (B) P(\"Class\" = \"Late\" | X)∝ 0.0013 (C) P(\"Class\"\n","= \"Late\" | X)∝ 0.0026 (D) P(\"Class\" = \"Late\" | X)∝ 0.0000 Câu hỏi 8:\n","Xác suất xả...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fe1a1c21-572d-4a1b-a6cf-d271ef513c91\n","Text: AI VIETNAM aivietnam.edu.vn 3. IRIS CLASSIFICATION Cho một tập\n","dữ liệu huấn luyện phân loại hoa Iris dựa vào chiều dài cánh hoa như\n","bảng dữ liệu bên dưới.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 20e25cb7-6009-4eea-84ac-9f1ff0ed1109\n","Text: Các bạn hãy trả lời các câu hỏi sau khi dùng Gaussian Naive\n","Bayes cho data Iris này. Length 1.4 1.0 1.3 1.9 2.0 1.8 3.0 3.8 4.1\n","3.9 4.2 3.4 Class 0 0 0 0 0 0 1 1 1 1 1 1 Bảng 3: Phân loại cánh hoa\n","Iris dựa vào chiều dài cánh hoa - Tập dữ liệu huấn luyện Câu hỏi 11:\n","Giá trị mean và variance của biến đầu vào (Length) cho \"Class\"=\"0\" lần\n","lượt là: a...\n","----------------------------------------------------------------------------------------------------\n","Node ID: aa3d12dc-54b0-4c97-98c5-f75bdf898037\n","Text: AI VIETNAM aivietnam.edu.vn 4. PLAY TENNIS CLASSIFIER\n","IMPLEMENTATION Cho trước dữ liệu thời tiết của 10 ngày (D1-D10, như\n","bảng 1). Hãy phát triển chương trình sử dụng mô hình phân loại Naive\n","Bayes để dự đoán xem ngày thứ 11 (D11), AD có thể chơi tennis hay\n","không? Day Outlook Temperature Humidity Wind PlayTennis D11 Sunny Cool\n","High Strong ?\n","----------------------------------------------------------------------------------------------------\n","Node ID: 226436e7-5ba1-4012-b0a8-0c4313a07331\n","Text: ?? Bảng 4: Play Tennis - Dữ liệu testing (a) \"Play Tennis\" =\n","\"Yes\" (b) \"Play Tennis\" = \"No\" Để hoàn thành bài tập này bạn cần hoàn\n","thành các function sau đây bằng cách sử dụng thư viên numpy: 4.1 Hoàn\n","thiện functioncreate_train_dataset() để tổ chức dữ liệu bảng 1 vào\n","array 2 chiều như bên dưới. 1 # ######################## 2 # Create\n","data 3 # ##...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c770036c-6ebb-4b79-94fa-461c86ba5eaa\n","Text: AI VIETNAM aivietnam.edu.vn Câu hỏi 14: Kết quả nào sau đây là\n","output từ chương trình trên: a) P(\"Play Tennis\" = \"Yes\") = 0.6,\n","P(\"Play Tennis\" = \"No\") = 0.4 b) P(\"Play Tennis\" = \"Yes\") = 0.3,\n","P(\"Play Tennis\" = \"No\") = 0.7 c) P(\"Play Tennis\" = \"Yes\") = 0.4,\n","P(\"Play Tennis\" = \"No\") = 0.8 d) P(\"Play Tennis\" = \"Yes\") = 0.4,\n","P(\"Play Tennis\" = \"No\") =...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1ff8088a-55d6-4b38-98a2-19d19927e030\n","Text: shape [1] -1) : 6 x_unique = np. unique ( data [: ,i]) 7\n","list_x_name . append ( x_unique ) 8 9 # your code here\n","******************** 10 11 conditional_probability . append (\n","x_conditional_probability ) 12 return conditional_probability ,\n","list_x_name 13 Câu hỏi 15: Hãy cho biết kết quả của đoạn chương trình\n","sau đây: 1 train_data = create_train_da...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f43c4279-16fc-4679-add1-cc8a2ca6fe08\n","Text: AI VIETNAM aivietnam.edu.vn d) x1 = [’Overcast’ ’Rain’ ’Sunny’]\n","x2 = [’Cool’ ’Hot’ ’Mild’] x3 = [’Strong’ ’Weak’] x4 = [’High’\n","’Normal’] 4.4 Hoàn thiện functionget_index_from_value để tính trả về\n","index tương ứng với feature name: 1 # This function is used to return\n","the index of the feature name 2 def get_index_from_value (\n","feature_name , list_fe...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6122799f-8232-47eb-9f9c-ae3d289b68dc\n","Text: round ( conditional_probability [0][1 , x1 ] ,2)) a)\n","P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.27 b)\n","P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.47 c)\n","P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.37 d)\n","P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.17 Câu hỏi 18: Hãy cho\n","biết kết quả của đoạn chương trình sau đây: 1 train_data =\n","create_train_data ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fc5ffdab-9f5a-4894-b0ed-d3b6309f12c9\n","Text: AI VIETNAM aivietnam.edu.vn a) P(’Outlook’=’Sunny’|Play\n","Tennis’=’Yes’) = 0.5 b) P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.4\n","c) P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.3 d)\n","P(’Outlook’=’Sunny’|Play Tennis’=’Yes’) = 0.2 4.5 Hoàn thiện\n","functiontrain_naive_bayes như bên dưới: 1 # ##########################\n","2 # Train Naive Bayes Model 3 # #####...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4f67798e-566d-4110-991c-ab4e638ee542\n","Text: Bảng 5: Play Tennis - Dữ liệu testing 1 # ################### 2\n","# Prediction 3 # ################### 4 def prediction_play_tennis (X,\n","list_x_name , prior_probability , conditional_probability ) : 5 6 x1=\n","get_index_from_value (X[0] , list_x_name [0]) 7 x2=\n","get_index_from_value (X[1] , list_x_name [1]) 8 x3=\n","get_index_from_value (X[2] , list_x_nam...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5b85567e-dff8-4d4d-b0fc-a61a72134774\n","Text: AI VIETNAM aivietnam.edu.vn 1 X = [’Sunny ’,’Cool ’, ’High ’, ’\n","Strong ’] 2 data = create_train_data () 3 prior_probability ,\n","conditional_probability , list_x_name = train_naive_bayes ( data ) 4\n","pred = prediction_play_tennis (X, list_x_name , prior_probability ,\n","conditional_probability ) 5 6 if( pred ): 7 print (\"Ad should go!\") 8\n","else : 9 print...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 075d43de-a33d-4c02-9c3a-2dab42b6fce3\n","Text: a) Ad should not go! b) Ad should go! 10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 86f5492c-963b-4c05-9e2f-1a178255e4e1\n","Text: AI VIETNAM aivietnam.edu.vn 5. (OPTIONAL) IRIS CLASSIFIER\n","IMPLEMENTATION Cho trước dữ liệu chứa thông tin về hoa Iris gồm có\n","sepal length, sepal width và petal length, và Species (bảng 6). Hãy\n","phát triển chương trình sử dụng mô hình phân loại Gausian Naive Bayes\n","để dự đoán chủng loại của hoa Iris. Dữ liệu hoa iris được lưu trữ\n","trong file iris_da...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b1c02e92-91c8-4376-8881-aac34ac871a5\n","Text: No. Sepal length Sepal width Petal length Petal width Species 1\n","5.1 3.5 1.4 0.2 Iris-setosa 2 4.9 3.0 1.4 0.2 Iris-setosa 3 6.4 3.1\n","5.5 1.8 Iris-virginica 4 6.0 3.0 4.8 1.8 Iris-virginica 5 6.0 2.2 4.0\n","1.0 Iris-versicolora ... ... ... ... .. ... Bảng 6: Iris flower - Tập\n","dữ liệu huấn luyện Dựa vào hướng dẫn dưới đây để thực thi mã nguồn cho\n","bài ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4c0e08ed-3734-453b-9af4-0adf9e3c18ef\n","Text: AI VIET NAM – AIO COURSE 2023 Project: Reinforcement Learning\n","From Human Feedback Quoc-Thai Nguyen và Quang-Vinh Dinh PR-Team: Đăng-\n","Nhã Nguyễn, Minh-Châu Phạm và Hoàng-Nguyên Vũ Ngày 1 tháng 4 năm 2024\n","Phần I. Giới thiệu Hình 1: Ứng dụng ChatGPT được cải tiến dựa vào mô\n","hình học tăng cường từ phản hồi người dùng. Hình 2: Ví dụ minh hoạ tóm\n","tắt v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 33f1dd67-249e-4efc-82f0-b8dfa32a2760\n","Text: 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: c7a4500d-4582-4a61-8574-5707a41e19ed\n","Text: AI VIETNAM aivietnam.edu.vn ChatGPT (Chat Generative Pre-\n","Training Transformer)là mô hình chatbot được phát triển bởi OpenAI ra\n","mắt vào tháng 11 năm 2022. ChatGPT được xây dựng dựa trên việc tinh\n","chỉnh mô hình ngôn ngữ lớn (Large Language Model) kết hợp với học tăng\n","cường dựa trên phản hồi của con người (Reinforcement Learning with\n","Human Feedback...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1fc947dd-f3a2-4616-b49f-682cd5844904\n","Text: Trong phần này chúng ta sẽ tập trung vào đi sâu tìm hiểu về\n","RLHF. Hình 3: Reinforcement Learning from Human Feedback. 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: fed32d95-8622-43cb-9118-e74a55f2b8ec\n","Text: AI VIETNAM aivietnam.edu.vn (a) Supervised fine-tuning (SFT)\n","Trong phần này chúng ta sử dụng 1 tập các prompt và response để tinh\n","chỉnh mô hình ngôn ngữ. Mô hình GPT2 được chọn với 12 khối\n","Transformer-Decoder. Ngoài ra, có nhiều mô hình ngôn ngữ có thể chọn\n","phù hợp với mục tiêu và open source như BLOOM, LLaMa,... (b) Reward\n","Modeling (RM) Trong p...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3bd557c8-1db3-4729-bce3-7570f3799d96\n","Text: Mục tiêu của phần này với mỗi prompt và response chúng ta cần\n","đưa ra điểm số tương ứng. (c) Reinforcement learning (RL) Trong phần\n","này chúng ta sử dụng bộ dữ liệu rm-static, kết hợp với mô hình RM để\n","tinh chỉnh mô hình SFT dựa vào phương pháp huấn luyện sử dụng Proximal\n","Policy Optimization (PPO). 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9d558ba0-8d2e-4642-973e-b9b80f535ca7\n","Text: AI VIETNAM aivietnam.edu.vn Phần II. Text Summarization using\n","RLHF Trong phần này, chúng ta sẽ tập trung ứng dụng cải tiến mô hình\n","dựa vào RLHF cho bài toán tóm tắt văn bản. Tóm tắt văn bản (Text\n","Summarization) nhận đầu vào là đoạn văn bản dài hoặc nhiều đoạn văn\n","bản dài. Thông qua mô hình các đoạn văn bản sẽ được tóm tắt ngắn gọn\n","thành vài câu.\n","----------------------------------------------------------------------------------------------------\n","Node ID: efa1fdf1-e078-4e15-bda1-9097a00236de\n","Text: Ví dụ minh hoạ về bài toán tóm tắt văn bản được minh hoạ như\n","hình 2. Các bước để huấn luyện và tối ưu mô hình được mô tả như sau:\n","Hình 4: Các bước huấn luyện mô hình tóm tắt văn bản sử dụng RLHF. RLHF\n","bao gồm 3 phần: • Supervised Fine-Tuning: Tinh chỉnh mô hình trên bộ\n","dữ liệu có nhãn cho bài toán tóm tắt • Reward Modeling: Huấn luyện mô\n","hình ch...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c0837923-cab5-4a10-a21e-97ca6ca62a13\n","Text: AI VIETNAM aivietnam.edu.vn 1. Supervised Fine-Tuning (SFT)\n","Trong phần này chúng ta sẽ tinh chỉnh mô hình tiền huấn luyện như GPT,\n","BLOOM, FlanT5,... trên bộ dữ liệu được gán nhãn. Bộ dữ liệu\n","openai_summarize_tldr bao gồm các cặp dữ liệu: văn bản và bản tóm tắt.\n","Hình 5: Supervised Fine-Tuning. 1.1. Dataset Chúng ta tải về bộ dữ\n","liệu từ thư viện t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 47467d7e-d4a9-4404-a4a0-2a5cb907cca2\n","Text: Để tăng tốc độ huấn luyện mô hình, chúng ta sẽ sử dụng kỹ thuật\n","quantization và LORA. 1 import torch 2 from trl import ModelConfig ,\n","get_quantization_config , get_kbit_device_map 3 from peft import\n","LoraConfig , PeftConfig , PeftModel , get_peft_model ,\n","prepare_model_for_kbit_training 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1edee873-c8c1-487b-b84b-0564d4bb7b45\n","Text: AI VIETNAM aivietnam.edu.vn 4 5 model_config = ModelConfig ( 6\n","model_name_or_path =’ facebook /opt -350 m’ 7 ) 8 9 torch_dtype = ( 10\n","model_config . torch_dtype 11 if model_config . torch_dtype in [\" auto\n","\", None ] 12 else getattr ( torch , model_config . torch_dtype ) 13 )\n","14 quantization_config = get_quantization_config ( model_config ) 15\n","mod...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d8d0dce2-ece7-41c5-9a10-1adb4981394b\n","Text: model_name_or_path , use_fast = True ) 25 tokenizer . pad_token\n","= tokenizer . eos_token 26 27 tokenizer . pad_token_id = tokenizer .\n","eos_token_id 28 29 30 # lora 31 peft_config = LoraConfig ( 32 r=16 ,\n","33 lora_alpha =32 , 34 lora_dropout =0.05 , 35 bias =\" none \", 36\n","task_type =\" CAUSAL_LM \", 37 ) 1.3. Metric Chúng ta sử dụng độ ROUGE\n","để đánh gi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9dd78363-4a92-43b8-9d34-57daf4f5e19c\n","Text: AI VIETNAM aivietnam.edu.vn 7 evaluation_strategy =\" epoch \", 8\n","save_strategy =’epoch ’, 9 per_device_train_batch_size =4 , 10\n","per_device_eval_batch_size =4 , 11 adam_beta1 =0.9 , 12 adam_beta2\n","=0.95 , 13 num_train_epochs = num_epochs , 14 load_best_model_at_end\n","=True , 15 ) 16 17 max_input_length = 512 18 trainer = SFTTrainer ( 19\n","model = model...\n","----------------------------------------------------------------------------------------------------\n","Node ID: fa9fb80d-451d-4c6d-96a9-36712195dfb7\n","Text: train () 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: a74bb88b-57d5-4414-a6eb-ff9623182ad3\n","Text: AI VIETNAM aivietnam.edu.vn 2. Reward Modeling Trong phần này\n","chúng ta sẽ xây dụng mô hình đánh giá chất lượng bản tóm tắt. Đầu vào\n","của mô hình là văn bản kết hợp với bản tóm tắt. Đầu ra là giá trị dự\n","đoán từ 0 đến 1. Chúng ta sẽ tinh chỉnh mô hình SFT cho bài toán phân\n","loại. Với bản tóm tắt phù hợp (Chosen) sẽ có nhãn là 1, và bản tóm tắt\n","không...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 26fa0fe1-6885-4433-929a-cd8aad1f71bf\n","Text: 2.1. Dataset Bộ dữ liệu openai_summarize_comparisons bao gồm 83\n","mẫu có nhãn: văn bản, bản tóm tắt phù hợp và bản tóm tắt không phù\n","hợp. Chúng ta sẽ kết hợp lần lượt văn bản với bản tóm tắt phù hợp và\n","không phù hợp để huấn luyện mô hình. 1 # load dataset 2 from datasets\n","import load_dataset 3 4 rw_ds_name = ’ CarperAI /\n","openai_summarize_comparison...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 552e5bd5-579b-4e6a-9bae-f5cae2b3542a\n","Text: AI VIETNAM aivietnam.edu.vn 29 preprocess_function , 30 batched\n","=True , 31 num_proc =4 , 32 ) 33 34 max_input_length = 512 35 36\n","rw_ds_filted = rw_ds_processed . filter ( 37 lambda x: len (x[\"\n","input_ids_chosen \"]) <= max_input_length 38 and len (x[\"\n","input_ids_rejected \"]) <= max_input_length 39 ) 40 41 rw_train =\n","rw_ds_filted [\" train \"] 42 rw_v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 43d71e38-1307-40f3-9fb5-427d968cc2c0\n","Text: model_name_or_path , use_fast = True ) 25 model =\n","AutoModelForSequenceClassification . from_pretrained ( 26 model_config\n",". model_name_or_path , num_labels =1 , ** model_kwargs 27 ) 28 29\n","peft_config = LoraConfig ( 30 r=16 , 31 lora_alpha =32 , 32\n","lora_dropout =0.05 , 33 bias =\" none \", 34 task_type =\" SEQ_CLS \", 35\n",") 2.3. Trainer Huấn luyện mô h...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 13c87d42-78cd-4aa2-80f1-223e8a9cf98a\n","Text: AI VIETNAM aivietnam.edu.vn 4 num_epochs = 1 # 10 5 6\n","reward_config = RewardConfig ( 7 output_dir =’./ save_rw_model ’, 8\n","evaluation_strategy =\" epoch \", 9 save_strategy =’epoch ’, 10\n","per_device_train_batch_size =4 , 11 per_device_eval_batch_size =4 , 12\n","num_train_epochs = num_epochs , 13 load_best_model_at_end =True , 14\n","max_length = max_input_...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6e4f1cc6-4178-4e85-9928-7526584725e3\n","Text: AI VIETNAM aivietnam.edu.vn 3. Reinforcement Learning Trong phần\n","này chúng ta tối ưu mô hình SFT dựa vào mô hình Reward sử dụng thuật\n","toán Proximal Policy Optimization (PPO). Hình 7: Proximal Policy\n","Optimization. 2.1.\n","----------------------------------------------------------------------------------------------------\n","Node ID: afa33f93-61e5-49ee-809e-7b3735b5ad50\n","Text: Dataset Chúng ta sử dụng bộ dữ liệu openai_summarize_tldr. Nối\n","phần văn bản và phần bản tóm tắt để huấn luyện mô hình. 1 from\n","transformers import AutoTokenizer 2 3 ppo_ds_name = ’ CarperAI /\n","openai_summarize_tldr ’ 4 ppo_ds = load_dataset ( sft_ds_name , split\n","=\" train \") 5 6 def build_dataset (ds , tokenizer , max_length =200) :\n","7 ds = ds. filt...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6ed5598a-4907-474c-b5bf-58f98ba3ff75\n","Text: AI VIETNAM aivietnam.edu.vn 9 bias =\" none \", 10 task_type =\"\n","SEQ_CLS \", 11 ) 12 13 model_path = \"./ save_sft_model / checkpoint\n","-1000 \" 14 model = AutoModelForCausalLMWithValueHead . from_pretrained\n","( 15 pretrained_model_name_or_path = model_path , 16 peft_config =\n","peft_config , 17 ) 2.3. Trainer 1 from trl import PPOConfig ,\n","PPOTrainer 2 3 def...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 305dbbd8-2e01-4f9b-a5b5-82f51beb48b4\n","Text: tokenizer . pad_token_id is None : 7 sentiment_pipe . tokenizer\n",". pad_token_id = tokenizer . pad_token_id 8 9 if sentiment_pipe .\n","model . config . pad_token_id is None : 10 sentiment_pipe . model .\n","config . pad_token_id = tokenizer . pad_token_id 2.5. Training Huấn\n","luyện mô hình với các tham số cài đặt cho quá trình sinh bản tóm tắt\n","và đánh giá ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8aef4b87-1a90-40fd-b1b0-53856c019d25\n","Text: eos_token_id , 9 \" max_new_tokens \": 200 , 10 } 11 sent_kwargs =\n","{\" return_all_scores \": True , \" function_to_apply \": \" none \", \"\n","batch_size \": 16} 12 13 for _epoch , batch in tqdm ( enumerate (\n","ppo_trainer . dataloader )): 14 query_tensors = batch [\" input_ids \"]\n","15 16 # Get response from gpt2 12\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8ee5ab9a-a6d3-47b4-bcf1-a80ba2854cfd\n","Text: AI VIETNAM aivietnam.edu.vn 17 response_tensors ,\n","ref_response_tensors = ppo_trainer . generate ( 18 query_tensors ,\n","return_prompt = False , generate_ref_response =True , **\n","generation_kwargs 19 ) 20 batch [\" response \"] = tokenizer .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2baba375-a0c5-4124-ae5e-7f34e56ae5b8\n","Text: batch_decode ( response_tensors ) 21 batch [\" ref_response \"] =\n","tokenizer . batch_decode ( ref_response_tensors ) 22 23 # Compute\n","sentiment score 24 texts = [q + r for q, r in zip ( batch [\" query \"],\n","batch [\" response \"])] 25 pipe_outputs = sentiment_pipe ( texts , **\n","sent_kwargs ) 26 rewards = [ torch . tensor ( output [1][ \" score \"])\n","for out...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f3b9a4c6-910b-433d-ba1a-22ff390a0818\n","Text: AI VIETNAM aivietnam.edu.vn Phần 4. Câu hỏi trắc nghiệm Câu hỏi\n","1Phương pháp huấn luyện của mô hình ChatGPT là gì? a) Tỉnh chỉnh mô\n","hình ngôn ngữ lớn GPT2 b) Tinh chỉnh mô hình ngôn ngữ lớn BLOOM c)\n","Tinh chỉnh mô hình mBART d) Tinh chỉnh mô hình ngôn ngữ lớn GPT\n","3.5(Large Language Model) với phương pháp học tăng cường dựa trên phản\n","hồi người dùn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 988e37c5-73c8-4f87-86bd-08083850b7b7\n","Text: a) Spacy b) Fasttext c) Langchain d) Fairseq Câu hỏi 5Dựa vào dữ\n","liệu đầu vào, bài toán tóm tắt văn bản được chia thành những loại nào?\n","a) Single-Document và Multi-Document b) Abstractive và Single-Document\n","c) Extractive và Abstractive d) Multi-Document và Extractive Câu hỏi\n","6Dựa vào dữ liệu đầu ra, bài toán tóm tắt văn bản được chia thành\n","những...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 214592ef-6de7-4bb9-94d4-4b87906d40a3\n","Text: AI VIETNAM aivietnam.edu.vn a) Accuracy b) F1 c) BLEU d) ROUGE\n","Câu hỏi 8Bộ dữ liệu sử dụng cho mô hình SFT là? a)\n","CarperAI/openai_summarize_tldr b) MNIST c) CIFAR10 d) CIFAR100 Câu hỏi\n","9Bài toán nào sau đây sử dụng để huấn luyện mô hình SFT?\n","----------------------------------------------------------------------------------------------------\n","Node ID: c87798b9-f0a8-46f8-930a-2590b29cfe1c\n","Text: a) Text Classification b) Machine Translation c) Text Generation\n","d) Question Answering Câu hỏi 10Bài toán nào sau đây sử dụng để huấn\n","luyện mô hình Reward? a) Text Classification b) Machine Translation c)\n","Text Generation d) Question Answering - Hết - 15\n","----------------------------------------------------------------------------------------------------\n","Node ID: 51b33831-eaf4-4f82-bafe-c7343e7ee641\n","Text: AI VIET NAM – COURSE 2024 Streamlit – Project Ngày 22 tháng 6\n","năm 2024 I. Lý thuyết Xây dựng và phát triển các ứng dụng AI bao gồm\n","các bước sau: • Data Handling: bao gồm các bước về thu thập và xử lý\n","dữ liệu • Exploratory Data Analysis (EDA): phân tích các đặc trưng của\n","dữ liệu • Modeling: từ các đặc trưng, xây dựng các mô hình và đánh giá\n","tính ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 395af445-9b1b-4300-af34-47fe339a4c63\n","Text: Hình 1: Khoảng cách chỉnh sửa Levenshtein 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: 40ecb1eb-a338-4f7e-9d20-59303c525ffb\n","Text: AI VIETNAM aivietnam.edu.vn Word Correction (Sửa lỗi chính tả)\n","là một trong những ứng dụng cơ bản của xử lý ngôn ngữ tự nhiên, với\n","mục đích xây dựng các ứng dụng nhận đầu vào là một từ, thông qua mô\n","hình sửa lỗi nếu từ đó bị sai thành một từ đúng. Ví dụ: người dùng\n","nhập vào từ ’hel’ là một từ sai, mô hình sẽ gợi ý sửa lỗi thành từ\n","’hello’. Để đơ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3c6a540f-9eeb-4c35-8a09-69d706828e8e\n","Text: Giao diện của ứng dụng được mô tả như hình sau: 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: e56021e8-8329-487a-bcab-ba2a037b11b8\n","Text: AI VIETNAM aivietnam.edu.vn Hình 2: Giao diện ứng dụng chỉnh sửa\n","chính tả 1 import streamlit as st 2 3 def main (): 4 st. title (\" Word\n","Correction using Levenshtein Distance \") 5 word = st. text_input\n","(’Word :’) 6 7 if st. button (\" Compute \"): 8 9 # compute levenshtein\n","distance 10 leven_distances = dict () 11 for vocab in vocabs : 12\n","leven_dist...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f455d0ef-9125-4fe4-bf88-638174233bac\n","Text: keys ()) [0] 17 st. write (’ Correct word : ’, correct_word ) 18\n","19 col1 , col2 = st. columns (2) 20 col1 . write (’ Vocabulary :’) 21\n","col1 . write ( vocabs ) 22 23 col2 . write (’ Distances :’) 24 col2 .\n","write ( sorted_distences ) 25 26 if __name__ == \" __main__ \": 27 main\n","() Cuối cùng, tất cả code sẽ được để trong file\n","levenshtein_distance.py....\n","----------------------------------------------------------------------------------------------------\n","Node ID: c19e8126-c05e-4722-add2-c4d3134e7338\n","Text: AI VIETNAM aivietnam.edu.vn Hình 3: Kết quả thực nghiệm. 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: dae187a2-8e4d-4f45-a9af-6803ff630ff2\n","Text: AI VIETNAM aivietnam.edu.vn 2. Object Detection Object Detection\n","là ứng dụng quan trọng điển hình của xử lý hình ảnh, với mục tiêu phát\n","hiện các khung hình chứa các đối tượng trong ảnh. Ví dụ minh hoạ về\n","ứng dụng như hình sau: Hình 4: Object Detection. Trong phần này chúng\n","ta sẽ xây dựng ứng dụng cho người dùng tải lên ảnh đầu vào, sử dụng mô\n","hì...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 43a75677-2156-47fc-993b-ad4f412b990a\n","Text: arange (0 , detections . shape [2]) : 20 confidence = detections\n","[0 , 0, i, 2] 21 22 if confidence > confidence_threshold : 23 #\n","extract the index of the class label from the ‘detections ‘, 24 # then\n","compute the (x, y)- coordinates of the bounding box for 25 # the\n","object 26 idx = int ( detections [0 , 0, i, 1]) 27 box = detections [0\n",", 0, i, 3:7...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4c5d41a4-a66a-4895-a2bd-36c128c5b560\n","Text: astype (\" int \") 29 cv2 . rectangle ( image , ( startX , startY\n","), (endX , endY ), 70 , 2) 30 return image 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: c8554fba-4c1f-4cc1-9e59-677a051f013a\n","Text: AI VIETNAM aivietnam.edu.vn Giao diện ứng dụng được mô tả như\n","hình sau: Hình 5: Giao diện ứng dụng object detection. 1 import numpy\n","as np 2 from PIL import Image 3 import streamlit as st 4 5 def main\n","(): 6 st.\n","----------------------------------------------------------------------------------------------------\n","Node ID: aadfdbae-48dc-4299-9b9b-fa4b26bc66c3\n","Text: title (’ Object Detection for Images ’) 7 file = st.\n","file_uploader (’ Upload Image ’, type = [’jpg ’,’png ’,’jpeg ’]) 8 if\n","file is not None : 9 st. image (file , caption = \" Uploaded Image \")\n","10 11 image = Image . open ( file ) 12 image = np. array ( image ) 13\n","detections = process_image ( image ) 14 processed_image =\n","annotate_image ( image , de...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4ff5631a-76a6-4008-b593-4aca53e84b25\n","Text: AI VIETNAM aivietnam.edu.vn 3. Chatbot Chatbot là ứng dụng được\n","chú ý phát triển mạnh những năm gần đây, các ứng dụng chatbot chủ yếu\n","tập trung vào các mô hình ngôn ngữ lớn có thể tương tác tốt với các\n","yêu cầu của người dùng. Ví dụ về chatbot được mô tả trong hình sau:\n","Hình 7: Ứng dụng chatbot. Trong phần này, chúng ta xây dựng ứng dụng\n","chatbot ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c3f0ecc6-c6f4-4973-ac16-52ba8835c234\n","Text: Sau khi được chấp nhận quyền truy cập, chúng ta xây dựng giao\n","diện ứng dụng được mô tả như sau: Hình 8: Giao diện ứng dụng chatbot.\n","Giao diện có bố cục gồm 2 phần: 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2890b15a-2390-4d6e-bbf0-cf69cd8a4414\n","Text: AI VIETNAM aivietnam.edu.vn • Phần bên trái: Chứa thông tin nhập\n","vào tài khoản và mật khẩu huggingface, được sử dụng để có quyền truy\n","cập vào hugchat 1 import streamlit as st 2 from hugchat import hugchat\n","3 from hugchat . login import Login 4 5 # App title 6 st. title (’\n","Simple ChatBot ’) 7 8 # Hugging Face Credentials 9 with st. sidebar :\n","10 st...\n","----------------------------------------------------------------------------------------------------\n","Node ID: feb1799d-1919-46d3-9018-20f946600cfc\n","Text: session_state . keys (): 3 st. session_state . messages = [{\"\n","role \": \" assistant \", \" content \": \" How may I help you ?\"}] 4 5 #\n","Display chat messages 6 for message in st. session_state . messages :\n","7 with st. chat_message ( message [\" role \"]): 8 st. write ( message\n","[\" content \"]) 9 Tương ứng với mỗi request gửi đến chatbot, mô hình sẽ\n","đăng nh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4182f6d9-c6df-40f6-984d-de96e87fb9ee\n","Text: messages . append ({\" role \": \" user \", \" content \": prompt })\n","13 with st. chat_message (\" user \"): 14 st. write ( prompt ) 15 16 #\n","Generate a new response if last message is not from assistant 17 if\n","st. session_state . messages [ -1][ \" role \"] != \" assistant \": 18\n","with st. chat_message (\" assistant \"): 19 with st. spinner (\" Thinking\n","... \"): 2...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9cc3cf52-7232-4ce2-8509-5fdae6ec2afe\n","Text: AI VIETNAM aivietnam.edu.vn Sau khi hoàn thiện code vào file\n","’chatbot.py’, chúng ta chạy lệnh: ’streamlit run chatbot.py’ và thử\n","nghiệm thu được kết quả như sau: Hình 9: Kết quả thực nghiệm ứng dụng\n","chatbot. 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: b96a3caa-6183-4ba9-b7d8-cde4f8030f1c\n","Text: AI VIETNAM aivietnam.edu.vn II. Câu hỏi trắc nghiệm Câu hỏi 1:\n","Hàm nào sau đây được sử dụng để hiển thị chuỗi văn bản trong\n","streamlit. a) st.text(...) b) st.image(...) c) st.selectbox(...) d)\n","st.slider(...) Câu hỏi 2: Đoạn code nào sau đây thể hiện đúng để hiển\n","thị cho giao diện sau: 1 a) 2 options = st.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9949cde8-0d2d-4512-a256-aa68d1be6758\n","Text: multiselect (\" Your favorite colors :\") 3 st. write (\" You\n","selected :\", options ) 4 5 b) 6 options = st. multiselect (\" Your\n","favorite colors :\", [\" Green \", \" Yellow \", \" Red \", \" Blue \"], [\"\n","Yellow \", \" Red \"]) 7 8 c) 9 options = st. multiselect (\" Your\n","favorite colors :\", [\" Green \", \" Yellow \", \" Red \", \" Blue \"], [\"\n","Yellow \", \" Red \"]) 10 st...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c0e5737a-4827-46aa-8ce8-82446e15259d\n","Text: AI VIETNAM aivietnam.edu.vn d) st.image(image_path, caption=’A\n","cat’, width=None, channels=’RGB’) Câu hỏi 5: Tính khoảng cách chỉnh\n","sửa levenshtein của 2 từ sau: \"elmets\" và \"elements\". a) 2 b) 3 c) 4\n","d) 5 Câu hỏi 6: Hàm st.session_state trong streamlit được sử dụng để\n","làm gì. a) Hiển thị chuỗi văn bản b) Hiển thị hình ảnh c) Lưu hình ảnh\n","d) Chia...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1f36e8ac-12a7-4a7c-8bc8-dc58fb71dc19\n","Text: columns (2) 3 f_name = col1 . text_input (’First N a m e ) 4\n","l_name = col2 . text_input (’Last N a m e ) 5 6 b) 7 with st. form (\"\n","my_form \"): 8 col1 , col2 = st. columns (2) 9 f_name = col1 .\n","text_input (’First N a m e ) 10 l_name = col2 . text_input (’Last N a\n","m e ) 11 12 c) 13 with st. form (\" my_form \"): 14 col1 , col2 = st.\n","columns (2) 15 f...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 904bc7a2-3c3f-4e89-a96a-e9f6413a60ee\n","Text: AI VIETNAM aivietnam.edu.vn 18 19 d) 20 with st. form (\" my_form\n","\"): 21 col1 , col2 = st.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9e6f36f1-b8e4-418e-a5e4-d26db185a89c\n","Text: columns (2) 22 f_name = col1 . text_input (’First N a m e ) 23\n","l_name = col2 . text_input (’Last N a m e ) 24 submitted = st.\n","form_submit_button (\" Submit \") 25 if submitted : 26 st. write (\"\n","First Name : \", f_name , 27 \" - Last Name :\", l_name ) Câu hỏi 8Hàm\n","nào sau đây cho phép người dùng tải lên nhiều file. a) uploaded_files\n","= st.file_uploade...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d53fd2a3-759f-4eda-b3c5-a1f4a13c851d\n","Text: AI VIET NAM – AI COURSE 2023 Diffusion-based Image Colorization\n","Tien-Huy Nguyen, Khanh Duong and Nhu-Tai Do Ngày 7 tháng 4 năm 2024\n","Phần I: Giới thiệu Hình 1: Ví dụ minh họa cho bài toán Image\n","Colorization. Sự bùng nổ của mô hìnhDiffusion trong những năm gần đây\n","đã tạo ra nhiều bước ngoặt trong vấn đề sinh dữ liệu, đặc biệt là vấn\n","đề tái tạo ảnh.\n","----------------------------------------------------------------------------------------------------\n","Node ID: f7925ebc-6f7e-477d-ac42-b41873fee94d\n","Text: Theo đó, ở quá trình forward, Diffusion thêm nhiễu vào ảnh một\n","cách có hệ thống, biến ảnh thành một nhiễu tuân theo phân phối Gauss.\n","Sau đó, ở quá trình ngược lại, mô hình này học cách dự đoán nhiễu và\n","tiến hành khử nhiễu dần dần từ một nhiễu chuẩn để tạo ra ảnh mới.\n","Trong bài báo mang tên Palette: Image-to-Image Diffusion Models, mô\n","hình Diffus...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f2372b93-b425-4fd1-9307-546a29bbacbb\n","Text: AI VIETNAM aivietnam.edu.vn Hình 2: Không gian màuLab. Trong dự\n","án này, chúng ta sẽ tiếp tục sử dụng không gian màuLab cho xử lý dữ\n","liệu. Theo đó, mô hình của chúng ta sẽ nhận đầu vào là kênhL như tấm\n","ảnh gray-scale, đại diện cho độ sáng, và sử dụng kênh ab như ground\n","truth của mô hình. Đối với vấn đề tô màu cho ảnh, có một thuật ngữ\n","được gọi là...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 29f73b50-3665-458c-893e-165b043d16de\n","Text: Input: Ảnh xám G (L channel). 2. Output: Trường ảnh màu C (ab\n","channels). 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5c070133-05ef-4adf-a2af-6fbe26ac198e\n","Text: AI VIETNAM aivietnam.edu.vn Phần II: Nội dung Trong phần này,\n","chúng ta sẽ triển khai mô hình Diffusion-based Image Colorization dựa\n","trên ý tưởng cơ bản của bài báo Palette: Image-to-Image Diffusion\n","Models để học cách biến đổi một hình ảnh xám đầu vào thành một hình\n","ảnh màu hợp lý. Cụ thể, ta sẽ xây dựng chương trình dựa trên bộ dữ\n","liệu CelebA (L...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4d227b66-a74c-4ec5-9263-6b355f430689\n","Text: Data Preparation Đầu tiên, chúng ta cần chuẩn bị bộ dữ liệu\n","CelebA. Bạn có thể tải bộ dữ liệu và giải nén tại đây. Khai báo các\n","thư viện: 1 import glob 2 import torch 3 import cv2 4 import numpy as\n","np 5 from torchvision import transforms 6 from torch . utils . data\n","import Dataset 7 from torch . utils . data import DataLoader 8 9 #\n","Load the paths...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 575847ae-df79-4e4f-a774-1b81b6df719a\n","Text: AI VIETNAM aivietnam.edu.vn 16 # Build ColorDataset 17 class\n","ColorDataset (): 18 def __init__ (self , img_paths , data_len =2880 ,\n","image_size =(128 , 128) ): 19 if data_len > 0: 20 self . img_paths =\n","img_paths [: int ( data_len )] 21 else : 22 self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 02d42509-989e-4984-9226-d7b8cc2a0781\n","Text: img_paths = img_paths 23 self . tfs = transforms . Resize ((\n","image_size [0] , image_size [1]) ) 24 25 def __getitem__ (self , index\n","): 26 img_path = self . img_paths [ index ] 27 arr_img_bgr = cv2 .\n","imread ( img_path ) 28 29 # Convert BGR to LAB 30 arr_img_lab = cv2 .\n","cvtColor ( arr_img_bgr , cv2 . COLOR_BGR2LAB ) 31 32 # Normalize from\n","[0..255]...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6f90127a-7b09-41ae-8ae6-be3d5716cd41\n","Text: AI VIETNAM aivietnam.edu.vn Hình 4: Minh họa quá trình đảo ngược\n","của mô hình Color Diffusion Ở quá trình đảo ngược, mô hình UNet nhận\n","đầu vào bao gồm: timestep embedding t và một ảnh 3 chiều (được hợp\n","thành bởi đầu ra của mô hình UNet tại thời điểmt + 1và thành phần điều\n","kiện là kênh màu xám). Sau đó, mô hình học cách dự đoán và trả về một\n","nhiễu...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f9c81d37-3090-4d0b-b1eb-b8f37b6ede68\n","Text: Kết quả này sau đó được kết hợp với trường màu ab tại timestep t\n","để tính toán raab channels tại timestep t - 1. Quá trình này được lặp\n","lại cho đến khi timestep t = 0. Lúc này ta nhận được trường màuab đã\n","được khử nhiễu. Kết hợp với kênh màu xám, ta thu được một ảnh màu hoàn\n","thiện. UNet model: 1 import math 2 import numpy as np 3 import torch 4\n","i...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 107d393d-b421-434c-9940-c4ae3cc59de4\n","Text: AI VIETNAM aivietnam.edu.vn 27 return module 28 29 30 def\n","mean_flat ( tensor ): 31 \"\"\" 32 Take the mean over all non - batch\n","dimensions . 33 \"\"\" 34 return tensor . mean ( dim = list ( range (1 ,\n","len ( tensor . shape )))) 35 36 37 def normalization ( channels ): 38\n","\"\"\" 39 Make a standard normalization layer . 40 41 : param channels :\n","number of in...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3debc170-fcd1-436c-bec3-bda9053c4154\n","Text: Module for normalization . 43 \"\"\" 44 return GroupNorm32 (32 ,\n","channels ) 45 46 47 48 def checkpoint (func , inputs , params , flag\n","): 49 \"\"\" 50 Evaluate a function without caching intermediate\n","activations , allowing for 51 reduced memory at the expense of extra\n","compute in the backward pass . 52 53 : param func : the function to\n","evaluate . 54 : p...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 696698fa-59bc-4b9f-89d1-e768937108ac\n","Text: grad ( 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: cf0dc5f7-58f5-4732-846d-e6b8fec12584\n","Text: AI VIETNAM aivietnam.edu.vn 86 output_tensors , 87 ctx .\n","input_tensors + ctx . input_params , 88 output_grads , 89 allow_unused\n","=True , 90 ) 91 del ctx . input_tensors 92 del ctx . input_params 93\n","del output_tensors 94 return (None , None ) + input_grads 95 96 97 def\n","count_flops_attn ( model , _x , y): 98 \"\"\" 99 A counter for the ‘thop\n","‘ package...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ce49f046-b01e-4592-ab58-00001e2c541b\n","Text: 122 : param dim : the dimension of the output . 123 : param\n","max_period : controls the minimum frequency of the embeddings . 124 :\n","return : an [N x dim ] Tensor of positional embeddings .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7d9d4418-aaa1-4500-811d-0ec67817c943\n","Text: 125 \"\"\" 126 half = dim // 2 127 freqs = torch . exp ( 128 -math\n",". log ( max_period ) * torch . arange ( start =0 , end =half , dtype =\n","torch . float32 ) / half 129 ).to( device = gammas . device ) 130 args\n","= gammas [: , None ]. float () * freqs [ None ] 131 embedding = torch\n",". cat ([ torch . cos ( args ), torch . sin ( args )], dim = -1) 132 if\n","...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8de93e35-9e94-4a51-b787-a8a9c9360e97\n","Text: AI VIETNAM aivietnam.edu.vn 10 return x * torch . sigmoid (x) 11\n","12 class EmbedBlock (nn. Module ): 13 \"\"\" 14 Any module where forward\n","() takes embeddings as a second argument . 15 \"\"\" 16 17\n","@abstractmethod 18 def forward (self , x, emb ): 19 \"\"\" 20 Apply the\n","module to ‘x‘ given ‘emb ‘ embeddings . 21 \"\"\" 22 23 class\n","EmbedSequential (nn. Sequent...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e19935cf-fb7d-4753-9343-09c59233a9cb\n","Text: 40 : param channels : channels in the inputs and outputs . 41 :\n","param use_conv : a bool determining if a convolution is applied . 42\n","43 \"\"\" 44 45 def __init__ (self , channels , use_conv , out_channel =\n","None ): 46 super (). __init__ () 47 self . channels = channels 48 self\n",". out_channel = out_channel or channels 49 self . use_conv = use_conv\n","50 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 252a87bf-f529-4cb8-a246-147f03aa0360\n","Text: Conv2d ( self . channels , self . out_channel , 3, padding =1)\n","52 53 def forward (self , x): 54 assert x. shape [1] == self .\n","channels 55 x = F. interpolate (x, scale_factor =2 , mode =\" nearest\n","\") 56 if self . use_conv : 57 x = self . conv (x) 58 return x 59 60\n","class Downsample (nn. Module ): 61 \"\"\" 62 A downsampling layer with an\n","optional conv...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5cd4cadf-1e3d-4fa9-b59d-895e5145e636\n","Text: AI VIETNAM aivietnam.edu.vn 70 self . out_channel = out_channel\n","or channels 71 self . use_conv = use_conv 72 stride = 2 73 if use_conv\n",": 74 self .op = nn. Conv2d ( 75 self . channels , self . out_channel ,\n","3, stride = stride , padding =1 76 ) 77 else : 78 assert self .\n","channels == self . out_channel 79 self .op = nn. AvgPool2d (\n","kernel_size = st...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f4865dbe-aa14-4ad0-8f45-848676865d2c\n","Text: use_checkpoint = use_checkpoint 120 self . use_scale_shift_norm\n","= use_scale_shift_norm 121 122 self . in_layers = nn. Sequential ( 123\n","normalization ( channels ), 124 SiLU () , 125 nn. Conv2d ( channels ,\n","self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 66c85c58-6b27-4e85-9091-dbf22efcfe76\n","Text: out_channel , 3, padding =1) , 126 ) 127 128 self . updown = up\n","or down 129 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e890ffc-3c98-44c7-80ea-7fbf67b39c5f\n","Text: AI VIETNAM aivietnam.edu.vn 130 if up: 131 self . h_upd =\n","Upsample ( channels , False ) 132 self . x_upd = Upsample ( channels ,\n","False ) 133 elif down : 134 self . h_upd = Downsample ( channels ,\n","False ) 135 self . x_upd = Downsample ( channels , False ) 136 else :\n","137 self . h_upd = self . x_upd = nn. Identity () 138 139 self .\n","emb_layers = nn....\n","----------------------------------------------------------------------------------------------------\n","Node ID: a00178c9-3c60-4e99-9e10-69529eca7ddd\n","Text: out_channel , 144 ), 145 ) 146 self . out_layers = nn.\n","Sequential ( 147 normalization ( self . out_channel ), 148 SiLU () ,\n","149 nn. Dropout (p= dropout ), 150 zero_module ( 151 nn. Conv2d ( self\n",".\n","----------------------------------------------------------------------------------------------------\n","Node ID: d29502ec-fca5-467d-878c-d4ccc3c686b7\n","Text: out_channel , self . out_channel , 3, padding =1) 152 ), 153 )\n","154 155 if self . out_channel == channels : 156 self . skip_connection\n","= nn. Identity () 157 elif use_conv : 158 self . skip_connection = nn.\n","Conv2d ( 159 channels , self . out_channel , 3, padding =1 160 ) 161\n","else : 162 self . skip_connection = nn. Conv2d ( channels , self .\n","out_ch...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9428fd05-471d-4fbb-9580-7bbda6910a36\n","Text: out_layers [1:] 10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e232336-1771-4687-82cf-09f3de6d40c5\n","Text: AI VIETNAM aivietnam.edu.vn 189 scale , shift = torch . chunk (\n","emb_out , 2, dim =1) 190 h = out_norm (h) * (1 + scale ) + shift 191 h\n","= out_rest (h) 192 else : 193 h = h + emb_out 194 h = self .\n","out_layers (h) 195 return self . skip_connection (x) + h 196 197 class\n","AttentionBlock (nn. Module ): 198 \"\"\" 199 An attention block that\n","allows spatial...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2b1c6345-462a-4af1-9167-faf68145b14e\n","Text: 201 https :// github . com / hojonathanho / diffusion / blob /1\n","e0dceb3b3495bbe19116a5e1b3596cd0706c543 / diffusion_tf / models / unet\n",".py# L66 . 202 \"\"\" 203 204 def __init__ ( 205 self , 206 channels ,\n","207 num_heads =1 , 208 num_head_channels =-1, 209 use_checkpoint =\n","False , 210 use_new_attention_order = False , 211 ): 212 super ().\n","__init__ (...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 100bc055-f4f6-41fa-b392-12f183e648c6\n","Text: norm (x)) 240 h = self . attention ( qkv ) 241 h = self .\n","proj_out (h) 242 return (x + h). reshape (b, c, * spatial ) 243 244\n","245 class QKVAttentionLegacy (nn. Module ): 246 \"\"\" 11\n","----------------------------------------------------------------------------------------------------\n","Node ID: d63c8e16-2385-4246-b347-f308720ef940\n","Text: AI VIETNAM aivietnam.edu.vn 247 A module which performs QKV\n","attention . Matches legacy QKVAttention + input / ouput heads shaping\n","248 \"\"\" 249 250 def __init__ (self , n_heads ): 251 super (). __init__\n","() 252 self . n_heads = n_heads 253 254 def forward (self , qkv ): 255\n","\"\"\" 256 Apply QKV attention . 257 : param qkv : an [N x (H * 3 * C) x\n","T] te...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b15d4919-8679-47ac-b6fa-579e67dd38d2\n","Text: type ( weight .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 271dd067-3d9d-440c-82a1-92fca6ba8dba\n","Text: dtype ) 269 a = torch . einsum (\"bts ,bcs -> bct \", weight , v)\n","270 return a. reshape (bs , -1, length ) 271 272 @staticmethod 273 def\n","count_flops ( model , _x , y): 274 return count_flops_attn ( model ,\n","_x , y) 275 276 277 class QKVAttention (nn. Module ): 278 \"\"\" 279 A\n","module which performs QKV attention and splits in a different order .\n","280 \"...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7ddb44d3-e3eb-4cf3-bfee-13187ebafa10\n","Text: type ( weight . dtype ) 303 a = torch . einsum (\"bts ,bcs -> bct\n","\", weight , v. reshape (bs * self . n_heads , ch , length )) 304\n","return a. reshape (bs , -1, length ) 12\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3185c892-1a95-4d83-a9f7-7b0df7a36462\n","Text: AI VIETNAM aivietnam.edu.vn 305 306 @staticmethod 307 def\n","count_flops ( model , _x , y): 308 return count_flops_attn ( model ,\n","_x , y) 309 310 class UNet (nn. Module ): 311 \"\"\" 312 The full UNet\n","model with attention and embedding . 313 : param in_channel : channels\n","in the input Tensor , for image colorization : Y_channels + X_channels\n",". 314 : pa...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4a03fee8-3e57-4544-8210-f1e5e47d4ed6\n","Text: 319 For example , if this contains 4, then at 4x downsampling ,\n","attention 320 will be used . 321 : param dropout : the dropout\n","probability . 322 : param channel_mults : channel multiplier for each\n","level of the UNet . 323 : param conv_resample : if True , use learned\n","convolutions for upsampling and 324 downsampling . 325 : param\n","use_checkpoint : ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4b54b18b-9c20-4bf5-8f79-53a57b0c7291\n","Text: AI VIETNAM aivietnam.edu.vn 363 self . image_size = image_size\n","364 self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: c6ccab0b-6eab-492b-9c85-c0d354f89b77\n","Text: in_channel = in_channel 365 self . inner_channel = inner_channel\n","366 self . out_channel = out_channel 367 self . res_blocks =\n","res_blocks 368 self . attn_res = attn_res 369 self . dropout = dropout\n","370 self . channel_mults = channel_mults 371 self . conv_resample =\n","conv_resample 372 self . use_checkpoint = use_checkpoint 373 self .\n","dtype = torch ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 83322017-defc-4030-9209-6453e4a624e7\n","Text: num_heads = num_heads 375 self . num_head_channels =\n","num_head_channels 376 self . num_heads_upsample = num_heads_upsample\n","377 378 cond_embed_dim = inner_channel * 4 379 self . cond_embed = nn.\n","Sequential ( 380 nn. Linear ( inner_channel , cond_embed_dim ), 381\n","SiLU () , 382 nn. Linear ( cond_embed_dim , cond_embed_dim ), 383 )\n","384 385 ch = input...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c65bb0b9-2123-4e62-8f90-2acb5ca9efb3\n","Text: AI VIETNAM aivietnam.edu.vn 423 ch , 424 cond_embed_dim , 425\n","dropout , 426 out_channel = out_ch , 427 use_checkpoint =\n","use_checkpoint , 428 use_scale_shift_norm = use_scale_shift_norm , 429\n","down =True , 430 ) 431 if resblock_updown 432 else Downsample ( 433 ch\n",", conv_resample , out_channel = out_ch 434 ) 435 ) 436 ) 437 ch =\n","out_ch 438 input_bl...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ac4773f2-0be7-4883-a0d6-37b5292e28d5\n","Text: output_blocks = nn. ModuleList ([]) 468 for level , mult in list\n","( enumerate ( channel_mults )) [:: -1]: 469 for i in range (\n","res_blocks + 1): 470 ich = input_block_chans . pop () 471 layers = [\n","472 ResBlock ( 473 ch + ich , 474 cond_embed_dim , 475 dropout , 476\n","out_channel = int ( inner_channel * mult ), 477 use_checkpoint =\n","use_checkpoint , 4...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e4fdc024-c4f2-45d8-99c4-a343ab842d4a\n","Text: AI VIETNAM aivietnam.edu.vn 483 layers . append ( 484\n","AttentionBlock ( 485 ch , 486 use_checkpoint = use_checkpoint , 487\n","num_heads = num_heads_upsample , 488 num_head_channels =\n","num_head_channels , 489 use_new_attention_order =\n","use_new_attention_order , 490 ) 491 ) 492 if level and i == res_blocks\n",": 493 out_ch = ch 494 layers . append ( 495 Res...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4da9e230-7cee-42cd-bcb7-0dce9f07244a\n","Text: Sequential ( 512 normalization (ch), 513 SiLU () , 514\n","zero_module (nn. Conv2d ( input_ch , out_channel , 3, padding =1) ),\n","515 ) 516 517 def forward (self , x, gammas ): 518 \"\"\" 519 Apply the\n","model to an input batch . 520 : param x: an [N x 2 x ...] Tensor of\n","inputs (B&W) 521 : param gammas : a 1-D batch of gammas . 522 : return\n",": an [N x C x ....\n","----------------------------------------------------------------------------------------------------\n","Node ID: f4a39c91-4f55-48c7-bdc2-3d8c2cbf6c59\n","Text: pop ()], dim =1) 535 h = module (h, emb ) 536 h = h. type (x.\n","dtype ) 537 return self . out (h) Color Diffusion Model 1 def\n","make_beta_schedule ( schedule , n_timestep , linear_start =1e -5 ,\n","linear_end =1e -2) : 2 if schedule == ’ linear ’: 3 betas = np.\n","linspace ( 16\n","----------------------------------------------------------------------------------------------------\n","Node ID: 29fb4986-8518-4e77-a6a8-4b77b8c6ac9a\n","Text: AI VIETNAM aivietnam.edu.vn 4 linear_start , linear_end ,\n","n_timestep , dtype =np. float64 5 ) 6 else : 7 raise\n","NotImplementedError ( schedule ) 8 return betas 9 10 def\n","get_index_from_list (vals , t, x_shape =(1 ,1 ,1 ,1) ): 11 \"\"\" 12\n","Returns a specific index t of a passed list of values vals 13 while\n","considering the batch dimension . 14 \"\"\" 15 b...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c62c5344-b268-43d8-943e-15a9f31abce4\n","Text: gather (-1, t) 17 return out . reshape ( batch_size , *((1 ,) *\n","( len ( x_shape ) - 1))).to( device ) 1 from tqdm import tqdm 2 from\n","functools import partial 3 4 class ColorDiffusion (nn. Module ): 5 def\n","__init__ (self , unet_config , beta_schedule , ** kwargs ): 6 super (\n","ColorDiffusion , self ). __init__ (** kwargs ) 7 self . denoise_fn =\n","UNet...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0a8632ca-47de-40f6-818a-96751400ac16\n","Text: sqrt ( alphas ) / (1. - gammas ))) 31 32 def set_loss (self ,\n","loss_fn ): 33 self . loss_fn = loss_fn 34 35 def\n","predict_start_from_noise (self , y_t , t, noise ): 36 return ( 37\n","get_index_from_list ( self . sqrt_recip_gammas , t, y_t . shape ) *\n","y_t - 38 get_index_from_list ( self . sqrt_recipm1_gammas , t, y_t .\n","shape ) * noise 39 ) 40 41 def q_...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b67ca312-2d0a-43da-bba2-615f88b8c04c\n","Text: AI VIETNAM aivietnam.edu.vn 42 \"\"\" 43 Compute the mean and\n","variance of the diffusion posterior : 44 45 q(x_{t -1} | x_t , x_0 )\n","46 47 \"\"\" 48 posterior_mean = ( 49 get_index_from_list ( self .\n","posterior_mean_coef1 , t, y_t .\n","----------------------------------------------------------------------------------------------------\n","Node ID: e0117617-7895-4248-8a4b-6f9454231e35\n","Text: shape ) * y_0_hat + 50 get_index_from_list ( self .\n","posterior_mean_coef2 , t, y_t . shape ) * y_t 51 ) 52\n","posterior_log_variance_clipped = get_index_from_list ( 53 self .\n","posterior_log_variance_clipped , t, y_t . shape 54 ) 55 return\n","posterior_mean , posterior_log_variance_clipped 56 57 def\n","p_mean_variance (self , y_t , t, clip_denoised : bool ,...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 595fe5b9-1acc-4647-ac23-7bf7ee7ba508\n","Text: device ) 59 y_0_hat = self . predict_start_from_noise ( 60 y_t ,\n","t=t, noise = self . denoise_fn ( torch . cat ([ y_cond , y_t ], dim\n","=1) , noise_level )) 61 62 if clip_denoised : 63 y_0_hat . clamp_ (\n","-1. , 1.) 64 65 model_mean , posterior_log_variance = self .\n","q_posterior ( 66 y_0_hat = y_0_hat , y_t =y_t , t=t) 67 return\n","model_mean , posterior...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4eeb4515-3db5-4d8a-b521-eec88b932f97\n","Text: AI VIETNAM aivietnam.edu.vn 99 # sampling from p( gammas ) 100\n","b, *_ = y_0 . shape 101 t = torch .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 63b73aff-42da-47c2-9e8c-91f62d8e8dd6\n","Text: randint (1 , self . num_timesteps , (b ,) , device = y_0 .\n","device ). long () 102 gamma_t1 = get_index_from_list ( self . gammas ,\n","t -1 , x_shape =(1 , 1)) 103 sqrt_gamma_t2 = get_index_from_list (\n","self . gammas , t, x_shape =(1 , 1)) 104 sample_gammas = (\n","sqrt_gamma_t2 - gamma_t1 ) * torch . rand ((b, 1) , device = y_0 .\n","device ) + gamma_t1 105 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0ba77778-2b42-4ddd-8210-ebab7d38ecfe\n","Text: 1 import torch .nn. functional as F 2 3 def mse_loss ( output ,\n","target ): 4 return F. mse_loss ( output , target ) 5 6 7 def mae (\n","input , target ): 8 with torch . no_grad (): 9 loss = nn. L1Loss () 10\n","output = loss ( input , target ) 11 return output 19\n","----------------------------------------------------------------------------------------------------\n","Node ID: 542c6f8d-e998-4830-8588-86872590d3dd\n","Text: AI VIETNAM aivietnam.edu.vn 4. Trainer Ở giai đoạn huấn luyện,\n","chúng ta sẽ xây dựng class Trainer dành cho việc huấn luyện mô hình\n","Dif- fusion. Ngoài ra, chúng ta cũng sẽ sử dụng một công cụ theo dõi\n","được sử dụng phổ biến trong việc huấn luyện các mô hình học máy, được\n","gọi làwandb. Theo đó, các kết quả bao gồm cả thông số mất mát của mô\n","hình và ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 65df3086-a635-4130-bd9b-6e3af3f50c93\n","Text: Hình 5: Ảnh minh họa cho việc sử dụngwandb. Khởi tạo class\n","Trainer 1 import time 2 3 class Trainer (): 4 def __init__ (self ,\n","model , optimizers , train_loader , 5 val_loader , epochs , sample_num\n",", 6 device , save_model , use_wandb = False ): 7 8 self . model =\n","model .to( device ) 9 self . optimizer = torch . optim . Adam ( list (\n","filter ( 10 l...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ce54189a-2ce6-467b-b42c-c708e25cfbc9\n","Text: parameters () 11 )), ** optimizers ) 12 self . model . set_loss\n","( mse_loss ) 13 self . model . set_new_noise_schedule ( device ) 14\n","self . sample_num = sample_num 15 self . train_loader = train_loader\n","16 self . val_loader = val_loader 17 self . device = device 18 self .\n","epochs = epochs 19 self . save_model = save_model 20 self . use_wandb\n","= use_...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c68f31bb-b1e8-4444-966b-bde41c6e5bed\n","Text: AI VIETNAM aivietnam.edu.vn 25 for original_gray , gray , color\n","in tqdm ( self . train_loader ): 26 cond_gray = gray .to( self .\n","device ) 27 gt_color = color .to( self . device ) 28 29 self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 211f5867-20b7-4803-8ce5-8939d095fbb3\n","Text: optimizer . zero_grad () 30 31 loss = self . model ( gt_color ,\n","cond_gray ) 32 loss . backward () 33 losses . append ( loss . item ())\n","34 self .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 95cf04ed-287a-46ce-95e2-82d6258a9654\n","Text: optimizer . step () 35 return sum ( losses )/ len ( losses ) 36\n","37 def val_step (self , epoch ): 38 self . model . eval () 39 losses ,\n","metrics = [] , [] 40 pred_images = [] 41 gt_images = [] 42 43 with\n","torch . no_grad (): 44 for i, ( original_gray , gray , color ) in tqdm\n","( enumerate ( self . val_loader )): 45 cond_gray = gray .to( self .\n","device...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d8210f50-b264-457f-905b-be12565fcd60\n","Text: numpy () 71 img_ab = img_ab . permute (1 , 2, 0). numpy () 72\n","img_ab = cv2 . resize ( img_ab , ( img_l . shape [1] , img_l . shape\n","[0]) , interpolation = cv2 . INTER_LINEAR ) 73 arr_lab = np.\n","concatenate ([ img_l , img_ab ], axis =2) 74 arr_lab = ( arr_lab +\n","1.0) * 255 / 2 75 arr_lab = np. clip ( arr_lab , 0, 255) . astype (np.\n","uint8 ) 76 arr_bg...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2e8e7a82-0ada-4c0c-9327-3c595d11b923\n","Text: AI VIETNAM aivietnam.edu.vn 78 79 80 def train ( self ): 81\n","best_mae = 100000 82 for epoch in range ( self . epochs ): 83\n","epoch_start_time = time . time () 84 train_loss = self . train_step ()\n","85 val_loss , val_mae , pred_images , gt_images = self . val_step (\n","epoch ) 86 87 # Log the results to WanDB 88 if self . use_wandb : 89\n","wandb . log ({ 90...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a7ee3334-5a29-4863-a897-e31dfd4d5280\n","Text: model .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4cf95dfd-e7c6-4557-b7c6-000bce05f229\n","Text: state_dict () , self . save_model ) 99 # Print loss , acc end\n","epoch 100 print (\"-\" * 59) 101 print ( 102 \"| End of epoch {:3 d} |\n","Time : {:5.2 f}s | Train Loss {:8.3 f} \" 103 \"| Valid Loss {:8.3 f} |\n","Valid MAE {:8.3 f} \". format ( 104 epoch +1 , time . time () -\n","epoch_start_time , 105 train_loss , val_loss , val_mae 106 ) 107 ) 108\n","print (\"-\" * ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 59a1af55-602e-4052-99b8-ea6b9381d63d\n","Text: AI VIETNAM aivietnam.edu.vn 1 if use_wandb : 2 wandb . init ( 3\n","# set the wandb project where this run will be logged 4 project =\"my\n","-diff - color \", 5 6 # track hyperparameters and run metadata 7 config\n","={ 8 \" learning_rate \": optimizers [\"lr\"], 9 \" weight_decay \":\n","optimizers [\" weight_decay \"], 10 \" architecture \": \" UNet \", 11 \"\n","dataset \": \" ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: aebf03ff-d069-4441-8424-183223d26cd5\n","Text: 1 trainer . train () Sau khi hoàn thành huấn luyện, hãy kết\n","thúcwandb session. 1 if use_wandb : 2 wandb . finish () Theo dõi quá\n","trình huấn luyện trênwandb. Hình 6: Theo dõi kết quả loss và metrics\n","theo từng epoch trênwandb. 23\n","----------------------------------------------------------------------------------------------------\n","Node ID: 724da76f-3522-419e-81de-a57819839ce8\n","Text: AI VIETNAM aivietnam.edu.vn Hình 7: Theo dõi quá trình tạo ảnh\n","của mô hình theo từng epoch trênwandb. 5. Inference Bạn có thể sử dụng\n","checkpoint sẵn có để tiến hành quá trình suy luận thử nghiệm. 1 #\n","Download the checkpoint 2 !\n","----------------------------------------------------------------------------------------------------\n","Node ID: 54cd8173-5cc7-4173-936b-c09f32092208\n","Text: gdown 1 -0 IcaofrE8cNbvUn1Ydo2WyShkxohv9r 1 # Load the model 2\n","colordiff_model = ColorDiffusion ( unet_config , beta_schedule ) 3\n","colordiff_model . set_new_noise_schedule ( device ) 4 load_state =\n","torch . load (’./ best_model .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0c6c3f2a-377c-4226-b6ce-21200fa3f6ed\n","Text: pth ’) 5 colordiff_model . load_state_dict ( load_state , strict\n","= True ) 6 colordiff_model . eval ().to( device ) 7 8 9 # Load\n","original image 10 showed_img_idx = 55 11 img_path = img_paths [\n","showed_img_idx ] 12 img_bgr = cv2 . imread ( img_path ) 13 14 img_lab\n","= cv2 . cvtColor (img , cv2 . COLOR_BGR2LAB ) 15 img_l = img_lab [: ,:\n",",:1] 16 plt . ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c745de7b-ff45-4912-9e2b-e443e9ca2a6e\n","Text: AI VIETNAM aivietnam.edu.vn 32 33 def inference ( model ,\n","test_sample ): 34 with torch . no_grad (): 35 output , visuals = model\n",". restoration ( 36 test_sample [1]. unsqueeze (0) .to( device ) 37 )\n","38 return output , visuals 39 40 output , visuals = inference (\n","colordiff_model , test_sample ) 41 42 43 # Show the results 44 def\n","show_tensor_image ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c3b89933-af69-4f82-9c9b-4328828c24b4\n","Text: show () Kết quả thực nghiệm mô hình sau khi huấn luyện Hình 8:\n","Kết quả thực nghiệm mô hình sau khi huấn luyện.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8142e1b1-29a7-40d1-9355-bcba670e89ff\n","Text: 25\n","----------------------------------------------------------------------------------------------------\n","Node ID: 48d24ef8-3c29-4777-b68b-3493be8d02eb\n","Text: AI VIETNAM aivietnam.edu.vn Phần III: Câu hỏi trắc nghiệm 1.\n","Trong Diffusion Model, Loss Function là: (a) ELBO (Evidence Lower\n","Bound) (b) VLB (Variational Lower Bound) (c) Simplified MSE (d) Tất cả\n","đáp án đều đúng. 2. Gray channel đóng vai trò gì trong quá trình huấn\n","luyện mô hình Diffusion-based Image Coloriza- tion? (a) Nó được sử\n","dụng làm điề...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c567d1e5-808c-4088-9254-419426aaa8b0\n","Text: (b) Nó bị bỏ qua trong quá trình tạo màu để chỉ tập trung vào\n","sắc độ. (c) Nó được tăng cường thông tin qua mô hình khuếch tán để cải\n","thiện độ trung thực của màu sắc ở đầu ra cuối cùng. 3. Đâu là nhược\n","điểm chính của DDPM trong việc sinh ảnh dữ liệu có chất lượng, độ phân\n","giải cao? (chọn phương án đúng nhất) (a) Tốn nhiều thời gian, tài\n","nguyên tí...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7e301e63-24b9-40df-ac6a-8cc81ad21203\n","Text: 4. Phát biểu nào sau đây đúng về Diffusion-based Image\n","Colorization: (a) Cần xác định tấm ảnh đầy màu sắc làm groundtruth.\n","(b) Ràng buộc trong việc sử dụng các kênh màu phổ biến RGB hoặc HSV.\n","(c) Quá trình Sampling là sự kết hợp giữa việc sử dụng color channel\n","được mô hình dự đoán (sau khi denoise) và việc sử dụng kênh màu gray-\n","scale để tạo nên ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4fe722ea-04fe-4557-90cb-50c9a8cdb506\n","Text: ANACONDA VSCODE INSTALLATION AND USAGE GUIDE Dinh-Tiem Nguyen và\n","Quang-Vinh Dinh 1 Mở đầu Anaconda là một nền tảng mã nguồn mở được\n","thiết kế đặc biệt cho khoa học dữ liệu và học máy, nó giúp người dùng\n","dễ dàng quản lí các thư viện và môi trường phát triển dự án. Ngoài ra\n","Anaconda còn cung cấp nhiều công cụ và thư viện phổ biến như Python,\n","R, Vsc...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1fd30fe3-9488-4cb6-97c3-6b121495df8f\n","Text: Để cài đặt Anaconda, bạn tiến hành tải về tại đây. Trong hướng\n","dẫn này, chúng ta sẽ cài đặt trên hệ điều hành Window. Hình 1:\n","Annconda hỗ trợ Window, Mac, Linux Sau khi tải về file cài đặt, chúng\n","ta tiến hành nhấn chuột hai lần vào file cài đặt để mở file và tiến\n","hành cài đặt. Việc cài đặt này khá dễ dàng, bạn thực hiện theo các\n","bước sau, lưu ý ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b7879287-502b-4ce6-b86b-8c1c5710ec0c\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Tiếp theo chọn \"Next->\n","\"I Agree-> \"Just me->\"Next->\"Next\". Khi cửa sổ Advanced xuất hiện,\n","tích vào lựa chọn \"add Anaconda to your PATH environment variable\".\n","Cuối cùng bạn chọn \"Install-> \"Next-> \"Finish\"để hoàn tất quá trình\n","cài đặt. Để kiểm tra đã cài đặt thành công hay chưa, bạn mở Command\n","Prompt hoặc ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 40897455-84d3-4340-84f7-3d821d8b9f42\n","Text: Nếu bạn chưa biết môi trường trong các dự án python thì nó là\n","không gian làm việc độc lập chứa các phiên bản 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4f83ffec-d27d-400b-a4c1-447e5c2329c5\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn riêng biệt của Python\n","và các gói liên quan. Mỗi môi trường có thể có các phiên bản Python,\n","cài đặt và quản lý gói riêng biệt, giúp cô lập và quản lý dễ dàng cho\n","từng dự án. 2.2.1 Hướng dẫn tạo một môi trường mới Để tạo một môi\n","trường mới, chúng ta sử dụng cú pháp: 1 conda create -- name\n","my_python_env pyth...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 49afffd9-3425-4c24-908c-4c35cc54935b\n","Text: • Chúng ta có thể quản lí các gói và môi trường tại Environments\n","• Chúng ta có thể dễ dàng cài đặt các công cụ như Vscode, Jupyter\n","Notebook, Spider... bằng cách nhấn vào nút install. Để mở các công cụ\n","này ta nhấn vào Launch, hoặc cũng có thể mở từ thanh tìm kiếm window.\n","3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 788f1885-64ff-4f7c-99c4-82f326084282\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 3 Cài đặt và sử dụng\n","Vscode Visual Studio Code (VSCode) là một trình soạn thảo mã nguồn mở\n","được phát triển bởi Microsoft. Nó cung cấp một loạt các tính năng hữu\n","ích cho lập trình Python và khoa học dữ liệu và nhiều ngôn ngữ lập\n","trình khác.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4e7732ec-b4c3-4e1c-baca-aa623aa8534d\n","Text: 3.1 Cài đặt Vscode Để cài đặt Vscode, chúng ta có thể cài đặt\n","theo hai cách, cách đầu tiên là chúng ta truy cập vào đường dẫn\n","https://code.visualstudio.com/download và tiến hành lựa chọn tải xuống\n","theo hệ điều hành trên máy, sau đó tiến hành cài đặt. Hình 2: Hai cách\n","cài đặt Vscode 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: 505f2f63-67bd-47ec-8166-70516bcd44a2\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Quá trình cài đặt khá\n","đơn giản, chỉ cần mở file cài đặt và tiến hành nhấn next theo mặc\n","định. Cách thứ hai thì đơn giản hơn, trong giao diện Anaconda\n","Navigator chúng ta tìm đến Vscode và nhấn install.\n","----------------------------------------------------------------------------------------------------\n","Node ID: bb6048c4-d632-4c77-880d-e89cfaa1175c\n","Text: 3.2 Cài đặt các tiện ích quan trọng trong Vscode Tiện ích là các\n","phần mở rộng được cung cấp bởi cộng đồng hoặc các nhà phát triển để mở\n","rộng tính năng của VSCode. Chúng ta sẽ cài đặt hai tiện ích là Python\n","và Jupyter trong đó: • Tiện ích Python hỗ trợ cho lập trình Python\n","trong VSCode, bao gồm hỗ trợ cú pháp, tự động hoàn thành, gỡ lỗi, và\n","nhiều...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5e0e7b62-8828-4fda-a96d-76e9578921dc\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Hình 4: Thực thi chương\n","trình python 3.4 Lựa chọn môi trường dự án Khi chạy chương trình trên,\n","chương trình được chạy trên môi trường mặc định của máy tính. Để chọn\n","môi trường cụ thể, hãy chọn \"View\"> \"Command Palette\"từ thanh menu\n","hoặc nhấn tổ hợp phím Ctrl + Shift + P sau đó nhập \"Select\n","Interpreter\"và ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b7ca84c3-ab60-4259-913c-f184d2799017\n","Text: Trong một số trường hợp nếu bạn đã làm theo các bước trên mà\n","không thấy môi trường được kích 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: fd713414-2715-4439-87a2-e78a44184302\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn hoạt có thể do Terminal\n","của bạn đang sử dụng Powershell, bạn hãy thay đổi sang Command Prompt\n","bằng cách nhấn vào biểu tượng dấu cộng trong cửa sổ terminal và chọn\n","Command Prompt. 3.5 Tạo notebook Jupyter Vscode hỗ trợ chúng ta sử\n","dụng file jupyter notebook mà không cần phải mở phần mềm này.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9d1a0355-b292-4e86-961d-7c505fad02e9\n","Text: Điều này rất hữu ích khi mà dự án của chúng ta vừa sử dụng file\n",".py vừa sử dụng file notebook .ipynb. Chúng ta sẽ tạo file với đuôi\n",".ipynb trong thư mục dự án tương tự như cách tạo file my_script.py ở\n","phần trên. Sau đó ta tiến hành viết code và thực thi code bằng cách\n","nhấn vào biểu tượng run ở đầu ô đó. Hình 6: Tạo file jupyter notebook\n","Với lần ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 31e467dc-ba43-4efa-8b2f-85d35e0c4451\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 Basic Python -\n","Data Analysis with Visualization Hoàng-Nguyên Vũ 1. Mô tả: Làm quen\n","với thư viện PygWalker • Thư viện PygWalkerlà một thư viện Python mã\n","nguồn mở giúp bạn dễ dàng chuyển đổi dữ liệu thành các ứng dụng phân\n","tích trực quan.\n","----------------------------------------------------------------------------------------------------\n","Node ID: aeddae2f-3247-4433-9730-a967175f08aa\n","Text: Thư viện này cung cấp một bộ công cụ mạnh mẽ để khám phá, tóm\n","tắt và trực quan hóa dữ liệu của bạn, giúp bạn hiểu rõ hơn về dữ liệu\n","và đưa ra quyết định sáng suốt hơn. • Điểm nổi bật của PyGWalker: +\n","Tạo bảng điều khiển tương tác:PyGWalker cho phép bạn tạo các bảng điều\n","khiển trực quan và dễ sử dụng để khám phá dữ liệu của bạn. Bạn có thể\n","dễ dàn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0b53ec66-2874-4c7c-9674-25bb2add4c34\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 2. Cách cài đặt\n","và sử dụng một số tính năng: Để cài đặt thư viện PygWalker, chúng ta\n","có thể cài trên Google Colab, hoặc ở máy cá nhân thông qua Jupyter\n","Notebook.\n","----------------------------------------------------------------------------------------------------\n","Node ID: c11c832f-bb8b-4d57-b4fd-2aac5657a8bf\n","Text: Cách cài đặt như sau: 1. Cài đặt thư viện PygWalker: + Để cài\n","đặt thư PygWalker, chúng ta sử dụng câu lệnh sau ở Google Colab: 1\n","!pip install pygwalker + Để cài thư viện trên máy cá nhân và chạy với\n","Jupyter Notebook, chúng ta sẽ chạy thông qua Terminal đối với hệ điều\n","hành MacOS và CMD đối với hệ điều hành Windows thông qua lệnh sau: 1\n","pip insta...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a818d82a-7f10-4a28-9d42-a391e819b715\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 Hình 3: Giao\n","diện PygWalker 2. Sử dụng một số tính năng trực quan hóa: + Tạo biểu\n","đồ cột:Tạo biểu đồ cột cho tập data mẫu trên để thể hiện dân số\n","(Population) theo quốc gia (Country).\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2f9680bc-0770-44ce-b6ea-9c89ff85acab\n","Text: (*) Ta sẽ thực hiện kéo 2 cột: Country vào X-Axis và Population\n","vào Y-Axis. Ứng với 2 thông số của biểu đồ cột mà chúng ta cần thực\n","hiện trực quan hóa biểu đồ: Trục Ox (Trục ngang) thể hiện cho Quốc gia\n","(Country) và Trục Oy (Trục dọc) thể hiện cho Dân số (Population)\n","www.facebook.com/aivietnam.edu.vn 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4aeffdfc-136e-457d-b141-3a9457ca0ca6\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 Hình 4: Biểu đồ\n","cột thể hiện dân số theo quốc gia + Lấy Top 20 Quốc Gia có giảm dần\n","theo dân số, và tô màu theo độ lớn của dân số: (*) Ta sẽ thực hiện kéo\n","2 cột: Chúng ta cũng thực hiện tương tự bài trên nhưng chúng sẽ thực\n","hiện sắp xếp Population giảm dần và Limit 20 dòng. Đồng thời gắn Color\n","là c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 244230f4-be2e-4a95-9782-25e113187ced\n","Text: Hình 5: Biểu đồ cột thể hiện dân số theo quốc gia + Vẽ biểu đồ\n","hộp thể hiện phân phối dữ liệu: www.facebook.com/aivietnam.edu.vn 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3f9697ae-eb47-4fd2-bc81-e962fad0efb8\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 Hình 6: Biểu đồ\n","hộp thể hiện dân số theo quốc gia + Vẽ bản đồ hộp thể hiện phân phối\n","dữ liệu: Hình 7: Bản đồ thể hiện dân số theo quốc gia 3. Bài tập: Hãy\n","đọc dữ liệu ở file: advertising.csv và khởi chạy thư viện PygWalker\n","sau đó thực hiện trực quan các biểu đồ sau đây: • Câu 1:Vẽ biểu đồ\n","phân phối...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f1fe2b33-4fcf-42bf-84fe-331c9d0c16a2\n","Text: www.facebook.com/aivietnam.edu.vn 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: df0477fd-b2f1-4c72-9643-7dd6ac5de572\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 Kết Quả: Hình 8:\n","Biểu đồ phân phối dữ liệu cho 3 loại: TV, Radio, Paper • Câu 2: Vẽ\n","biểu đồ cột thể hiện doanh số bán (Sales)⩾ 10 của cả 3 loại TV, Radio\n","và Newspaper. Kết quả: Hình 9: Biểu đồ doanh số bán hàng trên 10 sản\n","phẩm cho 3 loại: TV, Radio, Paper • Câu 3:Hãy vẽ bản đồ biểu diễn phân\n","bố dâ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9e56ea37-213e-459c-abd7-b611ec580436\n","Text: Daily AI Exercise (AIO) Ngày 1 tháng 4 năm 2024 Kết quả: Hình\n","10: Bản đồ thể hiện phân bố dân số theo thành phố của các nước thuộc\n","các nước: Việt Nam, Hàn Quốc, Nhật Bản, Singapore và Thái Lan - Hết -\n","www.facebook.com/aivietnam.edu.vn 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1f7855e0-8c96-45f4-9b48-2170c7c6018e\n","Text: XỬ LÝ TỆP PDF ĐƠN GIẢN VỚI PYPDF Dinh-Tiem Nguyen và Quang-Vinh\n","Dinh 1 Mở đầu Làm sao để trích xuất nội dung văn bản và hình ảnh trong\n","file pdf? Làm thế nào để ghép nhiều file pdf thành một file duy nhất?\n","Trong bài viết này, chúng ta sẽ trả lời những câu hỏi đó bằng cách sử\n","dụng thư viên pypdf-một thư viện xử lý file pdf hiệu quả. Yêu cầu: •\n","Máy...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e8a3c97f-37b0-4907-8638-6f10fc0e1603\n","Text: Ví dụ chương trình dưới đây, chúng ta khai báo sử dụng thư viện\n","pypdf và sử dụng PdfReader để đọc file yolov9.pdf sau đó lưu kết quả\n","vào biến reader. Tiếp theo ta sử dụng reader.pages, phương thức này\n","trả về một danh sách các trang trong tập tin PDF đã được đọc bằng\n","PdfReader. Mỗi phần tử trong danh sách này đại diện cho một trang\n","trong tập tin ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3195204e-9d55-472c-b82f-b57470eefb8d\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 1 from pypdf import\n","PdfReader 2 3 # Đọc file PDF 4 reader = PdfReader(\"yolov9.pdf\") 5 6 #\n","Lấy số trang 7 num_pages = len(reader.pages) 8 9 # Lấy nội dung trang\n","đầu tiên 10 page_1 = reader.pages[0] 11 page_1_txt =\n","page_1.extract_text() 12 13 # Lấy nội dung toàn bộ các trang 14\n","pages_txt = \"\" 15 for i in ra...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a6af7dd1-fe57-4b7f-a9bd-02a31396b18c\n","Text: Ta cùng xem ví dụ sau: 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2dd830b8-a82c-4993-aec4-68e8f445b3b6\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 1 # extract images 2\n","from pypdf import PdfReader 3 4 reader = PdfReader(\"yolov9.pdf\") 5\n","count = 0 6 for page in reader.pages: 7 for image_file_object in\n","page.images: 8 with open(str(count) + image_file_object.name, \"wb\") as\n","fp: 9 fp.write(image_file_object.data) 10 count += 1 Trong ví dụ trên,\n","chúng ta đọ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f248fa9d-5fe5-4c61-9e17-fc3b4b3d0437\n","Text: 1 # Merge PDFs 2 from pypdf import PdfWriter 3 4 merger =\n","PdfWriter() 5 for pdf in [\"yolov6.pdf\", \"yolov7.pdf\", \"yolov9.pdf\"]: 6\n","merger.append(pdf) 7 8 merger.write(\"merged-yolov-679.pdf\") 9\n","merger.close() 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: 846b9046-2575-49ef-8c9a-8f5a0fbe9bcd\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Trong ví dụ trên, ta\n","thực hiện nối ba bài báo yolov6, yolov7, yolov9 lại với nhau, đầu tiên\n","chúng ta sẽ import Pdfwriter từ thư viện pypdf, tiếp theo chúng ta tạo\n","đối tượng merger từ PdfWriter(), đối tượng này sẽ được sử dụng để\n","merge các tập tin PDF. Tiếp theo chúng ta duyệt qua từng file pdf và\n","thêm chú...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3ac59181-4290-461e-b312-05cad3c6cbb0\n","Text: Việc này không chỉ giúp tiết kiệm không gian lưu trữ mà còn làm\n","tăng tốc độ tải xuống và chia sẻ tập tin PDF. 1 from pypdf import\n","PdfWriter 2 3 writer = PdfWriter(clone_from=\"yolov9.pdf\") 4 5 for page\n","in writer.pages: 6 page.compress_content_streams(level=8) # This is\n","CPU intensive! 7 8 with open(\"out.pdf\", \"wb\") as f: 9 writer.write(f)\n","Trong ví...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 70074362-357a-43dd-914f-15a50a67bfc2\n","Text: AI VIET NAM – AI COURSE 2023 Ứng dụng cơ sở dữ liệu vector\n","Milvus cho hệ thống hỏi-đáp mở Dinh-Thang Duong và Quang-Vinh Dinh\n","Ngày 20 tháng 2 năm 2024 Milvus là một trong những hệ cơ sở dữ liệu\n","vector (vector database) mã nguồn mở, chuyên dùng cho việc lưu trữ các\n","vector embedding và tìm kiếm tương đồng (similarity search) giữa các\n","vector với nh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b85edd0d-b0f1-4399-9728-25b2d09521b6\n","Text: Các bạn có thể theo dõi và đọc thêm về thư viện này tại trang\n","chủ hoặc trang github của thư viện. Hình 1: Biểu tượng của Milvus\n","Trong bài viết này, chúng ta sẽ tìm hiểu cách cài đặt nhanh Milvus\n","trên máy tính cá nhân và ứng dụng Milvus trong việc tìm kiếm các tài\n","liệu (context) có liên quan nhằm hỗ trợ hệ thống hỏi-đáp mở (Open\n","Domain Question A...\n","----------------------------------------------------------------------------------------------------\n","Node ID: da05d93b-0cd3-482d-a3e6-0e016ca805be\n","Text: AI VIETNAM aivietnam.edu.vn Hình 2: Yêu cầu cấu hình về phần\n","cứng cho Milvus Tiếp theo, chúng ta đến với phần cài đặt Milvus. Như\n","đã đề cập ở trên, chúng ta sẽ cài đặt thông qua Docker.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0d747ca5-66c9-425e-ad4c-0a00d79c9793\n","Text: Vì vậy, chúng ta sẽ tiến hành cài đặt Docker và Docker compose\n","tại bước này. Các bạn hãy lên trang chủ của Docker và thực hiện theo\n","hướng dẫn cài đặt theo đúng hệ điều hành của máy mình tại đây: Hình 3:\n","Các lựa chọn cài đặt Docker cho từng hệ điều hành riêng biệt 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: b60e6f2b-0bcc-466f-8542-1aeb6b2c85e8\n","Text: AI VIETNAM aivietnam.edu.vn Khi quá trình cài đặt Docker hoàn\n","tất, chúng ta sẽ kiểm tra Docker đã sẵn sàng để sử dụng hay chưa. Đối\n","với MacOS/Windows, ta mở ứng dụng Docker Desktop để khởi động Docker:\n","Hình 4: Giao diện Docker Desktop trên hệ điều hành MacOS Sau đó, các\n","bạn mở Terminal/CMD lên và chạy lần lượt các dòng lệnh sau: 1 $ docker\n","-v 2 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8830fc95-2da8-4151-9606-9afbde17c300\n","Text: githubusercontent . com / milvus -io/ milvus / master / scripts\n","/ standalone_embed .sh Khi chạy xong lệnh này, tại vị trí chạy lệnh,\n","các bạn sẽ thấy filestandalone_embed.sh. Các bạn cần lưu ý vị trí tải\n","file này, vì chúng ta cần phải ở đúng vị trí tải file hoặc thay đổi\n","đường dẫn hợp lý thì mới chạy lệnh gọi file này được. (b) Chạy file\n","script: 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: eea2e4e0-0448-4163-973d-695a3295c5c9\n","Text: AI VIETNAM aivietnam.edu.vn 1 $ bash standalone_embed .sh start\n","Lưu ý rằng, lệnh này sẽ mất một khoảng thời gian để hoàn tất tùy vào\n","tốc độ mạng. (c) Kiểm tra cài đặt: Khi đã tải và triển khai xong, các\n","bạn có thể kiểm tra bằng lệnh sau: 1 $ docker ps Hình 6: Kết quả kiểm\n","tra cài đặt Milvus trên Terminal Như vậy, chúng ta đã hoàn tất cài đặt\n","và ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 29373816-3458-4eba-a462-f9bbc704e03d\n","Text: Kiểm tra hoạt động của Milvus:Chúng ta sẽ thử tương tác với\n","Milvus trong Python thông qua thư viện pymilvus. Milvus có sử dụng một\n","số từ khóa mới, song các bạn có thể nắm cơ bản rằng chúng ta sẽ có\n","cácCollection, một dạng bảng dữ liệu của Milvus. Như vậy, để lưu trữ\n","một vector database trong Milvus, chúng ta sẽ cần tạo một Collection,\n","từ đó kết ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8fb66b3b-f49b-41e8-b133-6c5d2a7893d4\n","Text: AI VIETNAM aivietnam.edu.vn Hình 7: Một vài thành phần trong\n","Milvus. Nguồn: link Các bạn có thể tìm hiểu những khái niệm khác trong\n","Milvus tại đây. Bây giờ, chúng ta sẽ thử kiểm tra danh sách các\n","Collection có trong Milvus hiện tại. Các bạn tạo một file .py bất kì,\n","ở đây mình tạm đặt tên làcheck_milvus.py, có nội dung như sau: 1 from\n","pymilvus im...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 11baec39-a005-4262-811b-7a38a8a3570d\n","Text: 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4c604eb3-7919-4d78-8254-fd5059e7129e\n","Text: AI VIETNAM aivietnam.edu.vn Hình 8: Pipeline của hệ thống End-\n","to-end QA trong bài Các bạn tạo một file code .py mới (ở đây mình sẽ\n","tạo filebuild_database.py) và thực hiện các bước sau đây: (a) Import\n","các thư viện cần thiết: 1 from pymilvus import ( 2 connections , 3\n","utility , 4 FieldSchema , 5 CollectionSchema , 6 DataType , 7\n","Collection , 8 ) 9...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d0850e6b-0fd8-4b90-a403-f29d3c6fff94\n","Text: AI VIETNAM aivietnam.edu.vn • INSERT_RATIO:Kích thước bộ dữ liệu\n","để đưa vào database. Ở đây mình chỉnh tỉ lệ rất thấp để việc demo trở\n","nên nhanh hơn. Các bạn muốn test nhiều hơn có thể tăng tỉ lệ này lên.\n","• LIMIT: Số lượng kết quả truy vấn trả về từ Milvus. Các bạn muốn tăng\n","số lượng tài liệu trả về có thể tăng tham số này lên. (c) Xây dựng hàm\n","...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 342b14a9-9a78-4685-9816-df09846397a1\n","Text: drop_collection ( collection_name ) 4 5 fields = [ 6 FieldSchema\n","( name =’id ’, dtype = DataType . INT64 , is_primary =True , auto_id =\n","True ), 7 FieldSchema ( name =’title ’, dtype = DataType . VARCHAR ,\n","max_length =1000) , 8 FieldSchema ( name =’ question ’, dtype =\n","DataType . VARCHAR , max_length =1000) , 9 FieldSchema ( name =’\n","context ’, dt...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ec00ef28-0885-4ee0-aee7-43cb97cfa847\n","Text: AI VIETNAM aivietnam.edu.vn và lấy final hidden state của token\n","[CLS] để làm vector embedding: 1 model = AutoModel . from_pretrained (\n","MODEL_NAME ) 2 def quest_embedding ( batch ): 3 sentence_embs = model\n","( 4 input_ids = batch [’ input_ids ’], 5 attention_mask = batch [’\n","attention_mask ’] 6 ) 7 batch [’ question_embedding ’] = sentence_embs\n",". la...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 016403b4-647c-4908-b7c4-63dd4e01485e\n","Text: map ( lambda val : {’ answer ’: val [’ answers ’][ ’text ’][0]}\n","if val [’ answers ’][ ’text ’] else {’ answer ’: ’’}, remove_columns\n","=[ ’ answers ’]) 5 6 # Generate the tokens for each entry . 7\n","squad_v2_dataset = squad_v2_dataset . map ( tokenize_question ,\n","batch_size = TOKENIZATION_BATCH_SIZE , batched = True ) 8\n","squad_v2_dataset . set_format ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 63ffa3c2-9f93-4f50-afb4-6d7b7905ee61\n","Text: AI VIETNAM aivietnam.edu.vn 15 ) 16 17 # Due to the varchar\n","constraint we are going to limit the question size when inserting 18\n","def insert_function ( batch ): 19 insertable = [ 20 batch [’title ’],\n","21 batch [’ question ’], 22 [x [:9995] + ’... ’ if len (x) > 9999 else\n","x for x in batch [’ context ’]] , 23 [x [:995] + ’... ’ if len (x) >\n","999 else...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 55b5c230-150d-4ea9-aa2a-5aa321272c75\n","Text: has_collection ( COLLECTION_NAME ): 3 qa_collection =\n","create_collection ( COLLECTION_NAME , DIMENSION ) 4 qa_collection .\n","load ( replica_number = REPLICA_NUMBER ) 5 else : 6 qa_collection =\n","Collection ( COLLECTION_NAME ) 7 qa_collection . load ( replica_number\n","= REPLICA_NUMBER ) 8 9 if qa_collection . is_empty : 10\n","create_squad_database ( qa_col...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d7e2e969-98c0-42ed-8c97-761d62ff6455\n","Text: AI VIETNAM aivietnam.edu.vn 21 contexts = [] 22 23 for hit in\n","hits : 24 ids . append ( hit .id) 25 distances .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7268ba73-42c1-4b64-8431-54a684dab65a\n","Text: append ( hit . distance ) 26 questions . append ( hit . entity .\n","get (’ question ’)) 27 contexts . append ( hit . entity . get (’\n","context ’)) 28 29 overall_id . append ( ids ) 30 overall_distance .\n","append ( distances ) 31 overall_question . append ( questions ) 32\n","overall_context . append ( contexts ) 33 34 return { 35 ’id ’:\n","overall_id , 36 ’ d...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f308a949-178e-4ab9-9faa-8c0d445ff6fd\n","Text: connect ( host = MILVUS_HOST , port = MILVUS_PORT ) 2 if utility\n",". has_collection ( COLLECTION_NAME ): 3 qa_collection = Collection (\n","COLLECTION_NAME ) 4 qa_collection . load ( replica_number =\n","REPLICA_NUMBER ) 5 else : 6 raise RuntimeError (c) Khai báo mô hình\n","QA:Chúng ta sẽ dùng mô hình đã huấn luyện ở buổi học về QA để sử dụng\n","trong chương tr...\n","----------------------------------------------------------------------------------------------------\n","Node ID: aa6213f8-29a3-478b-adba-39b8a425dd19\n","Text: AI VIETNAM aivietnam.edu.vn (d) Xây dựng hàm main cho chương\n","trình:Cuối cùng, ta viết code nhận đầu vào là câu hỏi từ command line,\n","thực hiện embedding câu hỏi và chạy hàm search. Từ đó, với các tài\n","liệu có liên quan, ta chạy mô hình QA để trả lời câu hỏi từ input: 1\n","def main (): 2 parser = argparse . ArgumentParser () 3 parser .\n","add_argument (’...\n","----------------------------------------------------------------------------------------------------\n","Node ID: dd3a0ee6-f75e-46e9-8402-c4ab551a583a\n","Text: Ở đây, mình sẽ chạy với câu hỏi sau (câu hỏi này thuộc bộ dữ\n","liệu SQuADv2): 1 $ python3 qa.py -- question ’In what year did Wesley\n","Clark retire ?’ 11\n","----------------------------------------------------------------------------------------------------\n","Node ID: 15161f56-725e-41e7-9d9f-d27f0c70e6b2\n","Text: AI VIETNAM aivietnam.edu.vn Hình 10: Kết quả End-to-end QA sử\n","dụng hàm search trên Milvus vector database Như vậy, thông qua việc\n","cài đặt theo các bước trên, các bạn đã thành công ứng dụng Milvus\n","vector database để xây dựng một chương trình về End-to-end Question\n","Answering. Các bạn muốn hiểu thêm về Milvus có thể tìm đọc code đính\n","kèm có filehel...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 75a69892-d343-499b-9241-103058477cb0\n","Text: 6. Trường hợp muốn ngắt kết nối với Milvus và xóa dữ liệu:Để\n","nhanh chóng ngắt kết nối với Milvus, các bạn hãy sử dụng lệnh sau\n","trong Terminal: 1 $ bash standalone_embed .sh stop Để xóa hẳn dữ liệu\n","được lưu trong Milvus, đầu tiên các bạn hãy chạy lệnh dưới đây trong\n","Terminal: 1 $ docker ps -a Tại đây, các bạn sẽ thấy một danh sách các\n","CONTAINER I...\n","----------------------------------------------------------------------------------------------------\n","Node ID: cb68cb13-7fe7-428f-90fd-61c5ee8e4504\n","Text: AI VIET NAM – AIO COURSE 2023 Project: Multi-Task Learning Quoc-\n","Thai Nguyen và Quang-Vinh Dinh Ngày 30 tháng 4 năm 2024 Phần I. Giới\n","thiệu Hình 1: Ví dụ về mô hình học đa tác vụ cho bài toán image\n","semantic segmentation và depth-image prediction Hình 2: Các phương\n","pháp huấn luyện mô hình học đa tác vụ. 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: 48a0e20c-20c9-498b-ac11-1fe3648e1fe1\n","Text: AI VIETNAM aivietnam.edu.vn Mô hình học đa tác vụ (Multi-Task\n","Learning)là phương pháp huấn luyện cho một mô hình nhưng có thể giải\n","quyết cho nhiều bài toán khác nhau. Ví dụ, chúng ta huấn luyện một mô\n","hình với đầu vào là một hình ảnh và đầu ra giải quyết cho hai bài toán\n","khác nhau như: semantic segmentation và depth-image prediction được mô\n","tả n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 474ae81b-4fde-4ab9-8c8b-a4d59122b9ee\n","Text: Deep Multi-Task Architectures. Bao gồm các phương pháp tập trung\n","vào xây dựng các mô hình chung để giải quyết các bài toán khác nhau.\n","Trong đó gồm 2 thành phần chính: thành phần thứ nhất bao gồm các layer\n","chung hoặc chia sẻ trọng số để học các đặc trưng thường gọi là shared-\n","encoder; thành phần thứ hai bao gồm các layer riêng để học các đặc\n","trưn...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8151d071-a40a-484b-9886-a45141e04d1e\n","Text: AI VIETNAM aivietnam.edu.vn Phần II. Multi-Task Learning for\n","Com- puter Vision Hai phương pháp chính bao gồm: hard parameter\n","sharing được mô tả trong hình 3 và soft parameter sharing được mô tả\n","trong hình 4. Hình 3: Hard parameter sharing. Hình 4: Soft parameter\n","sharing. Hard parameter sharing bao gồm các layer dùng chung cho tất\n","cả các task, sh...\n","----------------------------------------------------------------------------------------------------\n","Node ID: aa3f6306-cade-4123-9770-bfd46f068240\n","Text: 3\n","----------------------------------------------------------------------------------------------------\n","Node ID: c297a94f-cac3-4b25-9df1-f5c4d5f6a314\n","Text: AI VIETNAM aivietnam.edu.vn 1.1. Dataset 1 import os 2 import\n","torch 3 import fnmatch 4 import numpy as np 5 from torch . utils .\n","data import DataLoader 6 7 class NYUv2 ( torch . utils .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2853b072-6a99-4695-9900-84bfecf14a47\n","Text: data .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6bd620f7-8bd0-4883-ab5a-4dd7bdd607c5\n","Text: dataset . Dataset ): 8 def __init__ (self , root , train = True\n","): 9 self . train = train 10 self . root = os. path . expanduser (\n","root ) 11 12 # read the data file 13 if train : 14 self . data_path =\n","root + ’/ train ’ 15 else : 16 self . data_path = root + ’/ val ’ 17\n","18 # calculate data length 19 self . data_len = len ( fnmatch . filter\n","(os. l...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 82c804fb-85b6-4d17-80c7-2652b156a588\n","Text: load ( self . data_path + ’/ label /{: d}. npy ’. format ( index\n","))) 25 depth = torch . from_numpy (np. moveaxis (np. load ( self .\n","data_path + ’/ depth /{: d}. npy ’. format ( index )), -1, 0)) 26 27\n","return { 28 ’image ’: image . float () , 29 ’ semantic ’: semantic .\n","float () , 30 ’depth ’: depth . float () 31 } 32 33 def __len__ ( self\n","): 34 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: aaf4b70a-f51b-49b3-b2dd-4610bb8f97e3\n","Text: AI VIETNAM aivietnam.edu.vn 8 filter = [64 , 128 , 256 , 512 ,\n","512] 9 10 self . class_nb = 13 11 12 # define encoder decoder layers\n","13 self . encoder_block = nn. ModuleList ([ self . conv_layer ([3 ,\n","filter [0]]) ]) 14 self . decoder_block = nn. ModuleList ([ self .\n","conv_layer ([ filter [0] , filter [0]]) ]) 15 for i in range (4) : 16\n","self . enc...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 83e7df4d-18e2-4b12-9ef2-438c1dc4a6ab\n","Text: Conv2d ): 44 nn.\n","----------------------------------------------------------------------------------------------------\n","Node ID: f6cb7aec-387c-4228-a91e-861364357b70\n","Text: init . xavier_normal_ (m. weight ) 45 nn. init . constant_\n","(m.bias , 0) 46 elif isinstance (m, nn. BatchNorm2d ): 47 nn. init .\n","constant_ (m. weight , 1) 48 nn. init . constant_ (m.bias , 0) 49 elif\n","isinstance (m, nn. Linear ): 50 nn.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 2b014f80-1955-43d4-ab11-fb2f31c1b50c\n","Text: init . xavier_normal_ (m. weight ) 51 nn.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 81b72432-b58d-48fd-ab6f-d4dfe04815b1\n","Text: init . constant_ (m.bias , 0) 52 53 # define convolutional block\n","54 def conv_layer (self , channel ): 55 conv_block = nn. Sequential (\n","56 nn. Conv2d ( in_channels = channel [0] , out_channels = channel [1]\n",", kernel_size =3 , padding =1) , 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4d1fafa5-96f6-4f07-89f8-51f9e21feba9\n","Text: AI VIETNAM aivietnam.edu.vn 57 nn. BatchNorm2d ( num_features =\n","channel [1]) , 58 nn. ReLU ( inplace = True ) 59 ) 60 return\n","conv_block 61 62 def forward (self , x): 63 g_encoder , g_decoder ,\n","g_maxpool , g_upsampl , indices = ([0] * 5 for _ in range (5) ) 64 for\n","i in range (5) : 65 g_encoder [i], g_decoder [-i - 1] = ([0] * 2 for _\n","in range (2)...\n","----------------------------------------------------------------------------------------------------\n","Node ID: be73a208-21e8-49de-92ce-3f359dc2fe46\n","Text: float (). unsqueeze (1) .to( device ) 6 7 if task_type == ’\n","semantic ’: 8 # semantic loss : depth - wise cross entropy 9 loss = F.\n","nll_loss ( x_pred , x_output , ignore_index = -1) 10 11 if task_type\n","== ’depth ’: 12 # depth loss : l1 norm 13 loss = torch . sum ( torch .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 986427e0-6a84-420c-a8af-aa4ee4b3d247\n","Text: abs ( x_pred - x_output ) * binary_mask ) / torch . nonzero (\n","binary_mask , as_tuple = False ). size (0) 14 15 return loss 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: f568d1ad-ddeb-4c44-bebc-03d6da8b2180\n","Text: AI VIETNAM aivietnam.edu.vn 1.4. Training 1 from tqdm import\n","tqdm 2 3 def train_epoch ( train_loader , model , device , optimizer\n","): 4 # iteration for all batches 5 model . train () 6 losses = {’\n","semantic ’: [] , ’depth ’: [] , ’total ’: []} 7 for i, batch in tqdm (\n","enumerate ( train_loader )): 8 images = batch [’image ’]. to( device )\n","9 semanti...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8fdd1fae-8a9a-4b66-b1aa-2865c81a6d72\n","Text: append ( loss . item ()) 28 29 avg_losses = { task : sum (\n","task_loss )/ len ( task_loss ) for task , task_loss in losses . items\n","()} 30 31 return avg_losses 32 33 34 def evaluation_epoch ( val_loader\n",", model , device ): 35 # iteration for all batches 36 model . eval ()\n","37 losses = {’ semantic ’: [] , ’depth ’: [] , ’total ’:[]} 38 with\n","torch . n...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1046dc6b-53ba-424c-a38f-dae8ddfdc702\n","Text: append ( loss . item ()) 56 57 avg_losses = { task : sum (\n","task_loss )/ len ( task_loss ) for task , task_loss in losses . 7\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7f4379d5-ab35-49f2-a5b0-abf0fa1dacbd\n","Text: AI VIETNAM aivietnam.edu.vn items ()} 58 return avg_losses 59 60\n","def train ( train_loader , val_loader , model , device , optimizer ,\n","epochs ): 61 best_loss = 100. 62 for epoch in range ( epochs ): 63\n","train_loss = train_epoch ( train_loader , model , device , optimizer )\n","64 val_loss = train_epoch ( train_loader , model , device , optimizer\n",") 65 ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b8792846-f7cf-4d93-ab8b-bd5baf20f590\n","Text: lr_scheduler . StepLR ( optimizer , step_size =100 , gamma =0.5)\n","81 82 epochs = 10 83 model = train ( train_loader , val_loader , model\n",", device , optimizer , epochs ) 1.5. Inference 1 model_path = ’./\n","model / hard_parameter_sharing_model_weights .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1c5ee0f6-4e89-469a-9753-e4bde6901d0b\n","Text: pth ’ 2 model = HardParameterSharingModel () 3 model .\n","load_state_dict ( torch . load ( model_path )) 4 model . eval () 5\n","model .to( device ) 6 7 test_sample = next ( iter ( val_ds )) 8\n","test_sample = { task : test_sample [ task ]. unsqueeze (0) .to( device\n",") for task in test_sample . keys ()} 9 10 with torch . no_grad (): 11\n","output = model ( tes...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b478567d-70af-4db8-b9a1-6ea443f94699\n","Text: AI VIETNAM aivietnam.edu.vn Phần 3. Câu hỏi trắc nghiệm Câu hỏi\n","1Học đa tác vụ (Multi-Task Learning) là gì? a) Học cách thực hiện\n","nhiều tác vụ khác nhau một cách độc lập b) Học cách thực hiện nhiều\n","tác vụ cùng một lúc c) Học cách thực hiện một tác vụ duy nhất d) Học\n","cách thực hiện nhiều tác vụ theo thứ tự Câu hỏi 2Lợi ích chính của học\n","đa tác vụ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 28221c10-e955-47db-a00d-f4dcc6253831\n","Text: a) PAD-Net b) PAP-Net c) MTI-Net d) Cross-Stitch Networks Câu\n","hỏi 5Mô hình MTL nào sau đây tập trung vào tinh chỉnh khối decoder? a)\n","PAD-Net b) MTAN c) NDDR-CNN d) Cross-Stitch Networks Câu hỏi 6Hàm loss\n","để huấn luyện cho bài toán semantic segmentation là? a) Pixel-wise\n","Cross Entropy b) Huber Loss c) Recontruction Loss d) Contrastive Loss\n","Câu hỏ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c1904dac-45b1-4b57-b0d7-e2a3ed8d672a\n","Text: AI VIETNAM aivietnam.edu.vn c) L1 d) Contrastive Loss Câu hỏi\n","8Bộ dữ liệu sử dụng cho phần thực nghiệm là?\n","----------------------------------------------------------------------------------------------------\n","Node ID: bf9ab727-aedd-49ad-8495-5ad20d2bf1c8\n","Text: a) NYUD-V2 b) CIFAR10 c) CIFAR100 d) MNIST Câu hỏi 9Bộ dữ liệu\n","NYUD-V2 bao nhiêu sample? a) 499 b) 1119 c) 1449 d) 1999 Câu hỏi 10Độ\n","đo để đánh giá cho bài toán semantic segmentation là? a) IoU b)\n","F-Score c) Recall d) Precision - Hết - 10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 755b163e-cfdb-455b-b64b-459e7063ac85\n","Text: AI VIET NAM – AI COURSE 2023 Reinforcement Learning - Exercise\n","Dinh-Thang Duong và Quang-Vinh Dinh PR-Team: Hoàng-Nguyên Vũ, Đăng-Nhã\n","Nguyễn và Minh-Châu Phạm Ngày 18 tháng 4 năm 2024 Phần I: Giới thiệu\n","Reinforcement Learning (RL) (Tạm dịch: Học tăng cường), là một trong 3\n","nhánh chính trong Machine Learning (bên cạnh Supervised Learning và\n","Unsup...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ce3d292d-257c-4365-80bb-40e81fff524e\n","Text: Reinforcement learning thường được thử nghiệm trong môi trường\n","game giả lập, song vẫn góp mặt trong một vài các ứng dụng nổi tiếng\n","như: AlphaGo, ChatGPT... Để hiểu rõ hơn một số từ khóa sử dụng trong\n","bài viết, chúng ta sẽ mô tả lại ý tưởng chính của một bài toán Học\n","tăng cường qua framework sau: 1\n","----------------------------------------------------------------------------------------------------\n","Node ID: 35d76457-09a0-493c-864c-dafd03eea632\n","Text: AI VIETNAM aivietnam.edu.vn Hình 1: Reinforcement Learning\n","Framework. Mô tả tổng quan về bài toán Học tăng cường. Như đã nói ở\n","trên, Reinforcement Learning liên quan đến việc triển khai một tác tử\n","có khả năng tương tác với môi trường, từ đó học cách để tối ưu điểm\n","thưởng tích lũy kỳ vọng nhận được qua mỗi nước đi. Với hình minh họa\n","trên, ta có t...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1c78bd9e-7d5b-4a2d-9e04-45a0ec40d7a4\n","Text: 2. Dựa vào thông tin trạng tháiS0, tác tử thực hiện hành độngAt\n","lên môi trường. 3. Môi trường từ đó thay đổi sang trạng tháiS1. 4. Môi\n","trường đồng thời trả về cho tác tử điểm thưởngR1. Và cứ như vậy, quá\n","trình trên sẽ lặp đi lặp lại cho đến khi tác tử hoàn thành mục tiêu\n","của trò chơi hoặc hết giờ (timeout), trò chơi sẽ kết thúc (gọi là\n","terminate...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d2e7f381-0dee-4136-abec-977478f1f7f8\n","Text: AI VIETNAM aivietnam.edu.vn Phần II: Bài tập Để có thể quan sát\n","tác tử chơi game, các bạn cần chạy animation của game. Ở đây, chúng ta\n","có thể hiển thị animation game trên cả 2 nền tảng code (máy local và\n","Google Colab): • Đối với máy local:Các bạn hãy sử dụng môi trường ảo\n","conda (cách cài đặt conda các bạn hãy tham khảo tại đây) và cài đặt\n","thư vi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 8ff20b89-3ad9-41fe-a563-981595f576ca\n","Text: reset () 6 for _ in range (100) : 7 env . render () 8 env . step\n","( env . action_space . sample ()) 9 env . close () Sau đó, thực thi\n","filetest.py trong terminal bằng lệnh: 1 $ python3 test .py Nếu thực\n","thi thành công, các bạn sẽ thấy một cửa sổ animation game CartPole\n","hiện lên như sau: Hình 3: Hình ảnh trực quan của môi trường\n","CartPole-v1 • Đối v...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 0575aebb-1dbc-4006-8f3c-3ccb31ee0817\n","Text: AI VIETNAM aivietnam.edu.vn trạng thái của môi trường như các\n","frame, sau đó lưu lại thành một file .gif, từ đó có thể dễ dàng hiển\n","thị lên colab. Các bước thực hiện như sau: 1. Cài đặt một số package\n","cần thiết: 1 ! sudo apt - get update 2 !\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3f6d63fd-3b9d-4b10-bb68-58b0176de7a0\n","Text: apt install imagemagick 3 ! pip install ’gym [ all ]’ pygame 2.\n","Xây dựng hàm tạo ảnh gif:Hàm này nhận vào danh sách các frame, sau đó\n","xuất thành file có têndemo.gif: 1 import matplotlib . pyplot as plt 2\n","from matplotlib import animation 3 4 def save_frames_as_gif ( 5 frames\n",", 6 path =’./ ’, 7 filename =’demo . gif ’, 8 fps =1 9 ): 10\n","temp_frame ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9979e337-ae81-4724-9236-7d50fd13b3a2\n","Text: make ( env_id , render_mode =’ rgb_array ’) 7 images = [] 8\n","state , info = env . reset () 9 img = env . render () 10 images .\n","append ( img ) 11 12 for _ in range (100) : 13 action = env .\n","action_space . sample () 14 state , reward , terminated , truncated ,\n","info = env . step ( action ) 15 img = env . render () 16 images .\n","append ( img ) 17 18 if...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1a78da47-9868-469f-a0c4-4abd04b83349\n","Text: AI VIETNAM aivietnam.edu.vn Nếu toàn bộ các bước trên thành\n","công, ta sẽ thấy animation game hiển thị trong Colab như hình sau:\n","Hình 4: Hình ảnh trực quan của môi trường CartPole-v1 trong Google\n","Colab Để thuận tiện trong việc thực hiện bài tập này, bài tập 1 và 3\n","trong bài viết sẽ được thực hiện trong Google Colab. 5\n","----------------------------------------------------------------------------------------------------\n","Node ID: 44cc39e5-cea5-4faa-b301-50c6c4860916\n","Text: AI VIETNAM aivietnam.edu.vn Câu 1: Q-Learning Cho môi trường\n","game Taxi trong thư viện OpenAI Gym, với một số thông tin chính như\n","sau (các bạn có thể đọc thêm thông tin chi tiết về môi trường này tại\n","đây): Hình 5: Hình ảnh trực quan của game Taxi • Kiểu môi trường:Grid\n","World (Deterministic Environment). • Không gian hành động (Action\n","Space):6 (Di...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 74abcc98-74b1-48f6-a850-f47d71b7a78e\n","Text: pyplot as plt 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: 812f4c3e-8104-440b-b4b9-77bbb208b57a\n","Text: AI VIETNAM aivietnam.edu.vn 6 7 from IPython . display import\n","Image 8 from matplotlib import animation 9 from tqdm . notebook import\n","tqdm 2. Khởi tạo môi trường Taxi-v3: 1 env_id = ’Taxi -v3 ’ 2 env =\n","gym . make ( env_id , render_mode =’ rgb_array ’) 3. Xây dựng hàm khởi\n","tạo Q-Table: 1 def init_q_table ( state_space , action_space ): 2\n","q_table =...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 67ab8ba9-4eb9-465c-bcb3-ebbececd4894\n","Text: shape [1]) 8 9 return action 6. Khai báo một số siêu tham số cần\n","thiết: 1 n_training_episodes = 30000 2 n_eval_episodes = 100 3 lr =\n","0.7 4 5 max_steps = 99 6 gamma = 0.95 7 eval_seed = range (\n","n_eval_episodes ) 8 9 max_epsilon = 1.0 10 min_epsilon = 0.05 11\n","decay_rate = 0.0005 7. Xây dựng hàm training: 1 def train ( 2 env , 3\n","max_steps , 4 q_tab...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 324316f9-45f7-4ede-9ca9-980b44ff79aa\n","Text: AI VIETNAM aivietnam.edu.vn 9 lr , 10 gamma 11 ): 12 for episode\n","in tqdm ( range ( n_training_episodes )): 13 epsilon = min_epsilon + (\n","max_epsilon - min_epsilon ) * np. exp (- decay_rate * episode ) 14 15\n","state , info = env . reset () 16 step = 0 17 terminated = False 18\n","truncated = False 19 20 for step in range ( max_steps ): 21 action =\n","epsil...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 97c7ad15-4a77-4b34-b8ed-d1f10483f86d\n","Text: max ( q_table [ new_state ]) - q_table [ state , action ]) 25 26\n","if terminated or truncated : 27 break 28 29 state = new_state 30 31\n","return q_table 8. Thực hiện huấn luyện: 1 q_table = init_q_table (\n","state_space , action_space ) 2 trained_q_table = train ( 3 env , 4\n","max_steps , 5 q_table , 6 n_training_episodes , 7 min_epsilon , 8\n","max_epsilon , ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a7c2a4cf-462b-41d2-bdcf-6fe734fb58a7\n","Text: AI VIETNAM aivietnam.edu.vn Câu 2: Approximate Q-Learning Cho\n","môi trường game Pacman với một số thông tin chính như sau: Hình 6:\n","Hình ảnh trực quan của game Pacman • Kiểu môi trường:Grid World\n","(Stochastic Environment). • Không gian hành động (Actions Space):5\n","(Discrete). • Mục tiêu (Objective):Di chuyển Pacman ăn toàn bộ tất cả\n","các đồ ăn (chấm m...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 199777a1-e58e-4b96-97c6-c1723ccac6c7\n","Text: 9\n","----------------------------------------------------------------------------------------------------\n","Node ID: 60b1929b-a9f6-4985-9883-b5634b0e6ddd\n","Text: AI VIETNAM aivietnam.edu.vn • Demo 3:Tác tử khi được huấn luyện\n","qua 100 episodes. • Demo 4:Tác tử khi được huấn luyện qua 1000\n","episodes. Để thực hiện được bài tập này, các bạn có thể tham khảo cách\n","làm sau: 1. Cài đặt source code game Pacman:Dựa vào đường dẫn đã cung\n","cấp ở trên, các bạn hãy tải source code game Pacman thông qua lệnh\n","sau: 1 $ git...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5a39fff4-4b0d-4a10-bbf8-e69ae787a090\n","Text: 10\n","----------------------------------------------------------------------------------------------------\n","Node ID: 624c550c-ec84-4d56-9d7a-95a68111f538\n","Text: AI VIETNAM aivietnam.edu.vn Câu 3: Policy Gradient Cho môi\n","trường game LunarLander-v2 trong thư viện OpenAI Gym, với một số thông\n","tin chính như sau (các bạn có thể đọc thêm thông tin chi tiết về môi\n","trường này tại đây): Hình 8: Hình ảnh trực quan của môi trường\n","LunarLander • Kiểu môi trường:Box2D (Stochastic Environment). • Không\n","gian hành động ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f81ec09c-f813-4edb-ac47-f3ee3caeb42d\n","Text: • State Shape:(8,). • Mục tiêu (Objective):Điều khiển tàu tên\n","lửa đáp xuống đúng vị trí (càng sát càng tốt) đã định trên màn hình,\n","trong khi giữ cho lượng nguyên liệu tiêu thụ ít nhất, đặc biệt không\n","để tàu phát nổ. • Trạng thái kết thúc (Terminate State):Tàu tên lửa\n","đáp xuống thành công, hoặc phát nổ, hoặc bay khỏi giới hạn màn hình. •\n","Hàm điểm...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b72e6d43-1c06-43ef-bbe2-3d4cb5eda3df\n","Text: AI VIETNAM aivietnam.edu.vn 1 import numpy as np 2 import gym 3\n","import os 4 import tqdm 5 import matplotlib . pyplot as plt 6 7 import\n","torch 8 import torch .nn as nn 9 import torch .nn. functional as F 10\n","import torch . optim as optim 11 from torch . distributions import\n","Categorical 12 13 from collections import deque 14 from IPython .\n","display i...\n","----------------------------------------------------------------------------------------------------\n","Node ID: afe337fc-aacf-4b85-9c46-f5f8af6601c1\n","Text: action_space .n 3. Xây dựng Policy Network: 1 class Policy (nn.\n","Module ): 2 def __init__ (self , s_size , a_size , h_size ): 3 super (\n","Policy , self ). __init__ () 4 self . fc1 = nn. Linear ( s_size ,\n","h_size ) 5 self . fc2 = nn. Linear ( h_size , h_size * 2) 6 self . fc3\n","= nn. Linear ( h_size * 2, a_size ) 7 8 def forward (self , x): 9 x =\n","F. re...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6016e993-1fb9-4da3-963c-a51ec1e78efb\n","Text: float (). unsqueeze (0) .to( device ) 17 probs = self . forward\n","( state ). cpu () 18 m = Categorical ( probs ) 19 action = m. sample\n","() 20 21 return action . item () , m. log_prob ( action ) 4. Xây dựng\n","hàm training: 1 def reinforce ( 2 policy , 3 optimizer , 4\n","n_training_episodes , 5 max_steps , 6 gamma , 7 print_every 8 ): 9\n","scores_deque = deq...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9450c092-ee14-4076-a03b-3c897a8e3ed8\n","Text: AI VIETNAM aivietnam.edu.vn 10 scores = [] 11 12 for i_episode\n","in range (1 , n_training_episodes + 1): 13 saved_log_probs = [] 14\n","rewards = [] 15 state = env . reset () 16 17 for t in range (\n","max_steps ): 18 action , log_prob = policy . act ( state ) 19\n","saved_log_probs . append ( log_prob ) 20 state , reward , done , _,\n","info = env . step ( actio...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f8888a86-fb6a-4beb-a8ab-702eb85dc978\n","Text: eps . item () 35 36 returns = torch . tensor ( returns ) 37\n","returns = ( returns - returns .\n","----------------------------------------------------------------------------------------------------\n","Node ID: 9df18de8-2316-4c2d-ae80-aa6147411356\n","Text: mean ()) / ( returns . std () + eps ) 38 39 policy_loss = [] 40\n","for log_prob , disc_return in zip ( saved_log_probs , returns ): 41\n","policy_loss . append (- log_prob * disc_return ) 42 policy_loss =\n","torch . cat ( policy_loss ). sum () 43 44 optimizer . zero_grad () 45\n","policy_loss . backward () 46 optimizer . step () 47 48 if i_episode %\n","print_eve...\n","----------------------------------------------------------------------------------------------------\n","Node ID: f7c82a47-2f41-4a50-8d2c-ba0e5963dcb9\n","Text: AI VIETNAM aivietnam.edu.vn 3 optimizer , 4 n_training_episodes\n",", 5 max_steps , 6 gamma , 7 print_every =100 8 ) 14\n","----------------------------------------------------------------------------------------------------\n","Node ID: 6dbcf2e6-45c1-4d52-85c4-6a9481432318\n","Text: AI VIETNAM aivietnam.edu.vn Phần III: Câu hỏi trắc nghiệm 1.\n","Trong Reinforcement Learning, mục tiêu của tác tử là? (a) Tối ưu điểm\n","thưởng nhận được tại mỗi trạng thái. (b) Tối ưu điểm thưởng tích lũy\n","kỳ vọng. (c) Tối ưu thời gian hoàn thành mục tiêu. (d) Tối ưu tiền\n","thưởng nhận được trong trò chơi. 2. Theo Reinforcement Learning\n","Framework, sau k...\n","----------------------------------------------------------------------------------------------------\n","Node ID: df23344f-dbeb-4ba1-aeb9-f3832d21f4c8\n","Text: (b) Trạng tháiSt+1. (c) Điểm thưởngRt+1 và Trạng tháiSt+1. (d)\n","Điểm thưởngRt+1. 3. Đáp án nào sau đây là một dạng bài toán trong\n","Reinforcement Learning? (a) Adversarial (b) Recursive (c) Dynamic (d)\n","Episodic 4. Trong Reinforcement Learning, trạng thái (S) được hiểu là?\n","(a) Mô tả các hành động thực hiện được của tác tử. (b) Mô tả toàn cục\n","của môi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1114a231-cb61-42c8-b7d8-046b9dbd56eb\n","Text: 5. Trong trò chơi Super Mario Bros phiên bản OpenAI Gym (có\n","không gian trò chơi như ảnh minh họa trên), nhân vật Mario có thể thực\n","hiện được các hành động bao gồm: Di chuyển Trái/Phải, Ngồi xuống, Nhảy\n","lên và Đứng yên. Theo đó, không gian hành động (Action Space) của môi\n","trường này là? 15\n","----------------------------------------------------------------------------------------------------\n","Node ID: cf2295b5-c2d2-4168-ae4d-c19d2a13e2fc\n","Text: AI VIETNAM aivietnam.edu.vn (a) Discrete(4) (b) Discrete(5) (c)\n","Discrete(6) (d) Discrete(7) 6. Trong Reinforcement Learning, Policy là\n","gì? (a) Hàm ánh xạ hành động sang điểm thưởng.\n","----------------------------------------------------------------------------------------------------\n","Node ID: b5e58114-c449-48aa-ad11-b9eb580e37aa\n","Text: (b) Hàm ánh xạ trạng thái sang hành động. (c) Hàm ánh xạ điểm\n","thưởng sang hành động. (d) Hàm ánh xạ hành động sang trạng thái. 7.\n","Trong phương pháp Value-based, việc luôn lựa chọn hành động cho điểm\n","thưởng tích lũy cao nhất còn được gọi là gì? (a) Softmax Policy. (b)\n","Random Policy. (c) ε-greedy Policy. (d) Greedy Policy.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 1e70b0b5-9b48-405b-bf5c-4ff75bbf9f42\n","Text: 8. Dòng code nào sau đây biểu diễn chiến lược cân bằng giữa\n","exploration và exploitation? (a) np.argmax(Q[s]) (b)\n","np.random.choice(A) (c) np.argmin(Q[s]) (d) np.argmax(Q[s]) if\n","np.random.rand() > e else np.random.choice(A) 9. Mục đích của yếu tố\n","Hệ số chiết khấu (Discounting Factor) là gì? (a) Gia tăng kích thước\n","của không gian trạng thái. (b) Gi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3d546138-4e79-485c-a3cf-3d6441d23390\n","Text: AI VIETNAM aivietnam.edu.vn (b) V π(s) =P a∈A Qπ(s, a) (c) V\n","π(s) =P a∈A π(a|s)Qπ(s, a) (d) V π(s) =Eπ[Rt+1 + γRt+2 + γ2Rt+3 +\n","...|St = s] 13. Thuật toán nào sau đây thuộc nhóm thuật toán Policy\n","Gradient? (a) REINFORCE (b) Q-Learning (c) SARSA (d) Value Iteration\n","14. Trong Policy Gradient, hàm nào dưới đây biểu diễn stochastic\n","policy? (a) πθ(s) ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: b37fa2e6-26f6-4e55-9358-eeb7816214b0\n","Text: Cho đoạn code Q-Learning sau: 1 state = env . reset () 2 for t\n","in range ( max_steps ): 3 action = np. argmax (Q[ state ]) 4\n","next_state , reward , done , _ = env . step ( action ) 5 Q[ state ,\n","action ] = Q[ state , action ] + alpha * ( reward + gamma * np. max\n","(Q[ next_state ]) - Q[ state , action ]) 6 state = next_state 7 if\n","done : 8 break Loại ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c8f7ca2a-4fde-4a8b-8ee3-383e54d33ea2\n","Text: THỰC HÀNH TRÍCH XUẤT BẢNG TRONG PDF VỚI TABULA-PY VÀ PANDAS\n","Dinh-Tiem Nguyen và Quang-Vinh Dinh 1 Mở đầu PDF là một trong những\n","định dạng tài liệu phổ biến nhất được sử dụng trên Internet và trong\n","nhiều lĩnh vực khác nhau. Trích xuất dữ liệu từ các tài liệu PDF\n","thường gặp phải nhiều khó khăn, đặc biệt là đối với các bảng dữ liệu.\n","Tiếp nối bài vi...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 88472b2e-6be8-4944-9628-77609408674c\n","Text: Đối với máy tính window, chúng ta tải java tại đây, sau đó tiến\n","hành mở file và cài đặt theo các tùy chọn mặc định. Sau khi quá trình\n","cài đặt hoàn tất, ta cần thêm đường dẫn của java vào Environment\n","Variables của window theo các bước dưới đây: • Truy cập vào thư mục\n","java đã cài đặt trên máy và sao chép đường dẫn của thư mục này\n","C:\\Program Files ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: e50e34aa-4317-4ff1-bd4b-57f52f9bcaa0\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Sau khi thiết lập hoàn\n","tất, chúng ta có thể kiểm tra xem thư viện đã cài đặt thành công chưa\n","thông qua lệnh sau: 1 import tabula 2 !java -version\n","====================================== Output\n","======================================== java version \"1.8.0_411\"\n","Java(TM) SE Runtime Environment (build 1.8.0_411...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 471b8a51-0dcc-4155-9264-7c302d797e61\n","Text: 3 Trích xuất bảng với Tabula-py Trong tài liệu pdf, có rất nhiều\n","định dạng bảng khác nhau. Chúng ta sẽ tìm hiểu cách trích xuất các\n","loại bảng phổ biến nhất như: Bảng với cấu trúc rõ ràng, bảng không có\n","đường viền... 3.1 Bảng với cấu trúc rõ ràng Bảng có cấu trúc rõ ràng\n","là bảng có đường viền rõ ràng xung quanh cả bảng và các ô bên trong,\n","giúp dễ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4880cf7f-a92a-4bc2-929a-4815bce504e2\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn • lattice là một tham\n","số boolean cho biết liệu hàm có nên sử dụng chế độ \"lattice\"để trích\n","xuất bảng hay không. Khi lattice=True, tabula-py sẽ cố gắng phát hiện\n","các đường kẻ bảng (cả ngang và dọc) để xác định chính xác ranh giới\n","của các ô trong bảng. Kết quả trả về từ tabula.read_pdf là một danh\n","sách chứa...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ebdd3280-d7ef-444e-8019-17fa7cb99d10\n","Text: 3.3 Bảng với cấu trúc phức tạp Bảng có cấu trúc phức tạp là bảng\n","có hàng hoặc cột được gộp lại, chứa các bảng con, hoặc các bảng chứa\n","nhiều cấp độ của thông tin. Các bảng này thường xuất hiện trong các\n","tài liệu kỹ thuật, tài chính hoặc khoa học. Việc trích xuất dữ liệu từ\n","chúng khó hơn các loại bảng thông thường, đòi hỏi chúng ta phải xử\n","dụng cá...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c6a22e4b-6844-4739-913d-ed2579a5a3e7\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Bảng trên có cấu trúc\n","khá phức tạp, có những ô được gộp lại dẫn đến việc trích xuất đòi hỏi\n","chúng ta phải thử các tham số khác nhau và phải xử lý đầu ra với\n","pandas. Đầu tiên, để trích xuất bảng này từ file pdf, ta sẽ thử các\n","tham số khác nhau để thu được kết quả output phù hợp. Trong ví dụ này,\n","ta sẽ sử d...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 3482e9ea-ae41-4c85-9665-e9eb733fc23a\n","Text: 1 # Điền các giá trị NaN bằng các giá trị từ hàng trước đó 2\n","df[’Môn học’].fillna(method=’ffill’, inplace=True) 3 df[’Ngày\n","thi’].fillna(method=’ffill’, inplace=True) 4 df[’Giờ bắt\n","đầu’].fillna(method=’ffill’, inplace=True) 5 df[’Phòng\n","thi’].fillna(method=’ffill’, inplace=True) Bảng 5: Dataframe sau khi\n","đã fillna # Môn học Ngày thi Giờ bắt đầu Ph...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 368e4420-9d56-4da0-8910-503103b30748\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 1 df_cleaned =\n","df.drop([0, 3, 6]) Và cuối cùng, chúng ta sẽ thu được dataframe được\n","làm sạch. Bảng 6: Dataframe trích xuất từ bảng trong tệp pdf # Môn học\n","Ngày thi Giờ bắt đầu Phòng thi 1 Toán 20/12/2024 08:00 101A 2 Toán\n","21/12/2024 09:00 102B 4 Lý 22/12/2024 08:00 101A 5 Lý 22/12/2024 10:00\n","102B 7 Hóa 23...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a5bbd5e0-ac24-4544-9c79-ca31a425555d\n","Text: Tuy nhiên có một số trường hợp có thể khác phục bằng cách yêu\n","cầu tabula-py chỉ thực hiện trích xuất bảng tại vị trí được chỉ định\n","trên trang. Đây là cách làm thủ công và khá tốn sức, đầu tiên chúng ta\n","cần một công cụ để xác định vị trí của bảng trong tệp tin pdf. Trong\n","bài viết này, chúng ta sẽ sử dụng phần mềm Adobe reader, bạn có thể\n","tải và c...\n","----------------------------------------------------------------------------------------------------\n","Node ID: d506e83b-b930-4e06-861e-9cbb41bc8398\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn • top: Vị trí trên cùng\n","của bảng • left: Vị trí cạnh trái của bảng • width: Vị ví cạnh phải\n","của bảng • botton: Vị trí cạnh dưới cùng của bảng Chúng ta sẽ sử dụng\n","công cụ mesuaring tool trong phần mềm Adobe reader để đo khoảng cách\n","từ các cạnh của tài liệu đến các vị trí các cạnh của bảng tương ứng.\n","Khoảng...\n","----------------------------------------------------------------------------------------------------\n","Node ID: a00d7adb-5722-49cb-b522-86a8dfd4b952\n","Text: Mặc dù tabula-py còn khá nhiều hạn chế đối với cấu trúc bảng\n","phức tạp tuy nhiên nó cũng có nhiều ưu điểm đáng để ta ưu tiên sử dụng\n","khi trích xuất bảng trong pdf. 6\n","----------------------------------------------------------------------------------------------------\n","Node ID: 5ea2c620-dd08-4c79-a9f3-c88cb1c35191\n","Text: NÂNG CAO HIỆU QUẢ VIẾT CODE VỚI TYPE HINTS VÀ MYPY Dinh-Tiem\n","Nguyen và Quang-Vinh Dinh 1 Mở đầu Làm sao để viết code Python dễ đọc,\n","dễ hiểu và dễ bảo trì hơn? Làm thế nào để có thể kiểm tra tính đúng\n","đắn của code trước khi thực thi chúng?\n","----------------------------------------------------------------------------------------------------\n","Node ID: d77335c0-ec23-4b83-9b9d-cc3d33a437d1\n","Text: Và liệu có cách nào để phát hiện và sửa lỗi kiểu dữ liệu một\n","cách dễ dàng và nhanh chóng không? Trong bài viết này sẽ hướng dẫn sử\n","dụng Type Hints để xác định và gợi ý kiểu dữ liệu của các biến, giá\n","trị trả về của các hàm, phương thức... Đồng thời kết hợp với Mypy-một\n","công cụ kiểm tra kiểu dữ liệu static mạnh mẽ cho các chương trình\n","Python, giúp...\n","----------------------------------------------------------------------------------------------------\n","Node ID: bb19dc26-2b5b-4d65-9f5f-f80284b024cd\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 1 name : str = \"AI\n","VIETNAM \" 2 year : int = 2024 3 4 print (f\" Welcome to { name } { year\n","}!\") 5 print ( __annotations__ ) ================= Output\n","================ Welcome to AI VIETNAM 2024! {’name ’: <class ’str ’>,\n","’year ’: <class ’ int ’ >} ==========================================\n","Trong ví dụ trên ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 66a4eed9-d8fc-40ed-95f4-5be817426472\n","Text: Nhưng nếu ta cố tình sử dụng kiểu dữ liệu float thì chương trình\n","vẫn hoạt động bình thường, không một cảnh báo lỗi nào xuất hiện. 2\n","----------------------------------------------------------------------------------------------------\n","Node ID: eebd414b-2355-4f8e-a6a8-0b22e5263ff8\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 1 def add (x: int , y:\n","int ) -> int : 2 return x + y 3 4 if __name__ == \" __main__ \": 5 print\n","( add (x= 1.5 , y =2) ) 6 print ( add . __annotations__ )\n","================= Output ================ 3.5 {’x ’: <class ’int ’>,\n","’y ’: <class ’int ’>, ’return ’: <class ’int ’ >}\n","===================================...\n","----------------------------------------------------------------------------------------------------\n","Node ID: 7a467e51-f596-43bd-afd1-c73fdd21a9db\n","Text: name = name 7 self . age = age 8 9 def greet ( self ) -> str :\n","10 return f\" Xin chào, Tôi là { self . name } năm nay tôi { self . age\n","} tuổi.\" 11 12 if __name__ == \" __main__ \": 13 person_1 = Person (\"\n","Tom \", 25) 14 print ( person_1 . greet ())\n","====================================== Output\n","======================================== Tôi là Tom năm ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: c79d4fc9-39ba-484f-bf28-83f7018cb8ee\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn Trong lớp Person trên,\n","chúng ta gợi ý rằng thuộc tính name là một chuỗi (str), thuộc tính age\n","là một số nguyên (int) và phương thức greet trả về một chuỗi (str).\n","Lưu ý: Việc tạo type hints trong code chương trình là không bắt buộc,\n","không có chúng thì chương trình vẫn chạy bình thường. Vậy thì khi nào\n","nên ...\n","----------------------------------------------------------------------------------------------------\n","Node ID: ac740d15-484f-4fe8-ba7f-9495e9fa94ff\n","Text: Ta thấy mypy thật tiện lợi phải không? 4\n","----------------------------------------------------------------------------------------------------\n","Node ID: cc8f7cfc-1776-4fb4-b2f6-cc77a7ab7e43\n","Text: Daily AI Exercise (AIO) aivietnam.edu.vn 3 Bài tập Viết một\n","chương trình Python để kiểm tra xem một số có phải là số nguyên tố hay\n","không. Số nguyên tố là số nguyên dương lớn hơn 1 và chỉ có hai ước số\n","dương là 1 và chính nó.\n","----------------------------------------------------------------------------------------------------\n","Node ID: 4984385d-868f-4b1a-b743-9d24c7c63cd4\n","Text: Yêu cầu: • Viết một hàm có tên là is_prime nhận một số nguyên\n","dương n và trả về một giá trị boolean. Nếu n là số nguyên tố, hàm sẽ\n","trả về True, ngược lại trả về False. • Sử dụng Type Hints để gợi ý về\n","kiểu dữ liệu của tham số và giá trị trả về của hàm. • Sử dụng mypy để\n","kiểm tra kiểu dữ liệu trong chương trình 5\n"]}],"source":["noder = Chunking_Data(\"BAAI/bge-small-en-v1.5\",documents).Get_nodes()\n","for i in noder:\n","    print(\"-\" * 100)\n","    print(i)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7374,"status":"ok","timestamp":1742631790499,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"x2pPiYBRvRlc","outputId":"97eca436-f3e0-4c12-a99a-a8e0b6c90f1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting weaviate-client[embedded]\n","  Downloading weaviate_client-4.11.2-py3-none-any.whl.metadata (3.6 kB)\n","\u001b[33mWARNING: weaviate-client 4.11.2 does not provide the extra 'embedded'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client[embedded]) (0.28.1)\n","Collecting validators==0.34.0 (from weaviate-client[embedded])\n","  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting authlib<1.3.2,>=1.2.1 (from weaviate-client[embedded])\n","  Downloading Authlib-1.3.1-py2.py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client[embedded]) (2.10.6)\n","Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client[embedded]) (1.71.0)\n","Collecting grpcio-tools<2.0.0,>=1.66.2 (from weaviate-client[embedded])\n","  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n","Collecting grpcio-health-checking<2.0.0,>=1.66.2 (from weaviate-client[embedded])\n","  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client[embedded]) (43.0.3)\n","Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client[embedded]) (5.29.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client[embedded]) (75.1.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client[embedded]) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client[embedded]) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client[embedded]) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client[embedded]) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client[embedded]) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client[embedded]) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client[embedded]) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client[embedded]) (4.12.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client[embedded]) (1.3.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client[embedded]) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client[embedded]) (2.22)\n","Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Authlib-1.3.1-py2.py3-none-any.whl (223 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.8/223.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading grpcio_health_checking-1.71.0-py3-none-any.whl (18 kB)\n","Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading weaviate_client-4.11.2-py3-none-any.whl (353 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.0/354.0 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: validators, grpcio-tools, grpcio-health-checking, authlib, weaviate-client\n","Successfully installed authlib-1.3.1 grpcio-health-checking-1.71.0 grpcio-tools-1.71.0 validators-0.34.0 weaviate-client-4.11.2\n","4.11.2\n"]}],"source":["!pip install --upgrade \"weaviate-client[embedded]\"\n","import weaviate\n","print(weaviate.__version__)  # Cần >= 4.3.0\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rJqRIspPwXPk"},"source":["Embedding and Retriver"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7953,"status":"ok","timestamp":1742631807038,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"hU9GUy9qwE-u","outputId":"0b194457-4457-4ef8-ac25-6e93758b3435"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/weaviate/warnings.py:340: UserWarning: Con006: You're using the sync client in an async context. This usage is discouraged to avoid blocking your async event loop with sync I/O calls.\n","            We encourage you to update your code to use the async client instead when running inside async def functions!\n","  warnings.warn(\n","INFO:weaviate-client:Binary /root/.cache/weaviate-embedded did not exist. Downloading binary from https://github.com/weaviate/weaviate/releases/download/v1.26.6/weaviate-v1.26.6-Linux-amd64.tar.gz\n","INFO:weaviate-client:Started /root/.cache/weaviate-embedded: process ID 4133\n"]}],"source":["import weaviate\n","from weaviate.embedded import EmbeddedOptions\n","import os\n","\n","client = weaviate.WeaviateClient(\n","    embedded_options=EmbeddedOptions(\n","        additional_env_vars={\n","            \"ENABLE_MODULES\": \"backup-filesystem,text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai\",\n","            \"BACKUP_FILESYSTEM_PATH\": \"/tmp/backups\"\n","        }\n","    )\n","    # Add additional options here. For syntax, see the Python client documentation.\n",")\n","\n","# Run your client code in a context manager or call client.close()\n","# before exiting the client to avoid connection errors.\n","client.connect()  # Call `connect()` to connect to the server when you use `WeaviateClient`\n","\n","# Add your client code here."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":150,"status":"error","timestamp":1742545684526,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"eE5LuCrsq7vX","outputId":"86394df7-f87e-4d69-8c75-96dc4b269f6b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'llama_index.core.vector_stores.weaviate'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-139-ec09df3de7e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStorageContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweaviate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeaviateVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Vector_Store\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.core.vector_stores.weaviate'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5327,"status":"ok","timestamp":1742631817693,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"Wjd6Sv6CzYmA","outputId":"a30e09dd-84f4-4e0b-da67-9dd05bb3bf7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: llama-index 0.12.25\n","Uninstalling llama-index-0.12.25:\n","  Successfully uninstalled llama-index-0.12.25\n","Collecting llama-index\n","  Using cached llama_index-0.12.25-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: weaviate-client in /usr/local/lib/python3.11/dist-packages (4.11.2)\n","Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n","Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n","Requirement already satisfied: llama-index-core<0.13.0,>=0.12.25 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.25)\n","Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n","Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.9)\n","Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.26)\n","Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n","Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n","Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n","Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n","Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n","Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.28.1)\n","Requirement already satisfied: validators==0.34.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.34.0)\n","Requirement already satisfied: authlib<1.3.2,>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.3.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (2.10.6)\n","Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n","Requirement already satisfied: grpcio-tools<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n","Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client) (43.0.3)\n","Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client) (5.29.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (75.1.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.14.0)\n","Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.66.3)\n","Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (6.0.2)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.39)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.11.14)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.18)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.8)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2025.3.0)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (11.1.0)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (9.0.0)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.12.2)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.17.2)\n","Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.16)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n","Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n","Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n","Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.4.post1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.27.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.18.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n","Requirement already satisfied: llama-cloud-services>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.7)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.1.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (1.17.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.26.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (2.22)\n","Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n","Using cached llama_index-0.12.25-py3-none-any.whl (7.0 kB)\n","Installing collected packages: llama-index\n","Successfully installed llama-index-0.12.25\n"]}],"source":["!pip uninstall llama-index -y\n","!pip install --upgrade llama-index weaviate-client\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5617,"status":"ok","timestamp":1742631823322,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"xcL4Mcz23hUP","outputId":"a5cacbb4-f26e-42d6-df12-f3faff1c2dd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.25)\n","Requirement already satisfied: weaviate-client in /usr/local/lib/python3.11/dist-packages (4.11.2)\n","Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n","Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n","Requirement already satisfied: llama-index-core<0.13.0,>=0.12.25 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.25)\n","Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n","Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.9)\n","Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.26)\n","Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n","Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n","Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n","Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n","Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n","Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.28.1)\n","Requirement already satisfied: validators==0.34.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.34.0)\n","Requirement already satisfied: authlib<1.3.2,>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.3.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (2.10.6)\n","Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n","Requirement already satisfied: grpcio-tools<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n","Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client) (43.0.3)\n","Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client) (5.29.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (75.1.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.14.0)\n","Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.66.3)\n","Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (6.0.2)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.39)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.11.14)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.18)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.8)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2025.3.0)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (11.1.0)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (9.0.0)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.12.2)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.17.2)\n","Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.16)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n","Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n","Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n","Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.4.post1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.27.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.18.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n","Requirement already satisfied: llama-cloud-services>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.7)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.1.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (1.17.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.26.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (2.22)\n","Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n"]}],"source":["!pip install --upgrade llama-index weaviate-client\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":217,"status":"error","timestamp":1742631826793,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"AU87lrk1zyLV","outputId":"2fd82e38-67bf-4ac1-e6ac-6f194684f9b5"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'llama_index.vector_stores'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-dbc9c80951b4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStorageContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_stores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeaviateVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Vector_Store\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.vector_stores'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from llama_index.core import VectorStoreIndex, StorageContext\n","from llama_index.vector_stores import WeaviateVectorStore\n","\n","Name = \"Vector_Store\"\n","\n","vector_store = WeaviateVectorStore(weaviate_client=client, index_name=Name)\n","strong_content = StorageContext.from_defaults(vector_store=vector_store)\n","index = VectorStoreIndex.from_documents(noder, storage_context=strong_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1742546426342,"user":{"displayName":"Hùng Phạm","userId":"06860343330653376724"},"user_tz":-420},"id":"y8ogSddS1pZz","outputId":"bd41e3e6-e37a-48ae-8211-e415bd63ea95"},"outputs":[{"name":"stdout","output_type":"stream","text":["['__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'core', 'embeddings', 'llms', 'readers', 'vector_stores']\n"]}],"source":["import llama_index\n","print(dir(llama_index))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRh6Nrg33Fjv"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3f931f2157ee4b30a2b5d99a356e55df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d352a50e82c46b0bdacc82e09112106","IPY_MODEL_d462778df6e34e3580a474e78c0164d9","IPY_MODEL_48ff92440c804f5a842f8f421cb23f1e"],"layout":"IPY_MODEL_ef60bd5e3d684327a9233a125821de4b"}},"1d352a50e82c46b0bdacc82e09112106":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ba0d8390a0c40049e85f3e5b97083dd","placeholder":"​","style":"IPY_MODEL_5d876bd50e00416d80e425391e906ab9","value":"tokenizer.json: 100%"}},"d462778df6e34e3580a474e78c0164d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66fa533c442245ddbe597b6023a5f08f","max":711396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28c79584ab9a4f5da834c0bc43fe17ed","value":711396}},"48ff92440c804f5a842f8f421cb23f1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d669f1dcff9243dc88f523d81df35910","placeholder":"​","style":"IPY_MODEL_3cec7962a79942c1849a2d11b8ec5b38","value":" 711k/711k [00:00&lt;00:00, 1.64MB/s]"}},"ef60bd5e3d684327a9233a125821de4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ba0d8390a0c40049e85f3e5b97083dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d876bd50e00416d80e425391e906ab9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66fa533c442245ddbe597b6023a5f08f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28c79584ab9a4f5da834c0bc43fe17ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d669f1dcff9243dc88f523d81df35910":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cec7962a79942c1849a2d11b8ec5b38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bf48812a75c4c588ad7ac78d5f24f5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f1930e19619455f8072ac98f558d5aa","IPY_MODEL_a82a424c9e264c6cb17e5a8bb80461fd","IPY_MODEL_7cff5a88f08442c380faa0e1fa13b89e"],"layout":"IPY_MODEL_b790f3c270e648bca3c34c7626c2d22d"}},"4f1930e19619455f8072ac98f558d5aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da2b8b0772e47aaaacb5a3c0aa21a70","placeholder":"​","style":"IPY_MODEL_d0e48649c2f4405aa22e6e62d7d12924","value":"special_tokens_map.json: 100%"}},"a82a424c9e264c6cb17e5a8bb80461fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_234127b0aed44acd94af31847a31f349","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5821541f0634209afeace6ddf129521","value":125}},"7cff5a88f08442c380faa0e1fa13b89e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f3908be699746d9bb888c1792f10666","placeholder":"​","style":"IPY_MODEL_1d56ca7a1ff845a585325f1573c933b1","value":" 125/125 [00:00&lt;00:00, 14.5kB/s]"}},"b790f3c270e648bca3c34c7626c2d22d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da2b8b0772e47aaaacb5a3c0aa21a70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0e48649c2f4405aa22e6e62d7d12924":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"234127b0aed44acd94af31847a31f349":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5821541f0634209afeace6ddf129521":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f3908be699746d9bb888c1792f10666":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d56ca7a1ff845a585325f1573c933b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc0523b8934a41e28d67d4ce31405dce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89f818259b384468ace74f60eed18c94","IPY_MODEL_0443ca611c224f879f85f05f05195472","IPY_MODEL_f71aa89c29d447d2ae6b26dfe48008d7"],"layout":"IPY_MODEL_61b145aaf04843739562e7bbb5cd736c"}},"89f818259b384468ace74f60eed18c94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5869905b3a2145759d30164cea51604b","placeholder":"​","style":"IPY_MODEL_1472943eddcf49d591fff2cd26ca3b4c","value":"config.json: 100%"}},"0443ca611c224f879f85f05f05195472":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdc54f2767f4425982f861177484fd12","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d50d0722b27346058b82937177b67a5e","value":190}},"f71aa89c29d447d2ae6b26dfe48008d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24360bf86e8c4bc992181ea3523f0782","placeholder":"​","style":"IPY_MODEL_4a880d3b271c4f0cb182abd22bb278fe","value":" 190/190 [00:00&lt;00:00, 10.5kB/s]"}},"61b145aaf04843739562e7bbb5cd736c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5869905b3a2145759d30164cea51604b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1472943eddcf49d591fff2cd26ca3b4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdc54f2767f4425982f861177484fd12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d50d0722b27346058b82937177b67a5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24360bf86e8c4bc992181ea3523f0782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a880d3b271c4f0cb182abd22bb278fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}