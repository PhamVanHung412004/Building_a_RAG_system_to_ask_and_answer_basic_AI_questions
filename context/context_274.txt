Direct Preference Optimization: 
Your Language Model is Secretly a Reward Model
Nguyen Dinh Huan
introduction to 
reinforcement learning
proximal policy 
optimization (ppo)
02
reinforcement 
learning from human 
feedback (rlhf)
03
direct preference 
optimization (dpo)
04
agenda
01
introduction to 
reinforcement learning
01
reinforcement learning
â€¢
Há»c tÄƒng cÆ°á»ng (Reinforcement learning - RL) lÃ  má»™t nhÃ¡nh cá»§a há»c mÃ¡y (Machine learning -
ML), nghiÃªn cá»©u cÃ¡ch thá»©c má»™t thá»±c thá»ƒ(Agent) trong má»™t mÃ´i trÆ°á»ng (Environment) Ä‘ang á»Ÿ 
má»™t tráº¡ng thÃ¡i (State) thá»±c hiá»‡n má»™t hÃ nh Ä‘á»™ng (Action) Ä‘á»ƒ tá»‘i Æ°u hÃ³a má»™t pháº§n thÆ°á»Ÿng 
(Reward) chung.
â€¢
Há»c tÄƒng cÆ°á»ng lÃ  má»™t phÆ°Æ¡ng phÃ¡p phá»• biáº¿n Ä‘á»ƒ giáº£i cÃ¡c bÃ i toÃ¡n quyáº¿t Ä‘á»‹nh Markov (Markov 
Decision Process - MDP).
reinforcement learning
Má»¥c tiÃªu cá»§a RL lÃ  Ä‘á»ƒ thá»±c hiá»‡n má»™t chuá»—i cÃ¡c hÃ nh Ä‘á»™ng Ä‘á»ƒ sao cho tá»•ng pháº§n 
thÆ°á»Ÿng nháº­n láº¡i lÃ  lá»›n nháº¥t.
Reinforment learning
markov decision process (mdp)
TÃ­nh cháº¥t cá»§a Markov: 
â—
Pháº§n thÆ°á»Ÿng vÃ  tráº¡ng thÃ¡i cá»§a tÆ°Æ¡ng 
lai chá»‰ phá»¥ thuá»™c vÃ o hÃ nh Ä‘á»™ng vÃ  
tráº¡ng thÃ¡i á»Ÿ hiá»‡n táº¡i chá»© khÃ´ng pháº£i 
táº¥t cáº£ hÃ nh Ä‘á»™ng vÃ  tráº¡ng thÃ¡i á»Ÿ quÃ¡ 
khá»©.
Markov Process hay Markov Chain
â—
Markov Process : lÃ  má»™t chuá»—i cÃ¡c tráº¡ng thÃ¡i ngáº«u nhiÃªn tuÃ¢n theo tÃ­nh cháº¥t cá»§a Markov (hay 
cÃ²n gá»i lÃ  Memoryless random process)
â—
Markov Process bao gá»“m cÃ¡c thÃ nh pháº§n nhÆ° sau:
â—‹
ğ“¢lÃ  má»™t táº­p (há»¯u háº¡n) cÃ¡c tráº¡ng thÃ¡i (ğ’”âˆˆğ“¢)
â—‹
ğ“ŸlÃ  phÃ©p chuyá»ƒn Ä‘á»•i tráº¡ng thÃ¡i tá»«s sang sâ€™ ğ’‘ğ’”ğ’•+ğŸ= ğ’”â€² ğ’”ğ’•= ğ’”
â—
CÃ³ thá»ƒtháº¥y lÃ  á»Ÿ Ä‘Ã¢y chÆ°a cÃ³ thÃ nh pháº§n vá»pháº§n thÆ°á»Ÿng vÃ  hÃ nh Ä‘á»™ng.
â—
Náº¿u mÃ  cÃ³ N há»¯u háº¡n cÃ¡c tráº¡ng thÃ¡i thÃ¬ phÃ©p chuyá»ƒn Ä‘á»•i P cÃ³ thá»ƒÄ‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng 1 
ma tráº­n.
ğ‘ƒ=
ğ‘ƒ(ğ‘ 1|ğ‘ 1)
â‹¯
ğ‘ƒ(ğ‘ ğ‘|ğ‘ 1)
â‹®
â‹±
â‹®
ğ‘ƒ(ğ‘ 1|ğ‘ ğ‘)
â‹¯
ğ‘ƒ(ğ‘ ğ‘|ğ‘ ğ‘)
Markov Process hay Markov Chain
Markov Reward Process (MRP)
â—
Markov Reward Process: LÃ  Markov Proces mÃ  cÃ³ thÃªm pháº§n thÆ°á»Ÿng.
â—
Markov Reward Process bao gá»“m:
â—‹
ğ“¢lÃ  má»™t táº­p (há»¯u háº¡n) cÃ¡c tráº¡ng thÃ¡i (ğ’”âˆˆğ“¢)
â—‹
ğ“ŸlÃ  phÃ©p chuyá»ƒn Ä‘á»•i tráº¡ng thÃ¡i tá»«s sang sâ€™ ğ’‘ğ’”ğ’•+ğŸ= ğ’”â€² ğ’”ğ’•= ğ’”
â—‹
ğ“¡lÃ  hÃ m pháº§n thÆ°á»Ÿng ğ“¡ğ’”ğ’•= ğ’”= ğ”¼[ğ’“ğ’•|ğ’”ğ’•= ğ’”]
â—‹
Kháº¥u hao ğœ¸âˆˆ0,1
â—
CÃ³ thá»ƒtháº¥y ráº±ng á»Ÿ Ä‘Ã¢y váº«n chÆ°a cÃ³ Ä‘á»‹nh nghÄ©a nÃ o vá»hÃ nh Ä‘á»™ng.
â—
Náº¿u mÃ  cÃ³ N há»¯u háº¡n cÃ¡c tráº¡ng thÃ¡i thÃ¬ hÃ m pháº§n thÆ°á»Ÿng R cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i 
dáº¡ng 1 vector
ğ‘…=
ğ‘…(ğ‘ 1)
â‹®
ğ‘…(ğ‘ ğ‘)
Markov Reward Process (MRP)
markov decision process (mdp)
â—
Markov Decision Process (MDP): lÃ  má»™t cÃ´ng cá»¥ toÃ¡n há»c dÃ¹ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a bÃ i toÃ¡n 
decision-making trong má»™t mÃ´i trÆ°á»ng biáº¿n Ä‘á»™ng. Má»¥c Ä‘Ã­ch cá»§a MDP lÃ  Ä‘á»ƒtÃ¬m ra má»™t chiáº¿n
lÆ°á»£c (Policy) tá»‘i Æ°u Ä‘á»ƒcÃ³ Ä‘Æ°á»£c tá»•ng pháº§n thÆ°á»Ÿng tá»‘i Ä‘a.
â—
MDP gá»“m cÃ³:
â—‹
ğ“¢lÃ  má»™t táº­p (há»¯u háº¡n) cÃ¡c tráº¡ng thÃ¡i (ğ’”âˆˆğ“¢)
â—‹
ğ“lÃ  má»™t táº­p (há»¯u háº¡n) cÃ¡c hÃ nh Ä‘á»™ng cÃ³ thá»ƒthá»±c hiá»‡n (ğ’‚âˆˆğ“)
â—‹
ğ“Ÿğ’”ğ’‚lÃ  phÃ©p chuyá»ƒn Ä‘á»•i tráº¡ng thÃ¡i tá»«s sang sâ€™ khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng
ğ’‚(ğ’‘ğ’”ğ’•+ğŸ= ğ’”â€² ğ’”ğ’•= ğ’”, ğ’‚ğ’•= ğ’‚)
â—‹
ğ“¡lÃ  hÃ m pháº§n thÆ°á»Ÿng ğ“¡ğ’”ğ’•= ğ’”= ğ”¼[ğ’“ğ’•|ğ’”ğ’•= ğ’”, ğ’‚ğ’•= ğ’‚]
â—‹
Kháº¥u hao ğœ¸âˆˆ0,1
ğ‘´ğ‘«ğ‘·(ğ“¢, ğ“, ğ“Ÿğ’”ğ’‚, ğ“¡, ğœ¸)
markov decision process (mdp)
return
â—
Return (Gt) lÃ  tá»•ng giÃ¡ trá»‹ cÃ¡c pháº§n thÆ°á»Ÿng kháº¥u hao nháº­n Ä‘Æ°á»£c tÃ­nh tá»« thá»i Ä‘iá»ƒm t trá»Ÿ Ä‘i
ğºğ‘¡= ğ‘…ğ‘¡+ ğ›¾ğ‘…ğ‘¡+1 + ğ›¾2ğ‘…ğ‘¡+2 + â‹¯= à·
ğ‘˜=0
âˆ
ğ›¾ğ‘˜ğ‘…ğ‘¡+ğ‘˜
â—
ğœ¸lÃ  discount factor (kháº¥u hao) cÃ³ giÃ¡ trá»‹trong khoáº£ng [0,1] cÃ¡i nÃ y add vÃ o cÃ³ 2 má»¥c Ä‘Ã­ch:
â—‹
VÃ¬ tá»•ng nÃ y lÃ  vÃ´ háº¡n nÃªn cÃ³ discount factor vÃ o Ä‘á»ƒÄ‘áº£m báº£o tiáº¿n tá»›i 1 giÃ¡ trá»‹cá»‘Ä‘á»‹nh
thay vÃ¬ vÃ´ cá»±c.
â—‹
Láº¥y cáº£m há»©ng tá»«bÃªn khÃ¡i niá»‡m cá»§a kinh táº¿lÃ  Time value of Money (TÃ­nh Present 
Value dá»±a vÃ o Future Value).
â– 
ğ‘ƒğ‘‰=
ğ¹ğ‘‰
(1+ğ‘Ÿ)ğ‘›= ğ›¾ğ‘›ğ¹ğ‘‰Ä‘áº·ğ‘¡
1
(1+ğ‘Ÿ) ğ‘™Ã  ğ›¾
Value function
â—
State - value function lÃ  cÃ¡ch Ä‘á»ƒdá»±Ä‘oÃ¡n
pháº§n thÆ°á»Ÿng trong tÆ°Æ¡ng lai. 
â—
ThÆ°á»ng dÃ¹ng Ä‘á»ƒso sÃ¡nh Ä‘á»™tá»‘t/khÃ´ng tá»‘t
cá»§a má»™t tráº¡ng thÃ¡i (state)
â—
Tá»«káº¿t quáº£cá»§a value function ta cÃ³ thá»ƒcÄƒn
cá»©Ä‘á»ƒchá»n hÃ nh Ä‘á»™ng tÆ°Æ¡ng á»©ng.
ğ‘£ğ‘ = ğ”¼ğºğ‘¡ğ‘†ğ‘¡= ğ‘ 
= ğ”¼ğ‘…ğ‘¡+ ğ›¾ğ‘…ğ‘¡+1 + ğ›¾2ğ‘…ğ‘¡+2 + â‹¯ğ‘†ğ‘¡= ğ‘ 
= â„›ğ‘ + ğ›¾à·
ğ‘ âˆˆğ’®
ğ’«ğ‘ â†’ğ‘ â€² ğ‘£ğ‘ â€²
policy
â—
Policy lÃ  má»™t mÃ´ hÃ¬nh tÆ°á»£ng trÆ°ng cho 
hÃ nh vi cá»§a agent.
â—
NÃ³ lÃ  má»™t hÃ m Ã¡nh xáº¡ tá»« tráº¡ng thÃ¡i cá»§a 
mÃ´i trÆ°á»ng sang hÃ nh Ä‘á»™ng cá»§a agent.
â—
Policy lÃ  má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a táº¥t 
cáº£ cÃ¡c hÃ nh Ä‘á»™ng cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c 
khi á»Ÿ má»™t tráº¡ng thÃ¡i nháº¥t Ä‘á»‹nh.
â—
Policy trong Markov Decision Process chá»‰ 
phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i hiá»‡n táº¡i
ğœ‹ğ‘ğ‘ = ğ‘ƒğ´ğ‘¡= ğ‘ğ‘†ğ‘¡= ğ‘ ]
Value function when having policy
â—
State - value function trong MDP khi cÃ³ 1 
Policy lÃ :
ğ‘£ğ‘ = â„›ğ‘ + ğ›¾à·
ğ‘ âˆˆğ’®
ğ’«ğ‘ â†’ğ‘ â€²
ğ‘
ğ‘£ğ‘ â€²
â—
Khi cÃ³ policy action a sáº½ Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh 
dá»±a theo policy:
ğ‘= ğœ‹(ğ‘ )
â—
Action â€“ value function:
ğ‘ğ‘ , ğ‘= â„›ğ‘ , ğ‘+ ğ›¾à·
ğ‘ âˆˆğ’®
ğ’«(ğ‘ â€²|ğ‘ , ğ‘) ğ‘£ğœ‹ğ‘ â€²
Value function when having policy
model
â—
Model lÃ  1 mÃ´ hÃ¬nh cá»‘ gáº¯ng giáº£ láº­p láº¡i 
nhá»¯ng hÃ nh vi cá»§a mÃ´i trÆ°á»ng.
â—
Model sáº½há»c 2 pháº§n cá»§a MDP lÃ  ğ“Ÿğ’”ğ’”â€²
ğ’‚
(Transition probability) vÃ  ğ“¡ğ’”
ğ’‚(Reward 
function).
â—‹
ğ“Ÿğ’”ğ’”â€²
ğ’‚
dá»±Ä‘oÃ¡n tráº¡ng thÃ¡i káº¿tiáº¿p cá»§a
mÃ´i trÆ°á»ng. 
ğ’«ğ‘ ğ‘ â€²
ğ‘= â„™ğ‘†ğ‘¡+1 = ğ‘ â€² ğ‘†ğ‘¡= ğ‘ , ğ´ğ‘¡= ğ‘
â—‹
ğ“¡ğ’”
ğ’‚dá»±Ä‘oÃ¡n pháº§n thÆ°á»Ÿng (tá»©c thá»i) 
káº¿tiáº¿p.
â„›ğ‘ 
ğ‘= ğ”¼ğ‘…ğ‘¡+1 = ğ‘ â€² ğ‘†ğ‘¡= ğ‘ , ğ´ğ‘¡= ğ‘
Categorizing RL agents
Value based
Policy based
o
No Policy (Implicit)
o
Value Function
o
Policy
o
No Value Function
Categorizing RL agents
Model free
model based
o
Policy and/or Value Function
o
No Model
o
Policy and/or Value Function
o
Model
Categorizing RL agents
o
Policy
o
Value Function
Actor-critic
Find optimal policy through value function
â—
Náº¿u Policy ğœ‹âˆ—cÃ³ giÃ¡ trá»‹lá»›n nháº¥t táº¡i má»i
tráº¡ng thÃ¡i ğ‘ so vá»›i cÃ¡c policy khÃ¡c, ta gá»i ğœ‹âˆ—
lÃ  chiáº¿n lÆ°á»£c tá»‘i Æ°u.
â—
TÆ°Æ¡ng tá»±ta cÃ³ state-value function, action-
value function tá»‘i Æ°u khi sá»­dá»¥ng policy tá»‘i
Æ°u.
ğ‘£âˆ—ğ‘ = ğ‘£ğœ‹âˆ—ğ‘ â‰¥ğ‘£ğœ‹ğ‘ ,âˆ€ğ‘ , âˆ€ğœ‹= max ğ‘£ğœ‹ğ‘ 
ğ‘âˆ—ğ‘ , ğ‘= ğ‘ğœ‹âˆ—ğ‘ , ğ‘â‰¥ğ‘ğœ‹ğ‘ , ğ‘, âˆ€ğ‘ , âˆ€ğ‘, âˆ€ğœ‹
= max ğ‘ğœ‹ğ‘ , ğ‘
â—
Khi Ä‘Ã³ Policy tá»‘i Æ°u sáº½ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch:
ğœ‹âˆ—ğ‘ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘£ğœ‹ğ‘ 
ğœ‹âˆ—ğ‘ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘ğœ‹ğ‘ , ğ‘
Policy evaluation
â—
Khi chÃºng ta cÃ³ má»™t Policy cá»‘Ä‘á»‹nh trong
Markov Decision Process thÃ¬ chÃºng ta cÃ³
thá»ƒÄ‘o lÆ°á»ng Ä‘á»™tá»‘t/tá»‡cá»§a Policy Ä‘Ã³ báº±ng
Markov Reward Process.
â—
Khi Ä‘Ã³ Markove Reward Process sáº½bao 
gá»“m 4 pháº§n (ğ’®, â„›ğœ‹, ğ’«ğœ‹, ğ›¾) vá»›i:
ğ’«ğœ‹(ğ‘ â€²|ğ‘ ) = à·
ğ‘âˆˆğ’œ
ğœ‹ğ‘ğ‘ ğ’«(ğ‘ â€²|ğ‘ , ğ‘)
â„›ğœ‹(ğ‘ ) = à·
ğ‘âˆˆğ’œ
ğœ‹ğ‘ğ‘ â„›(ğ‘ , ğ‘)
â—
Khi Ä‘Ã³ chÃºng ta cÃ³ 2 cÃ¡ch Ä‘á»ƒÄ‘Ã¡nh giÃ¡ 1 cÃ¡i
Policy lÃ  tá»‘t hay khÃ´ng:
â—‹
Monte Carlo (MC) Policy Evaluation
â—‹
Temporal Difference (TD) Policy 
Evaluation
â—
CÃ¡ch tÃ¬m Policy tá»‘i Æ°u theo phÆ°Æ¡ng phÃ¡p vÃ²ng láº·p cáº£i tiáº¿n:
â—
GÃ¡n i= 0
â—
Khá»Ÿi táº¡o ngáº«u nhiÃªn Policy ğœ‹0 ğ‘ cho táº¥t cáº£cÃ¡c state s
â—
While i==0 hay ğœ‹ğ‘–âˆ’ğœ‹ğ‘–âˆ’1 1 > 0 (Kiá»ƒm tra xem Policy cÃ²n thay Ä‘á»•i hay khÃ´ng)
â—‹
ğ‘‰ğœ‹ğ‘–â†TÃ­nh ra giÃ¡ trá»‹Value báº±ng cho Policy ğœ‹ğ‘–
â—‹
ğœ‹ğ‘–+1 â†Cáº£i thiá»‡n Policy
â—‹
i = i+1
Policy iteration
Policy Evaluation â€“ Monte Carlos
â—
Trong MC Policy Evaluation chÃºng ta 
sáº½cháº¡y giáº£láº­p nhiá»u láº§n. Má»—i láº§n cháº¡y
giáº£láº­p sáº½cÃ³ má»™t chuá»—i cÃ¡c hÃ nh Ä‘á»™ng
khÃ¡c nhau Ä‘Æ°á»£c táº¡o ra nhá»Policy ğœ‹.
â—
ChÃºng ta tÃ­nh return cho má»—i láº§n cháº¡y
báº±ng cÃ´ng thá»©c:
ğºğ‘¡= ğ‘…ğ‘¡+ ğ›¾ğ‘…ğ‘¡+1 + ğ›¾2ğ‘…ğ‘¡+2 + â‹¯
â—
Sau Ä‘Ã³ tÃ­nh Value báº±ng cÃ¡ch láº¥y trung
bÃ¬nh cÃ¡c return:
ğ‘‰ğœ‹ğ‘ = ğ‘šğ‘’ğ‘ğ‘›ğºğ‘¡
â—
Cáº­p nháº­t láº¡i Value báº±ng cÃ¡ch cháº¡y
vÃ²ng láº·p, á»Ÿ vÃ²ng láº·p thá»©i:
ğ‘‰ğœ‹ğ‘ = ğ‘‰ğœ‹ğ‘ + ğ›¼(ğºğ‘–,ğ‘¡âˆ’ğ‘‰ğœ‹ğ‘ )
Policy Evaluation â€“ Monte Carlos
â—
Háº¡n cháº¿:
â—‹
Pháº£i Ä‘á»£i Ä‘áº¿n khi háº¿t má»—i láº§n cháº¡y má»›i cÃ³ return vÃ  má»›i tÃ­nh V Ä‘Æ°á»£c.
â—‹
YÃªu cáº§u má»—i láº§n cháº¡y pháº£i cÃ³ Ä‘iá»ƒm dá»«ng náº¿u khÃ´ng sáº½ cháº¡y khÃ´ng ngá»«ng.
â—‹
Cáº§n pháº£i cháº¡y nhiá»u láº§n (cáº§n nhiá»u data).
â—
Æ¯u Ä‘iá»ƒm:
â—‹
KhÃ´ng cáº§n giáº£ Ä‘á»‹nh cÃ¡c state pháº£i tuÃ¢n theo tÃ­nh cháº¥t cá»§a Markov.
â—‹
KhÃ´ng cáº§n mÃ´ hÃ¬nh reward vÃ  transition cá»§a MDP
Policy Evaluation â€“ temporal difference (TD(0))
â—
Temporal Difference muá»‘n tÃ¬m má»™t cÃ¡ch khÃ¡c Ä‘á»ƒ
cÃ³ thá»ƒÆ°á»›c lÆ°á»£ng Ä‘Æ°á»£c Gt mÃ  khÃ´ng cáº§n pháº£i Ä‘á»£i
Ä‘áº¿n háº¿t láº§n giáº£láº­p.
ğ‘‰ğœ‹ğ‘ = ğ‘‰ğœ‹ğ‘ + ğ›¼([ğ‘Ÿğ‘¡+ ğ›¾ğ‘‰ğœ‹ğ‘ ğ‘¡+1 ] âˆ’ğ‘‰ğœ‹ğ‘ )
Æ¯á»›c lÆ°á»£ng Gt báº±ng
cÃ¡ch nhÃ¬n vá»phÃ­a
trÆ°á»›c 1 bÆ°á»›c
Value Function Approximation
V(s) vÃ  Q(s,a) thÆ°á»ng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vector hay lÃ  ma tráº­n, nhÆ°ng trong thá»±c táº¿ 
cÃ³ nhá»¯ng mÃ´i trÆ°á»ng cÃ³ ráº¥t nhiá»u tráº¡ng thÃ¡i hoáº·c hÃ nh Ä‘á»™ng. 
Value Function Approximation
Value Function Approximation
â—
Lá»£i Ã­ch khi Ã¡p dá»¥ng value function approximation:
â—‹
Khiáº¿n agent trá»Ÿ nÃªn Generalization (tá»•ng quÃ¡t hÃ³a) Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c cÃ¡ch Ä‘Æ°a ra 
hÃ nh Ä‘á»™ng khi gáº·p má»™t tráº¡ng thÃ¡i mÃ  agent chÆ°a tá»«ng gáº·p.
â—‹
Giáº£m bá»™ nhá»› lÆ°u trá»¯.
â—‹
Giáº£m tÃ i nguyÃªn tÃ­nh toÃ¡n.
Value Function Approximation â€“ optimize weight
â—
TÃ¬m ğ‘¤sao cho L2 error giá»¯a giÃ¡ trá»‹xáº¥p xá»‰tá»«value function à·œğ‘£(ğ‘ , ğ‘¤) vÃ  giÃ¡ trá»‹thá»±c táº¿tá»«value 
function ğ‘£ğœ‹ğ‘ lÃ  nhá»nháº¥t.
ğ½ğ‘¤= ğ”¼ğœ‹[( à·œğ‘£ğ‘ , ğ‘¤âˆ’ğ‘£ğœ‹ğ‘ )2]
â—
Gradient descent thÃ¬ w Ä‘Æ°á»£c cáº­p nháº­t báº±ng cÃ¡ch:
ğ‘¤= ğ‘¤âˆ’ğ›¼ğ‘‘ğ½ğ‘¤
ğ‘‘ğ‘¤
= ğ‘¤âˆ’ğ›¼( à·œğ‘£ğ‘ , ğ‘¤âˆ’ğ‘£ğœ‹ğ‘ )ğ‘‘à·ğ‘£ğ‘ , ğ‘¤
ğ‘‘ğ‘¤
ğ‘‘ğ½(ğ‘¤)
ğ‘‘ğ‘¤
= 1
2 ğ”¼ğœ‹
à·œğ‘£ğ‘ , ğ‘¤âˆ’ğ‘£ğœ‹ğ‘ 
2
â€²
= 1
2 2ğ”¼ğœ‹à·œğ‘£ğ‘ , ğ‘¤âˆ’ğ‘£ğœ‹ğ‘ 
ğ‘‘à·œğ‘£ğ‘ , ğ‘¤âˆ’ğ‘£ğœ‹ğ‘ 
ğ‘‘ğ‘¤
= ( à·œğ‘£ğ‘ , ğ‘¤âˆ’ğ‘£ğœ‹ğ‘ ) ğ‘‘à·ğ‘£ğ‘ , ğ‘¤
ğ‘‘ğ‘¤
Value Function Approximation â€“ optimize weight
â—
Gá»i âˆ…(ğ‘ ) lÃ  1 linear layer hay neural network dÃ¹ng Ä‘á»ƒ encode tráº¡ng thÃ¡i thÃ nh embedding 
vector cá»§a tráº¡ng thÃ¡i hay cÃ³ thá»ƒ gá»i lÃ  feature vector:
âˆ…ğ‘ =
âˆ…1 ğ‘ 
â‹®
âˆ…ğ‘›ğ‘ 
â—
Náº¿u biá»ƒu diá»…n value function dÆ°á»›i dáº¡ng 1 linear layer thÃ¬:
à·œğ‘£ğ‘ , ğ‘¤= âˆ…ğ‘ ğ‘‡ğ‘¤
â—
Trong thá»±c táº¿thÃ¬ ta sáº½thay tháº¿ğ‘£ğœ‹ğ‘ báº±ng return ğºğ‘¡. Khi Ä‘Ã³ ğ‘¤Ä‘Æ°á»£c cáº­p nháº­t báº±ng cÃ¡ch:
ğ‘¤= ğ‘¤âˆ’ğ›¼ğ‘‘ğ½ğ‘¤
ğ‘‘ğ‘¤
= ğ‘¤âˆ’ğ›¼âˆ…ğ‘ ğ‘‡ğ‘¤âˆ’ğºğ‘¡
ğ‘‘âˆ…ğ‘ ğ‘‡ğ‘¤
ğ‘‘ğ‘¤
= ğ‘¤âˆ’ğ›¼âˆ…ğ‘ ğ‘‡ğ‘¤âˆ’ğºğ‘¡âˆ…ğ‘ 
Step-size
Prediction error
Feature value
proximal policy 
optimization (ppo)
02
PPO
â—
PPO: lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»c Policy Ä‘á»ƒcáº£i thiá»‡n
quÃ¡ trÃ¬nh há»c táº­p cá»§a agent má»™t cÃ¡ch á»•n Ä‘á»‹nh báº±ng
cÃ¡ch trÃ¡nh cáº­p nháº­t nhá»¯ng thay Ä‘á»•i lá»›n Ä‘á»‘i vá»›i
Policy.
â—
LÃ½ do nÃªn trÃ¡nh cáº­p nháº­t nhá»¯ng thay Ä‘á»•i lá»›n cho
Policy:
â—‹
Khi cáº­p nháº­t nhá»¯ng thay Ä‘á»•i nhá»vá»›i Policy sáº½
dá»…Ä‘i Ä‘áº¿n káº¿t quáº£tá»‘i Æ°u (Optimal solution).
â—‹
Thay Ä‘á»•i Policy quÃ¡ khÃ¡c biá»‡t vá»›i Policy cÅ© cÃ³
thá»ƒgÃ¢y ra káº¿t quáº£giá»‘ng nhÆ° â€œrÆ¡i xuá»‘ng nÃºiâ€ 
vÃ  sáº½tá»‘n ráº¥t nhiá»u thá»i gian hoáº·c khÃ´ng bao 
giá»tÃ¬m Ä‘Æ°á»£c Policy tá»‘t (leo lÃªn láº¡i nÃºi)
Advantage Actor-Critic (A2C)
â—
Actor â€“ Critic cÃ³ thá»ƒhiá»ƒu á»Ÿ trÆ°á»ng há»£p Ä‘Æ¡n giáº£n
nháº¥t lÃ  cÃ³ 2 model 1 model Ä‘Ã³ng vai lÃ  actor vÃ  1 
model Ä‘Ã³ng vai critic.
â—
Actor lÃ  model dÃ¹ng Ä‘á»ƒhá»c ra policy. Critic lÃ  1 
baseline Ä‘Æ°a ra cÃ¡c pseudo groundtruth
â—
Trong quÃ¡ trÃ¬nh há»c:
â—‹
Critic sáº½ cáº­p nháº­t action-value function Ä‘á»ƒ 
Ä‘Æ°a ra value tá»‘t hÆ¡n (value based).
â—‹
Actor sáº½ cáº­p nháº­t policy dá»±a vÃ o hÆ°á»›ng dáº«n 
cá»§a critic (policy based).
Advantage Actor-Critic (A2C)
â—
Advantage function ğ´lÃ  cÃ¡ch critic dÃ¹ng Ä‘á»ƒ
Ä‘Ã¡nh giÃ¡ má»‘i tÆ°Æ¡ng quan giá»¯a hÃ nh Ä‘á»™ng mÃ 
policy chá»n thá»±c hiá»‡n Ä‘á»‘i vá»›i hÃ nh Ä‘á»™ng khÃ¡c
á»Ÿ trong cÃ¹ng 1 tráº¡ng thÃ¡i. 
ğ´= ğ‘„ğ‘ , ğ‘âˆ’ğ‘‰(ğ‘ )
â—
Náº¿u A > 0 nghÄ©a lÃ  hÃ nh Ä‘á»™ng mÃ  actor chá»n
thá»±c hiá»‡n cho káº¿t quáº£tá»‘t hÆ¡n nhá»¯ng hÃ nh
Ä‘á»™ng khÃ¡c. 
ğ½ğœƒ= ğ”¼ğ‘¡[logğœ‹ğœƒğ‘ğ‘¡ğ‘ ğ‘¡ğ´(ğ‘ğ‘¡,ğ‘ ğ‘¡)]
Generalized advantage estimation (GAE)
â€¢
Q(s,a) cÃ³ thá»ƒ Ä‘Æ°á»£c Æ°á»›c lÆ°á»£ng báº±ng nhiá»u cÃ¡ch nhÆ° sau:
Generalized advantage estimation (GAE)
â€¢
Generalized advantage estimation lÃ  1 cÃ¡ch Ä‘á»ƒ Æ°á»›c lÆ°á»£ng Q(s,a) báº±ng cÃ¡ch káº¿t há»£p giá»¯a 
phÆ°Æ¡ng phÃ¡p TD vÃ  MC:
Generalized advantage estimation (GAE)
Importance Sampling
â—
Importance Sampling lÃ  má»™t ká»¹ thuáº­t Æ°á»›c tÃ­nh giÃ¡ trá»‹ ká»³ 
vá»ng cá»§a f(x) trong Ä‘Ã³ x cÃ³ phÃ¢n phá»‘i dá»¯ liá»‡u p. Tuy nhiÃªn, 
Thay vÃ¬ láº¥y máº«u tá»« p, chÃºng ta sáº½ tÃ­nh káº¿t quáº£ tá»« viá»‡c láº¥y 
máº«u q.
ğ¸ğ‘[ğ‘“(ğ‘¥)] = ğ¸ğ‘(ğ‘“ğ‘‹ğ‘ğ‘‹
ğ‘(ğ‘‹)
)
â—
Äá»ƒcÃ³ thá»ƒviá»‡c Æ°á»›c lÆ°á»£ng Ä‘Æ°á»£c chÃ­nh xÃ¡c thÃ¬ phÃ¢n phá»‘i p vÃ 
q pháº£i gáº§n giá»‘ng nhau cÃ³ nghÄ©a lÃ 
ğ‘ğ‘‹
ğ‘(ğ‘‹) pháº£i khÃ´ng Ä‘Æ°á»£c
cÃ¡ch biá»‡t nhau quÃ¡ lá»›n. Do Ä‘Ã³ tá»‘t nháº¥t thÃ¬ 
ğ‘ğ‘‹
ğ‘(ğ‘‹) chá»‰nÃªn náº±m
trong khoáº£ng nÃ o Ä‘Ã³ 0 Ä‘áº¿n 1.
KL Divergence
â—
Trong thá»‘ng kÃª vÃ  lÃ½ thuyáº¿t thÃ´ng tin, 
Ä‘á»™ Ä‘o Kullbackâ€“Leibler divergence (cÃ²n 
hay gá»i lÃ  Entropy tÆ°Æ¡ng Ä‘á»‘i, viáº¿t táº¯t KL 
divergence) lÃ  má»™t Ä‘á»™ Ä‘o má»©c Ä‘á»™ lá»‡ch 
cá»§a má»™t phÃ¢n bá»‘ Ä‘á»‘i vá»›i phÃ¢n bá»‘ Ä‘Æ°á»£c 
chá»‰ Ä‘á»‹nh. 
â—
NÃ³i má»™t cÃ¡ch Ä‘Æ¡n giáº£n, KL divergence lÃ  
Ä‘á»™ Ä‘o sá»± khÃ¡c nhau giá»¯a hai phÃ¢n bá»‘ xÃ¡c 
suáº¥t.
ğ·ğ¾ğ¿(ğ‘||ğ‘) = âˆ’à·
ğ‘–=1
ğ‘›
ğ‘ğ‘–logğ‘
ğ‘ğ‘–
ğ‘ğ‘–
Policy gradient
Put all together
ğ¿ğ‘‰ğ¹(ğœƒ) = à·¢
ğ”¼ğ‘¡[ ğ‘‰ğœƒğ‘ ğ‘¡âˆ’ğºğ‘¡2]
ğ‘Ÿğ‘¡(ğœƒ) = ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)
ğœ‹ğœƒğ‘œğ‘™ğ‘‘(ğ‘ğ‘¡|ğ‘ ğ‘¡)
ğ¿ğ‘ƒğº(ğ¶ğ¿ğ¼ğ‘ƒ)(ğœƒ) = à·¢
ğ”¼ğ‘¡[min(ğ‘Ÿğ‘¡ğœƒà·¢
ğ´ğ‘¡, ğ‘ğ‘™ğ‘–ğ‘ğ‘Ÿğ‘¡ğœƒ,1 âˆ’ğœ€, 1 + ğœ€à·¢
ğ´ğ‘¡)]
Or
ğ¿ğ‘ƒğº(ğ¾ğ¿ğ‘ƒğ¸ğ‘)(ğœƒ) = à·¢
ğ”¼ğ‘¡[ğ‘Ÿğ‘¡ğœƒà·¢
ğ´ğ‘¡âˆ’ğ›½ğ¾ğ¿(ğœ‹ğœƒ. ğ‘ ğ‘¡, ğœ‹ğœƒğ‘œğ‘™ğ‘‘(. |ğ‘ ğ‘¡))]
ğ¿
ğœƒ= ğ¿ğ‘ƒğº+ ğ‘1ğ¿ğ‘‰ğ¹
A2C
Important sampling
VÃ¬ chÃºng ta láº¥y important 
sampling nÃªn chÃºng ta pháº£i
Ä‘áº£m báº£o 2 phÃ¢n phá»‘i khÃ´ng
nÃªn quÃ¡ khÃ¡c biá»‡t
Put all together
reinforcement 
learning from human 
feedback (rlhf)
03
overview
Pretraining language models 
Reward model training 
Reward model
â—
Objective function cá»§a reward model xuáº¥t
phÃ¡t tá»«Bradley-Terry model. ÄÃ³ lÃ  má»™t mÃ´
hÃ¬nh xÃ¡c suáº¥t dÃ¹ng Ä‘á»ƒso sÃ¡nh káº¿t quáº£tráº£vá»
cá»§a 2 Ä‘á»‘i tÆ°á»£ng. á» Ä‘Ã¢y ta dÃ¹ng nÃ³ Ä‘á»ƒso 
sÃ¡nh Ä‘á»™Æ°a thÃ­ch giá»¯a 2 cÃ¢u sau khi Ä‘Æ°á»£c
con ngÆ°á»i Ä‘Ã¡nh giÃ¡.
ğ‘ğ‘¦1 > ğ‘¦2 =
exp ğ‘Ÿğœƒğ‘¥, ğ‘¦1
exp ğ‘Ÿğœƒğ‘¥, ğ‘¦1
+ exp(ğ‘Ÿğœƒ(ğ‘¥, ğ‘¦2))
ğ‘ğ‘¦1 > ğ‘¦2 = ğœ(ğ‘Ÿğœƒğ‘¥, ğ‘¦1 âˆ’ğ‘Ÿğœƒğ‘¥, ğ‘¦2 )
Reward model
â—
Objective function cá»§a reward model xuáº¥t phÃ¡t tá»«Bradley-Terry model. ÄÃ³ lÃ  má»™t mÃ´ hÃ¬nh
xÃ¡c suáº¥t dÃ¹ng Ä‘á»ƒso sÃ¡nh káº¿t quáº£tráº£vá»cá»§a 2 Ä‘á»‘i tÆ°á»£ng. á» Ä‘Ã¢y ta dÃ¹ng nÃ³ Ä‘á»ƒso sÃ¡nh Ä‘á»™Æ°a
thÃ­ch giá»¯a 2 cÃ¢u sau khi Ä‘Æ°á»£c con ngÆ°á»i Ä‘Ã¡nh giÃ¡.
ğ‘ğ‘¦1 > ğ‘¦2 =
exp ğ‘Ÿğœƒğ‘¥, ğ‘¦1
exp ğ‘Ÿğœƒğ‘¥, ğ‘¦1
+ exp(ğ‘Ÿğœƒ(ğ‘¥, ğ‘¦2))
ğ‘ğ‘¦1 > ğ‘¦2 = ğœ(ğ‘Ÿğœƒğ‘¥, ğ‘¦1 âˆ’ğ‘Ÿğœƒğ‘¥, ğ‘¦2 )
â—
Loss function:
ğ¿ğ‘Ÿğœƒ= âˆ’log(ğœğ‘Ÿğœƒğ‘¥, ğ‘¦1 âˆ’ğ‘Ÿğœƒğ‘¥, ğ‘¦2
)
â—
Khi reward tráº£ra tá»«reward model dÃ¹ng cho PPO tÃ¡c giáº£add thÃªm KL Divergence:
R(ğ‘¥, ğ‘¦) = ğ‘Ÿğœƒğ‘¥, ğ‘¦
âˆ’ğ›½ğ¾ğ¿(ğœ‹ğœƒ
ğ‘…ğ¿ğ‘¦ğ‘¥||ğœ‹ğœƒ(ğ‘œğ‘™ğ‘‘)
ğ‘†ğ¹ğ‘‡(ğ‘¦|ğ‘¥))
Fine-tuning with rl - ppo
ppo
direct preference 
optimization (dpo)
04
overview
â€¢
ÄÃ¢y lÃ  má»™t ká»¹thuáº­t dÃ¹ng Ä‘á»ƒhuáº¥n language model sao cho model Ä‘Æ°a ra cÃ¢u tráº£lá»i Ä‘Ãºng theo
Ã½ ngÆ°á»i dÃ¹ng má»™t cÃ¡ch trá»±c tiáº¿p.
â€¢
TrÆ°á»›c Ä‘Ã¢y cÃ¡c Language model thÆ°á»ng Ä‘Æ°á»£c train theo hÆ°á»›ng maximum likelihood estimation 
(MLE) hoáº·c reinforment learning (RL).
overview 
overview 
Secret behind DPO
â—
Tá»« RLHF ta cÃ³ nhá»¯ng cÃ´ng thá»©c sau:
ğ‘ğ‘¦1 > ğ‘¦2 =
exp ğ‘Ÿğœƒğ‘¥, ğ‘¦1
exp ğ‘Ÿğœƒğ‘¥, ğ‘¦1
+ exp(ğ‘Ÿğœƒ(ğ‘¥, ğ‘¦2))
ğ¿ğ‘Ÿğœƒğ‘…ğ‘€= âˆ’log(ğœğ‘Ÿğœƒğ‘…ğ‘€ğ‘¥, ğ‘¦1 âˆ’ğ‘Ÿğœƒğ‘…ğ‘€ğ‘¥, ğ‘¦2
)
ğ‘Ÿğœƒğ‘…ğ‘€ğ‘–ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
R ğ‘¥, ğ‘¦= ğ‘Ÿğœƒğ‘…ğ‘€ğ‘¥, ğ‘¦
âˆ’ğ›½ğ¾ğ¿(ğœ‹ğœƒ
ğ‘…ğ¿ğ‘¦ğ‘¥||ğœ‹ğœƒ(ğ‘œğ‘™ğ‘‘)
ğ‘†ğ¹ğ‘‡(ğ‘¦|ğ‘¥))
= ğ‘Ÿğœƒğ‘…ğ‘€ğ‘¥, ğ‘¦
âˆ’ğ›½log ğœ‹ğœƒ
ğ‘…ğ¿ğ‘¦ğ‘¥
âˆ’log ğœ‹ğœƒğ‘œğ‘™ğ‘‘
ğ‘†ğ¹ğ‘‡
ğ‘¦ğ‘¥
Secret behind DPO
Secret behind DPO
Secret behind DPO
Demo
Thanks for listening
â—
http://www.juyang.co/reinforcement-learning-ii-markov-decision-process-and-rl-agent/
â—
https://www.davidsilver.uk/teaching/
â—
https://www.researchgate.net/figure/Classification-of-RL-algorithms-inspired-from-UCL-
Course-on-RL-by-David-Silver-41_fig2_346808682
â—
https://www.researchgate.net/figure/Reinforcement-Learning-Agent-
Taxonomy_fig2_327733315
â—
https://medium.com/@khalil.hennara.247/value-function-approximation-dqn-
43df289a3380
â—
https://tek4.vn/phuong-phap-importance-sampling-hoc-tang-cuong
â—
https://jonathan-hui.medium.com/rl-policy-gradients-explained-advanced-topic-
20c2b81a9a8b
â—
https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-
reinforcement-learning-79cf1e83d565
â—
https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-
learning-14ac0b4493cc/
references
â—
https://www.eeweb.com/tools/calculus-derivatives-and-limits-reference-sheet/
â—
https://huggingface.co/blog/deep-rl-ppo
â—
https://arshren.medium.com/unlocking-the-secrets-of-actor-critic-reinforcement-learning-a-
beginners-guide-3c5953b13551
â—
https://huggingface.co/blog/deep-rl-a2c
â—
https://hugocisneros.com/notes/kullback_leibler_divergence/
â—
https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b864
48f951-Paper.pdf
â—
https://arxiv.org/pdf/1707.06347.pdf
â—
https://blog.tylertaewook.com/post/proximal-policy-optimization
â—
https://huggingface.co/learn/deep-rl-course/
â—
https://lilianweng.github.io/posts/2018-02-19-rl-overview/
â—
https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/13-multistep.pdf
â—
https://www.interconnects.ai/p/the-dpo-debate
â—
https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/
references
