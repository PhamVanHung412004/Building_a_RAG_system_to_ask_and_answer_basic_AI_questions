<<<<<<< HEAD
Node ID: eed8168c-45ce-42f8-9e8b-d9d486fe7435
Text: Tương tự như trong kiến trúc VQ-VAE. Khối Decoder: Khối decoder
của C-ViViT đơn giản là một khối encoder được lật ngược. Đầu tiên
token được chuyển đổi thành các embedding. Sau đó là temporal
transformer, tiếp theo là spatial transformer. Đầu ra sau đó được áp
dụng một phép chiếu tuyến tính đơn để ánh xạ các token trở lại không
gian pixel. 4.3.2...
=======
Tương tự như trong kiến trúc VQ-VAE.
Khối Decoder: Khối decoder của C-ViViT đơn giản là một khối encoder được lật ngược. Đầu
tiên token được chuyển đổi thành các embedding. Sau đó là temporal transformer, tiếp theo là spatial
transformer. Đầu ra sau đó được áp dụng một phép chiếu tuyến tính đơn để ánh xạ các token trở lại
không gian pixel.
4.3.2 Tạo sinh video từ văn bản với bidirectional transformers
Hình 11: Quá trình training bidirectional transformer
Việc chuyển đổi từ văn bản sang video có thể được định nghĩa như một tác vụ sequence-to-sequence,
thường được giải quyết bằng cách sử dụng các mô hình autoregressive transformer dự đoán tuần tự các
token video từ nhúng văn bản. Tuy nhiên, cách tiếp cận này trở nên không hiệu quả cho các video dài
do thời gian lấy mẫu tăng tuyến tính.
Để cải thiện hiệu quả, tác giả sử dụng mô hình bidirectional transformer có khả năng dự đoán đồng
thời nhiều token video, giảm đáng kể thời gian lấy mẫu bất kể độ dài của chuỗi video. Trong quá trình
huấn luyện, một phần các token video được che kín và các token này được dự đoán bằng các embedding
văn bản và các token không bị che kín, với mô hình học thông qua hàm loss cross-entropy.
Phương pháp này, lấy cảm hứng từ các kỹ thuật như được sử dụng trong MaskGIT, giảm đáng kể
số bước lấy mẫu cần thiết (thông thường từ 12 đến 48 bước), có khả năng cải thiện chất lượng video
được tạo ra trong khi đảm bảo quá trình xử lý nhanh hơn.
Quá trình inferenceTrong quá trình inferece, trước tiên tất cả các video token được đánh dấu
là token đặc biệt [MASK]. Sau đó, tại mỗi bước inference, tất cả các token video bị che được dự đoán
cùng một lúc dựa trên các embedding văn bản và các token video không bị che (đã dự đoán). Sau đó,
một tỷ lệβi của các token được dự đoán tại bước lấy mẫu i được giữ lại, và các token còn lại sẽ được
che lại và dự đoán lại ở bước tiếp theo.
11
>>>>>>> HEAD@{1}
