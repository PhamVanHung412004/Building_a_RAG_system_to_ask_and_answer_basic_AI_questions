Applications of 
Transformer Encoder
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
â¢Layer Normalization
â¢Transformers Block
â¢BERT and Text Classification
â¢Vision Transformer and Image Classification
â¢For Time-series and Tabular Data
Outline
https://arxiv.org/pdf/1803.08494.pdf
ğœğ‘=
1
ğ‘Ã— ğ»Ã— ğ‘Šà·
ğ‘–=1
ğ‘
à·
ğ‘—=1
ğ»
à·
ğ‘˜=1
ğ‘Š
ğ¹ğ‘–ğ‘—ğ‘˜âˆ’ğœ‡ğ‘
2
ğœ‡ğ‘=
1
ğ‘Ã— ğ»Ã— ğ‘Šà·
ğ‘–=1
ğ‘
à·
ğ‘—=1
ğ»
à·
ğ‘˜=1
ğ‘Š
ğ¹ğ‘–ğ‘—ğ‘˜
Normalization
AI VIETNAM
All-in-One Course
â– Overview
1
Batch Normalization
ğœ–= 10âˆ’5
AI VIETNAM
All-in-One Course
ğœ‡= [2.0, 3.0]
ğœ2 = [3.5, 3.25]
ğ›¾= 1.0
Î² = 0.0
ğ‘‹=  
, 
à· ğ‘Œ= â€¦
batch-size = 2
input_shape = (2, 2, 2, 2)
sample 1 sample 2
6 4
5 2
0 5
3 0
2 3
0 2
1 4
3 0
à· ğ‘‹=  
, 
sample 1 sample 2
1.6 
0.5
1.1 
âˆ’0.5
1.6 
âˆ’1.1
âˆ’1.1 
0.5
âˆ’0.5 1.1
0.5 
âˆ’1.1
âˆ’0.5 
0.0
âˆ’1.6 
âˆ’0.5
Batch-Norm Layer
2
Layer Normalization
AI VIETNAM
All-in-One Course
ğ‘‹= 5 
1
2 
8
shape=(1, 2, 2, 1)
Layer Norm
(mean=4 & std=2.73)
à· ğ‘‹= 0.36 
âˆ’1.09
âˆ’0.73 
1.46
shape=(1, 2, 2, 1)
ğœğ‘›=
1
ğ¶Ã— ğ»Ã— ğ‘Šà·
ğ‘=1
ğ¶
à·
ğ‘—=1
ğ»
à·
ğ‘˜=1
ğ‘Š
ğ¹ğ‘ğ‘—ğ‘˜âˆ’ğœ‡ğ‘›
2
ğœ‡ğ‘›=
1
ğ¶Ã— ğ»Ã— ğ‘Šà·
ğ‘=1
ğ¶
à·
ğ‘—=1
ğ»
à·
ğ‘˜=1
ğ‘Š
ğ¹ğ‘ğ‘—ğ‘˜
https://arxiv.org/pdf/
1607.06450.pdf
3
ğ‘‹=
8 
6
2 
4 ; 0 
2
1 
5
input_shape=(2, 2, 2, 1)
Layer Norm
(mean1=5.0 & mean2=2.0)
 (std1=2.23 & std2=1.87)
shape=(2, 2, 2, 1)
à· ğ‘‹=
1.34 
0.44
âˆ’1.34 âˆ’0.44 ; âˆ’1.06 
0.0 
âˆ’0.53 
1.6
sample 1
sample 2
Layer Normalization
sample 1
sample 2
ğ‘‹=
8 
6
2 
4 , 0 
2
1 
5
input_shape=(1, 2, 2, 2)
Layer Norm
(mean=3.5 & std=2.54)
shape=(1, 2, 2, 2)
à· ğ‘‹=
1.76 
0.98
âˆ’0.58 
0.19 , âˆ’1.37 
âˆ’0.58
âˆ’0.98 
0.58
a sample 
4
Instance Normalization
AI VIETNAM
All-in-One Course
ğœğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’=
1
ğ»Ã— ğ‘Šà·
ğ‘–=1
ğ»
à·
ğ‘—=1
ğ‘Š
ğ¹ğ‘–ğ‘—âˆ’ğœ‡ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’
2
ğœ‡ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’=
1
ğ»Ã— ğ‘Šà·
ğ‘–=1
ğ»
à·
ğ‘—=1
ğ‘Š
ğ¹ğ‘–ğ‘—
ğ‘‹= 1 
5
4 
0
shape=(1, 2, 2, 1)
à· ğ‘‹= âˆ’0.7276 
1.2127
 0.7276 
âˆ’1.2127
shape=(1, 2, 2, 1)
https://arxiv.org/pdf/
1607.08022.pdf
Instance-Norm Layer
ğœ‡= 2.5
ğœ2 = 2.06
5
Instance Normalization
ğœ‡= [2.5, 4.75]
ğœ2 = [4.25, 6.18]
à· ğ‘‹=
âˆ’0.72 
1.21
 0.72 
âˆ’1.21
0.1 
1.3
0.1 
âˆ’1.5
ğ‘‹=  
batch-size = 1
input_shape = (1, 2, 2, 2)
5 8
5 1
1 5
4 0
sample 1
AI VIETNAM
All-in-One Course
ğ›¾= 1.0
Î² = 0.0
ğœ–= 10âˆ’5
ğœğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’=
1
ğ»Ã— ğ‘Šà·
ğ‘–=1
ğ»
à·
ğ‘—=1
ğ‘Š
ğ¹ğ‘–ğ‘—âˆ’ğœ‡ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’
2
ğœ‡ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’=
1
ğ»Ã— ğ‘Šà·
ğ‘–=1
ğ»
à·
ğ‘—=1
ğ‘Š
ğ¹ğ‘–ğ‘—
Instance-Norm Layer
6
Instance Normalization
AI VIETNAM
All-in-One Course
ğœ‡= [2.5, 5.0]
ğœ2 = [4.25, 7.7]
ğ›¾= 1.0
Î² = 0.0
ğ‘‹=  
, 
à· ğ‘Œ= â€¦
batch-size = 2
input_shape = (2, 2, 2, 2)
sample 1 sample 2
9 2
6 3
1 5
4 0
0 2
1 4
6 3
1 7
à· ğ‘‹=  
, 
sample 1 sample 2
1.46 
âˆ’1.09
0.36 
âˆ’0.73
âˆ’0.72 
1.21
 0.72 âˆ’1.21
 0.73 âˆ’0.52
âˆ’1.36 
1.15
âˆ’1.12 
0.16
âˆ’0.5 
âˆ’1.52
Instance-Norm Layer
ğœ–= 10âˆ’5
ğœ‡= [4.25, 1.75]
ğœ2 = [5.68, 2.18]
7
N
C
H, W
Group Normalization
AI VIETNAM
All-in-One Course
ğ‘›ğ‘¢ğ‘š_ğ‘â„ğ‘ğ‘›ğ‘’ğ‘™ğ‘ (ğ¶) = 4
ğ‘›ğ‘¢ğ‘š_ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ (ğº) = 2
ğœğ‘”ğ‘Ÿ=
1
ğ‘šà·
ğ‘˜âˆˆğ‘†ğ‘–
ğ‘¥ğ‘˜âˆ’ğœ‡ğ‘”ğ‘Ÿ
2
ğœ‡ğ‘”ğ‘Ÿ= 1
ğ‘šà·
ğ‘˜âˆˆğ‘†ğ‘–
ğ‘¥ğ‘˜
ğ‘†ğ‘–= ğ‘˜ | ğ‘˜ğ‘›= ğ‘–ğ‘›, á‰
á‰Ÿ
ğ‘˜ğ¶
ğ¶/ğº= á‰
á‰Ÿ
ğ‘–ğ¶
ğ¶/ğº
Group-Norm Layer
ğ‘‹1 =
1 5
4 7 , 1 2
4 0 , 9 2
0 3 , 6 3
1 8
ğ‘‹2 =
5 2
6 3 , 1 7
0 7 , 0 2
3 3 , 1 4
2 5
ğœ‡= 3
ğœ2 = 5
ğœ‡= 4
ğœ2 = 9.5
ğœ‡= 3.8
ğœ2 = 6.6
ğœ‡= 2.5
ğœ2 = 2.25
shape=(2, 2, 2, 4)
à· ğ‘‹1 =
âˆ’0. 89 0.89
0.44 1.78 , âˆ’0.9 âˆ’0.44
0.44 âˆ’1.3 ,
1.62 âˆ’0.64
âˆ’1.29 âˆ’0.32 , 0.64 âˆ’0.32
âˆ’0.97 1.29
à· ğ‘‹2 =
0.43 âˆ’0.72
0.82 âˆ’0.34 , âˆ’1.11 1.21
âˆ’1.5 
1.21 ,
âˆ’1.66 âˆ’0.33
0.33 
0.33 , âˆ’1.0 
1.0
âˆ’0.33 1.67
shape=(2, 2, 2, 4)
ğ‘›ğ‘¢ğ‘š_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ =  2
8
Group Normalization
AI VIETNAM
All-in-One Course
ğ‘›ğ‘¢ğ‘š_ğ‘â„ğ‘ğ‘›ğ‘’ğ‘™ğ‘ = 6
ğ‘›ğ‘¢ğ‘š_ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ = 1
ğ‘›ğ‘¢ğ‘š_ğ‘â„ğ‘ğ‘›ğ‘’ğ‘™ğ‘ = 6
ğ‘›ğ‘¢ğ‘š_ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ = 6
9
â¢Layer Normalization
â¢Transformers Block
â¢BERT and Text Classification
â¢Vision Transformer and Image Classification
â¢For Time-series and Tabular Data
Outline
https://arxiv.org/pdf/
1803.08494.pdf
Input Embedding
(N, Seq_len, E_dim)
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Output
(N, Seq_len, E_dim) 
Transformer Block (-)
10
1
ğ‘§1
ğ‘§2
ğ‘§ğ‘›
Softmax
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥ğ‘›
à·œy1
à·œy2
à·œyğ‘›
â€¦
â€¦
â€¦
X
Conv
Conv
Y
+
Skip Connection
MLP + Softmax
X
Conv
Conv
Y
f
11
Group Normalization
AI VIETNAM
All-in-One Course
Multi-head Attention
Transformer Block
âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
ğ‘Šğ‘„=
âˆ’0.35 
0.51 
0.50
 0.36 âˆ’0.47 âˆ’0.29
âˆ’0.51 âˆ’0.14 âˆ’0.56
ğ‘Šğ¾=
âˆ’0.49 
âˆ’0.68 0.18
âˆ’0.44 
âˆ’0.46 0.18
 0.07 
âˆ’0.10 0.44
ğ‘Šğ‘‰=
âˆ’0.41 
0.39 âˆ’0.65
âˆ’0.40 âˆ’0.07 âˆ’0.34
âˆ’0.55 âˆ’0.13 âˆ’0.29
ğ‘Šğ‘‚=
âˆ’0.36 âˆ’0.08 0.32
 0.27 
0.05 0.15
âˆ’0.05 âˆ’0.28 0.05
ğ‘Œ= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ‘„ğ¾ğ‘‡
ğ‘‘
ğ‘‰
bs = 1 
sequence_length = 2
        embed_dim = 3
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
âˆ’0.029 âˆ’0.028 0.065
âˆ’0.025 âˆ’0.025 0.058
bs = 1 
sequence_length = 2
        embed_dim = 3
12
Multi-head Attention
Transformer Block
âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
âˆ’0.029 âˆ’0.028 0.065
âˆ’0.025 âˆ’0.025 0.058
+
âˆ’0.12 
0.07 
0.36
 0.37 âˆ’1.12 âˆ’0.24
Input Embedding
(N, Seq_len, E_dim)
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Output
(N, Seq_len, E_dim) 
13
ğ‘Œ= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ğ‘„ğ¾ğ‘‡
ğ‘‘
ğ‘‰
Multi-head Attention
Transformer Block
âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
âˆ’0.029 âˆ’0.028 0.065
âˆ’0.025 âˆ’0.025 0.058
+
âˆ’0.12 
0.07 
0.36
 0.37 âˆ’1.12 âˆ’0.24
Input Embedding
(N, Seq_len, E_dim)
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Output
(N, Seq_len, E_dim) 
Layer Norm
âˆ’1.14 âˆ’0.15 1.29
 1.14 âˆ’1.29 0.14
https://openaccess.thecvf.com/content/ICCV2021W/Ne
urArch/papers/Yao_Leveraging_Batch_Normalization_
for_Vision_Transformers_ICCVW_2021_paper.pdf
14
Multi-head Attention
Transformer Block
âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
Input Embedding
(N, Seq_len, E_dim)
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Output
(N, Seq_len, E_dim) 
Layer Norm
âˆ’1.14 âˆ’0.15 1.29
 1.14 âˆ’1.29 0.14
Feed Forward
ğ‘Š=
 0.15 
0.39 âˆ’0.34
âˆ’0.41 
0.54 
0.49
 0.50 âˆ’0.07 âˆ’0.41
0.53 âˆ’0.62 âˆ’0.22
0.78 âˆ’0.26 âˆ’1.08
15
Multi-head Attention
Transformer Block
âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
Input Embedding
(N, Seq_len, E_dim)
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Output
(N, Seq_len, E_dim) 
Layer Norm
âˆ’1.14 âˆ’0.15 1.29
 1.14 âˆ’1.29 0.14
Feed Forward
Layer Norm
âˆ’0.59 âˆ’0.81 
1.40
 1.39 âˆ’0.90 âˆ’0.49
bs = 1 
sequence_length = 2
        embed_dim = 3
bs = 1 
sequence_length = 2
        embed_dim = 3
16
https://arxiv.org/pdf/
1803.08494.pdf
Positional 
Embedding
Input Embedding
+
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
Transformer Models for Text Classification
NÃ—
17
ğ‘£pos
0 , ğ‘£pos
1 , ğ‘£pos
2 , ğ‘£pos
3 , â€¦ , ğ‘£pos
ğ‘‘âˆ’2, ğ‘£pos
ğ‘‘âˆ’1
ğ‘˜= ğ‘‘
2
ğ‘ ğ‘–ğ‘›ğ‘¤ğ‘˜Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤ğ‘˜Ã— pos
ğ‘˜= ğ‘‘âˆ’1
2
ğ‘¤ğ‘˜=
1
ğ‘‡2ğ‘˜/ğ‘‘
ğ’—pos =
ğ‘ ğ‘–ğ‘›ğ‘¤0 Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤0 Ã— pos
ğ‘ ğ‘–ğ‘›ğ‘¤1 Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤1 Ã— pos
â€¦â€¦
ğ‘ ğ‘–ğ‘›ğ‘¤ğ‘‘
2
Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤ğ‘‘âˆ’1
2
Ã— pos
ğ‘‡= 10000
Transformer
AI VIETNAM
All-in-One Course
â– Positional encoding
0 â‰¤pos â‰¤99
ğ‘‡= 10
0 â‰¤d â‰¤127
ğ‘‡= 100
ğ‘‡= 1000
ğ‘‡= 10000
19
0 â‰¤pos â‰¤9
ğ‘‡= 100
0 â‰¤d â‰¤127
0 â‰¤pos â‰¤99
0 â‰¤pos â‰¤499
0 â‰¤pos â‰¤199
ğ‘¤ğ‘˜=
1
ğ‘‡2ğ‘˜/ğ‘‘
ğ’—pos =
ğ‘ ğ‘–ğ‘›ğ‘¤0 Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤0 Ã— pos
ğ‘ ğ‘–ğ‘›ğ‘¤1 Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤1 Ã— pos
â€¦â€¦
ğ‘ ğ‘–ğ‘›ğ‘¤ğ‘‘
2
Ã— pos
ğ‘ğ‘œğ‘ ğ‘¤ğ‘‘âˆ’1
2
Ã— pos
ğ‘‡= 100
0 â‰¤pos â‰¤2
0 â‰¤d â‰¤3
ğ’—=
ğ’—0
ğ’—1
ğ’—2
=
ğ‘ ğ‘–ğ‘›ğ‘¤0 Ã— 0 , ğ‘ğ‘œğ‘ ğ‘¤0 Ã— 0 , ğ‘ ğ‘–ğ‘›ğ‘¤1 Ã— 0 , ğ‘ğ‘œğ‘ ğ‘¤1 Ã— 0
ğ‘ ğ‘–ğ‘›ğ‘¤0 Ã— 1 , ğ‘ğ‘œğ‘ ğ‘¤0 Ã— 1 , ğ‘ ğ‘–ğ‘›ğ‘¤1 Ã— 1 , ğ‘ğ‘œğ‘ ğ‘¤1 Ã— 1
ğ‘ ğ‘–ğ‘›ğ‘¤0 Ã— 2 , ğ‘ğ‘œğ‘ ğ‘¤0 Ã— 2 , ğ‘ ğ‘–ğ‘›ğ‘¤1 Ã— 2 , ğ‘ğ‘œğ‘ ğ‘¤1 Ã— 2
=
0.0 
1.0 
0.0 
1.0 
0.84 
0.54 0.09 0.99
0.91 âˆ’0.41 0.19 0.98
https://kazemnejad.com/blog/transformer
_architecture_positional_encoding/
â– Positional encoding
â¢Layer Normalization
â¢Transformers Block
â¢BERT and Text Classification
â¢Vision Transformer and Image Classification
â¢For Time-series and Tabular Data
Outline
-
50,000 movie review for sentiment analysis (data)
-
Consist of:
+ 25,000 movie review for training
+ 25,000 movie review for testing
-
Label: positive â€“ negative = 1 â€“ 1
â€œA wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-
BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece 
â€¦ â€
positive
â€œThis show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years 
were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's 
continued its decline further to the complete waste of time it is todayâ€¦.â€
negative
â€œI thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air 
conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty 
and the characters are likable (even the well bread suspected serial killer)â€¦.â€
positive
â€œBTW Carver gets a very annoying sidekick who makes you wanna shoot him the first three minutes he's 
on screen.â€
negative
Text Classification
AI VIETNAM
All-in-One Course
â– IMDB dataset
Positional 
Embedding
Input Embedding
+
Transformer Models for Text Classification
â– Embedding
23
Transformer Models for Text Classification
â– Transformer block
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
24
Positional 
Embedding
Input Embedding
+
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
NÃ—
Transformer Models for Text Classification
25
Transformer Models 
for Text Classification
â– Results
40
50
60
70
80
90
100
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Training Accuracy
Test Accuracy
Train Transformer from Scratch
Test accuracy: ~82%
transfer learning
Text Deep Models
Long short-term memory
Word-1
Word-2
Word-500
LSTM Cell
LSTM Cell
LSTM Cell
LSTM Cell
LSTM Cell
LSTM Cell
hidden_dim=64
dim=2
embed_dim = 128
50
55
60
65
70
75
80
85
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Train Accuracy
Test Accuracy
(RNN) Test accuracy: ~68%
50
60
70
80
90
100
110
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Train Accuracy
Test Accuracy
(LSTM) Test accuracy: ~87%
27
-
Training samples: 7613
-
Total Number of disaster tweets: 4342
-
Total Number of non-disaster tweets: 3271
Text Classification
AI VIETNAM
All-in-One Course
â– Tweets dataset
28
Text Classification
AI VIETNAM
All-in-One Course
â– Tweets dataset
Text Classification
AI VIETNAM
All-in-One Course
â– Tweets dataset
Transformer Models for Text Classification
Positional 
Embedding
Input Embedding
+
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
NÃ—
31
Text Classification
AI VIETNAM
All-in-One Course
â– Results
40
50
60
70
80
90
100
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Training Accuracy
Test Accuracy
Train Transformer from Scratch
Test accuracy: ~78%
Model Inputs
Model Architecture
Trained on Wikipedia (~2.5B words) and Googleâ€™s 
BooksCorpus (~800M words)
64 TPUs trained BERT over the course of 4 days
DistilBERT offers a lighter version of BERT; runs 60% 
faster while maintaining over 95% of BERTâ€™s performance.
https://arxiv.org/pdf/1810.04805.pdf
Bidirectional Encoder 
Representations from Transformers
Bidirectional Encoder 
Representations from Transformers
https://huggingface.co/blog/bert-101
35
https://github.com/jessevig/bertviz
Attention Visualization
â¢Layer Normalization
â¢Transformers Block
â¢BERT and Text Classification
â¢Vision Transformer and Image Classification
â¢For Time-series and Tabular Data
Outline
ğ‘Œ= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ‘„ğ¾ğ‘‡
ğ‘š
ğ‘‰
ğ‘‚= ğ´ğ‘Šğ‘‚
word-0
word-1
word-n
â€¦
ğ‘‹0
ğ‘‹1
ğ‘‹ğ‘›
â€¦
ğ‘„0
ğ¾0
ğ‘‰0
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„1
ğ¾1
ğ‘‰1
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„ğ‘›
ğ¾ğ‘›
ğ‘‰ğ‘›
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
Self-Attention
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
Vision Transformer
Embedding
37
Projection and Embedding
ğ‘Œ= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ‘„ğ¾ğ‘‡
ğ‘š
ğ‘‰
ğ‘‚= ğ´ğ‘Šğ‘‚
patch-0
patch-1
patch-n
â€¦
ğ‘‹0
ğ‘‹1
ğ‘‹ğ‘›
â€¦
ğ‘„0
ğ¾0
ğ‘‰0
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„1
ğ¾1
ğ‘‰1
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„ğ‘›
ğ¾ğ‘›
ğ‘‰ğ‘›
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
Self-Attention
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
Vision Transformer
38
Vision Transformer
â–From text to image
AI VIETNAM
All-in-One Course
39
Vision Transformer
â–Get patches
AI VIETNAM
All-in-One Course
40
Vision Transformer
â–Get patches
AI VIETNAM
All-in-One Course
41
Vision Transformer
â–Get patches
AI VIETNAM
All-in-One Course
1
2
3
4
5
6
7
8
9
42
Vision Transformer
â–Patch and position embedding
Linear
Linear
Linear
Linear
Linear
Linear
Linear
Linear
Linear
4800
128
0
1
2
3
4
5
6
7
8
128
128
position 
embedding
+
+
+
+
+
+
+
+
+
flatten
Pretrained Vision 
Transformer
â–JFT-300M
Internal Google dataset
300M images
For training image classification models
Vision Transformer
From text to image
https://arxiv.org/pdf/2010.11929.pdf
Performance of ViT using Cifar-10 dataset
Train from scratch
Transfer Learning
78%
98%
* You may have different results in your own experiments
â¢Layer Normalization
â¢Transformers Block
â¢BERT and Text Classification
â¢Vision Transformer and Image Classification
â¢For Time-series and Tabular Data
Outline
Patch 
Time-Series 
Transformer
â–For time-series data
https://github.com/yu
qinie98/PatchTST
Patch 
Time-Series 
Transformer
â–For time-series data
https://github.com/yuqinie98/PatchTST
47
Patch 
Time-Series 
Transformer
â–For time-series data
TabTransformer
â–For Tabular Data
AI VIETNAM
All-in-One Course
TabTransformer
â–For Tabular Data
AI VIETNAM
All-in-One Course
Categorical Embeddings
https://towardsdatascience.com/transformers-for-tabular-
data-tabtransformer-deep-dive-5fb2438da820
50
TabTransformer
â–For Tabular Data
AI VIETNAM
All-in-One Course
https://keras.io/examples/structured_data/tabtransformer/
Contextual Embeddings
https://towardsdatascience.com/transformers-for-tabular-
data-tabtransformer-deep-dive-5fb2438da820
51
