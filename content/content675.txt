1 # Tạo chức năng tiền xử lý để mã hóa văn bản và cắt bớt các chuỗi không
dài hơn độ dài đầu vào tối đa của mã thông báo
2 def preprocess_function(examples):
3 samples = tokenizer(examples["text"], truncation=True)
4 # Không cần attention_mask
5 # Cụ thể hơn về token masking của mamba có thể tham khảo: https://
github.com/state-spaces/mamba/issues/49
6 samples.pop(’attention_mask’)
7 return samples
8
9 # Thực hiện mã hóa văn bản
10 tokenized_imdb = imdb.map(preprocess_function, batched=True)
11
12 # Set seed cho hàm random
13 random.seed(42)
14
15 # Tạo tập train và test
16 train_dataset = tokenized_imdb["train"]
17 test_dataset = tokenized_imdb["test"]
18
19 # Tạo tập evaluation để đánh giá trong lúc train
20 # Do số lượng tập test lớn nên chỉ lấy mẫu 1% tập dữ liệu test để đánh
giá
21 total_samples = len(test_dataset)
22 eval_samples = int(0.1 * total_samples)
23 eval_indices = random.sample(range(total_samples), eval_samples)
24 eval_dataset = test_dataset.select(eval_indices)
(e) Evaluation metric:Để đánh giá performance của model ta sẽ sử dụng metric accuracy từ
thư viện evaluate:
1 # Tải module "accuracy" từ thư viện evaluate.
2 accuracy = evaluate.load("accuracy")
3
6