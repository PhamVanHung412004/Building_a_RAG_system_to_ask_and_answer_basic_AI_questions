Introduction to POS 
Tagging
AI VIETNAM
All-In-One Course
➢Problem Introduction
➢Simple Examples
➢Code Implementation
Outline
Problem Introduction
AI VIETNAM
All-In-One Course
❖Definition
Part of Speech (POS) tagging: assign each word in a sentence with an appropriate POS
●Input: a string of words + a tagset
●Output: a best tag for each word
Words often have more than one POS:
●The back door    (adjective)
●On my back    (noun)
●Win the voters back    (particle)
●Promised to back the bill    (verb)
➢Due to ambiguity (and unknown words), we cannot rely on a dictionary to look up 
the correct POS tags.
Problem Introduction
AI VIETNAM
All-In-One Course
❖Why POS tagging?
●POS tagging is one of the first steps in the NLP pipeline (right after tokenization, 
segmentation).
●POS tagging is traditionally viewed as a prerequisite for further analysis:
○Syntactic Parsing: What words are in the sentence?
○Information extraction: Finding names, dates, relations, …
Application Area
Description
Sentiment Analysis
POS tags help identify crucial sentiment indicators like adjectives.
Named Entity Recognition (NER)
Improves NER by providing context, such as identifying proper nouns as potential named entities.
Automatic Summarization
Identifies key verbs and nouns to extract main ideas for summaries.
Text Generation and Chatbots
Ensures grammatical correctness and enhances the naturalness of generated text.
Grammar Checking and Proofreading
Utilized to identify grammatical errors and suggest corrections.
Problem Introduction
❖English word classes
AI VIETNAM
All-In-One Course
Problem Introduction
❖English word classes
AI VIETNAM
All-In-One Course
Problem Introduction
AI VIETNAM
??? Course
1
❖Example from Penn Treebank
Example 
from 
Penn 
Treebank
Simple Examples
❖Creating a POS Tagger
Step 1
Obtain a 
POS-tagged 
corpus
Step 2
Choose a POS 
tagging model
Step 3
Train your 
model on your 
training corpus
Step 4
Evaluate your 
model on your 
test corpus
AI VIETNAM
All-In-One Course
Simple Examples
❖POS Tagging
TargSet:
(NNP, CD, JJ, …)
Tagged Text
MODEL
Raw Text
AI VIETNAM
All-In-One Course
Simple Examples
❖Text Classification
AI VIETNAM
All-In-One Course
POS 
Part-of-speed 
Tagging 
Simple Examples
❖POS Part-of-speed Tagging 
●Neural sequence models (RNNs or Transformers)
●Large Language Models (like BERT), finetuned
AI VIETNAM
All-In-One Course
Simple Examples
❖Evaluation Metric: Test Accuracy
●How many words in the unseen test data can you tag correctly?
➩ How many sentences can you tag correctly?
Metric
Description
Importance
Accuracy
Proportion of words correctly tagged
Measures overall effectiveness
Precision
Correctly predicted tags for a specific POS out of all predicted for that POS
Important for understanding model's performance on each tag
Recall
Correctly predicted tags for a specific POS out of all actual instances of that POS
Helps identify if the model is missing any specific POS tags
F1 Score
Weighted average of Precision and Recall
Useful in cases of uneven distribution of POS tags
AI VIETNAM
All-In-One Course
Code Implementation
❖POS Tagging Code Overview
0: Import Library 
1: Data Preparation
2: Modeling
4: Plot Results
3: Train and 
Evaluate
2
Split Data
3
Tokenizer
1
Load Raw Data
4
Pad and Truncate
2
Build Model
1
Conﬁg Model
2
Evaluate model
1
Train model
2
Train/Test Acc
1
Train/Test Loss
AI VIETNAM
All-In-One Course
Code Implementation
❖Import Library
import torch: Imports PyTorch, a library for tensor computation and deep learning.
import torch.nn as nn : Brings in 
PyTorch's neural network module, 
aliased as nn, for building network 
layers.
import torch.nn.functional as F : Imports PyTorch's functional 
interface, providing access to activation and loss functions.
AI VIETNAM
All-In-One Course
❖Conference on Computational Natural Language Learning (CoNLL-2003) Dataset
{
    "chunk_tags": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],
    "id": "0",
    "ner_tags": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    "pos_tags": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],
    "tokens": ["The", "European", "Commission", "said", "on", "Thursday", "it", "disagreed", "with", "German", "advice", "to", "consumers", "to", "shun", 
"British", "lamb", "until", "scientists", "determine", "whether", "mad", "cow", "disease", "can", "be", "transmitted", "to", "sheep", "."]
}
Code Implementation
❖Data Preparation
{'"': 0, "''": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,  'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 
'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,  'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 
28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,  'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 
40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,  'WP': 44, 'WP$': 45, 'WRB': 46}
tokens: a list of string features.
pos_tags: a list of classification labels (int). Full tagset with indices:
AI VIETNAM
All-In-One Course
Code Implementation
❖Data Splits
AI VIETNAM
All-In-One Course
Code Implementation
❖AutoTokenizer
Automatic Tokenizer Loading: The AutoTokenizer 
class can automatically detect and load the 
appropriate tokenizer for a given pre-trained model. 
This is highly convenient because different models in 
the Transformers library (like BERT, GPT, T5, etc.) 
require different tokenizers due to their distinct 
architectures and training setups.
AI VIETNAM
All-In-One Course
Code Implementation
❖PosTagging_Dataset Example
sample1 
sample2 
sample1 
sample2 
sample1 
sample2 
Step1: get input and label of 1 sample 
Step2: tokenize 
Step3:Pad and Truncate 
AI VIETNAM
All-In-One Course
Code Implementation
❖PosTagging_Dataset
AI VIETNAM
All-In-One Course
Code Implementation
❖PosTagging_Dataset Example
sample1 
sample2 
sample1 
sample2 
sample1 
sample2 
Step1: get input and label of 1 sample 
Step2: tokenize 
Step3:Pad and Truncate 
AI VIETNAM
All-In-One Course
Code Implementation
❖PosTagging_Dataset Example
sample1 
sample2 
Step1: get input and label of 1 sample 
AI VIETNAM
All-In-One Course
Code Implementation
❖PosTagging_Dataset Example
sample1 
sample2 
Step2: tokenize 
AI VIETNAM
All-In-One Course
Code Implementation
❖PosTagging_Dataset Example
sample1 
sample2 
Step3:Pad and Truncate 
AI VIETNAM
All-In-One Course
Code Implementation
❖POS_Model
Input (x)
Embedding
fc1
fc2
fc3
AI VIETNAM
All-In-One Course
Code Implementation
❖Evaluate
AI VIETNAM
All-In-One Course
Code Implementation
❖Train
Example1: N=1, C=4
N
Loss = -[(log(0.1998) + log(0.9760)]/2 ≃ 
1.2
3.6
0.1
-2.5
3
0.0808
0.8903
0.0269
0.0020
6.2146
Loss = -log(0.0020) ≃
softmax(z) =
y (N,)
 z (N,C)
Example2:  N=1, C=3, d=2
1.2
3.6
0.1
-2.5
0.1
-0.2
1
0
0.8173
softmax(z) =
y (N,d)
 z (N,C, d)
 z (N,C, d)
1.2
3.6
0.1
3.6
-2.5
-0.2
0.6003
0.1998
0.1998
0.9760
0.0022
0.0218
0
1
2
0
1
2
3
AI VIETNAM
All-In-One Course
Code Implementation
❖Train
N
Example3:  N=1, C=3, d=2, ignore_index=1
1.2
3.6
0.1
-2.5
0.1
-0.2
1
0
0.0243
softmax(z) =
y (N,d)
 z (N,C, d)
 z (N,C, d)
1.2
3.6
0.1
3.6
-2.5
-0.2
0.6003
0.1998
0.1998
0.9760
0.0022
0.0218
Loss = -log(0.9760) ≃ 
softmax(z) =
0.6003
0.1998
0.1998
0.9760
0.0022
0.0218
0
1
2
0
1
2
AI VIETNAM
All-In-One Course
Code Implementation
AI VIETNAM
??? Course
❖Train
❖Train
Code Implementation
❖Plot Results
AI VIETNAM
All-In-One Course
32
