TOOLFORMER: 
LANGUAGE MODELS CAN TEACH 
THEMSELVES TO USE TOOLS
Timo Schick
Jane Dwivedi-Yu
Roberto DessÃ¬â€ 
Roberta Raileanu
Maria Lomeli
Luke Zettlemoyer
Nicola Cancedda
Thomas Scialom
Meta AI Research
â€ Universitat Pompeu Fabr
9 Feb 2023
arxiv.org/pdf/2302.04761.pdf
â€¢ Clarify the researcher's ideas
â€¢ Experiments & My comments
â€¢ Implement (code)
TABLE OF CONTENTS
CLARIFY THE RESEARCHER'S IDEAS
Key steps for question answering tool
Sampling 
API Calls
Executing 
API Calls
Filtering API 
Calls
Model 
Finetuning
Inference
APPROACH
Datasetof plain texts (C)
ğ¶= {ğ‘¥1, â€¦ , ğ‘¥ğ¶}
API call (c)
ğ‘= (ğ‘ğ‘, ğ‘–ğ‘)
ğ‘ğ‘: name of API
ğ‘–ğ‘: corresponding input
Linearized sequences of API (e)  
ğ‘’ğ‘= < ğ´ğ‘ƒğ¼> ğ‘ğ‘ğ‘–ğ‘</ğ´ğ‘ƒğ¼>
ğ‘’ğ‘, ğ‘Ÿ= < ğ´ğ‘ƒğ¼> ğ‘ğ‘ğ‘–ğ‘â†’ğ‘Ÿ</ğ´ğ‘ƒğ¼>
r:  API result
â€œ [â€, â€œ]â€, â€œ->â€ instead of â€œ<API>â€, â€œ</API>â€, â€œâ†’â€ (special 
tokens) to work without modifying LMâ€™s vocabulary
Finetuning language 
model M on C*
Dataset with API Calls (C*)
ğ¶âˆ—= {ğ‘¥âˆ—1, â€¦ , ğ‘¥âˆ—ğ¶}
C* contains the exact same texts as C
APPROACH
STEP BY STEP
â€¢ Sampling API Calls
â€¢ Executing API Calls
â€¢ Filtering API Calls
â€¢ Model Finetuning
â€¢ Inference
SAMPLING API CALLS
â€¢ Sequence ğ‘¥= ğ‘¥1, â€¦ , ğ‘¥ğ‘›
â€¢ Prompt P(x)
â€¢ ğ‘ğ‘€= ğ‘ğ‘€ğ‘§ğ‘›+1 ğ‘§1, â€¦ , ğ‘§ğ‘›probability that M assigns to token ğ‘§ğ‘›+1 as a continuation for 
the sequence ğ‘§1, â€¦ , ğ‘§ğ‘›â”For each i âˆˆ{1, . . . , n} â”ğ‘ğ‘–= ğ‘ğ‘€< ğ´ğ‘ƒğ¼> ğ‘ƒğ‘‹, ğ‘¥1:ğ‘–âˆ’1
probability that M assigns to starting an API call at position i
â€¢ Sampling threshold Ï„ğ‘ â”keep all positions I = {i| ğ‘ğ‘–> Ï„ğ‘ } â”k candidate positions â”
keep top k positions
â€¢ For each position i âˆˆI â”m API calls ğ‘ğ‘–
1, â€¦ , ğ‘ğ‘–
ğ‘šâ”
[P(x), ğ‘¥1, â€¦ , ğ‘¥ğ‘–âˆ’1, <API> ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘“ğ‘œğ‘Ÿğ´ğ‘ƒğ¼</ğ´ğ‘ƒğ¼>]
â€¢ M does not generate </API> â”discard example
Sampling 
API Calls
Executing 
API Calls
Filtering 
API Calls
PROMPT P(X)
â€¢ Question Answering (2)
â€¢ Calculator (5)
â€¢ Wikipedia Search (3)
â€¢ Machine Translation System (3)
â€¢ Calendar (5)
EXECUTING API CALLS
â€¢ How this is done depends entirely on the API itself
â€¢ Calling another neural network
â€¢ Executing a Python script
â€¢ Using a retrieval system to perform search over a large corpus
â€¢ API call ğ‘ğ‘–â”single text sequence ğ‘Ÿğ‘–
Sampling 
API Calls
Executing 
API Calls
Filtering 
API Calls
FILTERING API CALLS
â€¢ At position i in sequence ğ‘¥= ğ‘¥1, â€¦ , ğ‘¥ğ‘›â”API call ğ‘ğ‘–â”
response ğ‘Ÿğ‘–â”ğ‘’ğ‘ğ‘–, ğ‘Ÿğ‘–
â€¢ A sequence of weights ğ‘¤ğ‘–ğ‘–âˆˆğ‘
â€¢ z â‰œğ‘’ğ‘ğ‘–, ğ‘Ÿğ‘–as a prefix instead of inserting it at position i 
â€¢ Over the tokens ğ‘¥1, â€¦ , ğ‘¥ğ‘›â”weighted cross entropy loss for M 
ğ¿ğ‘–ğ‘§= âˆ’Ïƒğ‘—=ğ‘–
ğ‘›
ğ‘¤ğ‘—âˆ’ğ‘–âˆ™ğ‘™ğ‘œğ‘”ğ‘ğ‘€(ğ‘¥ğ‘—|ğ‘§, ğ‘¥1:ğ‘—âˆ’1)
Sampling 
API Calls
Executing 
API Calls
Filtering 
API Calls
FILTERING API CALLS
â€¢ Weighted cross entropy loss ğ¿ğ‘–ğ‘§= âˆ’Ïƒğ‘—=ğ‘–
ğ‘›
ğ‘¤ğ‘—âˆ’ğ‘–âˆ™ğ‘™ğ‘œğ‘”ğ‘ğ‘€(ğ‘¥ğ‘—|ğ‘§, ğ‘¥1:ğ‘—âˆ’1)
â€¢ Consider 3 cases:
â€¢ (a) doing no API call 
â€¢ (b) doing an API call and providing the response
â€¢ (c) doing an API call, but not providing the response
â€¢ Efficiency of API call â”compare (b) with (c) or (a) 
â€¢ ğ¿ğ‘–
ğµğ‘§< ğ¿ğ‘–
ğ´ğ‘§or ğ¿ğ‘–
ğµğ‘§< ğ¿ğ‘–
ğ¶ğ‘§â”ğ¿ğ‘–
ğµğ‘§< min(ğ¿ğ‘–
ğ´ğ‘§, ğ¿ğ‘–
ğ¶ğ‘§)
Sampling 
API Calls
Executing 
API Calls
Filtering 
API Calls
FILTERING API CALLS
â€¢ Weighted cross entropy loss ğ¿ğ‘–ğ‘§= âˆ’Ïƒğ‘—=ğ‘–
ğ‘›
ğ‘¤ğ‘—âˆ’ğ‘–âˆ™ğ‘™ğ‘œğ‘”ğ‘ğ‘€(ğ‘¥ğ‘—|ğ‘§, ğ‘¥1:ğ‘—âˆ’1)
â€¢ Efficiency of API call â”ğ¿ğ‘–
ğµğ‘§< min(ğ¿ğ‘–
ğ´ğ‘§, ğ¿ğ‘–
ğ¶ğ‘§)
â€¢ Empty sequence ğœ€â”
â€¢ ğ¿ğ‘–
+ = ğ¿ğ‘–ğ‘’ğ‘ğ‘–, ğ‘Ÿğ‘–
â€¢ ğ¿ğ‘–
âˆ’= min(ğ¿ğ‘–(ğœ€), ğ¿ğ‘–(ğ‘’ğ‘ğ‘–, ğœ€))
â€¢ Filtering threshold ğœğ‘“â”keep API that ğ¿ğ‘–
âˆ’âˆ’ğ¿ğ‘–
+â‰¥ğœğ‘“
Sampling 
API Calls
Executing 
API Calls
Filtering 
API Calls
MODEL FINETUNING
â€¢ LM Dataset C â”LM Dataset with API Calls C*
â€¢ Original input ğ‘¥= ğ‘¥1, â€¦ , ğ‘¥ğ‘›â”new input ğ‘¥âˆ—= ğ‘¥1:ğ‘–âˆ’1, ğ‘’ğ‘ğ‘–, ğ‘Ÿğ‘–, ğ‘¥ğ‘–:ğ‘›
â€¢ Finetune M on C*, then based on its feedback ğ‘’ğ‘ğ‘–, ğ‘Ÿğ‘–, M will learn 
to decide when and how to use which tool can help predict future 
tokens.
INFERENCE
â€¢ ğ‘¥âˆ—= ğ‘¥1:ğ‘–âˆ’1, ğ‘’ğ‘ğ‘–, ğ‘Ÿğ‘–, ğ‘¥ğ‘–:ğ‘›
â€¢ ğ‘’ğ‘, ğ‘Ÿ= < ğ´ğ‘ƒğ¼> ğ‘ğ‘ğ‘–ğ‘â†’ğ‘Ÿ</ğ´ğ‘ƒğ¼>
â€¢ Finetuned M â”generated text t â”regular decoding, when M
produces the â€œâ†’â€ token â”interrupt decoding â”M produces
</API> token â”continue decoding.
Fine-tuning
Inference
TOOLS
â€¢ Question Answering
â€¢ QA system based on another LM - Atlas retrieval-augmented LM fine-tuned on Natural Questions
â€¢ Calculator
â€¢ Simple numeric calculations - four operations, rounded to two decimal places
â€¢ Wikipedia Search
â€¢ Search engine, given a search term, extract from comprehensive information and returns short
text - a BM25 retriever that indexes the Wikipedia dump from KILT
â€¢ Machine Translation System
â€¢ Detect source language using the fast-Text classifier, translate into English from any language
using multilingual machine translation model - 600M parameter NLLB (200 languages)
â€¢ Calendar
â€¢ Query, return the current date without taking any inputs, provide temporal context for predictions that
require some awareness of time
EXPERIMENTS & MY COMMENTS
Finetuning 
M on C*
API call (c)
ğ‘= (ğ‘ğ‘, ğ‘–ğ‘)
Linearized sequences of API (e)  
ğ‘’ğ‘= < ğ´ğ‘ƒğ¼> ğ‘ğ‘ğ‘–ğ‘</ğ´ğ‘ƒğ¼>
ğ‘’ğ‘, ğ‘Ÿ= < ğ´ğ‘ƒğ¼> ğ‘ğ‘ğ‘–ğ‘â†’ğ‘Ÿ</ğ´ğ‘ƒğ¼>
Dataset (C*)
ğ¶âˆ—= {ğ‘¥âˆ—1, â€¦ , ğ‘¥âˆ—ğ¶}
Dataset (C)
ğ¶= {ğ‘¥1, â€¦ , ğ‘¥ğ¶}
Subset of CCNet
GPT-J
more likely
to be helpful
Computational cost â”define heuristics
Default:  Ï„ğ‘ = 0.05, Ï„ğ‘“= 1.0, ğ‘˜= 5, ğ‘š= 5
Calculator, machine translation: Ï„ğ‘ = 0.0, Ï„ğ‘“= 0.5, ğ‘˜= 20, ğ‘š= 10
obtaining C* from C
Make sure that API is actually helpful â”weighting function
â€¢ GPT-J: w/o finetuning 
â€¢ GPT-J + CC: finetuned on C
â€¢ Toolformer: GPT-J finetuned on C*
â€¢ Toolformer (disabled): disable API calls (decode)
batch size: 128
learning rate: 1Â·10âˆ’5
linear warmup: first 10% of training
Filtering threshold ğœğ‘“â”keep API that ğ¿ğ‘–
âˆ’âˆ’ğ¿ğ‘–
+= ğœğ‘“
DOWNSTREAM TASKS 
â€¢ SETUP
â€¢ prompted zero shot setup - instructed to solve but do not provide any
in-context examples
â€¢ greedy decoding - selects the word with the highest probability as the
next word ğ‘¤ğ‘¡= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥
ğ‘¤
ğ‘ƒ(ğ‘¤|ğ‘¤1:ğ‘¡âˆ’1), but one modification
â€¢ Standard: the most likely token â”Modification: the k most likely
tokens â”that ğ‘¤ğ‘¡= <API> â”k = 10
â€¢ one API call/input â”does not get stuck in a loop in case API call not
providing the response
DOWNSTREAM TASKS 
Standard:the most likely token â”Modification: the k most likely tokens â”that ğ‘¤ğ‘¡= <API> â”k = 10
increasing k â”model doing API calls for more examples
k = 1 â”model is calibrated to some extent â”decide to call 
APIs (better perform) instead of NOT call APIs (badly perform). 
higher values of k â”lose this calibration
EVALUATE MODELS
â€¢ Considering the following criterions:
â€¢ The factual and commonsense knowledge contained in pretrained language models
(on SQuAD, Google RE and T-REx subsets of the LAMA)
â€¢ Performance on specific tasks when compared with other models
â€¢ Mathematical reasoning abilities on ASDiv, SVAMP, MAWPS
â€¢ Question Answering on Web Questions, Natural Questions,TriviaQA
â€¢ Multilingual Question Answering on MLQA
â€¢ Calendar APIâ€™s utility on TEMPLAMA, generated DATESET
â€¢ Language modeling performance on WikiText, CCNet not used in training (10,000)
â€¢ Ability to ask external tools for help affects performance using GPT-J and GPT-2 family
(124M, 355M, 775M, 1.6B) for tools: question answering, calculator,Wikipedia search.
DOWNSTREAM TASKS 
The factual and commonsense knowledge on SQuAD, Google RE and T-REx subsets of the LAMA
Task: complete a short statement with a missing fact (e.g., a date or a place)
LAMA was designed to evaluate masked language models â”filter out mask token is not the final token. 
Lenient evaluation criteria instead of exact match, the correct word is in the top five predicted words.
LAMA is based on statements (Wikipedia), prevent Toolformer from using the Wikipedia Search API
question answering tool 
(98.1%);  a different tool 
(0.7%) or no tool (1.2%)
DOWNSTREAM TASKS 
Performance on specific tasks
Mathematical reasoning abilities on ASDiv, SVAMP, MAWPS
Lenient evaluation criterion, output is a number â”check for the first predicted number
Surmise: finetuned on many examples of API calls and their results â”improving mathematical capabilities
calculator tool 
(97.9%)
Operations: â€œ+â€, â€œâˆ’â€, â€œâˆ—â€, â€œ/â€
Heuristic filters for CCNet & 
processing criterions
â€¢
window of 100 tokens
â”>= 3
numbers, one is the result of the
operation of the other two.
â€¢
â€œ=â€, â€œequalsâ€, â€œequal toâ€, â€œtotal 
ofâ€, â€œaverage ofâ€ + a number 
â€¢
>= 3 numbers (random 1%)
DOWNSTREAM TASKS 
Performance on specific tasks
Question Answering on Web Questions, Natural Questions,TriviaQA
Lenient evaluation criteria instead of exact match, the correct word is in the top 20 predicted words.
Question-answering tool:  Atlas model finetuned on  Natural Questions. Atlas large (obtaining C*),  Atlas-
xxl (Inference) â”task solving is trivial â”disable question-answering tool
Wikipedia 
search (99.3%)
Simplicity (not a good match)
Inability to interact with itself, 
reformulating or browsing through 
the top results â”future work
DOWNSTREAM TASKS 
Performance on specific tasks
Multilingual Question Answering on MLQA
Machine translation tool: NLLB 600M (training & inference), fastText classifier (source language detection)
Text chunks (multilingual language) â”target language (English)
Preprocessing:
â€¢
Text chunks size: 10 tokens
â€¢
Middle text chunk is in a language other than English (fastText classifier, confidence > 0.8)
â€¢
Any text chunks only numbers or special symbols
Input: è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§
ç”Ÿçš„æ­Œæ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³En un estudio que aparece el 29 de septiembre en la revista Heliyon, los
investigadores utilizan el estilo de los gÃ©neros musicales existentes para guiar la estructura de las
canciones proteicas y hacerlas mÃ¡s musicales ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œå“çš„é£æ ¼ä¸ºæŒ‡å¯¼ï¼Œ
å°†è›‹ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨å¤§éŸ³ä¹æ€§çš„æ­Œæ›²
DOWNSTREAM TASKS 
Performance on specific tasks
Multilingual Question Answering on MLQA
Input â”API calls (Machine translation tool) â”è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°
å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ­Œæ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³<API> response of API </API> En un
estudio que aparece el 29 de septiembre en la revista Heliyon, los investigadores utilizan el estilo de los
gÃ©neros musicales existentes para guiar la estructura de las canciones proteicas y hacerlas mÃ¡s
musicales <API> response of API </API> ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œå“çš„é£æ ¼ä¸ºæŒ‡å¯¼ï¼Œå°†è›‹
ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨å¤§éŸ³ä¹æ€§çš„æ­Œæ›²<API> response of API </API>
<API> response of API </API>
è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ­Œ
æ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³<API> response of API </API> En un estudio que aparece el 29 de septiembre en la
revista Heliyon, los investigadores utilizan el estilo de los gÃ©neros musicales existentes para guiar la
estructura de las canciones proteicas y hacerlas mÃ¡s musicales ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œ
å“çš„é£æ ¼ä¸ºæŒ‡å¯¼ï¼Œå°†è›‹ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨å¤§éŸ³ä¹æ€§çš„æ­Œæ›²
DOWNSTREAM TASKS 
Performance on specific tasks
Multilingual Question Answering on MLQA
Input â”API calls (Machine translation tool) â”è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°
å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ­Œæ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³<API> response of API </API> En un
estudio que aparece el 29 de septiembre en la revista Heliyon, los investigadores utilizan el estilo de los
gÃ©neros musicales existentes para guiar la estructura de las canciones proteicas y hacerlas mÃ¡s
musicales <API> response of API </API> ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œå“çš„é£æ ¼ä¸ºæŒ‡å¯¼ï¼Œå°†è›‹
ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨å¤§éŸ³ä¹æ€§çš„æ­Œæ›²<API> response of API </API>
<API> response of API </API>
è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ­Œ
æ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³En un estudio que aparece el 29 de septiembre en la revista Heliyon, los investigadores
utilizan el estilo de los gÃ©neros musicales existentes para guiar la estructura de las canciones proteicas y
hacerlas mÃ¡s musicales <API> response of API </API> ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œå“çš„é£æ ¼
ä¸ºæŒ‡å¯¼ï¼Œå°†è›‹ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨å¤§éŸ³ä¹æ€§çš„æ­Œæ›²
DOWNSTREAM TASKS 
Performance on specific tasks
Multilingual Question Answering on MLQA
Input â”API calls (Machine translation tool) â”è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°
å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ­Œæ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³<API> response of API </API> En un
estudio que aparece el 29 de septiembre en la revista Heliyon, los investigadores utilizan el estilo de los
gÃ©neros musicales existentes para guiar la estructura de las canciones proteicas y hacerlas mÃ¡s
musicales <API> response of API </API> ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œå“çš„é£æ ¼ä¸ºæŒ‡å¯¼ï¼Œå°†è›‹
ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨å¤§éŸ³ä¹æ€§çš„æ­Œæ›²<API> response of API </API>
<API> response of API </API>
è¿‘å¹´æ¥ï¼Œç§‘å­¦å®¶ä»¬æ ¹æ®è›‹ç™½è´¨çš„ç»“æ„åˆ›ä½œäº†éŸ³ä¹ï¼Œä½œä¸ºæ›´å¥½åœ°å‘å…¬ä¼—æ™®åŠç§‘å­¦çš„åˆ›é€ æ€§æ–¹å¼ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„æ­Œ
æ›²å¹¶ä¸æ€»æ˜¯æ‚¦è€³En un estudio que aparece el 29 de septiembre en la revista Heliyon, los investigadores
utilizan el estilo de los gÃ©neros musicales existentes para guiar la estructura de las canciones proteicas y
hacerlas mÃ¡s musicales ç ”ç©¶äººå‘˜ä»¥è‚–é‚¦çš„å¹»æƒ³å³å…´æ›²å’Œå…¶ä»–å¤å…¸ä½œå“çš„é£æ ¼ä¸ºæŒ‡å¯¼ï¼Œå°†è›‹ç™½è´¨è½¬åŒ–ä¸ºå…·æœ‰å·¨
å¤§éŸ³ä¹æ€§çš„æ­Œæ›²<API> response of API </API>
DOWNSTREAM TASKS 
Performance on specific tasks
Multilingual Question Answering on MLQA
Context in English, question in Arabic, German, Spanish, Hindi, Vietnamese, Simplified Chinese â”
understand paragraph and question. Percentage of times that correct word is in the top 10 predicted words.
Machine translation tool 
(63.8-94.9%), Hindi (7.3%)
*Distribution shift: CCNet compared to GPT-Jâ€™s 
original pretraining data
*Multilingual aspect: OPT and GPT-3 fail to
provide answers in English with language
differences between context and question.
Context
and
question
in
English
â”
best
performance (GPT-3)
DOWNSTREAM TASKS 
Performance on specific tasks
Calendar APIâ€™s utility
TEMPLAMA (Wikidata) contains
cloze queries about facts
that change with time.
DATESET generated through a series of templates, but
populated using a combination of random dates/durations.
Randomly 500 â€œcurrent datesâ€ â”randomly within four-year
range â€œpast dateâ€ and â€œfuture dateâ€ â”fill templates
Assumption: The calendar date is created document date.
Extract from the URL, if present. Otherwise, filter out and
leave ~18%.
DOWNSTREAM TASKS 
Performance on specific tasks
Calendar APIâ€™s utility on TEMPLAMA, generated DATESET
Lenient evaluation criteria instead of exact match, the correct word is in the top five predicted words.
Mostly to Wikipedia search and question 
answering, calendar (0.2%) for TEMPLAMA. 
But calendar (54.8%) for DATESET.
A little help (exact date) because of specific 
and rare on TEMPLAMA
The best action: query current date (calendar), 
then query question answering with this date. 
But two API calls per example &  
Independently sampled API calls on training.
DOWNSTREAM TASKS 
Language modeling performance on WikiText, CCNet not used in training (10,000)
Original pretraining data for GPT-J is more similar
to WikiText than randomly subset of CCNet. 
Training on C* compared to C does not lead to an 
increase in perplexity when API calls are disabled 
at inference time.
Ability to ask external tools for help affects performance using GPT-J and GPT-2 family (124M, 355M,
775M, 1.6B) for tools: question answering, calculator,Wikipedia search engine
Leverage the provided tools at models ~775M. Smaller models achieve similar performance w/ and
w/o tools. But achieve difference for QA benchmarks (Wikipedia search engine mainly be used).
When the model size grows, it can solve tasks without API calls. So, it also can make good use of the
provided API. And the gap between model predictions with and without API calls remains high.
DOWNSTREAM TASKS 
Ability to ask external tools for help affects performance using GPT-J and GPT-2 family (124M, 355M, 775M, 1.6B)
for tools: question answering, calculator,Wikipedia search engine
LIMITATIONS
Generated independently tools â”limitation in using tools in chain (output of one as input for
another)
NOT allow to use tool in an interactive way (search engines) â”Solution: take advantage of
possible results or refine search query by itself
LMs - sensitive to a prompt â”sensitive to exact wording (input when deciding to call APIs)
Sample-inefficient (tool-dependent): million results but few thousand useful calls â”Solution:
iteratively apply our approach, similar to bootstrapping approaches
API calls â”NOT consider computational cost for tool-dependent
My confused
Lenient evaluation criteria instead of exact match (tool-dependent), the correct word is in
the top k predicted words â”what happened in case they used exact match? It too bad?
C* was obtained from C (C* contains the exact same texts as C) and their assumption that
finetuning M on C* exposes it to the same content as on C â”Why is this true in case there
are response of API calls in C* while not in C? Mabe the reason is that when obtaining C*
from C, they filtered alters the distribution of training and assume that the remaining
examples are close enough to the original distribution so that Mâ€™s language modeling
abilities remain unaffected? But how?
They used the weighting function to make sure that API calls happen close to where the
information provided by the API is actually helpful for the model BUT I can find more
clearly explaination for this function nowhere in their paper.
IMPLEMENT (CODE)
â€¢ lucidrains/toolformer-pytorch: Implementation of Toolformer, Language Models That
Can Use Tools, by MetaAI (github.com)
â€¢ conceptofmind/toolformer(github.com) - Tool Former Source Code: LLM using API
(youtube.com)
â€¢ xrsrke/toolformer: Implementation of Toolformer: Language Models Can Teach
Themselves to Use Tools (github.com)
