From Linear Regression
to Logistic Regression
Year 2023
Quang-Vinh Dinh
PhD in Computer Science
AI VIETNAM
All-in-One Course
â¢Optimization Review
â¢Linear Regression Review
â¢Logistic Regression
â¢Examples
â¢Vectorization
â¢Implementation (optional)
Outline
Optimization
â– Gradient descent
1
AI VIETNAM
All-in-One Course
ğ‘‘
ğ‘‘ğ‘¥ğ½(ğ‘¥) = lim
âˆ†ğ‘¥â†’0
ğ½ğ‘¥ + âˆ†ğ‘¥ âˆ’ğ½(ğ‘¥ )
âˆ†ğ‘¥ 
ğ½
ğ‘¥
ğ‘¥+ âˆ†ğ‘¥ğ‘¥
J(x)
âˆ†ğ‘¥
âˆ†ğ½
ğ‘¥
ğ½
ğ‘¥op
ğ‘¥2
ğ
ğğ± ğ‰ğ±ğŸ< ğŸ
ğ
ğğ± ğ‰ğ±ğŸ> ğŸ
ğ‘¥1
xğ‘›ğ‘’ğ‘¤= xğ‘œğ‘™ğ‘‘âˆ’ ğœ‚d
dx J xğ‘œğ‘™ğ‘‘
Derivate at xğ‘œğ‘™ğ‘‘
learning rate
Optimization
ğ‰ğœƒ
Khá»Ÿi táº¡o giÃ¡ trá»‹ ğœ½
d
dğœƒJ ğœƒ> 0
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a trÃ¡i
ğœƒ value
Di chuyá»ƒn ğœ½ ngÆ°á»£c hÆ°á»›ng Ä‘áº¡o hÃ m
d
dğœƒJ ğœƒ> 0
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a trÃ¡i
d
dğœƒJ ğœƒ> 0
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a trÃ¡i
ğœƒ value
J ğœƒ
ğœƒ value
d
dğœƒJ ğœƒ< 0
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a pháº£i
Cá»© tiáº¿p tá»¥c di chuyá»ƒn ngÆ°á»£c 
hÆ°á»›ng Ä‘áº¡o hÃ m
d
dğœƒJ ğœƒ> 0
Dá»‹ch chuyá»ƒn ğœƒ vá» phÃ­a trÃ¡i
J ğœƒ
â– Gradient descent
2
Optimization
AI VIETNAM
All-in-One Course
â– Square function
Compute 
derivative at x
Move x 
opposite to dx
Initialize x
ğ‘“ğ‘¥= ğ‘¥2
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
3
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 0.1
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
4
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 0.001
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
5
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 0.8
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
6
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 1. 1
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
7
Optimization
AI VIETNAM
All-in-One Course
â– Optimization: 2D function
âˆ’100 â‰¤ğ‘¥, ğ‘¦â‰¤100
ğ‘¥, ğ‘¦âˆˆâ„•
ğ‘“ğ‘¥, ğ‘¦= ğ‘¥2 + ğ‘¦2
8
Derivative
AI VIETNAM
All-in-One Course
â– Optimization: 2D function
âˆ’100 â‰¤ğ‘¥, ğ‘¦â‰¤100
ğ‘¥, ğ‘¦âˆˆâ„•
ğ‘“ğ‘¥, ğ‘¦= ğ‘¥2 + ğ‘¦2
ğ‘¥= ğ‘¥âˆ’ğœ‚ğœ•ğ‘“(ğ‘¥, ğ‘¦)
ğœ•ğ‘¥
ğ‘¦= ğ‘¦âˆ’ğœ‚ğœ•ğ‘“(ğ‘¥, ğ‘¦)
ğœ•ğ‘¦
ğ‘¥0 = 3.0
ğ‘¦0 = 4.0
ğœ•ğ‘“(ğ‘¥0, ğ‘¦0)
ğœ•ğ‘¥
= 6.0
ğœ•ğ‘“(ğ‘¥0, ğ‘¦0)
ğœ•y
= 8.0
ğ‘¥1 = 2.0
ğ‘¦1 = 3.0
ğœ•ğ‘“(ğ‘¥1, ğ‘¦1)
ğœ•ğ‘¥
= 4.0
ğœ•ğ‘“(ğ‘¥1, ğ‘¦1)
ğœ•y
= 6.0
ğ‘¥2 = 1.0
ğ‘¦2 = 2.0
ğœ•ğ‘“(ğ‘¥2, ğ‘¦2)
ğœ•ğ‘¥
= 2.0
ğœ•ğ‘“(ğ‘¥2, ğ‘¦2)
ğœ•y
= 4.0
ğ‘¥3 = 0.0
ğ‘¦3 = 1.0
ğœ•ğ‘“(ğ‘¥3, ğ‘¦3)
ğœ•ğ‘¥
= 0.0
ğœ•ğ‘“(ğ‘¥3, ğ‘¦3)
ğœ•y
= 0.0
ğ‘¥4 = 0.0
ğ‘¦4 = 0.0
ğœ‚= 1.0
9
Optimization
â– For composite function
AI VIETNAM
All-in-One Course
ğ‘“ğ‘¥= 2ğ‘¥âˆ’1
ğ‘”ğ‘“= ğ‘“âˆ’3 2
ğ‘”ğ‘“ğ‘¥
ğ‘¥
ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘¥
ğ‘“
ğ‘”
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘‘
ğ‘‘ğ‘¥ğ‘”ğ‘“ğ‘¥
=
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
âˆ—
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘¥
ğ‘”
ğ‘“ğ‘¥
ğ‘”ğ‘“
Optimization
â– For composite function
AI VIETNAM
All-in-One Course
ğ‘¥
ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘¥
ğ‘“
ğ‘”
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘‘
ğ‘‘ğ‘¥ğ‘”ğ‘“ğ‘¥
=
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
âˆ—
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘“ğ‘¥= 2ğ‘¥âˆ’1
ğ‘”ğ‘“= ğ‘“âˆ’3 2
ğ‘”ğ‘¥= 2ğ‘¥âˆ’1 âˆ’3 2
= 2ğ‘¥âˆ’4 2
ğ‘”â€² ğ‘¥= 4 2ğ‘¥âˆ’4
= 8ğ‘¥âˆ’16
11
Optimization
â– For composite function and chain rule
AI VIETNAM
All-in-One Course
ğ‘¥
ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘¥
ğ‘“
ğ‘”
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘‘
ğ‘‘ğ‘¥ğ‘”ğ‘“ğ‘¥
=
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
âˆ—
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘“ğ‘¥= 2ğ‘¥âˆ’1
ğ‘”ğ‘“= ğ‘“âˆ’3 2
ğ‘‘ğ‘”
ğ‘‘ğ‘¥= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘¥
ğ‘“â€² ğ‘¥= 2
ğ‘”â€² ğ‘“= 2 ğ‘“âˆ’3
= 2 ğ‘“âˆ’3 2
= 4 2ğ‘¥âˆ’1 âˆ’3
= 8ğ‘¥âˆ’16
12
â¢Optimization Review
â¢Linear Regression Review
â¢Logistic Regression
â¢Examples
â¢Vectorization
â¢Implementation (optional)
Outline
House 
Price 
Prediction
Feature
Label
House price data
price = wâˆ—area + ğ‘
if area=6.0, price=7.28
price = ğ‘¤1 âˆ—area + ğ‘1
price = ğ‘¤2 âˆ—area + ğ‘2
price = ğ‘¤3 âˆ—area + ğ‘3
if area=6.0, price=?
Feature
Label
House price data
Linear Regression
AI VIETNAM
All-in-One Course
predicted_price = w âˆ—area + ğ‘
error = predicted_price âˆ’real_price 2
â– Area-based house price prediction
ğ¿(à·œğ‘¦, ğ‘¦) = à·œğ‘¦âˆ’ğ‘¦2
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
w = -0.34
b = 0.04
14
w = 1.17
b = 0.26
Linear Regression
AI VIETNAM
All-in-One Course
â– Area-based house price prediction
predicted_price = w âˆ—area + ğ‘
error = predicted_price âˆ’real_price 2
ğ¿(à·œğ‘¦, ğ‘¦) = à·œğ‘¦âˆ’ğ‘¦2
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
15
Linear Regression
â– Area-based house 
     price prediction
w = -0.34
b = 0.04
w = 1.17
b = 0.26
ğ¿(à·œğ‘¦, ğ‘¦) =
à·œğ‘¦âˆ’ğ‘¦2
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
AI VIETNAM
All-in-One Course
How to change w and b 
so that ğ¿(à·œğ‘¦, ğ‘¦) reduces
16
Linear Regression
â– Understanding the loss function
AI VIETNAM
All-in-One Course
Different b values with a fixed w value
Different w values with a fixed b value
ğ¿(à·œğ‘¦, ğ‘¦) =
à·œğ‘¦âˆ’ğ‘¦2
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
How to change w and b 
so that ğ¿(à·œğ‘¦, ğ‘¦ğ‘–) reduces
17
Linear Regression
AI VIETNAM
All-in-One Course
Error (loss) computation
Idea: compare predicted values à·œğ‘¦ and label values y 
Squared loss
L(à·œğ‘¦, ğ‘¦) = (à·œğ‘¦âˆ’ğ‘¦)2
Linear equation
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
where à·œğ‘¦ is a predicted value,
            ğ‘¤ and ğ‘ are parameters
            and ğ‘¥ is input feature
Compute output à·œğ‘¦
Compute loss
Compute derivative 
for each parameter
Update parameters
Training 
data
Pick a sample 
(x, y)
x=area and y=price
Initialize ğ‘¤ and ğ‘
18
Use gradient descent to minimize the loss function
ğœ•ğ¿
ğœ•ğ‘¤= ğœ•ğ¿
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘¤= 2ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= ğœ•ğ¿
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘= 2(à·œğ‘¦âˆ’ğ‘¦)
Compute derivate for each parameter
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
Update parameters
is learning rate
ğœ‚
Linear Regression
AI VIETNAM
All-in-One Course
Find better w and b
Error (loss) computation
Idea: compare predicted values à·œğ‘¦ and label values y 
Squared loss
L(à·œğ‘¦, ğ‘¦) = (à·œğ‘¦âˆ’ğ‘¦)2
Linear equation
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
where à·œğ‘¦ is a predicted value,
            ğ‘¤ and ğ‘ are parameters
            and ğ‘¥ is input feature
Linear Regression
â– Example
Compute output à·œğ‘¦
Compute loss
Compute derivative 
for each parameter
Update parameters
Training 
data
Pick a sample 
(x, y)
x=area and y=price
Initialize ğ‘¤ and ğ‘
Feature
Label
Model
Input
Label
Loss
Parameters
ğ‘¥
ğ‘¤
ğ‘
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
(à·œğ‘¦âˆ’ğ‘¦)2
ğ‘¦
20
Given
sample
data
Model
ğ‘¥ = 6.7
ğ‘= 0.04
w = -0.34
ğ‘¦ = 9.1
à·œğ‘¦= ğ‘¥ğ‘¤+ ğ‘ = -2.238 
Input
Label
Loss
à·œğ‘¦âˆ’ğ‘¦2 = 128.5
Parameters
Forward 
propagation
House price prediction
Linear Regression
AI VIETNAM
All-in-One Course
Initialize 
b=0.04 and 
w=-0.34
Feature
Label
1
21
Model
ğ‘¥ = 6.7
ğ‘= 0.26676
ğ‘¤ = 1.17929
ğ‘¦ = 9.1
à·œğ‘¦= ğ‘¥ğ‘¤+ ğ‘ = -2.238 
Input
Label
Loss
Parameters
Backpropagation
à·œğ‘¦âˆ’ğ‘¦2 = 128.5
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥à·ğ‘¦âˆ’ğ‘¦
 
= âˆ’151.9292
ğœ•ğ¿
ğœ•ğ‘= 2 à·ğ‘¦âˆ’ğ‘¦
 
= âˆ’22.676
ğœ‚= 0.01
New w and b help 
the loss reduce
Forward 
propagation
Model
ğ‘¥ = 6.7
ğ‘¦ = 9.1
à·œğ‘¦= ğ‘¥ğ‘¤+ ğ‘ = -2.238 
Input
Label
Loss
Parameters
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
à·œğ‘¦âˆ’ğ‘¦2 = 0.868
ğ‘= 0.26676
ğ‘¤ = 1.17929
w = w âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
Linear Regression
AI VIETNAM
All-in-One Course
3
2
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
w = w âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
w = -0.34
b = 0.04
L = 128.55
w = 1.179292 b = 0.26676
L = 0.868
Model prediction before and after the first update
Before updating
After updating
Linear Regression
AI VIETNAM
All-in-One Course
â– Toy example
23
Linear Regression
AI VIETNAM
All-in-One Course
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿(à·œğ‘¦, ğ‘¦) = (à·œğ‘¦âˆ’ğ‘¦)2
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2(à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
â– Summary (one feature and one sample)
Model
Input
Label
Loss
Parameters
ğ‘¥
ğ‘¤
ğ‘
à·œğ‘¦= ğ‘¤ğ‘¥+ ğ‘
(à·œğ‘¦âˆ’ğ‘¦)2
ğ‘¦
24
â¢Optimization Review
â¢Linear Regression Review
â¢Logistic Regression
â¢Examples
â¢Vectorization
â¢Implementation (optional)
Outline
Idea of Logistic Regression
à·œğ‘¦= ğœ½ğ‘‡ğ’™= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦âˆˆâˆ’âˆ 
+ âˆ 
â– Linear regression
Find the line à·œğ‘¦= ğœ½ğ‘‡ğ’™ that is best fitting to given 
data, then use à·œğ‘¦ to predict for new data 
Feature
Label
Area-based House Price Data
ğ’™
ğ’š
error
error
error
error
Training data
Model
construct
AI VIETNAM
All-in-One Course
25
Idea of Logistic Regression
â– Given a new kind of data
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
Assign numbers 
to categories
Feature
Feature
Category 0
Category 1
Plot data
A line is not suitable 
for this data
AI VIETNAM
All-in-One Course
26
Idea of Logistic Regression
ğœ(ğ‘¢) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§âˆˆâˆ’âˆ 
+ âˆ 
ğœ(ğ‘¢) âˆˆ0 1 
âˆ€ğ‘§1ğ‘§2 âˆˆğ‘ ğ‘ and ğ‘§1 â‰¤ğ‘§2
â†’ğœ(ğ‘§1) â‰¤ğ‘§(ğ‘¢1)
ğ‘§
+âˆ
âˆ’âˆ
ğ‘§1
ğ‘§2
ğœ
ğœ1
ğœ2
Sigmoid function
Property
AI VIETNAM
All-in-One Course
27
Idea of Logistic Regression
ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§âˆˆâˆ’âˆ 
+ âˆ 
ğœ(ğ‘§) âˆˆ0 1 
ğ‘¥
ğ‘§
ğ‘¥
ğ‘§
ğ‘§
ğœ
ğ‘§
ğœ
AI VIETNAM
All-in-One Course
Idea of Logistic Regression
ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§âˆˆâˆ’âˆ 
+ âˆ 
ğœ(ğ‘§) âˆˆ0 1 
ğ‘¥
ğ‘§
ğ‘¥
ğ‘§
ğ‘§
ğœ
ğ‘§
ğœ
AI VIETNAM
All-in-One Course
Feature
Label
Category 0
Category 1
Idea of Logistic Regression
ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ(ğ‘§) âˆˆ0 1 
ğ‘§= 0.535 âˆ—ğ‘¥âˆ’0.654
ğ‘¥
ğœ
ğ‘§
ğœ(ğ‘§)
AI VIETNAM
All-in-One Course
30
Feature
Label
Category 0
Category 1
Idea of Logistic Regression
ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ(ğ‘§) âˆˆ0 1 
ğ‘§= 2.331 âˆ—ğ‘¥âˆ’5.156
ğ‘¥
ğœ
ğ‘§
ğœ(ğ‘§)
AI VIETNAM
All-in-One Course
31
Feature
Label
Category 0
Category 1
Idea of Logistic Regression
ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ(ğ‘§) âˆˆ0 1 
ğ‘§(ğ‘¥)
ğ‘¥
AI VIETNAM
All-in-One Course
32
How to evaluate the performance of a model?
ğœ(ğ‘§)
ğ‘¦= ğ‘’ğ‘¥
ğ‘¦= 2ğ‘¥
ğ‘¦=
1
2
ğ‘¥
ğ‘¦=
1
6
ğ‘¥
â– Suggested Functions
ğ‘¦= log(ğ‘¥)
ğ‘¦= âˆ’log(ğ‘¥)
ğ‘¦= log(1 âˆ’ğ‘¥)
ğ‘¦= âˆ’log(1 âˆ’ğ‘¥)
ğ‘¦= ğ‘¥2
Idea of Logistic Regression
â– Loss function
AI VIETNAM
All-in-One Course
ğ¿(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
ğ¿(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
-log(à·œğ‘¦)
with y = 1
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
à·œğ‘¦
-log(1-à·œğ‘¦)
with y = 0
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
à·œğ‘¦
How to 
remove if?
Feature
Label
Category 0
Category 1
ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ(ğ‘§) âˆˆ0 1 
Idea of Logistic Regression
â– Loss function
AI VIETNAM
All-in-One Course
Feature
Label
Output
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
ğ¿(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ¿(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
à·œğ‘¦
à·œğ‘¦
L(y, à·œğ‘¦) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
Introduce the loss function in another way
Idea of Logistic Regression
â– Given a new kind of data
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
Assign numbers 
to categories
Sigmoid function 
could fit the data
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦âˆˆ0 1 
1
1 + ğ‘’âˆ’ğœ½ğ‘‡ğ’™ 
Feature
Error
if ğ‘¦= 1
error = 1 âˆ’à·œğ‘¦
if ğ‘¦= 0
error = à·œğ‘¦
error =1-à·œğ‘¦
error = à·œğ‘¦
Feature
For some ğœ½
AI VIETNAM
All-in-One Course
36
Idea of Logistic Regression
error =1-à·œğ‘¦
error = à·œğ‘¦
Error
if ğ‘¦= 1
error = 1 âˆ’à·œğ‘¦
if ğ‘¦= 0
error = à·œğ‘¦
belief = à·œğ‘¦
belief = 1 âˆ’à·œğ‘¦
Belief
if ğ‘¦= 1
belief = à·œğ‘¦
if ğ‘¦= 0
belief = 1 âˆ’à·œğ‘¦
Minimize error ~ maximize belief ~ Minimize (-belief)
ğ‘ƒ= à·œğ‘¦ ğ‘¦1 âˆ’à·œğ‘¦1âˆ’ğ‘¦
AI VIETNAM
All-in-One Course
â– Construct loss
37
Idea of Logistic Regression
â– Construct loss
belief = à·œğ‘¦
belief = 1 âˆ’à·œğ‘¦
Belief
belief = ğ‘ƒ
log_belief = logğ‘ƒ
log_belief = ğ‘¦logà·œğ‘¦+ 1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
loss = âˆ’log _belief
L(à·œğ‘¦, ğ‘¦) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
= âˆ’[ğ‘¦logà·œğ‘¦+ 1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦]
if ğ‘¦= 1
belief = à·œğ‘¦
if ğ‘¦= 0
belief = 1 âˆ’à·œğ‘¦
ğ‘ƒ= à·œğ‘¦ ğ‘¦1 âˆ’à·œğ‘¦1âˆ’ğ‘¦
One sample
AI VIETNAM
All-in-One Course
38
logğ‘ğ‘= 1
logğ‘ğ‘¥ğ‘¦= logğ‘ğ‘¥+ logğ‘ğ‘¦
CÃ´ng thá»©c phá»• biáº¿n
HÃ m log lÃ  hÃ m Ä‘Æ¡n Ä‘iá»‡u (~thá»© tá»± 
khÃ´ng thay Ä‘á»•i)
âˆ€ğ‘¥1ğ‘¥2 âˆˆğ‘ ğ‘ vÃ  ğ‘¥1 â‰¤ğ‘¥2
Logarithm
â†’log(ğ‘¥1) â‰¤log(ğ‘¥1)
á»¨ng dá»¥ng trong Machine Learning
ğ‘“ğ‘¥
ğ‘“ğ‘¥
log ğ‘“ğ‘¥
log ğ‘“ğ‘¥
TÃ¬m bá»™ tham sá»‘ ğ›‰ cho má»™t model sao 
cho model mÃ´ táº£ Ä‘Æ°á»£c dá»¯ liá»‡u training 
argmax
Î¸
ğ‘“Î¸ = argmax ğ‘ƒÎ¸ training data
Vá»›i data sample Ä‘Æ°á»£c thu nháº­p Ä‘á»™c láº­p vá»›i nhau
argmax
Î¸
ğ‘ƒÎ¸ sample_1 âˆ—â‹¯âˆ—ğ‘ƒÎ¸ sample_ğ‘›
argmax
Î¸
ğ‘“Î¸ =
DÃ¹ng hÃ m log
argmax
Î¸
logğ‘“Î¸ = argmax
Î¸
[log ğ‘ƒÎ¸ sample_1 + â‹¯+ logğ‘ƒÎ¸ sample_ğ‘›]
VÃ­ trÃ­ cá»±c Ä‘áº¡i cá»§a hÃ m ğ’‡ğœ½ vÃ  ğ¥ğ¨ğ ğ’‡ğœ½ khÃ´ng thay Ä‘á»•i 
Idea of Logistic Regression
belief = à·œğ‘¦
belief = 1 âˆ’à·œğ‘¦
Belief
ğ‘ƒğ‘–= à·œğ‘¦ğ‘–
ğ‘¦ğ‘–1 âˆ’à·œğ‘¦ğ‘–1âˆ’ğ‘¦ğ‘–
belief = à·‘
ğ‘–=1
ğ‘›
ğ‘ƒğ‘–
log_belief = à·
ğ‘–=1
ğ‘›
logğ‘ƒğ‘–
log_belief = à·
ğ‘–=1
ğ‘›
ğ‘¦ğ‘–logà·œğ‘¦ğ‘–+ 1 âˆ’ğ‘¦ğ‘–log 1 âˆ’à·œğ‘¦ğ‘–
loss = âˆ’log _belief
since iid
L = 1
ğ‘âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’šâˆ’(ğŸâˆ’ğ’šğ‘‡)ğ‘™ğ‘œğ‘”ğŸâˆ’à·ğ’š
Binary cross-entropy
= âˆ’à·
ğ‘–=1
ğ‘›
ğ‘¦ğ‘–logà·œğ‘¦ğ‘–+ 1 âˆ’ğ‘¦ğ‘–log 1 âˆ’à·œğ‘¦ğ‘–
if ğ‘¦ğ‘–= 1
belief = à·œğ‘¦ğ‘–
if ğ‘¦ğ‘–= 0
belief = 1 âˆ’à·œğ‘¦ğ‘–
N samples
â– Construct loss
AI VIETNAM
All-in-One Course
40
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğœ•ğ¿
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘§
ğœ•ğ‘§
ğœ•ğœƒğ‘–
ğœ•à·œğ‘¦
ğœ•ğ‘§= à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ‘§
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
Derivative
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•à·œğ‘¦= âˆ’ğ‘¦
à·œğ‘¦+ 1 âˆ’ğ‘¦
1 âˆ’à·œğ‘¦=
à·œğ‘¦âˆ’ğ‘¦
à·œğ‘¦(1 âˆ’à·œğ‘¦)
Idea of Logistic Regression
L(à·œğ‘¦âˆ’ğ‘¦) = âˆ’ğ‘¦log à·œğ‘¦âˆ’(1 âˆ’ğ‘¦)log 1 âˆ’à·œğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
Model and Loss
â– Construct loss
AI VIETNAM
All-in-One Course
41
-log(à·œğ‘¦)
with y = 1
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
à·œğ‘¦
-log(1-à·œğ‘¦)
with y = 0
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
à·œğ‘¦
Idea of Logistic Regression
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
1
1 + ğ‘’âˆ’ğ‘§
1
1 + ğ‘’âˆ’ğ‘§
AI VIETNAM
All-in-One Course
42
â¢Optimization Review
â¢Linear Regression Review
â¢Logistic Regression
â¢Examples
â¢Vectorization
â¢Implementation (optional)
Outline
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ½ğ‘‡ = [ğ‘ ğ‘¤]
ğ’™ğ‘‡ = [1 
ğ‘¥]
Model
Label
Loss
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¦
ğ‘¥
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘
ğ‘¤
AI VIETNAM
All-in-One Course
43
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
Logistic Regression-Stochastic
ğ’™ =
1
1.4
ğ’š= 0
Dataset
Model
Loss
0.1
ğ‘¦
ğ‘¥
-0.1
ğ‘¥= 1.4
ğ‘§= âˆ’0.0399
à·œğ‘¦= 0.49
ğ‘¦= 0
L = 0.6733
ğ‘
ğ‘¤
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
AI VIETNAM
All-in-One Course
44
Logistic Regression-Stochastic
ğ’™ =
1
1.4
ğ’š= 0
Dataset
ğœ‚= 0.01
ğ¿ğ‘
â€²
ğ¿w
â€²
=
1 âˆ—0.49
1.4 âˆ—0.49 =
0.49
0.686
ğ‘= 0.1 âˆ’ğœ‚0.49=0.095
Model
Loss
0.1
ğ‘¦
ğ‘¥
-0.1
ğ‘¥= 1.4
ğ‘§= âˆ’0.0399
à·œğ‘¦= 0.49
L = 0.6733
ğ‘
ğ‘¤
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¦= 0
ğ‘¤ = âˆ’0.1 âˆ’ğœ‚0.686=âˆ’0.1068
AI VIETNAM
All-in-One Course
45
Logistic Regression-Stochastic
ğ’™ =
1
1.4
ğ’š= 0
Dataset
Model
Loss
0.095
ğ‘¦
ğ‘¥
âˆ’0.1068
ğ‘¥= 1.4
ğ‘§= âˆ’0.0545
à·œğ‘¦= 0.486
L = 0.666
ğ‘
ğ‘¤
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¦= 0
previous L = 0.6733
AI VIETNAM
All-in-One Course
46
Another example
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¤ğ‘–= ğ‘¤ğ‘–âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
ğœ½ğ‘‡ = [ğ‘ ğ‘¤1 ğ‘¤2]
ğ’™ğ‘‡ = [1 ğ‘¥1 ğ‘¥2]
Model
Label
Loss
ğ‘¥1
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¦
ğ‘¥2
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
ğ‘¤1
ğ‘
ğ‘¤2
AI VIETNAM
All-in-One Course
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
Model
Label
Loss
ğ‘¥1
0.5
0.1
ğ‘¦
ğ‘¥2
-0.1
ğ‘¥1 = 1.4
ğ‘¥2 = 0.2
ğ‘§= 0.78
à·œğ‘¦= 0.6856
ğ‘¦= 0
ğ¿= 1.1573
ğ‘¤1
ğ‘
ğ‘¤2
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
Logistic Regression-Stochastic
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
AI VIETNAM
All-in-One Course
ğ’™ =
1
1.4
0.2
ğ’š= 0
Dataset
48
Logistic Regression-Stochastic
Model
Loss
ğ‘¥1
0.5
0.1
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
ğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘¥2
-0.1
ğœ‚= 0.01
ğ‘¤1
ğ‘
ğ‘¤2
ğ¿ğ‘
â€²
ğ¿ğ‘¤1
â€²
ğ¿ğ‘¤2
â€²
=
1 âˆ—0.6856
1.4 âˆ—0.6856
0.2 âˆ—0.6856
=
0.6856
0.9599
0.1371
ğ‘= 0.1 âˆ’ğœ‚0.6856
ğ‘¤1= 0.5 âˆ’ğœ‚0.9598
ğ‘¤2= âˆ’0.1+ğœ‚0.1371
ğ¿ğ‘
â€²
ğ¿ğ‘¤1
â€²
ğ¿ğ‘¤2
â€²
ğ‘§= 0.78
à·œğ‘¦= 0.6856
ğ‘¦= 0
ğ¿= 1.1573
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¥1 = 1.4
ğ‘¥2 = 0.2
AI VIETNAM
All-in-One Course
ğ’™ =
1
1.4
0.2
ğ’š= 0
Dataset
=0.0931
=0.4990
=âˆ’0.1013
Logistic Regression-Stochastic
Model
Loss
ğ‘¥1
0.4904
0.0931
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
ğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘¥2
-0.1013
ğœ‚= 0.01
ğ‘¤1
ğ‘
ğ‘¤2
ğ¿ğ‘
â€²
ğ¿ğ‘¤1
â€²
ğ¿ğ‘¤2
â€²
à·œğ‘¦= 0.6856
ğ¿= 1.1573
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘§= 0.78
ğ‘¦= 0
AI VIETNAM
All-in-One Course
ğ’™ =
1
1.4
0.2
ğ’š= 0
Dataset
ğ¿ğ‘
â€²
ğ¿ğ‘¤1
â€²
ğ¿ğ‘¤2
â€²
=
1 âˆ—0.6856
1.4 âˆ—0.6856
0.2 âˆ—0.6856
=
0.6856
0.9599
0.1371
ğ‘= 0.1 âˆ’ğœ‚0.6856
ğ‘¤1= 0.5 âˆ’ğœ‚0.9598
ğ‘¤2= âˆ’0.1+ğœ‚0.1371
=0.0931
=0.4990
=âˆ’0.1013
ğ‘¥1 = 1.4
ğ‘¥2 = 0.2
Logistic Regression-Stochastic
Model
Loss
ğ‘¥1
0.4904
0.0931
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
ğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘¥2
-0.1013
ğ‘¤1
ğ‘
ğ‘¤2
à·œğ‘¦= 0.6812
ğ¿= 1.1432
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘§= 0.75
ğ’™ =
1
1.4
0.2
ğ’š= 0
Dataset
previous ğ‘³= 1.1573
ğ‘¥1 = 1.4
ğ‘¥2 = 0.2
ğ‘¦= 0
AI VIETNAM
All-in-One Course
51
â¢Optimization Review
â¢Linear Regression Review
â¢Logistic Regression
â¢Examples
â¢Vectorization
â¢Implementation (optional)
Outline
Review
Multiply with a number
ğ›¼ğ‘¢= ğ›¼
ğ‘¢1
â€¦
ğ‘¢ğ‘›
=
ğ›¼ğ‘¢1
â€¦
ğ›¼ğ‘¢ğ‘›
=
data 
1
2
3
2
*
result 
2
4
6
1
3
2
4
T
1
2
3
4
A =
ğ‘11 â€¦  ğ‘1ğ‘›
â€¦ â€¦ . . .
ğ‘ğ‘š1 â€¦  ğ‘ğ‘šğ‘›
Ağ‘‡=
ğ‘11 â€¦  ğ‘ğ‘š1
â€¦ â€¦ â€¦
ğ‘1ğ‘› â€¦  ğ‘ğ‘šğ‘›
Ô¦ğ‘£=
ğ‘£1
â€¦
ğ‘£ğ‘›
Ô¦ğ‘£ğ‘‡= ğ‘£1 â€¦ ğ‘£ğ‘›
1
2
T
1
2
AI VIETNAM
All-in-One Course
Transpose
52
Review
Dot product
Ô¦ğ‘£=
ğ‘£1
â€¦
ğ‘£ğ‘›
ğ‘¢=
ğ‘¢1
â€¦
ğ‘¢ğ‘›
Ô¦ğ‘£âˆ™ğ‘¢= ğ‘£1 Ã— ğ‘¢1 + â‹¯+ ğ‘£ğ‘›Ã— ğ‘¢ğ‘›
v 
1
2
2
3
w 
=
result 
8
53
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Vectorization
AI VIETNAM
All-in-One Course
ğ‘§= ğ‘¤ğ‘¥+ ğ‘1 = ğ‘ ğ‘¤
1
ğ‘¥= ğœ½ğ‘‡ğ’™
dot product
Traditional
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ½= ğ‘
ğ‘¤
â†’ğœ½ğ‘‡= ğ‘ ğ‘¤
Feature
Label
ğ‘¥
ğ‘¦
54
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§= ğœ½ğ‘‡ğ’™
ğ¿à·œğ‘¦, ğ‘¦= (à·œğ‘¦âˆ’ğ‘¦)2
numbers
What will we do?
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
55
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Traditional
Vectorization
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦âˆ’ğ‘¦Ã— 1
à·œğ‘¦âˆ’ğ‘¦Ã— ğ‘¥ =
à·œğ‘¦âˆ’ğ‘¦
1 
ğ‘¥ =
à·œğ‘¦âˆ’ğ‘¦ğ’™=
ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ¿
ğœ•ğ‘¤
= ğ›»ğœ½ğ¿
common factor
â†’
ğ›»ğœ½ğ¿= 2ğ’™( Æ¸ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥à·œğ‘¦âˆ’ğ‘¦=
à·œğ‘¦âˆ’ğ‘¦Ã— ğ‘¥
ğœ•ğ¿
ğœ•ğ‘=
à·œğ‘¦âˆ’ğ‘¦ =
à·œğ‘¦âˆ’ğ‘¦Ã— 1
56
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğœ½ğ‘‡ğ’™
ğ‘¤ =  ğ‘¤ âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘ =  ğ‘ âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğ›»ğœ½ğ¿=
ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ¿
ğœ•ğ‘¤
ğœ½
ğœ½
ğ›»ğœ½ğ¿
â†’
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
57
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ’™(à·œğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
Traditional
Vectorized
Vectorization
AI VIETNAM
All-in-One Course
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
Vectorization
AI VIETNAM
All-in-One Course
â– Implementation (using Numpy)
# Given X and y
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ’™(à·œğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
59
Model
Input
Label
Loss
ğ’™
ğœ½=
0.1
0.5
âˆ’0.1
à·œğ‘¦= ğœğœ½ğ‘‡ğ’™ = 0.6856
ğ¿= 1.1573
ğ‘¦= 0
ğ’™=
1
ğ‘¥1
ğ‘¥2
=
1
1.4
0.2
Given ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
ğ›»ğœ½ğ¿= ğ’™à·œğ‘¦âˆ’ğ‘¦=
1
1.4
0.2
0.6856 =
0.6856
0.9599
0.1371
=
ğ¿ğ‘
â€²
ğ¿ğ‘¤1
â€²
ğ¿ğ‘¤2
â€²
ğ›‰âˆ’Î·Lğ›‰
â€² =
0.1
0.5
âˆ’0.1
âˆ’Î·
0.6856
0.9599
0.1371
=
0.093
0.499
âˆ’0.101
ğœ‚= 0.01
1
3
Dataset
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ’™(à·œğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
4
5
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿ğœ½= âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ±(à·œy âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
ğ‘§= ğœ½ğ‘‡ğ’™
Demo
AI VIETNAM
All-in-One Course
ğ’™ =
1
1.4
0.2
ğ’š= 0
Dataset
61
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿ğœ½= âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ±(à·œy âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
ğ‘§= ğœ½ğ‘‡ğ’™
