amp . GradScaler ()
9
10 for epoch in range ( num_epochs ):
11 epoch_loss = []
12 model . train ()
13 for idx , batch in zip ( tqdm ( range ( len ( train_dataloader )), desc =f’ Training batch : {
epoch +1} ’), train_dataloader ):
14 input_ids = batch . pop (’ input_ids ’).to( device )
15 pixel_values = batch . pop (’ pixel_values ’).to( device )
16 attention_masked = batch . pop (’ attention_mask ’).to( device )
17 labels = batch . pop (’ labels ’).to( device )
18
19 with torch . amp . autocast ( device_type =’cuda ’, dtype = torch . float16 ):
20 outputs = model (
21 input_ids = input_ids ,
22 pixel_values = pixel_values ,
23 labels = labels
24 )
25
26 loss = outputs . loss
27 epoch_loss . append ( loss . item ())
28
29 optimizer . zero_grad ()
30 scaler . scale ( loss ). backward ()
31 scaler . step ( optimizer )
32 scaler . 