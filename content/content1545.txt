<<<<<<< HEAD
Node ID: a9f6a33d-d8bf-4be1-aa71-4d9eb9002c8c
Text: 8. Dòng code nào sau đây biểu diễn chiến lược cân bằng giữa
exploration và exploitation? (a) np.argmax(Q[s]) (b)
np.random.choice(A) (c) np.argmin(Q[s]) (d) np.argmax(Q[s]) if
np.random.rand() > e else np.random.choice(A) 9. Mục đích của yếu tố
Hệ số chiết khấu (Discounting Factor) là gì? (a) Gia tăng kích thước
của không gian trạng thái. (b) Gi...
=======
8. Dòng code nào sau đây biểu diễn chiến lược cân bằng giữa exploration và exploitation?
(a) np.argmax(Q[s])
(b) np.random.choice(A)
(c) np.argmin(Q[s])
(d) np.argmax(Q[s]) if np.random.rand() > e else np.random.choice(A)
9. Mục đích của yếu tố Hệ số chiết khấu (Discounting Factor) là gì?
(a) Gia tăng kích thước của không gian trạng thái.
(b) Giảm kích thước của không gian trạng thái.
(c) Tăng điểm thưởng tích lũy kỳ vọng nhận được.
(d) Cân bằng độ quan trọng giữa điểm thưởng nhận được tức khắc và tương lai.
10. Trong Q-learning, hàm nào sau đây miêu tả tìm kiếm optimal policy?
(a) π∗(s) = arg maxa Q∗(s, a)
(b) π∗(s) = arg maxa V ∗(s)
(c) π∗(s) = arg mina Q∗(s, a)
(d) π∗(s) = arg mina V ∗(s)
11. Trong Q-Learning, công thức nào dưới đây biểu diễn hàm cập nhật Q-Value của trạng tháis và
hành độnga?
(a) Q(s, a) =Q(s, a) +α[r + γ P
a′ Q(s′, a′) − Q(s, a)]
(b) Q(s, a) =Q(s, a) +α[r + γQ(s′, a′) − Q(s, a)]
(c) Q(s, a) =Q(s, a) +α[r + γ maxa′ Q(s′, a′) − Q(s, a)]
(d) Q(s, a) =Q(s, a) +α[r − γ maxa′ Q(s′, a′) − Q(s, a)]
12. Hàm nào sau đây không phải là hàm điểm thưởng tích lũy kỳ vọng?
(a) V π(s) =Eπ
τt∼p(τt)[P∞
i=0 γi · rt+i|st = s]
16
>>>>>>> HEAD@{1}
