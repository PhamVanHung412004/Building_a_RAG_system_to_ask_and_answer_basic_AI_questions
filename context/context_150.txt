IMAGE COLORIZATION PROBLEM USING 
(VARIATIONAL) AUTO-ENCODER
Prepared by: 
Khanh Duong, Tien-Huy Nguyen, Nhu-Tai Do
taidn@ueh.edu.vn
Agenda
2
1. Introduction
2. Image Colorization Problem
3. Context Auto-Encoder Approach
4.    Variational Auto-Encoder Approach
Introduction about (Variational) 
Auto-Encoder
3
4
Introduction
[1] Russ Salakhutdinov, Deep Unsupervised Learning, Slides, CMU
Massive increase in the amount 
of the data
Mostly Unlabeled
Bag of Word
Deep Unsupervised Model
Learned latent code
(Hinton & Salakhutdinov, 
Science 2006)
Reuters dataset: 804,414
newswire stories: unsupervised
Inference and discover 
structure at multiple levels
underlying structure, 
cause, or statistical
correlation
5
What is Unsupervised Learning?
Supervised Learning
Unsupervised Learning
Data
(x, y) - x is data, y is label
x - Just data, no labels
Goal
Learn a function to map x -> y
Learn some underlying hidden 
structure of the data
Application
Classification, Regression, Object 
detection, Semantic segmentation, 
Image Captioning, â€¦
Clustering, Dimensionality reduction, 
Feature learning, Density estimation, â€¦
Clustering
Dimensionality reduction
Feature learning
[2] CS231n Convolutional Neural Networks for Visual Recognition, Standford
Training data is cheap
Solve unsupervised learning => understand structure of visual world
6
Technical mind-map in Unsupervised Learning
Unsupervised Learning
Non-probabilistic 
(Parametric) Models
Probabilistic 
(Generative) Models
Explicit Density
p(x)
Implicit Density
p(x)
Approximate 
Density
Tractable 
Density
- Sparse Coding
- Autoencoders
- Others (e.g. k-means)
- Fully observed Belief Nets
- NADE
- PixelRNN
- Boltzmann Machines
- Variational Autoencoders
-Helmholtz Machines
-Many othersâ€¦
- Generative Adversarial Networks
- Moment Matching Networks
- Markov Chain
[1] Russ Salakhutdinov, Deep Unsupervised Learning, Slides, CMU
Figure copyright and adapted from 
Ian Goodfellow, Tutorial on 
Generative Adversarial Networks, 
2017
Dimensionality Reduction Problem
â€¢ Given input data X with N samples in D dimension space
ğ‘‹= ğ‘‹ğ‘= ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘, ğ‘¥ğ‘–âˆˆâ„ğ·
â€¢ Find feature matrix W: ğ‘Š= ğ‘Šğ‘€= ğ‘¤1, ğ‘¤2, â€¦ , ğ‘¤ğ‘€, ğ‘¤ğ‘–âˆˆâ„ğ·
â€¢ Use W to transform X into weight matrix à·¨ğ‘ : à·¨ğ‘= ğ‘Šğ‘‡ğ‘‹
â€¢ Find a good representation?
â€¢ Reduce redundancy in the data?
7
=
features
samples
dimensions
samples
features
dimensions
Input Matrix
Feature Matrix
Weight Matrix
Dimensionality Reduction Problem
â€¢ Desirable feature features:
â€“ Avoid feature similarity â†’    ğ‘¤ğ‘–
ğ‘‡ğ‘¤ğ‘—= 0 â†’ linear combination
â€“ Give â€œsimpleâ€ weights   â†’    ğ¶ğ‘œğ‘£ğ‘§ğ‘–, ğ‘§ğ‘—= ğ¼ â†’ minimize relation of the 
two dimensions
â€¢ Satisfy minimising the total squared reconstruction error:
ğ‘Šğ·ğ‘‹âˆ’ğ‘Šğ‘€ğ‘‹2 â†’ğ‘šğ‘–ğ‘›
Where ğ‘€â‰ªğ·, ğ‘Šğ‘€âŠ‚ğ‘Šğ·
8
PCA 2D
[3] https://en.wikipedia.org/wiki/Principal_component_analysis
Feature Learning
Motivation
â€¢
Training very deep neural networks is difficult:
â€“ Magnitudes of gradients in lower layers and in higher layers are different
â€“ The landscape of objective function is difficult for SGD to find a good local 
optimum
â€“ Many parameters to remember training data and do not generalize well
â€¢
The goal of pretraining is to address the above problems:
â€“ Pretraining step: train a sequence of shallow autoencoders, greedily one layer at 
a time, using unsupervised data
â€“ Fine-tuning step 1: train the last layer using supervised data
â€“ Fine-tuning step 2: use backpropagation to fine-tune the entire network using 
supervised data
9
[7] Quoc V.Le, A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks, Google 
Brain, robotics.stanford.edu/~quocle/tutorial2.pdf
Feature Learning
General Architecture
10
[8] T.Paine, An analysis of unsupervised pre-training in light of recent advances, ICLR 2015
ğ‘¥
à·¤ğ‘¥
Hidden 1
Hidden 2
Hidden 3
ğ‘¥âˆ’Çğ‘¥2
ğŸ
ğ‘¥
Hidden 1
Hidden 2
FC
Softmax
â‰ˆğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ 
Phase 1
Train the Autoencoder 
using all data
Phase 2
Train the Autoencoder 
on â€œlabeledâ€ data
Work often better:
â€¢
learns internal data representation: may 
be useful features
â€¢
initializes optimization from more 
favorable initial approximation: good for 
solving vanishing gradient problem
â€¢
especially useful when few labelled 
examples and many unlabeled
Deep Learning 
General AutoEncoders
â€¢
Autoencoders: artificial neural networks
â€“
Capable of learning efficient representations of the input data, called latent code
â€“
Without any supervision, simply learning to reconstruct original data
â€“
Need to constrain complexity: (1) by architectural constraint (2) by penalty on internal 
representation
11
ğ‘¥
à·¤ğ‘¥
ğ‘§
Encoder
Decoder
Input  Data
Features
(Latent Code)
Reconstructed 
Data
ğ‘¥âˆ’Çğ‘¥2
ğŸ
Loss function
Goal: Train such that features used 
to reconstruct original data, donâ€™t 
use labels
Hidden layer z: features
+ smaller than x (dimensionality 
reduction)
+ sparse constraint (larger than x) 
Encoder, Decoder:
+ Linear + Nonlinearity (sigmoid)
+ Deep, fully â€“ connected
+ ReLU CNN
[2] CS231n Convolutional Neural Networks for Visual Recognition, Standford
12
Vanilla (Undercomplete) AutoEncoder
Input
(None, 784)
Dense
(None, 32)
Dense
(None, 784)
Encoder
Decoder
Auto Encoder
Sigmoid
Normalize 
[0, 1]
Normalize 
[0, 1]
PCA Role
ğ‘‹
ğ‘§
à·¨ğ‘‹
ğ‘Š
ğ‘‰
+ Encoding: X (input data), f (activation 
function)
 ğ‘§= ğ’‡ğ‘Šğ‘‹
+ Decoding: g (activation function)
à·©ğ‘‹ = ğ’ˆğ‘‰ğ‘§= ğ’ˆğ‘‰ğ’‡ğ‘Šğ‘‹
+ If g, f is linear function:
à·©ğ‘‹ = ğ‘‰ğ‘Šğ‘‹
+ Loss function MSE:
min
ğ‘Š,ğ‘‰ğ‘‹âˆ’à·¨ğ‘‹
min
ğ‘Š,ğ‘‰ğ‘‹âˆ’ğ‘‰ğ‘Šğ‘‹ 
Dimensionality reduction with z as new 
subspace for input data X, ability 
reconstruct X with à·¨ğ‘‹.
If g, f is non-linear function (sigmoid) â†’ 
Non-Linear PCA
Denoising Autoencoder
â€¢
To avoid overfitting and improve the robustness, the input is partially corrupted by
adding noises to or masking some values of the input vector in a stochastic manner
13
Variational Autoencoder
â€¢
Instead of mapping the input into a fixed vector, we want to map it into a
distribution.
14
Image Colorization Problem
15
Introduction
â€¢
Problem: Image Colorization is the task of colorizing gray-scale images.
â€¢
Practical applications: coloring old black and white images, movies etc.
â€¢
Main approaches: Scribble-based, Example-based, and Fully Automatic.
16
User Stroke on Image
Reference image
Gray-scale image
Gray-scale image
Scribble-based colorization
Example-based colorization
Fully Automatic colorization
Introduction
â€¢
Our problem focuses on Fully Automatic Colorization: Given the grayscale 
image, produce a plausible colorization to fool a human observer.
â€“ Input: Grayscale image in grids of pixels from 0 â€“ 255
â€“ Output: Channel a, b of color image in CIE Lab color space
â€“ 94% of the cells in our eyes determine brightness, only 6% for 
colors â†’ grayscale image is a lot sharper than the color layers.
17
https://www.freecodecamp.org/news/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d/
Related works
â€¢
Non-parametric methods: transfer color reference images onto input image 
from analogous regions
â€¢
Parametric methods: learn prediction functions from large datasets
â€“ Problem define: (1) regression onto continuous color space, (2) classification of 
quantized color values
â€“ Approach: (1) Hand-engineered Features  (2) Deep networks
18
Related works
â€¢
Parametric methods: Hand-engineered Features
â€“ Cheng et al.1: adaptive image clustering according to global information, every 
neural network trained on specific cluster for colorization with L2 Regression 
loss, using joint bilateral filtering for post-processing.
â€“ Charpiat et al.2: deal with multimodality in colorization with the probability 
distribution of all possible colors on every pixel, use graph-cut to maximize 
the probability, discretization of the color space.
19
[1] Z. Cheng, Q. Yang, and B. Sheng, â€œDeep colorization,â€ IEEE International Conference on Computer Vision, pp. 415â€“423, 2015.
[2] G. Charpiat, M. Hofmann, and B. SchÃ¶lkopf, â€œAutomatic image colorization via multimodal predictions,â€ Lecture Notes in Computer 
Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 5304 LNCS, pp. 126â€“139, 2008.
Cheng et al.
Charpiat et al.
Regression
Classification
Related works
â€¢
Parametric methods: Deep Learning Approach:
â€“ Larsson et al.1:  use un-rebalanced classification loss, build on hypercolumns on a 
VGG network,  train on ImageNet, evaluate on PSNR, RMSE.
â€“ Iizuka et al.2: use a regression loss, build a two-stream architecture fusing global 
and local features, train on Places scene dataset, evaluate on naturalness of the 
colorizations by user asking
20
[1] G. Larsson, M. Maire, and G. Shakhnarovich, â€œLearning Representations for Automatic Colorization,â€ in Lecture Notes in Computer 
Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 9908 LNCS, 2016, pp. 577â€“593.
[2] S. Iizuka, E. Simo-Serra, and H. Ishikawa, â€œLet there be Color: Joint End-to-end Learning of Global and Local Image Priors for Automatic 
Image Colorization with Simultaneous Classificatio,â€ ACM Transactions on Graphics, vol. 35, no. 4, pp. 1â€“11, Jul. 2016.
Larsson et al.
Iizuka et al.
Regression
Classification
Related works
â€¢
Zhang et. at1: Main idea
â€“ Multinomial classification problem by quantize ab space into grid size 10, keep 
313 bins in gamut.
â€“ Cross entropy loss with class rebalancing to encourage learning of rare colors.
â€“ Post-processing: per-pixel color distribution to single point estimate by 
interpolating  between mean and mode with annealed-mean.
21
[1] R. Zhang, P. Isola, and A. A. Efros,Colorful Image Colorization, ECCV, pp. 649â€“666, 2016
Deep network model
Related works
â€¢
Quantization process in classification approach from Richard Zhang et. al.:
â€“ Quantization Lab Color Space into 313 bins
â€“ Using soft-encoding scheme instead of nearest searching
â€¢
Benefits from this quantization process to classify:
â€“ Prevent the averaging effect of regression loss: easy to favor grayish, desaturated 
results
â€“ Increase the correlation between nearest color pixels by soft-encoding.
22
ğ¿áˆ˜ğ‘, ğ‘= âˆ’1
ğ»ğ‘Šà·
â„,ğ‘¤
ğ‘£ğ‘â„,ğ‘¤à·
ğ‘
ğ‘â„,ğ‘¤,ğ‘ğ‘™ğ‘œğ‘”áˆ˜ğ‘â„,ğ‘¤,ğ‘
Rarity weighting
Target distribution
Predicted distribution
Category Cross entropy loss 
Related works
â€¢
Smoothing the color prior probability:
23
Distribution of probability vs smoothness probability
Smoothness of color probability, Invert Probability
Related works
â€¢
More details: The ab color distribution
â€“
Soft-Encoding Process:
â€¢ Step 1: For every pixel of image, convert from ab values to color index q (encoding) 
using K-Nearest
â€¢ Step 2: Convert to one-hot encoding representation
â€¢ Step 3: Apply label smoothing 
â€“ Use K-Nearest neighbors  to get 4 color indexes nearest q, 
â€“ Generate 5 gaussian values, and normalize 
24
ğ¼ğ‘ğ‘ğ‘= (ğ‘, ğ‘)
q âˆˆ[0,312]
â€¦
0
0
1
0
0
â€¦
â€¦
0.05
0.24
0.42
0.24
0.05
â€¦
quantize ab space  with grid size 
10 (313 bins)
0
n-1
0
312
â€¦
0
312
â€¦
q
q
Context Auto-Encoder Approach
25
Challenges
â€¢
Averaging effect: grayish, desaturated results 
due to 94% of the cells in our eyes determine 
brightness, only 6% for colors. Grayscale 
image is a lot sharper than the color layers.
â€¢
Rare colors in images: strongly biased due to 
the appearance of backgrounds such as 
clouds, pavement, dirt, and walls.
â€¢
Semantic information matters: In order to 
colorize any kind of image, a system must 
interpret the semantic composition of the 
scene (what is in the image: faces, cars, 
plants, . . . ) as well as localize objects (where 
things are). 
26
Context-Aware Colorization
â€¢
Objectives:
â€“ Integrate scene-context classification and pixel-wise semantic segmentation
27
pixel-wise semantic segmentation
+ what object the pixels belong to
scene-context classification
+ global scene information
Segmentation classes in Coco-Stuff
(0: unlabeld, 1 â€“ 182: objects & stuffes)
Scene-context classes
(totally 365 classes)
Context-Aware Colorization
â€¢
Objectives:
â€“ Use ab color distribution to encourage rare color (rebalancing colors), and multi-
modal in colorization
28
0
n-1
ab color distribution
vs.
ab color value
With a pixel
Gray
Ours
GT
Sky (common colors)
Tree
Multi-Modal Attribute or Bias
(many choice in colorization)
leading to
Grayish or Desaturated Effect
GT
Gray
Grayish result
Shirt (diversity colors, rare colors)
Semantic Image Colorization Auto-Encoder
â€¢
Take advantage of skip connections between the contracting and expanding path at 
the same depth level using U-Net model (prevent dying ReLU and vanishing problem)
â€¢
Use multi-task learning with end-end training from gray-scale image to four outputs 
for learning mutual benefits of global/local context, content accuracy and color 
biases.
29
flexible colorize for rare 
color
Global-style context from 
Scene Types of Places365
Local context at pixel-level 
with 183 objects and 
stuffes from Coco-Stuff
365
W x H x 183
W x H x 313
W x H x 2
Content accuracy 
(grayish effects)
encoding features
(7x7x1024)
decoding feature map
(W x H x 64) 
Input image
(224 x 224 x 1) 
[1] Nhu-Tai Do et al. "Image colorization using the global scene-context style and pixel-wise semantic segmentation." IEEE Access 8 (2020): 
214098-214114.
Scene-context classification
â€¢
Extract the scene probabilities of training dataset (without scene-context 
ground-truth) based on pre-trained model on Places3651.
â€¢
Label Smoothing2 with top-5 prediction: keep 5 highest probabilities, set all 
remain values  to 0, and normalize the probabilities with sum 1.
30
Image
Pre-trained 
Model
0
1
2
3
â€¦
n-1
Scene probability with n=365
Top-1: Cafeteria (0.179)
Top-2: Restaurant (0.167)
Top-3: dining_hall (0.091)
Top-4: coffe_shop (0.086)
Top-5: restaurant_patio (0.080)
Top-1: Cafeteria (0.297)
Top-2: Restaurant (0.277)
Top-3: dining_hall (0.151)
Top-4: coffe_shop (0.143)
Top-5: restaurant_patio (0.132)
[1] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, â€œPlaces: A 10 Million Image Database for Scene Recognition,â€ IEEE transactions on pattern analysis and machine 
intelligence (TPAMI), vol. 40, no. 6, pp. 1452â€“1464, 2018 
[2] R. MÃ¼ller, S. Kornblith, and G. Hinton, â€œWhen Does Label Smoothing Help?,â€ In Advances in Neural Information Processing Systems (NeurIPS), pp.4696-4705, 2019.
Regression/Color Distribution/Segmentation 
Branches
â€¢
Compute backward gradients of three branches to enhance decoding feature map 
Xmap and encoding feature Xenc
â€“
regression branch to keep the accuracy between prediction/ground-truth â†’ output results 
with grayish and desaturated effects (not used as colorized result)
â€“
color distribution branch to encourage rare color (rebalancing colors) and multi-modal in 
colorization â†’output results with more vivid
â€“
segmentation branch to help the system understand what object the pixels belong to (with 
183 object & stuff labels) â†’output results with more precise edge
31
Gray
Ground-Truth
Reg
Soft
Seg
Learning mutual benefits at 
pixel-level
colorize 
result
Quantitative comparisons
32
â€“
Larsson et al.: better on PSNR for ImageNet,DIV2K, and COCO-Stuff and on SSIM results for 
ImageNet and DIV2K.
â€“
Our methods: better on L2ab metric for DIV2K, Places365, and COCO-Stuff
â€“
Semantic segmentation played an important role in enhancing the colorization results, and 
it helped our method improve the accuracy of the ab channels.
Qualitive Comparisions
33
SegClasRegSoft
RegSoft
Ground-truth
Input
Zhang
Iizuka
Larsson
ClasRegSoft
Div2k
Coco-Stuff
ImageNet
Places365
â–SUCCESSFUL CASES
Results were more vibrant and had more precise edges than the other methods. Moreover, the 
yellow color noise also was reduced in our ClasRegSoft versions comparison on RegSoft version.
Qualitive Comparisions
34
â–SOME FAIL CASES
SegClasRegSoft
RegSoft
Ground-truth
Input
Zhang
Iizuka
Larsson
ClasRegSoft
My results met difficulties for colorization with incorrect colors, noise occurrences. These defects are 
similar to the results of Iizuka et al. and Larsson et al..
Project: VAE-Based Image 
Colorization
35
THANKS FOR LISTENING!
Waiting for question!
36
