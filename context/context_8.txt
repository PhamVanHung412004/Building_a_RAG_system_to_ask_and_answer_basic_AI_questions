Text Generation
Year 2024
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
â¢Motivation to Text Generation
â¢Simple Models
â¢Using RNNs
â¢Examples
â¢Using Masked Encoder (~Decoder)
Outline
Self-Supervision Using Image Data
AI VIETNAM
All-in-One Course
Model
Noisy and grayscale images
Clean and color images
Model
Model
Noisy images
Clean images
Grayscale images
Color images
1
Self-Supervision Using Image Data
AI VIETNAM
All-in-One Course
https://arxiv.org/pdf/2111.06377.pdf
2
Self-Supervision Using Text Data
â– How?
natural language processing is a branch of artificial intelligence
natural
language
X
y
Text Model
natural
language
language
à·œğ‘¦
ğ‘¦
loss
update
3
Text Generation
AI VIETNAM
All-in-One Course
â– Applications
â– From the viewpoint of users
Embedding
Vectorization
Model
Output
Input
Inference
4
Simple Model
RNN Cell
RNN Cell
ho
Wih
Whh
bih
Wih
bih
h1
h2
bhh
ai
há»c
Ä‘i
4
3
há»c
[-1.27,  0.84,  0.04]
Embedded sample 2
[-0.88,  0.81,  0.77]
Embedded sample 1
Model
Model
ai
há»c
Output
Loss
optimizer
há»c
input
Ä‘i
há»c
ai
data
ai
label
Ä‘i
há»c
Label
ho
Whh
bhh
Output
Label
Loss
optimizer
Simple Model
5
RNN Cell
RNN Cell
ho
Wih
Whh
bih
Wih
bih
h1
h2
bhh
?
?
?
?
?
?
Model
Model
?
?
Output
Loss
optimizer
cÃ³
lÃ m
má»›i
cÃ³
Äƒn
data
Label
ho
Whh
bhh
Output
Label
Loss
optimizer
input & label
Discussion and Finding out problems
Another Example
6
Text Generation
AI VIETNAM
All-in-One Course
â– Input is a set of tokens
data = 'trÄƒm nÄƒm trong cÃµi ngÆ°á»i ta'
Embedding
Vectorization
Model
A Token
Input
7
Text Generation
AI VIETNAM
All-in-One Course
Text Generation Model
interesting
Learning AI is
Vectorization
5
2
6
Embedding
Model
Softmax
â€¦
size?
Where is â€œinterestingâ€ from? 
size = vocab_size
Another Example
Learning AI is interesting
AI is a CS Topic
8
Text Generation
Text Generation Model
interesting
Learning AI is
Vectorization
5
2
6
Embedding
Model
0.x
0.x
0.x
0.x
0.8
0.x
0.x
0.x
index
0
1
2
3
4
5
6
7
word
pad
[UNK]
ai
we
interesting
learning
is
cs
Text Generation Model
is
Learning AI
Vectorization
5
2
Embedding
Model
0.x
0.x
0.x
0.x
0.x
0.x
0.8
0.x
Abstract view
9
Text Generation
Text Generation Model
AI
Learning
Vectorization
5
Embedding
Model
0.x
0.x
0.8
0.x
0.x
0.x
0.x
0.x
index
0
1
2
3
4
5
6
7
word
pad
[UNK]
ai
we
interesting
learning
is
cs
Text Generation Model
Learning
?
Vectorization
Embedding
Model
0.x
0.x
0.x
0.x
0.x
0.8
0.x
0.x
use [startoftext]
?
Abstract view
Learning AI is interesting
index
0
1
2
3
4
5
6
7
word
pad
[UNK]
ai
we
interesting
learning
is
cs
sequence_length = 6
Text Generation Model
interesting
Learning AI is
Vectorization
5
2
6
0
0
0
Embedded Vectors
Model
0.x
0.x
0.x
0.x
0.8
0.x
0.x
0.x
Text Generation Model
is
Learning AI
Vectorization
Embedded Vectors
Model
0.x
0.x
0.x
0.x
0.x
0.x
0.8
0.x
5
2
0
0
0
0
11
. . .
. . .
â¢Motivation to Text Generation
â¢Simple Models
â¢Using RNNs
â¢Examples
â¢Using Masked Encoder (~Decoder)
Outline
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
Ä‘i
há»c
input
Ä‘i
há»c
ai
data
ai
label
ai
Model
Output
Loss
optimizer
Implementation Using RNN
Label
way 1
Extract context from input
12
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
<pad>
Ä‘i
1
3
Ä‘i
input
Ä‘i
há»c
ai
data
há»c
label
Model
Output
Loss
optimizer
Label
way 1
há»c
há»c
Extract context from input
Implementation Using RNN
13
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
Ä‘i
há»c
ai
data
ai
Model
Output
Loss
optimizer
Label
way 2
Linear
Ä‘i
há»c
input
ai
label
Using all the features
Implementation Using RNN
14
Text 
Generation
â– Practice
Text Generation Model
a Token
Input Text
Vectorization
Embedding
Model
0.x
0.x
0.x
0.x
0.x
â€¦
0.x
0.x
5
0
0 â€¦ 0
0
Classifier
Linear
shape = (40,)
shape = (40, 128)
shape = (40, 128)
shape = (40, 128)
shape = (1000,)
â€˜AIâ€™
â€˜Learning AI is interestingâ€™
ğ‘¥=
ğ‘¦=
â€˜Learningâ€™
sequence_length = 40
embed_dim = 128
vocab_size = 1000
15
Text 
Generation
â– Practice
Text Generation Model
a Token
Input Text
Vectorization
Embedding
Model
0.x
0.x
0.x
0.x
0.x
â€¦
0.x
0.x
5
2
0 â€¦ 0
0
Classifier
Linear
shape = (40,)
shape = (40, 128)
shape = (40, 128)
shape = (40, 128)
shape = (1000,)
â€˜isâ€™
â€˜Learning AI is interestingâ€™
ğ‘¥=
ğ‘¦=
â€˜Learning AIâ€™
sequence_length = 40
embed_dim = 128
vocab_size = 1000
16
Text 
Generation
â– Practice
Text Generation Model
a Token
Input Text
Vectorization
Embedding
Model
0.x
0.x
0.x
0.x
0.x
â€¦
0.x
0.x
5
2
8
0 â€¦ 0
sequence_length = 40
shape = (40,)
embed_dim = 128
shape = (40, 128)
Classifier
shape = (40, 128)
shape = (40, 128)
vocab_size = 1000
Linear
shape = (1000,)
â€˜interestingâ€™
â€˜Learning AI is interestingâ€™
ğ‘¥=
ğ‘¦=
â€˜Learning AI isâ€™
17
Loss function?
Copus
Vocab
Id
Text
0
Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y
1
CÃ³ chÃ­ thÃ¬ nÃªn
Tokenizer
Ä‚n
quáº£
cÃ¢y
trá»“ng
káº»
nhá»›
cÃ³
chÃ­
thÃ¬
nÃªn
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
<unk>
<pad>
<sos>
Build vocab
Special tokens
Add
Example
18
Id
Text
0
Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y
1
CÃ³ chÃ­ thÃ¬ nÃªn
Input tokens
Target token
<sos>
Ä‚n
<sos> Ä‚n
quáº£
<sos> Ä‚n quáº£
nhá»›
<sos> Ä‚n quáº£ nhá»›
káº»
<sos> Ä‚n quáº£ nhá»› káº»
trá»“ng
<sos> Ä‚n quáº£ nhá»› káº» trá»“ng
cÃ¢y
<sos> 
CÃ³
<sos> CÃ³
chÃ­
<sos> CÃ³ chÃ­
thÃ¬
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Next token prediction dataset
Input ids
Target ids
[2, 1, 1, 1, 1, 1]
12
[2, 12, 1, 1, 1, 1]
9
[2, 12, 9, 1, 1, 1]
7
[2, 12, 9, 7, 1, 1]
6
[2, 12, 9, 7, 6, 1]
11
[2, 12, 9, 7, 6, 11]
4
[2, 1, 1, 1, 1, 1]
5
[2, 5, 1, 1, 1, 1]
3
[2, 5, 3, 1, 1, 1]
10
[2, 5, 3, 10, 1, 1]
8
padding
Vocab
Training data
Example
19
sequence_length = 6
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
Token
Id
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
Training
X ids
[2, 1, 1, 1, 1, 1]
[2, 12, 1, 1, 1, 1]
[2, 12, 9, 1, 1, 1]
[2, 12, 9, 7, 1, 1]
[2, 12, 9, 7, 6, 1]
[2, 12, 9, 7, 6, 11]
[2, 1, 1, 1, 1, 1]
[2, 5, 1, 1, 1, 1]
[2, 5, 3, 1, 1, 1]
[2, 5, 3, 10, 1, 1]
Model
(RNN / Transformer)
target ids
12
9
7
6
11
4
5
3
10
8
predict ids
12
9
7
6
12
4
5
3
9
8
CrossEntropy
Loss
Update params
Example
AI VIETNAM
All-in-One Course
20
Input tokens
Target token
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Input ids
Target ids
[2, 5, 3, 10]
[8]
Embedding 
layer
2
5
3
10
ğ‘‹1 = âˆ’0.7521
1.6487
âˆ’0.3925
âˆ’1.4036
ğ‘‹2 = âˆ’0.7581
1.0783
0.8008
1.6806
ğ‘‹3 = âˆ’0.7279
âˆ’0.5594
âˆ’0.7688
0.7624
ğ‘‹4 = âˆ’0.8371
âˆ’0.9224
1.8113
0.1606
0.3035
âˆ’0.2523
0.2980
0.4578
âˆ’0.1187
0.1524
0.3399
âˆ’0.1687
0.2860
0.1057
âˆ’0.3626
âˆ’0.1773
âˆ’0.3885
âˆ’0.1275
âˆ’0.2669
âˆ’0.4838
ğ‘¾ğ’‰ğ’‰
âˆ’0.2863
0.1249
âˆ’0.0660
âˆ’0.3629
ğ‘ğ‘–â„
ğ‘¾ğ’Šğ’‰
âˆ’0.2982
0.4811
âˆ’0.3363
âˆ’0.2582
âˆ’0.2982
âˆ’0.4126
0.2025
âˆ’0.3409
0.4497
âˆ’0.4959
0.1790
0.2653
0.1666
âˆ’0.3912
0.4155
âˆ’0.2021
0.0117
âˆ’0.3415
âˆ’0.4242
âˆ’0.2753 
ğ‘â„â„
â„0
â„1
â„2
â„3
ğ‘‹1
0
0
0
0
ğ’ƒğ’Šğ’‰
â„4
ğ‘‹4
ğ‘‹3
ğ‘‹2
RNN
ğ’ƒğ’‰ğ’‰
ğ‘¾ğ’Šğ’‰
ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’Šğ’‰
ğ‘¾ğ’Šğ’‰
ğ‘¾ğ’Šğ’‰
ğ’ƒğ’‰ğ’‰
ğ’ƒğ’‰ğ’‰
ğ’ƒğ’‰ğ’‰
â„1 = âˆ’0.7409
âˆ’0.4739
âˆ’0.5055
âˆ’0.6786
â„2 = 0.2170
âˆ’0.9590
0.6681
âˆ’0.6519
â„3 = 0.4735
âˆ’0.2915
âˆ’0.4692
âˆ’0.1583
â„4 = 0.8327
âˆ’0.8839
0.2449
0.6446
FC
Flatten
ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’‰ğ’‰
ğ’ƒğ’Šğ’‰
ğ’ƒğ’Šğ’‰
ğ’ƒğ’Šğ’‰
Feed Forward
21
sequence_length = 4
hidden_dim = 4
FC
Probability
0.1121
0.1334
0.0742
0.0896
0.0648
0.0569
0.0956
0.0744
0.0751
0.0809
0.0636
0.0490
0.0304
Target
0
0
0
0
0
0
0
0
1
0
0
0
0
CrossEntropyLoss (L)
Loss
0.0896
âˆ’0.0079
0.0049
âˆ’0.0619
âˆ’0.0204
âˆ’0.0960
âˆ’0.1445
0.0827
0.0843
0.0819
âˆ’0.0518
âˆ’0.1736
0.0201
âˆ’0.0723
âˆ’0.1252
0.0291
ğ›ğ‘¾ğ’‰ğ’‰ğ‘³
ğ›ğ‘¾ğ’Šğ’‰ğ‘³
0.0187
âˆ’0.1285
âˆ’0.2816
âˆ’0.0437
âˆ’0.1589
0.1600
0.0904
0.1782
âˆ’0.0991
âˆ’0.1825
0.1933
0.2889
âˆ’0.0988
âˆ’0.0411
0.1014
âˆ’0.0192
âˆ’0.0235
0.1788
0.3628
0.0449
ğ›ğ‘â„â„ğ‘³
âˆ’0.0235
0.1788
0.3628
0.0449
ğ›ğ‘ğ‘–â„ğ‘³
2.5884
backward
backward
Back-Propagation
ğ‘¾ğ’‰ğ’‰= ğ‘¾ğ’‰ğ’‰ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’Šğ’‰= ğ‘¾ğ’Šğ’‰ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’Šğ’‰
0.2945
âˆ’0.2515
0.2975
0.4640
âˆ’0.1166
0.1620
0.3544
âˆ’0.1770
0.2776
0.0975
âˆ’0.3574
âˆ’0.1599
âˆ’0.3905
âˆ’0.1203
âˆ’0.2544
âˆ’0.4867
âˆ’0.2840
0.1070
âˆ’0.1023
âˆ’0.3674
[âˆ’0.3001
0.4940
âˆ’0.3082
[âˆ’0.2538
âˆ’0.2823
âˆ’0.4286
0.1935
âˆ’0.3587
0.4596
âˆ’0.4777
0.1597
0.2364
0.1765
âˆ’0.3871
0.4053
âˆ’0.2002
0.0141
âˆ’0.3594
âˆ’0.4605
âˆ’0.2798 
ğ’ƒğ’‰ğ’‰= ğ’ƒğ’‰ğ’‰âˆ’ğ’ğ’“Ã— ğ’…ğ’ƒğ’‰ğ’‰
ğ’ƒğ’Šğ’‰= ğ’ƒğ’Šğ’‰âˆ’ğ’ğ’“Ã— ğ’…ğ’ƒğ’Šğ’‰
Update Parameters
SGD(ğ‘™ğ‘Ÿ= 0.1)
Probability
0.1044
0.1163
0.0659
0.0865
0.0575
0.0521
0.0894
0.0673
0.1506
0.0754
0.0578
0.0488
0.0280
Target
0
0
0
0
0
0
0
0
1
0
0
0
0
CrossEntropyLoss (L)
Loss
1.8929
Forward again
2nd Feed Forward
23
â¢Motivation to Text Generation
â¢Simple Models
â¢Using RNNs
â¢Examples
â¢Using Masked Encoder (~Decoder)
Outline
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
há»c
Ä‘i
há»c
input
Ä‘i
há»c
ai
data
há»c
ai
label
ai
há»c
Model
Output
Loss
optimizer
Label
way 3
RNNs encode input sequentially
Implementation Using RNN
24
*Linear can be added (where?)
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
há»c
[-0.88,  0.81,  0.77]
[-1.27,  0.84,  0.04]
Embedded Input
[0.25,   0.05,  0.17]
[-0.11,  0.35,  0.22]
[0.44, -0.22, -0.42]
[0.38,  0.36, -0.01]
[-0.08,  0.68, -0.08, -0.53]
[-0.30,  0.77, -0.15, -0.58]
Output
[-0.30,  0.77, -0.15, -0.58]
Hidden
[0.38, -0.06,  0.37, -0.19]
[-0.16,  0.42,  0.08, -0.02]
[-0.36, -0.01,  0.17,  0.39]
[-0.43,  0.09,  0.33,  0.03]
Wih
[-0.47,  0.13,  0.46, -0.15]
bih
Whh
[0.43,  0.16,  0.34, -0.39]
bhh
seq_len = 2
way 3
RNNs encode input sequentially
Implementation Using RNN
*Linear can be added
hidden_dim = vocab_size = 4
RNN Cell
RNN Cell
ho
h1
h2
Wxh
Whh
bxh
Wxh
Whh
bxh
h1
h2
bhh
bhh
Whh
bhh
ai
há»c
Ä‘i
4
3
há»c
RNN Cell
h3
Wxh
bxh
h2
Whh
bhh
<eos>
2
ai
Ä‘i
há»c
ai
<eos>
Training
Loss
ai
há»c
<eos>
label
Add more special tokens
seq_len = 3
27
RNN Cell
RNN Cell
ho
h1
h3
Wxh
Whh
bxh
Wxh
Whh
bxh
h1
h3
bhh
bhh
Whh
bhh
ai
há»c
<sos>
4
1
RNN Cell
h4
Wxh
bxh
h4
Whh
bhh
<eos>
2
ai
RNN Cell
h2
Wxh
bxh
Whh
h2
bhh
Ä‘i
3
há»c
Ä‘i
<sos>
Ä‘i
há»c
ai
<eos>
Training
Loss
ai
<eos>
há»c
Ä‘i
label
seq_len = 4
28
Text 
Generation
â– Practice
Text Generation Model
Outputs
Input
Vectorization
Embedding
Model
0.x
0.x
0.x
0.x
0.x
â€¦
0.x
0.x
5
2
7 â€¦ 0
0
Classifier
Linear
shape = (40,)
shape = (40, 128)
shape = (40, 128)
shape = (40, 128)
shape 
(40, 1000)
What about?
â€˜AI is â€¦  very interestingâ€™
â€˜Learning AI is â€¦ very interestingâ€™
ğ‘¥=
ğ‘¦=
â€˜Learning AI is â€¦ veryâ€™
sequence_length = 40
embed_dim = 128
vocab_size = 1000
29
0.x
0.x
0.x
0.x
0.x
â€¦
0.x
0.x
â¢Motivation to Text Generation
â¢Simple Models
â¢Using RNNs
â¢Examples
â¢Using Masked Encoder (~Decoder)
Outline
https://arxiv.org/pdf/
1803.08494.pdf
Positional 
Embedding
Input Embedding
+
Multi-head 
Self-Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
Transformer Encoder
NÃ—
30
Softmax
n
d
n
d
d
d
n
n
n
n
n
ğ‘‘ğ‘
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘‘ğ‘
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘Šğ‘
ğ‘Šğ‘˜
ğ‘Šğ‘£
Q
K
V
ğ‘„ğ¾ğ‘‡
Y
A
n
n
ğ‘‘ğ‘= ğ‘‘ğ‘˜= ğ‘‘ğ‘£
Embedding size
Sequence length
Input
ğ‘‘ğ‘£
Self-Attention
31
Positional 
Embedding
Input Embedding
+
Multi-head 
Self-Attention
Add & Norm
Feed Forward
Add & Norm
Positional 
Embedding
Output Embedding
+
Masked 
Multi-head 
Self-Attention
Add & Norm
Multi-head 
Self-Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
Transformer Models
NÃ—
NÃ—
32
word-0
word-1
word-n
â€¦
ğ‘‹0
ğ‘‹1
ğ‘‹ğ‘›
â€¦
ğ‘„0
ğ¾0
ğ‘‰0
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„1
ğ¾1
ğ‘‰1
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„ğ‘›
ğ¾ğ‘›
ğ‘‰ğ‘›
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
Transformer
AI VIETNAM
All-in-One Course
â– Masked self-attention
33
ğ·0
ğ·1
ğ·ğ‘›
â€¦
a word vector
a word vector
a word vector
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ›¼00
ğ›¼01
ğ›¼0ğ‘›
ğ‘Œ0 = ğ›¼00ğ·0 + ğ›¼01ğ·1 + â‹¯+ ğ›¼0ğ‘›ğ·ğ‘›
ğ‘Œ0 = ğ›¼00ğ·0 + 0 Ã— ğ·1 + â‹¯+ 0 Ã— ğ·ğ‘›
ğ›¼0 = softmax(ğ·0ğ·ğ‘‡
ğ‘‘
) =
ğ›¼00
ğ›¼01
â€¦
ğ›¼0ğ‘›
ğ›¼00
0
â€¦
0
How to obtain 
kind of â†’
ğ›¼0 = softmax ğ·0ğ·ğ‘‡
ğ‘‘
âˆ—
1
0
â€¦
0
=
ğ›¼00
0
â€¦
0
?
Masked 
self-attention
ğ·0
ğ·1
ğ·ğ‘›
â€¦
a word vector
a word vector
a word vector
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ›¼00
ğ›¼01
ğ›¼0ğ‘›
ğ‘Œ0 = ğ›¼00ğ·0 + ğ›¼01ğ·1 + â‹¯+ ğ›¼0ğ‘›ğ·ğ‘›
ğ‘Œ0 = ğ›¼00ğ·0 + 0 Ã— ğ·1 + â‹¯+ 0 Ã— ğ·ğ‘›
ğ›¼0 = softmax(ğ·0ğ·ğ‘‡
ğ‘‘
) =
ğ›¼00
ğ›¼01
â€¦
ğ›¼0ğ‘›
ğ›¼00
0
â€¦
0
How to obtain 
kind of â†’
ğ›¼0 = softmax
ğ·0ğ·ğ‘‡
ğ‘‘
âˆ—
1
0
â€¦
0
=
ğ›¼00
0
â€¦
0
?
Mask 
self-attention
ğ·0
ğ·1
ğ·ğ‘›
â€¦
a word vector
a word vector
a word vector
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ›¼00
ğ›¼01
ğ›¼0ğ‘›
ğ‘Œ0 = ğ›¼00ğ·0 + ğ›¼01ğ·1 + â‹¯+ ğ›¼0ğ‘›ğ·ğ‘›
ğ‘Œ0 = ğ›¼00ğ·0 + 0 Ã— ğ·1 + â‹¯+ 0 Ã— ğ·ğ‘›
ğ›¼0 = softmax(ğ·0ğ·ğ‘‡
ğ‘š) =
ğ›¼00
ğ›¼01
â€¦
ğ›¼0ğ‘›
ğ›¼00
0
â€¦
0
How to obtain 
kind of â†’
ğ›¼0 = softmax
ğ·0ğ·ğ‘‡
ğ‘‘
+
0
âˆ’âˆ
â€¦
âˆ’âˆ
=
ğ›¼00
0
â€¦
0
?
Mask 
self-attention
ğ·0
ğ·1
ğ·ğ‘›
â€¦
a word vector
a word vector
a word vector
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ›¼10
ğ›¼11
ğ›¼1ğ‘›
ğ‘Œ1 = ğ›¼10ğ·0 + ğ›¼11ğ·1 + â‹¯+ ğ›¼1ğ‘›ğ·ğ‘›
ğ‘Œ1 = ğ›¼10ğ·0 + ğ›¼11ğ·1 + â‹¯+ 0 Ã— ğ·ğ‘›
ğ›¼1 = softmax
ğ·1ğ·ğ‘‡
ğ‘‘
+
0
0
âˆ’âˆ
â€¦
âˆ’âˆ
=
ğ›¼10
ğ›¼11
0
â€¦
0
Mask 
self-attention
37
ğ·0
ğ·1
ğ·ğ‘›
â€¦
a word vector
a word vector
a word vector
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ›¼ğ‘›0
ğ›¼ğ‘›1
ğ›¼ğ‘›ğ‘›
ğ‘Œğ‘›= ğ›¼ğ‘›0ğ·0 + ğ›¼ğ‘›1ğ·1 + â‹¯+ ğ›¼ğ‘›ğ‘›ğ·ğ‘›
?
ğ›¼ğ‘›= softmax
ğ·ğ‘›ğ·ğ‘‡
ğ‘‘
+
0
0
0
â€¦
0
=
ğ›¼ğ‘›0
ğ›¼ğ‘›1
ğ›¼ğ‘›2
â€¦
ğ›¼ğ‘›ğ‘›
Mask 
self-attention
38
ğ·0
ğ·1
ğ·ğ‘›
â€¦
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ·0
ğ·1
ğ·ğ‘›
â€¦
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
ğ›¼= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ·ğ·ğ‘‡
ğ‘‘
ğ›¼= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ·ğ·ğ‘‡
ğ‘‘
+ ğ‘€
ğ‘€=
0 
âˆ’âˆ 
âˆ’âˆâ€¦ âˆ’âˆ
0 
0 
âˆ’âˆâ€¦ âˆ’âˆ
â€¦
0 
0 
0 
â€¦  
0
ğ‘Œ= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ‘„ğ¾ğ‘‡
ğ‘‘
+ ğ‘€ğ‘‰
ğ‘€=
0 
âˆ’âˆ 
âˆ’âˆâ€¦ âˆ’âˆ
0 
0 
âˆ’âˆâ€¦ âˆ’âˆ
â€¦
0 
0 
0 
â€¦  
0
Masked self-attention
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
word-0
word-1
word-n
â€¦
ğ‘‹0
ğ‘‹1
ğ‘‹ğ‘›
â€¦
ğ‘„0
ğ¾0
ğ‘‰0
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„1
ğ¾1
ğ‘‰1
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„ğ‘›
ğ¾ğ‘›
ğ‘‰ğ‘›
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
40
ğ´= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ‘„ğ¾ğ‘‡
ğ‘‘
+ ğ‘€ğ‘‰
ğ‘Œ= ğ´ğ‘Šğ‘‚
Masked self-attention
ğ‘Œ0
ğ‘Œ1
ğ‘Œğ‘›
â€¦
word-0
word-1
word-n
â€¦
ğ‘‹0
ğ‘‹1
ğ‘‹ğ‘›
â€¦
ğ‘„0
ğ¾0
ğ‘‰0
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„1
ğ¾1
ğ‘‰1
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘„ğ‘›
ğ¾ğ‘›
ğ‘‰ğ‘›
ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
Masked 
Self-Attention
41
ğ‘‹= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
ğ‘Šğ‘„=
âˆ’0.35 
0.51 
0.50
 0.36 âˆ’0.47 âˆ’0.29
âˆ’0.51 âˆ’0.14 âˆ’0.56
ğ‘Šğ¾=
âˆ’0.49 
âˆ’0.68 0.18
âˆ’0.44 
âˆ’0.46 0.18
 0.07 
âˆ’0.10 0.44
ğ‘Šğ‘‰=
âˆ’0.41 
0.39 âˆ’0.65
âˆ’0.40 âˆ’0.07 âˆ’0.34
âˆ’0.55 âˆ’0.13 âˆ’0.29
ğ‘Šğ‘‚=
âˆ’0.36 âˆ’0.08 0.32
 0.27 
0.05 0.15
âˆ’0.05 âˆ’0.28 0.05
ğ‘„= ğ‘‹ğ‘Šğ‘„= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
âˆ’0.35 
0.51 
0.50
 0.36 âˆ’0.47 âˆ’0.29
âˆ’0.51 âˆ’0.14 âˆ’0.56
ğ¾= ğ‘‹ğ‘Šğ¾= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
âˆ’0.49 
âˆ’0.68 
0.18
âˆ’0.44 
âˆ’0.46 
0.18
 0.07 
âˆ’0.10 
0.44
ğ‘‰= ğ‘‹ğ‘Šğ‘‰= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
âˆ’0.41 
0.39 âˆ’0.65
âˆ’0.40 âˆ’0.07 âˆ’0.34
âˆ’0.55 âˆ’0.13 âˆ’0.29
head = 1
Masked Multi-
head Attention
ğ‘€= 0 
âˆ’âˆ
0 
0
= âˆ’0.08 âˆ’0.14 âˆ’0.24
âˆ’0.39 0.77 0.69
= 0.02 âˆ’0.01 0.13
0.27 0.27 âˆ’0.26
= âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 0.05
42
approximately
Masked Multi-head Attention
AI VIETNAM
All-in-One Course
â– Example 
ğ‘Œ= ğ´ğ‘Šğ‘‚= âˆ’0.16 âˆ’0.08 âˆ’0.05
 0.12 
0.08 
0.06
âˆ’0.36 âˆ’0.08 0.32
 0.27 
0.05 0.15
âˆ’0.05 âˆ’0.28 0.05
=
0.03 0.02 âˆ’0.06
âˆ’0.02 âˆ’0.02 0.05
ğ´= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ğ‘„ğ¾ğ‘‡
ğ‘‘
+ ğ‘€ğ‘‰
= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘
âˆ’0.08 âˆ’0.14 âˆ’0.24
âˆ’0.39 
0.77 
0.69
 0.02 
0.27
âˆ’0.01 
0.27
 0.13 âˆ’0.26
1
ğ‘‘
+ 0 
âˆ’âˆ
0 
0
âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 
0.05
=
1.0 
0.0
0.52 0.48
âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 
0.05 = âˆ’0.16 âˆ’0.08 âˆ’0.05
 0.12 
0.08 0.06
= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘
âˆ’0.019 
0.002
 0.043 
âˆ’0.046 + 0 
âˆ’âˆ
0 
0
âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 
0.05
43
n
d
n
d
d
d
n
n
n
n
n
ğ‘‘ğ‘
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘‘ğ‘
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘Šğ‘
ğ‘Šğ‘˜
ğ‘Šğ‘£
Q
K
V
ğ‘„ğ¾ğ‘‡
Z
ğ‘‘ğ‘= ğ‘‘ğ‘˜= ğ‘‘ğ‘£
Embedding size
Sequence length
Input
ğ‘‘ğ‘£
0
âˆ’âˆ
âˆ’âˆ
âˆ’âˆ
âˆ’âˆ
0
0
âˆ’âˆ
âˆ’âˆ
âˆ’âˆ
0
0
0
âˆ’âˆ
âˆ’âˆ
0
0
0
0
âˆ’âˆ
0
0
0
0
0
ğ‘€ğ‘ğ‘ ğ‘˜
Softmax
A
n
n
Masked-Attention
Training process
44
Positional 
Embedding
Output Embedding
+
Masked 
Multi-head 
Self-Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Decoder
NÃ—
Text Generation
AI VIETNAM
All-in-One Course
â– Architecture
interesting
Learning AI is
Text Encoding
Softmax
Dataset
(x, y)
45
Encoder in PyTorch
Ä‘i
há»c
3
4
index
word
0
[UNK]
1
[pad]
2
ai
3
Ä‘i
4
há»c
â€¦
â€¦
Embedding
index
Embedding
0
[-0.188, â€¦,  0.7013]
1
[1.7840â€¦,  1.3586]
2
[1.0281, â€¦,  0.4211]
3
[-1.308,  â€¦, -0.3680]
4
[0.2293,  â€¦,  2.0501]
â€¦
â€¦
Transformer Encoder
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Encoder 
in Pytorch
(N, 2, 3)
(N, 2, 3)
46
Ä‘i
há»c
3
4
Embedding
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Encoder 
in Pytorch
(N, 2, 3)
(N, 2, 3)
[0.69,  0.72, -1.41]
[0.21,  1.10, -1.31]
[0.97, 0.39, -1.37]
[0.58, 0.82, -1.40]
Transformer Encoder
47
Masked Encoder in PyTorch
Ä‘i
há»c
ai
3
4
2
index
word
0
[UNK]
1
[pad]
2
ai
3
Ä‘i
4
há»c
â€¦
â€¦
Embedding
index
Embedding
0
[-0.188, â€¦,  0.7013]
1
[1.7840â€¦,  1.3586]
2
[1.0281, â€¦,  0.4211]
3
[-1.308,  â€¦, -0.3680]
4
[0.2293,  â€¦,  2.0501]
â€¦
â€¦
Transformer Encoder
Masked 
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Masked Encoder 
in Pytorch
(N, 3, 3)
(N, 3, 3)
48
Ä‘i
há»c
ai
3
4
2
Embedding
Masked 
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Masked Encoder 
in Pytorch
(N, 3, 3)
(N, 3, 3)
[0.69,  0.72, -1.41]
[0.21,  1.10, -1.31]
[-0.88,  0.60, -0.31]
[0.97, 0.39, -1.37]
[0.58, 0.82, -1.40]
[-0.85, 1.40, -0.54]
Transformer Encoder
49
Transformer in PyTorch
(N, 2, 3)
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
query
value
key
50
Transformer in PyTorch
AI VIETNAM
All-in-One Course
â– Transformer Encoder 
(N, 2, 3)
Masked 
Multi-head 
Self-Attention
Add & 
Norm
(N, 3, 3)
Masked 
Multi-head 
Self-Attention
query
value
key
query
value
key
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
Masked 
Multi-head 
Self-Attention
Add & 
Norm
(N, 3, 3)
Masked 
Multi-head 
Self-Attention
query
value
key
query
value
key
query
value
key
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Example
Input tokens
Target token
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Input ids
Target ids
[2, 5, 3, 10]
[8]
Embedding layer
ğ‘‹1 = âˆ’0.7521
1.6487
âˆ’0.3925
âˆ’1.4036
ğ‘‹2 = âˆ’0.7581
1.0783
0.8008
1.6806
ğ‘‹3 = âˆ’0.7279
âˆ’0.5594
âˆ’0.7688
0.7624
ğ‘‹4 = âˆ’0.8371
âˆ’0.9224
1.8113
0.1606
Masked Multi-
Head Attention
Feed Forward
Add & Norm
Add & Norm
Linear
2
5
3
10
0.3035
âˆ’0.2523
0.2980
0.4578
âˆ’0.1187
0.1524
0.3399
âˆ’0.1687
0.2860
0.1057
âˆ’0.3626
âˆ’0.1773
âˆ’0.3885
âˆ’0.1275
âˆ’0.2669
âˆ’0.4838
ğ‘¾ğ’Œ_ğ’‘ğ’“ğ’ğ’‹
0.0782
0.0014
âˆ’0.3432
0.4192
âˆ’0.4400
âˆ’0.1861
âˆ’0.2917
âˆ’0.0992
âˆ’0.2154
âˆ’0.0346
âˆ’0.1711
0.4302
âˆ’0.2993
âˆ’0.3388
âˆ’0.3946
0.1558
ğ‘¾ğ’_ğ’‘ğ’“ğ’ğ’‹
âˆ’0.2863
0.0117
âˆ’0.4376
0.1541
0.1249
âˆ’0.3415
âˆ’0.3184
âˆ’0.4663
âˆ’0.0660
âˆ’0.4242
0.4998
âˆ’0.3284
âˆ’0.3629
âˆ’0.2753
0.0944
âˆ’0.1664
ğ‘¾ğ’—_ğ’‘ğ’“ğ’ğ’‹
Linear (fc2)
Feed Forward
Linear (fc1)
GeLU
Transformer
âˆ’0.2982
0.4811
âˆ’0.3363
âˆ’0.2582
âˆ’0.2982
âˆ’0.4126
0.2025
âˆ’0.3409
0.4497
âˆ’0.4959
0.1790
0.2653
0.1666
âˆ’0.3912
0.4155
âˆ’0.2021
ğ‘¾ğ’’_ğ’‘ğ’“ğ’ğ’‹
ğ‘¾ğ’‡ğ’„_ğŸ
0.2792
âˆ’0.3735
0.1161
0.2610
âˆ’0.1278
0.1783
0.2583
0.2628
âˆ’0.2853
0.3870
0.0907
0.1870
âˆ’0.1712
âˆ’0.4707
âˆ’0.1781
âˆ’0.0879
âˆ’0.1324
0.3196
0.0073
âˆ’0.4541
0.0535
0.4297
âˆ’0.0299
âˆ’0.1845
âˆ’0.0883
âˆ’0.0495
0.1202
0.4211
âˆ’0.1490
âˆ’0.1119
0.1401
0.1948
âˆ’0.3125
âˆ’0.2288
âˆ’0.2465
âˆ’0.0035
âˆ’0.2126
0.2581
âˆ’0.3489
âˆ’0.2689
âˆ’0.0558
âˆ’0.1608
âˆ’0.2871
âˆ’0.3029
0.3420
âˆ’0.0709
0.2636
âˆ’0.3307
0.0511
âˆ’0.3517
0.1697
0.1447
âˆ’0.0916
0.2366
0.2975
âˆ’0.1736
0.1463
0.2679
0.1852
âˆ’0.0712
âˆ’0.1347
0.1289
0.0895
âˆ’0.2035
ğ–ğŸğœ_ğŸ
Linear
Linear
Linear
Scaled Dot-Product 
Attention
Linear
V
K
Q
Masked Multi-Head Attention
Decoder
Probability
0.1098
0.0881
0.1008
0.1022
0.0764
0.0552
0.0719
0.0356
0.0639
0.0781
0.0314
0.1317
0.0547
Target
0
0
0
0
0
0
0
0
1
0
0
0
0
CrossEntropyLoss (L)
Linear
0.0226
âˆ’0.0292
âˆ’0.0036
0.0103
âˆ’0.0454
0.0681
âˆ’0.0774
0.0547
0.0152
âˆ’0.0030
âˆ’0.0410
0.0288
0.0237
âˆ’0.0166
âˆ’0.0150
0.0079
0.0099
0.0117
âˆ’0.0254
0.0038
âˆ’0.0103
0.0277
âˆ’0.1183
0.1009
âˆ’0.0523
0.0269
0.0294
âˆ’0.0041
âˆ’0.1596
0.2036
0.0265
âˆ’0.0706
ğ’…ğ‘³
ğ’…ğ‘¾ğ’‡ğ’„_ğŸ
ğ’…ğ‘³
ğ’…ğ‘¾ğ’‡ğ’„_ğŸ
âˆ’0.0086
âˆ’0.1358
âˆ’0.0030
0.0203
0.0251
0.0574
âˆ’0.0024
âˆ’0.0668
âˆ’0.0023
0.0340
âˆ’0.0033
0.0041
âˆ’0.0143
0.0445
0.0087
0.0425
0.0681
âˆ’0.0266
âˆ’0.0638
âˆ’0.1115
âˆ’0.0519
âˆ’0.0736
0.0198
0.0300
âˆ’0.0352
0.0213
0.0126
0.0305
0.0190
0.0789
0.0315]
0.0509]
0.0293
0.0130
0.0123
âˆ’0.0020
âˆ’0.0253
âˆ’0.0116
0.0327
0.0192
âˆ’0.0122
âˆ’0.0029
âˆ’0.0392
âˆ’0.0113
âˆ’0.0568
âˆ’0.0259
0.0138
0.0187
ğ’…ğ‘³
ğ’…ğ‘¾ğ’’_ğ’‘ğ’“ğ’ğ’‹
4.129e âˆ’4
2.1878e âˆ’5
âˆ’2.6279e âˆ’4
âˆ’6.7103e âˆ’4
âˆ’3.752e âˆ’3
3.1984e âˆ’2
âˆ’1.1805e âˆ’2
âˆ’3.4454e âˆ’2
6.2969e âˆ’3
âˆ’6.6984e âˆ’2
âˆ’2.3536e âˆ’3
2.0683e âˆ’2
4.7672e âˆ’2
âˆ’1.9498e âˆ’1
âˆ’1.6597e âˆ’2
3.0328e âˆ’2
ğ’…ğ‘³
ğ’…ğ‘¾ğ’Œ_ğ’‘ğ’“ğ’ğ’‹
âˆ’0.0805
0.0609
âˆ’0.0342
0.0538
0.0741
âˆ’0.1002
âˆ’0.1142
0.1403
0.0290
âˆ’0.0820
0.0412
0.0118
0.1113
âˆ’0.0876
âˆ’0.1077
0.0840
ğ’…ğ‘³
ğ’…ğ‘¾ğ’_ğ’‘ğ’“ğ’ğ’‹
backward
Loss
2.7498
backward
backward
backward
0.0805
âˆ’0.0260
0.0046
0.0316
âˆ’0.1119
0.0188
âˆ’0.0373
âˆ’0.0858
âˆ’0.0405
0.0059
âˆ’0.0205
âˆ’0.0154
âˆ’0.1720
âˆ’0.0532
âˆ’0.0527
âˆ’0.0120
ğ’…ğ‘³
ğ’…ğ‘¾ğ’—_ğ’‘ğ’“ğ’ğ’‹
54
ğ‘¾ğ’‡ğ’„_ğŸ
= ğ‘¾ğ’‡ğ’„_ğŸ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’‡ğ’„_ğŸ
Update Parameters
SGD(ğ‘™ğ‘Ÿ= 0.1)
âˆ’0.3148
âˆ’0.2259
[âˆ’0.2462
[âˆ’0.0045
âˆ’0.2081
0.2512
âˆ’0.3412
âˆ’0.2743
âˆ’0.0573
âˆ’0.1605
âˆ’0.2830
âˆ’0.3058
0.3396
âˆ’0.0692
0.2651
âˆ’0.3315
0.0502
âˆ’0.3529
0.1723
0.1443
âˆ’0.0905
0.2339
0.3093
âˆ’0.1837
âˆ’0.1515
0.2652
0.1823
âˆ’0.0707
âˆ’0.1187
0.1085
0.0868
âˆ’0.1964
ğ‘¾ğ’‡ğ’„_ğŸ
= ğ‘¾ğ’‡ğ’„_ğŸ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’‡ğ’„_ğŸ
0.2800
âˆ’0.3600
0.1164
0.2589
âˆ’0.1303
0.1726
0.2585
0.2694
âˆ’0.2851
0.3836
0.0910
0.1866
âˆ’0.1698
âˆ’0.4752
âˆ’0.1789
âˆ’0.0921
âˆ’0.1392
0.3223
0.0137
âˆ’0.4430
0.0587
0.4371
âˆ’0.0318
âˆ’0.1875
âˆ’0.0848
âˆ’0.0516
0.1189
0.4180
âˆ’0.1509
âˆ’0.1198
0.1370
0.1897
ğ‘¾ğ’_ğ’‘ğ’“ğ’ğ’‹
= ğ‘¾ğ’_ğ’‘ğ’“ğ’ğ’‹ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’_ğ’‘ğ’“ğ’ğ’‹
0.0862
âˆ’0.0047
âˆ’0.3398
0.4139
âˆ’0.4474
âˆ’0.1760
âˆ’0.2803
âˆ’0.1133
âˆ’0.2183
âˆ’0.0264
âˆ’0.1753
0.4290
âˆ’0.3105
âˆ’0.3301
âˆ’0.3839
0.1474
ğ‘¾ğ’’_ğ’‘ğ’“ğ’ğ’‹
= ğ‘¾ğ’’_ğ’‘ğ’“ğ’ğ’‹ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’’_ğ’‘ğ’“ğ’ğ’‹
âˆ’0.3011
0.4798
âˆ’0.3376
âˆ’0.2580
âˆ’0.2957
âˆ’0.4115
0.1992
âˆ’0.3428
0.4509
âˆ’0.4956
0.1830
0.2664
0.1723
âˆ’0.3886
0.4141
âˆ’0.2040
ğ‘¾ğ’Œ_ğ’‘ğ’“ğ’ğ’‹
= ğ‘¾ğ’Œ_ğ’‘ğ’“ğ’ğ’‹ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’Œ_ğ’‘ğ’“ğ’ğ’‹
0.3034
âˆ’0.2523
0.2981
0.4579
âˆ’0.1183
0.1492
0.3411
âˆ’0.1653
0.2854
0.1124
âˆ’0.3624
âˆ’0.1793
âˆ’0.3933
âˆ’0.1080
âˆ’0.2653
âˆ’0.4868
ğ‘¾ğ’—_ğ’‘ğ’“ğ’ğ’‹
= ğ‘¾ğ’—_ğ’‘ğ’“ğ’ğ’‹ âˆ’ğ’ğ’“Ã— ğ’…ğ‘¾ğ’—_ğ’‘ğ’“ğ’ğ’‹
âˆ’0.2944
0.0143
âˆ’0.4381
0.1509
0.1361
âˆ’0.3434
âˆ’0.3146
âˆ’0.4578
âˆ’0.0619
âˆ’0.4248
0.5019
âˆ’0.3268
âˆ’0.3457
âˆ’0.2700
0.0997
âˆ’0.1652
Probability
0.0503
0.0227
0.0684
0.0322
0.0225
0.0170
0.0445
0.0075
0.6174
0.0274
0.0108
0.0522
0.0270
Target
0
0
0
0
0
0
0
0
1
0
0
0
0
CrossEntropyLoss (L)
Loss
0.1699
Forward again
55
Precision and Recall of Words
AI VIETNAM
All-in-One Course
Predict/Candidate/Output: 
       TÃ´i há»c NLP cá»§a AI VIET NAM
                         Reference: 
       TÃ´i Ä‘ang há»c lá»›p AI  cá»§a AI VIET NAM
correct
reference_length = 6
9
Recall 1-gram
correct
candidate_length = 6
7
Precision 1-gram
F1-score 1-gram
precision Ã— recall
(precision + recall)/2 = 0.75
BLEU Score
AI VIETNAM
All-in-One Course
N-gram overlap between machine translation 
candidate and reference translation
Compute precision for n-grams of size 1 to 4
With 4-gram and add brevity penalty 
(for too short translations):
â–BLEU score
BLEU = min 1, candidate_length
reference_length
à·‘
i=1
4
Precisioni
1/4
correct
reference_length = 6
9
Recall 1-gram
correct
candidate_length = 6
7
Precision 1-gram
Precision and Recall of Words
AI VIETNAM
All-in-One Course
Predict/Candidate/Output:           TÃ´i há»c NLP cá»§a AI VIET NAM
                           Reference:           TÃ´i Ä‘ang há»c lá»›p CV vÃ  NLP  cá»§a AI 
Precision
1-gram
2-gram
3-gram
4-gram
6/7
3/6
2/5
1/4
Multiple reference: N-grams may match in any of 
the reference and closest reference length used
Brevity penalty = 7/9
                 BLEU = 0.35
BLEU = min 1, candidate_length
reference_length
à·‘
i=1
4
Precisioni
1/4
