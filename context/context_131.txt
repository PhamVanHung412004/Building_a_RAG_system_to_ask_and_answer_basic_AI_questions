NLP Project
Low-Resource
Neural Machine Translation
Year 2023
AI VIET NAM
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
√ò Introduction
√ò Pre-trained LMs: mBART50, mT5
√ò Back-Translation
Outline
3
Introduction
Translate a sentence w(s) in a source language (input) to a sentence w(t) in the 
target language (output)
!
4
Introduction
Translate a sentence w(s) in a source language (input) to a sentence w(t) in the 
target language (output)
!
√ò Can be formulated as an optimization problem:
!ùë§(") = argmax
$(")
ùúÉ( ùë§(%), ùë§(&))
Where ùúÉis a scoring function over source and target sentences
√ò Requires two components:
q Learning algorithm to compute parameters of ùúÉ
q Decoding algorithm for computing the best translation !ùë§(")
5
Introduction
Translate a sentence w(s) in a source language (input) to a sentence w(t) in the 
target language (output)
!
Source 
sentence
Target 
sentence
Train a model
6
Introduction
Low-resource Machine Translation
!
Source 
sentence
Target 
sentence
Train a model
Language Pair
Parallel Sentence
En-De
800M
En-Ko
500M
En-Vi
0.17M
De-Vi
0.05M
7
Introduction
Low-resource Machine Translation
!
Source
8
√ò Introduction
√ò Pre-trained LMs: mBART50, mT5
√ò Back-Translation
Outline
9
Pre-trained LMs
Pre-trained LMs
!
10
Pre-trained LMs
Pre-trained LMs
!
Source
11
Pre-trained LMs
Pre-trained LMs
!
√ò BERT and GPT: a great catalyst for NLU
√ò But, less successful for sequence-to-sequence tasks: machine translation, text 
summarization,‚Ä¶
Missing tokens are predicted independently,
soBERT
cannot
easily
be
used
for
generation
Tokens can only condition on leftward
context, so it cannot learn bidirectional
interactions
12
Pre-trained LMs
BART
!
√ò BART (Denoising Sequence-to-Sequence Pre-Training for Natural Language 
Generation, Translation and Comprehension.
13
Pre-trained LMs
BART
!
√ò BART (Denoising Sequence-to-Sequence Pre-Training for Natural Language 
Generation, Translation and Comprehension.
√ò Fine-Tuning
Classification Task
Machine Translation Task
14
Pre-trained LMs
BART
!
√ò mBART: Multilingual Denoising Pre-Training
15
Pre-trained LMs
BART
!
√ò mBART50: Multilingual Translation with Extensible Multilingual Pretraining
16
Pre-trained LMs
BART
!
√ò mBART50
√ò PhoMT Dataset
17
Pre-trained LMs
BART
!
√ò mBART50
√ò PhoMT Dataset
18
Pre-trained LMs
T5
!
√ò T5 (Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer)
√ò Every task, one format!
√ò [‚ÄúTask-specific prefix]: [Input text]‚Äù => ‚Äú[Output text]‚Äù
19
Pre-trained LMs
T5
!
√ò Baseline Objective
20
Pre-trained LMs
T5
!
√ò Different Attention Mask Patterns
21
Pre-trained LMs
T5
!
√ò Transformer Architecture Variants
22
Pre-trained LMs
T5
!
√ò Different Unsupervised Objectives
23
Pre-trained LMs
T5
!
√ò Multi-task Learning
24
Pre-trained LMs
T5
!
√ò Multi-task Learning
25
Pre-trained LMs
T5
!
√ò Multi-task Learning
26
Pre-trained LMs
T5
!
√ò Multi-task Learning
27
Pre-trained LMs
T5
!
√ò Multi-task Learning
28
Pre-trained LMs
T5
!
√ò mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
29
Pre-trained LMs
Pre-trained LMs
!
# MBart50TokenizerFast.from_pretrained(model_name, 
src_lang="en_XX",tgt_lang = "vi_VN")
model_name = "facebook/mbart-large-50-many-to-many-mmt" 
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# prefix: translate English to Vietnamese
model_name = ‚Äùgoogle/mt5-base" 
tokenizer = T5TokenizerFast.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
30
√ò Introduction
√ò Pre-trained LMs: mBART50, mT5
√ò Back-Translation
Outline
31
Back-Translation
Back-Translation Technique
!
Bilingual Data
(En-Vi)
Train En-Vi Model
(mBART50)
Bilingual Data
(En-Vi)
Train En-Vi Model
(mBART50)
Train Vi-En Model
(mBART50)
Synthetic Data
(Vi-En)
Merge
32
Back-Translation
Back-Translation with Monolingual Dataset
!
Bilingual Data
(En-Vi)
Train En-Vi Model
(mBART50)
Train Vi-En Model
(mBART50)
Synthetic Data
(Vi-En)
Merge
Bilingual Data
(En-Vi)
Train En-Vi Model
(mBART50)
Train Vi-En Model
(mBART50)
Synthetic Data
(Vi-En)
Merge
Monolingual
(Vi)
Crawl,‚Ä¶
33
Back-Translation
Data Selection
!
Bilingual Data
(En-Vi)
Train En-Vi Model
(mBART50)
Train Vi-En Model
(mBART50)
Synthetic Data
(Vi-En)
Merge
Monolingual
(Vi)
Crawl,‚Ä¶
Domain Mismatch
Bilingual En-Vi
Monolingual Vi
News
Medical
Law
News
Cosine Similarity
TF-IDF
34
Back-Translation
Data Selection
!
Bilingual Data
(En-Vi)
Train En-Vi Model
(mBART50)
Train Vi-En Model
(mBART50)
Synthetic Data
(Vi-En)
Merge
Monolingual
(Vi)
Crawl,‚Ä¶
Synthetic Data Filtering
Cosine Similarity
Round-Trip BLEU
Reference
35
Back-Translation
Experiment
!
v Dataset: IWSLT‚Äô15 English-Vietnamese
Training: 133 317
Validation: 1 553
Test: 1 269
Experiment
Model
ScareBLEU
#1
Standard Transformer (Greedy Search)
24.66
#2
BERT-to-BERT (Greedy Search)
25.41
#3
BERT-to-GPT2 (Greedy Search)
23.56
#4
mBART50
34.87
#5
Back-Translation (Monolingual)
35.22
Thanks!
Any questions?
36
