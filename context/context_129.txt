Machine Learning
Linear Regression
Logistic Regression
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Linear Regression
(2) â€“ Logistic Regression
(3) â€“ Code
1 â€“ Linear Regression
Linear Regression
!
3
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Level
Salary
3.5
???
10
???
Data
Learning
Prediction
1 â€“ Linear Regression
Linear Regression
!
4
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Visualization
y = 6x + 7
y = f(x): linear function
1 â€“ Linear Regression
Linear Regression
!
5
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Visualization
y = 6x + 7
y = f(x): linear function
Modeling
y = wx + b
Find w and b to fit 
the data
1 â€“ Linear Regression
Linear Regression using Gradient Descent
!
6
Modeling
y = wx + b
Visualization
y = 2x + 2
y = 2x + 2
Init ğœƒ
lr = 0.1
Data 
y = 2x + 2 = 6
x = [1  2]
y = 18 
Loss
Difference between 
predicted and actual value
L = (6 â€“ 18)2 = 144
Lâ€™ = âˆ’24
âˆ’48
k = -24
ğœƒ= 4.4
6.8
1 â€“ Linear Regression
!
7
Modeling
y = wx + b
Visualization
y = 2x + 2 
y = 2x + 2
Linear Regression using Gradient Descent
y = 6.8x + 4.4
y = 6.8x + 4.4 
Updated 
1 â€“ Linear Regression
!
8
Implement for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
"ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥("ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2("ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
"ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™("ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
Traditional
Basic Python
Vectorized
Numpy
1 â€“ Linear Regression
!
9
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
"ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥("ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2("ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
1 â€“ Linear Regression
!
10
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
"ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥("ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2("ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
1 â€“ Linear Regression
!
11
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
"ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥("ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2("ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
1 â€“ Linear Regression
!
12
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
"ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥("ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2("ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
1 â€“ Linear Regression
!
13
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
"ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥("ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2("ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
1 â€“ Linear Regression
!
14
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
"ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™("ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
1 â€“ Linear Regression
!
15
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
"ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™("ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
1 â€“ Linear Regression
!
16
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
"ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™("ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
1 â€“ Linear Regression
!
17
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
"ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™("ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
1 â€“ Linear Regression
!
18
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
"ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= ("ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™("ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
1 â€“ Linear Regression
!
19
Pratice
Modeling
y = wx + b
y = 2x + 2
Init ğœƒ
lr = 0.1
Data 
y = ?
x = [1  3]
y = 22 
L = ?
Lâ€™ = ?
ğœƒ= ?
1 â€“ Linear Regression
!
20
Pratice
Modeling
y = wx + b
y = 2x + 2
Init ğœƒ
lr = 0.1
Data 
y = 2x + 2 = 8
x = [1  3]
y = 22 
L = (6 â€“ 18)2 = 196
Lâ€™ = âˆ’28
âˆ’84
ğœƒ= 4.8
10.4
y = 2x + 2
y = 10.4x + 4.8
1 â€“ Linear Regression
!
21
Problem
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Data #1
Any value
Values: [0, 1]
Discrete values:{0, 1} 
y = ax + b
Need flexible model?
2 â€“ Logistic Regresion
!
22
Problem
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Hours
Pass
0.25
???
4.5
???
Learning
Prediction
2 â€“ Logistic Regresion
!
23
Problem
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Visualization
Modeling
y = f(x)
Find a function to 
fit the data
Sigmoid function
2 â€“ Logistic Regresion
!
24
Sigmoid Function
ğœ(ğ‘¢) =
1
1 + ğ‘’$%
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(ğ‘¢) âˆˆ0
1
âˆ€ğ‘§!ğ‘§" âˆˆğ‘ğ‘and ğ‘§! â‰¤ğ‘§"
â†’ğœ(ğ‘§!) â‰¤ğ‘§(ğ‘¢!)
ğ‘§
+âˆ
âˆ’âˆ
ğ‘§!
ğ‘§"
ğœ
ğœ!
ğœ"
Sigmoid function
Property
2 â€“ Logistic Regresion
!
25
Sigmoid Function
ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ‘§= ğœ½"ğ’™
ğ‘§= ğœ½"ğ’™
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(ğ‘§) âˆˆ0
1
ğ‘¥
ğ‘§
ğ‘¥
ğ‘§
ğ‘§
ğœ
ğ‘§
ğœ
2 â€“ Logistic Regresion
!
26
Logistic Regression using Gradient Descent
ğœ½! = [ğ‘
ğ‘¤]
ğ’™! = [1
0.5]
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
ğœ½! = [0.1
0.1]
ğ’š= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğœ‚= 0.1
2 â€“ Logistic Regresion
!
27
Logistic Regression using Gradient Descent
ğ’™! = [1
0.5]
ğœ½! = [0.1
0.1]
ğ’š= 0
Model
0.1
0.1
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğ‘§= 0.15
)ğ‘¦= 0.54
ğœ‚= 0.1
is learning rate
2 â€“ Logistic Regresion
!
28
Logistic Regression using Gradient Descent
ğ’™! = [1
0.5]
ğœ½! = [0.1
0.1]
ğ’š= 0
Model
Loss
0.1
ğ‘¦
0.1
ğ‘¦= 0
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğ‘§= 0.15
)ğ‘¦= 0.54
L = 0.771
ğœ‚= 0.1
is learning rate
2 â€“ Logistic Regresion
!
29
Logistic Regression using Gradient Descent
ğ’™! = [1
0.5]
ğœ½! = [0.1
0.1]
ğ’š= 0
Model
Loss
0.1
ğ‘¦
0.1
ğ‘¦= 0
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğ‘§= 0.15
)ğ‘¦= 0.54
L = 0.771
ğ›»!ğ¿= ğ±()y âˆ’ğ‘¦)
=
1
0.5
0.54
= 0.54
0.27
= ğ¿"
#
ğ¿$
#
ğœ‚= 0.1
is learning rate
2 â€“ Logistic Regresion
!
30
Logistic Regression using Gradient Descent
ğ’™! = [1
0.5]
ğœ½! = [0.1
0.1]
ğ’š= 0
Model
Loss
0.1
ğ‘¦
0.1
ğ‘¦= 0
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğ‘§= 0.15
)ğ‘¦= 0.54
L = 0.771
ğ›»!ğ¿= ğ±()y âˆ’ğ‘¦)
=
1
0.5
0.54
= 0.54
0.27
= ğ¿"
#
ğ¿$
#
ğ‘= 0.1 âˆ’ğœ‚0.54
=0.046
ğ‘¤= 0.1 âˆ’ğœ‚0.27
=0.073
ğœ‚= 0.1
is learning rate
2 â€“ Logistic Regresion
!
31
Logistic Regression using Gradient Descent
ğ’™! = [1
0.5]
ğœ½! = [0.046
0.073]
ğ’š= 0
Model
Loss
0.046
ğ‘¦
0.073
ğ‘¦= 0
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğ‘§= 0.0825
)ğ‘¦= 0.52
L = 0.735
ğ‘= 0.1 âˆ’ğœ‚0.54
=0.046
ğ‘¤= 0.1 âˆ’ğœ‚0.27
=0.073
ğœ‚= 0.1
Previous L = 0.777
is learning rate
2 â€“ Logistic Regresion
!
32
Logistic Regression using Gradient Descent
ğœ½! = [ğ‘
ğ‘¤]
ğ’™! = [1
1.0]
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
ğœ½! = [0.046
0.073]
ğ’š= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğœ‚= 0.1
is learning rate
2 â€“ Logistic Regresion
!
33
Logistic Regression using Gradient Descent
ğ’™! = [1
1.0]
ğœ½! = [0.046 0.073]
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğ‘§= 0.119
)ğ‘¦= 0.53
L = 0.755
âˆ‡%L = ğ±()y âˆ’ğ‘¦)
=
1
1.0
0.53
= 0.53
0.53
= ğ¿"
#
ğ¿$
#
ğ‘= 0.046 âˆ’ğœ‚0.53
= âˆ’0.007
ğ‘¤= 0.073 âˆ’ğœ‚0.53
=0.02
ğœ‚= 0.1
0.046
0.073
is learning rate
2 â€“ Logistic Regresion
!
34
Logistic Regression using Gradient Descent
ğ’™! = [1
1.0]
ğœ½! = [âˆ’0.007 0.02]
ğ’š= 0
Model
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚
ğ‘§= ğœ½!ğ’™
ğœ‚= 0.1
âˆ’0.007
0.02
is learning rate
2 â€“ Logistic Regresion
!
35
Prediction
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Hours
Pass
0.25
???
4.5
???
Learning
Prediction
ğœ½! = [0.046 0.073]
ğ‘
ğ‘¤
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’0.007
0.02
Prediction
2 â€“ Logistic Regresion
!
36
Prediction
Hours
Pass
0.25
???
4.5
???
Prediction
Prediction
ğ‘
ğ‘¤
&ğ‘¦= ğœğ‘§=
1
1 + ğ‘’"# = 0.499
ğ‘§= ğ‘¤ğ‘¥+ ğ‘= âˆ’0.002
âˆ’0.007
0.02
Hours
Pass
0.25
???
4.5
???
Prediction
Prediction
ğ‘
ğ‘¤
&ğ‘¦= ğœğ‘§=
1
1 + ğ‘’"# = 0.52
ğ‘§= ğ‘¤ğ‘¥+ ğ‘= 0.083
âˆ’0.007
0.02
Thresholds = 0.5
ğ‘¦&'(): 0
Thresholds = 0.5
ğ‘¦&'(): 1
2 â€“ Logistic Regresion
!
37
Multivariable Logistic Regression
Data #2
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
Day
Hours
Pass
2
0.25
???
1
4.5
???
Learning
Prediction
2 â€“ Logistic Regresion
!
38
Multivariable Logistic Regression using Gradient Descent
ğœ½! = [ğ‘
ğ‘¤% ğ‘¤&]
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
ğ’š= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğœ‚= 0.1
Data #2
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
2 â€“ Logistic Regresion
!
39
Multivariable Logistic Regression using Gradient Descent
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
40
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
Model
ğ‘
ğ‘¤G
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§= 0.35
)ğ‘¦= 0.59
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤H
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
41
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
ğœ‚= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
42
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤G
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
ğ‘§= 0.35
)ğ‘¦= 0.59
L = 0.883
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤H
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
43
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
ğœ‚= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
44
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤G
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
ğ‘§= 0.35
)ğ‘¦= 0.59
L = 0.883
âˆ‡%L = ğ±()y âˆ’ğ‘¦)
=
1
1.0
0.5
0.59 =
0.59
0.59
0.295
=
ğ¿"
#
ğ¿&#
#
ğ¿&$
#
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤H
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
45
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
ğœ‚= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
46
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤G
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog"yâˆ’(1âˆ’y)log(1âˆ’"y )
ğ‘§= 0.35
)ğ‘¦= 0.59
L = 0.883
âˆ‡%L = ğ±()y âˆ’ğ‘¦)
=
1
1.0
0.5
0.59 =
0.59
0.59
0.295
=
ğ¿"
#
ğ¿&#
#
ğ¿&$
#
ğ‘= 0.1 âˆ’ğœ‚0.59 = 0.041
ğ‘¤'= 0.2 âˆ’ğœ‚0.59 =0.141
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤H
ğ‘¤(= 0.1 âˆ’ğœ‚0.295
=0.0706
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
47
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
ğœ‚= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
48
Multivariable Logistic Regression using Gradient Descent
ğ’š= 0
Model
ğ‘
ğ‘¤G
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘= 0.1 âˆ’ğœ‚0.59 = 0.041
ğ‘¤'= 0.2 âˆ’ğœ‚0.59 =0.141
ğœ‚= 0.1
0.041
0.141
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.041
0.141 0.0706]
0.0706
ğ‘¤H
ğ‘¤(= 0.1 âˆ’ğœ‚0.295
=0.0705
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
2 â€“ Logistic Regresion
!
49
Prediction
Prediction
Prediction
&ğ‘¦= ğœğ‘§=
1
1 + ğ‘’"# = 0.58
ğ‘§= ğ‘¤ğ‘¥+ ğ‘= 0.3417
Prediction
Thresholds = 0.5
ğ‘¦&'(): 1
Day
Hours
Pass
2
0.25
???
1
4.5
???
Day
Hours
Pass
2
0.25
???
1
4.5
???
Model
ğ‘
ğ‘¤G
0.041
0.141
0.0706
ğ‘¤H
Prediction
&ğ‘¦= ğœğ‘§=
1
1 + ğ‘’"# = 0.622
ğ‘§= ğ‘¤ğ‘¥+ ğ‘= 0.5
Thresholds = 0.5
ğ‘¦&'(): 1
Model
ğ‘
ğ‘¤G
0.041
0.141
0.0706
ğ‘¤H
3 â€“ Code Demo
50
Summary
51
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’"#
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡$L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
Data #2
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
Thanks!
Any questions?
52
