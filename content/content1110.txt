AI VIETNAM (AIO2024) aivietnam.edu.vn
Về độ chính xác, dựa trên ý tưởng liên quan đến receptive field và phép self-attention,
nhóm tác giả thực hiện hiệu chỉnh các nội dung sau:
- Large-kernel convolution:Ở các phiên bản YOLOv10 kích thước nhỏ, nhóm tác
giả thực hiện tăng kích thước kernel từ 3x3 lên 7x7 của phép depth-wise convolution
trong CIB nhằm cải thiện receptive field. Việc gia tăng này chỉ được áp dụng ở các
stage cuối.
- Partial self-attention (PSA):Để tận dụng sức mạnh của phép self-attention. Kể từ
sau stage 4 của mô hình, nhóm tác giả tách features sau phép point-wise convolution
của CIB làm hai phần. Một phần sẽ được đẩy vàoNPSA block bao gồm sự kết hợp của
lớp Multi-head self-attention (MHSA) và Feed-forward network (FFN), khá giống với
Transformer Encoder. Phần LayerNorm sẽ được thay thế bằng BatchNorm để tăng
tốc độ xử lý. Sau đó, kết quả của bước này sẽ được kết hợp với phần tách còn lại bởi
phép point-wise convolution.
Hình 14: Minh họa Partial self-attention (PSA). Ảnh: [10].
