Deep Multi-Task Architectures. Bao gồm các phương pháp tập trung vào xây dựng các mô hình
chung để giải quyết các bài toán khác nhau. Trong đó gồm 2 thành phần chính: thành phần thứ
nhất bao gồm các layer chung hoặc chia sẻ trọng số để học các đặc trưng thường gọi là shared-
encoder; thành phần thứ hai bao gồm các layer riêng để học các đặc trưng để tối ưu cho từng bài
toán riêng thường được gọi là task-specific layer.
2. Optimization Strategy. Với mỗi bài toán sẽ có hàm loss đánh giá khác nhau. Trong phần này
sẽ tập trung xây dựng các thuật toán tối ưu trong quá trình huấn luyện và cập nhật trọng số.
Điển hình trong đó là các phương pháp cân bằng trọng số khi tổng hợp hàm loss, như gradient
normalization, uncertainy weighting,...
Trong phần này để bước đầu hiểu rõ về các phương pháp huấn luyện mô hình MTL, chúng ta sẽ triển
khai mô hình MTL cơ bản tập trung vào tinh chỉnh phần encoder là hard parameter sharing model.
2