Random Forest and 
AdaBoost
(Warm-up Class)
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
Random Forest
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
Decision Tree
AI VIETNAM
All-in-One Course
❖ Observation
https://www.fs.usda.gov/wildflowers/
beauty/iris/flower.shtml
 
 
Petal_Length Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
1
Decision Tree
AI VIETNAM
All-in-One Course
❖ Observation
https://www.fs.usda.gov/wildflowers/
beauty/iris/flower.shtml
Petal_Length Petal_Width Sepal_length
Label
1
0.2
5.1
0
1.3
0.6
4.9
0
0.9
0.7
4.7
0
1.7
0.5
4.8
1
1.8
0.9
6.6
1
1.2
1.3
5.2
1
 
 
2
Decision Tree
AI VIETNAM
All-in-One Course
❖ Observation
https://www.fs.usda.gov/wildflowers/
beauty/iris/flower.shtml
Petal_Length Petal_Width
Sepal_length Sepal_Width Label
1
0.2
5.1
3.5
0
1.3
0.6
4.9
3
0
0.9
0.7
4.7
3.2
0
1.7
0.5
4.8
2.8
1
1.8
0.9
6.6
3.3
1
1.2
1.3
5.2
2.4
1
 
 
3
Discussion
4
problem 1
problem 2
5
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
6
Create a new dataset
Select features randomly
Enough?
Build decision tree
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
7
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
1
0.2
0
1.8
0.9
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
1
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
8
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Label
1
0
1.3
0
1
0
1.8
1
1.8
1
1.2
1
1
 
 
RF-Classification
❖ Simple IRIS
9
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Label
1
0
1.3
0
1
0
1.8
1
1.8
1
1.2
1
1
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
10
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
2
Petal_Length
Petal_Width
Label
1.3
0.6
0
1.3
0.6
0
0.9
0.7
0
0.9
0.7
0
1.8
0.9
1
1.2
1.3
1
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
11
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
2
Petal_Width
Label
0.6
0
0.6
0
0.7
0
0.7
0
0.9
1
1.3
1
RF-Classification
❖ Simple IRIS
12
Create a new dataset
Select features randomly
Enough?
Build decision tree
2
Petal_Width
Label
0.6
0
0.6
0
0.7
0
0.7
0
0.9
1
1.3
1
 
 
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
13
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
3
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
1.2
1.3
1
1.8
0.9
1
1.8
0.9
1
1.2
1.3
1
RF-Classification
AI VIETNAM
All-in-One Course
❖ Simple IRIS
14
Create a new dataset
Select features randomly
Enough?
Build decision tree
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
3
Petal_Length
Label
1
0
1.3
0
1.2
1
1.8
1
1.8
1
1.2
1
 
 
RF-Classification
❖ Simple IRIS
15
Create a new dataset
Select features randomly
Enough?
Build decision tree
3
Petal_Length
Label
1
0
1.3
0
1.2
1
1.8
1
1.8
1
1.2
1
RF-Classification
❖ Simple IRIS
16
inference
 
 
                       
    
          
       
3
 
 
                       
    
          
       
1
2
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length = 1.7
Petal_Width = 0.8
 
 
                       
    
          
       
RF-Classification
❖ Using sklearn
Another experiment 
RF-Classification
❖ Using sklearn
 
 
 
 
 
 
18
Using all the training samples
Outlook
Temp
Humidity
Wind
Play Tennis
Sunny
Hot
High
Weak
No
Sunny
Hot
High
Strong
No
Overcast
Hot
High
Weak
Yes
Rain
Mild
High
Weak
Yes
Rain
Cool
Normal
Weak
Yes
Rain
Cool
Normal
Strong
No
Overcast
Cool
Normal
Strong
Yes
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Rain
Mild
Normal
Weak
Yes
Sunny
Mild
Normal
Strong
Yes
Overcast
Mild
High
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Rain
Mild
High
Strong
No
Entropy:
𝐸𝑆= − ෍
𝑐∈𝐶
𝑝𝑐𝑙𝑜𝑔2𝑝𝑐
𝐼𝐺𝑆, 𝐹= 𝐸𝑆− ෍
𝑓∈𝐹
𝑆𝑓
𝑆𝐸(𝑆𝑓)
Information Gain
Combine 
Option_1: Sunny - (Overcast, Rain) 
Option_2: Overcast - (Sunny, Rain)
Option_3: Rain – (Sunny, Overcast)
Category = 3 > 2
𝐼𝐺𝑆, 𝑂𝑝𝑡𝑖𝑜𝑛_1  
 
= 𝐸𝑆−5
14 𝐸𝑆𝑆𝑢𝑛𝑛𝑦−9
14 𝐸𝑆𝑂𝑣𝑒𝑟𝑐𝑎𝑠𝑡,𝑅𝑎𝑖𝑛 
 
= 0.94 −5
14 ∗0.97 −9
14 ∗0.764 = 0.102
𝑆𝑆𝑢𝑛𝑛𝑦= {2: 𝑌𝑒𝑠, 3: 𝑁𝑜}
𝑆𝑂𝑣𝑒𝑟𝑐𝑎𝑠𝑡,𝑅𝑎𝑖𝑛= {7: 𝑌𝑒𝑠, 2: 𝑁𝑜}
𝐸𝑆𝑆𝑢𝑛𝑛𝑦= 0.97
𝐸𝑆𝑂𝑣𝑒𝑟𝑐𝑎𝑠𝑡,𝑅𝑎𝑖𝑛= 0.764
Gain(S, Outlook) = max ൞
𝐼𝐺𝑆, 𝑂𝑝𝑡𝑖𝑜𝑛_1 = 0.102
𝐼𝐺𝑆, 𝑂𝑝𝑡𝑖𝑜𝑛_2 = 0.226 
𝐼𝐺𝑆, 𝑂𝑝𝑡𝑖𝑜𝑛_3 = 0.003 
Gain(S, Outlook) = 0.226
Gain(S, Temp) = 0.015
Gain(S, Humidity) = 0.151
Gain(S, Wind) = 0.048
Choose Outlook 
with highest Gain 
score for root node
Option_2 is used to 
split 
𝐸𝑆= −9
14 𝑙𝑜𝑔2
9
14 −5
14 𝑙𝑜𝑔2
5
14
= 0.94
𝑆= {9: 𝑌𝑒𝑠, 5: 𝑁𝑜}
𝐺𝑎𝑖𝑛𝑆, 𝑊𝑖𝑛𝑑= 𝐸𝑆−8
14 𝐸𝑆𝑤𝑒𝑎𝑘−6
14 𝐸𝑆𝑆𝑡𝑟𝑜𝑛𝑔
 = 0.94 −
8
14 ∗0.811 −
6
14 ∗1 = 0.048
𝑆𝑤𝑒𝑎𝑘= {6: 𝑌𝑒𝑠, 2: 𝑁𝑜}
𝑆𝑆𝑡𝑟𝑜𝑛𝑔= {3: 𝑌𝑒𝑠, 3: 𝑁𝑜}
𝐸𝑆𝑤𝑒𝑎𝑘= −6
8 𝑙𝑜𝑔2
6
8 −2
8 𝑙𝑜𝑔2
6
8
= 0.811
𝐸𝑆𝑆𝑡𝑟𝑜𝑛𝑔= −3
6 𝑙𝑜𝑔2
3
6 −3
6 𝑙𝑜𝑔2
3
6
= 1
Category = 2
Decision Tree
19
n = 4
Play_Tennis = Yes
n = 3
Play_Tennis = No
n = 3
Play_Tennis = Yes
n = 3
Play_Tennis = No
n = 3
Play_Tennis = Yes
n = 3
Play_Tennis = No
n = 3
Play_Tennis = Yes
20
Decision Tree
 
 
 
 
Outlook
Temp
Humidity
Wind
Label
Sunny
Hot
High
Weak
No
Sunny
Hot
High
Strong
No
Overcast
Hot
High
Weak
Yes
Rain
Mild
High
Weak
Yes
Rain
Cool
Normal
Weak
Yes
Rain
Cool
Normal
Strong
No
Overcast
Cool
Normal
Strong
Yes
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Rain
Mild
Normal
Weak
Yes
Sunny
Mild
Normal
Strong
Yes
Overcast
Mild
High
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Rain
Mild
High
Strong
No
Bootstrapping
(random 
sampling with 
replacement)
Outlook
Temp
Humidity
Wind
Label
Sunny
Hot
High
Strong
No
Rain
Cool
Normal
Weak
Yes
Rain
Cool
Normal
Strong
No
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Rain
Mild
Normal
Weak
Yes
Sunny
Mild
Normal
Strong
Yes
Sunny
Mild
Normal
Strong
Yes
Sunny
Mild
Normal
Strong
Yes
Sunny
Mild
Normal
Strong
Yes
Outlook
Temp
Humidity
Wind
Label
Sunny
Hot
High
Strong
No
Overcast
Hot
High
Weak
Yes
Overcast
Hot
High
Weak
Yes
Rain
Cool
Normal
Strong
No
Sunny
Mild
High
Weak
No
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Sunny
Mild
Normal
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Rain
Mild
High
Strong
No
Outlook
Temp
Humidity
Wind
Label
Sunny
Hot
High
Weak
No
Rain
Mild
High
Weak
Yes
Rain
Mild
High
Weak
Yes
Overcast
Cool
Normal
Strong
Yes
Overcast
Cool
Normal
Strong
Yes
Rain
Mild
Normal
Weak
Yes
Rain
Mild
Normal
Weak
Yes
Overcast
Mild
High
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Overcast
Hot
Normal
Weak
Yes
 
 
Build tree
Build tree
Build tree
Training phase
Tree 1
Tree 2
Tree 3
Forest
Tree_3
Tree_1
Tree_2
n_sample = 10
replacement
21
 
 
Test = <outlook=Sunny, temperature=Hot, humidity=High, Wind=Weak>
Tree 1
 
 
Tree 2
 
 
Tree 3
predict
predict
predict
Voting
No
Yes
Yes
Final predict: Yes
Test phase
22
 
 
Using sklearn
Another run
Outlook
Temp
Humidity
Wind
Play Tennis
Sunny
Hot
High
Weak
No
Sunny
Hot
High
Strong
No
Overcast
Hot
High
Weak
Yes
Rain
Mild
High
Weak
Yes
Rain
Cool
Normal
Weak
Yes
Rain
Cool
Normal
Strong
No
Overcast
Cool
Normal
Strong
Yes
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Rain
Mild
Normal
Weak
Yes
Sunny
Mild
Normal
Strong
Yes
Overcast
Mild
High
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Rain
Mild
High
Strong
No
Tree 1
23
Using sklearn
Another run
Outlook
Temp
Humidity
Wind
Play Tennis
Sunny
Hot
High
Weak
No
Sunny
Hot
High
Strong
No
Overcast
Hot
High
Weak
Yes
Rain
Mild
High
Weak
Yes
Rain
Cool
Normal
Weak
Yes
Rain
Cool
Normal
Strong
No
Overcast
Cool
Normal
Strong
Yes
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Rain
Mild
Normal
Weak
Yes
Sunny
Mild
Normal
Strong
Yes
Overcast
Mild
High
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Rain
Mild
High
Strong
No
Tree 2
 
 
24
 
 
Using sklearn
Another run
Tree 3
25
Decision Tree - Regression
AI VIETNAM
All-in-One Course
❖ Salary prediction
26
When Experience = 5.3,
Salary = ?
𝜇= 1
𝑆෍
𝑖
𝑆𝑖= 55.14
𝑚𝑠𝑒= 1
𝑆෍
𝑖
𝑆𝑖−𝜇2 = 1417.97
𝜇𝑅= 1
𝑅෍
𝑖
𝑅𝑖= 59.38
𝑚𝑠𝑒𝑅= 1
𝑅෍
𝑖
𝑅𝑖−𝜇2 = 1275.15
𝜇𝐿= 1
𝐿෍
𝑖
𝐿𝑖= 0
𝑚𝑠𝑒𝐿= 1
𝐿෍
𝑖
𝐿𝑖−𝜇2 = 0
𝑎𝑚𝑠𝑒= 𝐿
𝑆𝑚𝑠𝑒𝐿+ 𝑅
𝑆𝑚𝑠𝑒𝑅
= 1
14 ∗0 + 13
14 ∗1275.15
= 1184.07
27
𝜇= 1
𝑆෍
𝑖
𝑆𝑖= 55.14
𝑚𝑠𝑒= 1
𝑆෍
𝑖
𝑆𝑖−𝜇2 = 1417.97
𝜇𝑅= 1
𝑅෍
𝑖
𝑅𝑖= 77.2
𝑚𝑠𝑒𝑅= 1
𝑅෍
𝑖
𝑅𝑖−𝜇2 = 282.35
𝜇𝐿= 1
𝐿෍
𝑖
𝐿𝑖= 0
𝑚𝑠𝑒𝐿= 1
𝐿෍
𝑖
𝐿𝑖−𝜇2 = 0
𝑎𝑚𝑠𝑒= 𝐿
𝑆𝑚𝑠𝑒𝐿+ 𝑅
𝑆𝑚𝑠𝑒𝑅
= 4
14 ∗0 + 10
14 ∗282.35
= 201.68
𝑎𝑚𝑠𝑒= 1184.07
𝑎𝑚𝑠𝑒= 911.19
𝑎𝑚𝑠𝑒= 588.68
𝑎𝑚𝑠𝑒= 201.68
𝑎𝑚𝑠𝑒= 383.92
𝑎𝑚𝑠𝑒= 526.52
𝑎𝑚𝑠𝑒= 543.51
𝑎𝑚𝑠𝑒= 575.09
𝑎𝑚𝑠𝑒= 613.34
𝑎𝑚𝑠𝑒= 758.4
𝑎𝑚𝑠𝑒= 947.73
𝑎𝑚𝑠𝑒= 1090.05
𝑎𝑚𝑠𝑒= 1256.21
29
Decision Tree - Regression
AI VIETNAM
All-in-One Course
❖ Salary prediction
30
Decision Tree 
Regression
31
Random Forest 
Regression
❖ Salary prediction
32
 
 
 
 
 
 
Random Forest
❖ Bernoulli Random variables
AI VIETNAM
All-in-One Course
𝑝𝑥= 𝑝𝑋= 𝑥= ቊ𝑝 
𝑤ℎ𝑒𝑛 𝑥= 1
1 −𝑝 
𝑤ℎ𝑒𝑛 𝑥= 0
Toss a coin
Sample space: S = {tail, head}
A numerical description of the outcome 
of a statistical experiment
X= {0, 1}
𝑝𝑥, 𝑛, 𝑘=
1 −1
𝑛
𝑛
𝑝𝑥, 𝑛, 𝑘= 𝐶𝑛𝑘
1
𝑛
𝑘
1 −1
𝑛
𝑛−𝑘
with 𝑘= 0
33
Adaptive Boosting
(Warm-up Class)
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
 
 
AdaBoost
❖ Idea
AI VIETNAM
All-in-One Course
https://www.fs.usda.gov/wildflowers/
beauty/iris/flower.shtml
Node
Leaf
Leaf
Stump
Learning from mistakes
Correctly
Wrongly
Correctly
Wrongly
34
AdaBoost
❖ Discussion
AI VIETNAM
All-in-One Course
Node
Leaf
Leaf
Stump
Learning from mistakes
Correctly
Wrongly
Correctly
Wrongly
1) Are a wide range of features used to build a forest? 
2) How to create a new dataset?
35
AdaBoost
❖ Idea
AI VIETNAM
All-in-One Course
How to balance the two groups’ values?
Each person has a value
Balance #sources
36
AdaBoost
❖ Idea
AI VIETNAM
All-in-One Course
Any other ideas?
Each person has a value
???
37
AdaBoost
❖ Create a new dataset
AI VIETNAM
All-in-One Course
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label Evaluation
1
0.2
0
T
1.3
0.6
0
F
0.9
0.7
0
T
1.7
0.5
1
T
1.8
0.9
1
F
1.2
1.3
1
T
Naïve 
Petal_Length
Petal_Width
Label Evaluation
1.3
0.6
0
F
1.3
0.6
0
F
1.8
0.9
1
F
1.8
0.9
1
F
Petal_Length
Petal_Width
Label Evaluation
1
0.2
0
T
0.9
0.7
0
T
1.7
0.5
1
T
1.2
1.3
1
T
38
⚫
Roulette Wheel Selection
❖
The probability of selecting a given chromosome is proportional to its fitness 
⚫
Tournament Selection
❖
Combine the fitness proportional concept with the random selection
Individual 1
Individual 2
Individual n
Individual 4
Individual 4
Individual 1
Individual 3
Individual 1
Competition
tth generation
t+1th generation
t+1th generation
Roulette
wheel
Tournament
(pair-wise)
Individual 1
Individual 2
Individual k
Individual 4
Individual m
Individual 5
Individual 3
Individual k
Individual 3
Ideas from Genetic Algorithms 
39
AdaBoost
❖ Create a new dataset
AI VIETNAM
All-in-One Course
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label Evaluation
1
0.2
0
T
1.3
0.6
0
F
0.9
0.7
0
T
1.7
0.5
1
T
1.8
0.9
1
F
1.2
1.3
1
T
Add more randomness 
Petal_Length
Petal_Width
Label
Evaluation
Score
Probability
1
0.2
0
T
1
0.125
1.3
0.6
0
F
2
0.25
0.9
0.7
0
T
1
0.125
1.7
0.5
1
T
1
0.125
1.8
0.9
1
F
2
0.25
1.2
1.3
1
T
1
0.125
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
1.2
1.3
1
1.7
0.5
1
1.8
0.9
1
1.8
0.9
1
40
normalize
AdaBoost
❖ Create a new dataset
AI VIETNAM
All-in-One Course
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label
Evaluation
Score
Probability
1
0.2
0
T
1
0.125
1.3
0.6
0
F
2
0.25
0.9
0.7
0
T
1
0.125
1.7
0.5
1
T
1
0.125
1.8
0.9
1
F
2
0.25
1.2
1.3
1
T
1
0.125
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
1.2
1.3
1
1.7
0.5
1
1.8
0.9
1
1.8
0.9
1
Case 1
Petal_Length
Petal_Width
Label
Evaluation
Score
Probability
1
0.2
0
T
1
0.142
1.3
0.6
0
T
1
0.142
1.2
1.3
1
F
2
0.29
1.7
0.5
1
T
1
0.142
1.8
0.9
1
T
1
0.142
1.8
0.9
1
T
1
0.142
41
AdaBoost
❖ Create a new dataset
AI VIETNAM
All-in-One Course
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
Petal_Length
Petal_Width
Label
Evaluation
Score
Probability
1
0.2
0
T
1
0.125
1.3
0.6
0
F
2
0.25
0.9
0.7
0
T
1
0.125
1.7
0.5
1
T
1
0.125
1.8
0.9
1
F
2
0.25
1.2
1.3
1
T
1
0.125
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
1.2
1.3
1
1.7
0.5
1
1.8
0.9
1
1.8
0.9
1
Petal_Length
Petal_Width
Label
Evaluation
Score
Probability
1
0.2
0
T
1
0.142
1.3
0.6
0
F
2
0.29
1.2
1.3
1
T
1
0.142
1.7
0.5
1
T
1
0.142
1.8
0.9
1
T
1
0.142
1.8
0.9
1
T
1
0.142
Case 2
Problem and solution?
42
AdaBoost
❖ Create a new dataset
AI VIETNAM
All-in-One Course
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
True
False
Update
How much?
43
𝑓(𝑥) = 𝑒−𝑥
For incorrect samples 
When a model is good (small error), 
scaled weights should increase/decrease slightly/significantly?
Error < 0.5
𝑓(𝑥) = 𝑒𝑥
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑓(𝑥) = 𝑥
𝑓𝑥= −𝑥
44
AdaBoost
For incorrect samples 
AI VIETNAM
All-in-One Course
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑓(𝑥) = 1 −𝑥
𝑥
When a model has a small error, increase significantly
Error < 0.5
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑔(𝑥) =
1 −𝑥
𝑥
45
AdaBoost
For correct samples 
AI VIETNAM
All-in-One Course
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑘𝑥=
1
𝑓(𝑥) =
𝑥
1 −𝑥
ℎ𝑥=
1
𝑔(𝑥) =
𝑥
1 −𝑥
Decrease significantly
Error < 0.5
❖ Create a new dataset
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
True
False
Update
𝑔(𝐸) =
1 −𝐸
𝐸
𝑝𝑖= 𝑝𝑖𝑔𝐸
𝑝𝑖= 𝑝𝑖ℎ(𝐸)
ℎ(𝐸) =
𝐸
1 −𝐸
= 0.166 ∗1.41 = 2.347
= 0.166 ∗0.707 = 1.17
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
1.17
1.3
0.6
0
2.347
0.9
0.7
0
1.17
1.7
0.5
1
1.17
1.8
0.9
1
2.347
1.2
1.3
1
1.17
46
❖ Create a new dataset
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
True
False
Update
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.124
1.3
0.6
0
0.25
0.9
0.7
0
0.124
1.7
0.5
1
0.124
1.8
0.9
1
0.25
1.2
1.3
1
0.124
Normalized
𝑔(𝐸) =
1 −𝐸
𝐸
𝑝𝑖= 𝑝𝑖𝑔𝐸
𝑝𝑖= 𝑝𝑖ℎ(𝐸)
ℎ(𝐸) =
𝐸
1 −𝐸
= 0.166 ∗1.41 = 2.347
= 0.166 ∗0.707 = 1.17
47
❖ Create a new dataset
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.166
1.3
0.6
0
0.166
0.9
0.7
0
0.166
1.7
0.5
1
0.166
1.8
0.9
1
0.166
1.2
1.3
1
0.166
True
False
Update
𝑔𝐸=
1 −𝐸
𝐸
𝑝𝑖= 𝑝𝑖𝑔𝐸= 𝑝𝑖𝑒ln 𝑔𝐸
𝑝𝑖= 𝑝𝑖ℎ𝐸= 𝑝𝑖 𝑒ln ℎ𝐸
ℎ(𝐸) =
𝐸
1 −𝐸
Petal_Length
Petal_Width
Label
Probability
1
0.2
0
0.124
1.3
0.6
0
0.25
0.9
0.7
0
0.124
1.7
0.5
1
0.124
1.8
0.9
1
0.25
1.2
1.3
1
0.124
Normalized
= 𝑝𝑖𝑒
ln
1−𝐸
𝐸
= 𝑝𝑖𝑒
1
2ln 1−𝐸
𝐸
= 𝑝𝑖𝑒
ln
𝐸
1−𝐸= 𝑝𝑖𝑒−1
2ln 1−𝐸
𝐸
48
AdaBoost
For incorrect samples 
AI VIETNAM
All-in-One Course
https://mbernste.github.io/files/notes/AdaBoost.pdf
𝑔(𝑥) = 𝑒
1
2ln 1−𝑥
𝑥
Increase significantly
𝑒𝑟𝑟𝑜𝑟 𝑥
Error < 0.5
𝑒𝑟𝑟𝑜𝑟 𝑥
𝑔(𝑥) =
1 −𝑥
𝑥
49
AdaBoost
For correct samples 
AI VIETNAM
All-in-One Course
𝑒𝑟𝑟𝑜𝑟 𝑥
ℎ𝑥=
1
𝑔(𝑥) =
𝑥
1 −𝑥
Decrease significantly
Error < 0.5
𝑒𝑟𝑟𝑜𝑟 𝑥
ℎ(𝑥) = 𝑒−1
2ln 1−𝑥
𝑥
Implementation 
using sklearn
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
AdaBoost
AI VIETNAM
All-in-One Course
AdaBoost
For incorrect samples 
AI VIETNAM
All-in-One Course
𝑔(𝑥) = 𝑒
1
2ln 1−𝑥
𝑥
Increase
𝑒𝑟𝑟𝑜𝑟 𝑥
Error > 0.5
Decrease
𝑒𝑟𝑟𝑜𝑟 𝑥
ℎ(𝑥) = 𝑒−1
2ln 1−𝑥
𝑥
For correct samples 
Implementation 
using sklearn
Petal_Length
Petal_Width
Label
1
0.2
0
1.3
0.6
0
0.9
0.7
0
1.7
0.5
1
1.8
0.9
1
1.2
1.3
1
AdaBoost
AI VIETNAM
All-in-One Course
How to do 
inference?
