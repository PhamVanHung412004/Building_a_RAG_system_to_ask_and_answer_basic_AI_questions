Optimization Algorithms 
in Deep Learning
Year 2023
Quang-Vinh Dinh
PhD in Computer Science
AI VIETNAM
All-in-One Course
â¢Quick Introdution
â¢Stochastic Gradient Descent
â¢Adagrad
â¢RMSProp
â¢Momentum
â¢Adam
â¢Case studies
Outline
Optimization Algorithms
AI VIETNAM
All-in-One Course
Overview
1
Optimization Algorithms
Loss functions
Discontinuous 
Functions
Continuous 
Function
From â€œMachine Learning Simplifiedâ€
Continuous non-differentiable functions
2
Optimization Algorithms
AI VIETNAM
All-in-One Course
Challenges
Local minima
Global minima
Saddle points
https://blog.paperspace.com/intro-
to-optimization-in-deep-learning-
gradient-descent/
Optimization Algorithms
AI VIETNAM
All-in-One Course
Challenges
Local minima
Global minima
Saddle points
https://vitalflux.com/local-
global-maxima-minima-
explained-examples/
4
Optimization Algorithms
AI VIETNAM
All-in-One Course
Challenges: Local minima
From â€œMachine Learning Simplifiedâ€
5
Optimization Algorithms
AI VIETNAM
All-in-One Course
Challenges
Saddle points
https://www.wikiwand.com/en/Saddle_point
6
Optimization Algorithms
AI VIETNAM
All-in-One Course
Challenges: Gradient vanishing
sigmoid ğ‘¥=
1
1 + ğ‘’âˆ’ğ‘¥
sigmoidâ€² ğ‘¥= sigmoid ğ‘¥
1 âˆ’sigmoid ğ‘¥
tanhâ€² ğ‘¥= 1 âˆ’tanh2(ğ‘¥)
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥
7
Optimization Algorithms
AI VIETNAM
All-in-One Course
Learning rate
From â€œMachine Learning Simplifiedâ€
â¢Quick Introdution
â¢Stochastic Gradient Descent
â¢Adagrad
â¢RMSProp
â¢Momentum
â¢Adam
â¢Case studies
Outline
Optimizers
AI VIETNAM
All-in-One Course
Optimizer Selection
https://www.kdnuggets.com/2019/06/gradient-descent-algorithms-cheat-sheet.html
Define a way to update parameters
9
Äáº¡o hÃ m=
ğ‘‡â„ğ‘ğ‘¦ Ä‘á»•ğ‘– ğ‘¡â„ğ‘’ğ‘œ ğ‘¦
ğ‘‡â„ğ‘ğ‘¦ Ä‘á»•ğ‘– ğ‘¡â„ğ‘’ğ‘œ ğ‘¥=
âˆ†ğ‘¦ 
âˆ†ğ‘¥ 
ğ‘‘
ğ‘‘ğ‘¥ ğ‘“(ğ‘¥) = lim
âˆ†ğ‘¥â†’0
ğ‘“ğ‘¥+ âˆ†ğ‘¥âˆ’ğ‘“(ğ‘¥)
âˆ†ğ‘¥
âˆ†ğ‘¥ cáº§n tiáº¿n vá» 0 Ä‘á»ƒ 
Ä‘Æ°á»ng tiáº¿p tuyáº¿n tiáº¿n 
vá» hÃ m f(x) trong vÃ¹ng
 lÃ¢n cáº­n táº¡i x
Äáº¡o hÃ m cho hÃ m liÃªn tá»¥c
Derivative/Gradient
AI VIETNAM
All-in-One Course
f(x)
âˆ†ğ‘¥
âˆ†ğ‘¦
ğ‘¥
ğ‘¦
ğ‘¥
ğ‘¥+ âˆ†ğ‘¥
10
TÃ¬m giÃ¡ trá»‹ min
ğ‘¥
ğ‘¦
ğ‘¥op
ğ‘¥2
ğ
ğğ± ğŸğ±ğŸ< ğŸ
ğ
ğğ± ğŸğ±ğŸ> ğŸ
ğ‘¥1
Quan sÃ¡t: ğ‘¥op á»Ÿ vá»‹ trÃ­ ngÆ°á»£c hÆ°á»›ng Ä‘áº¡o hÃ m 
táº¡i ğ±ğŸ vÃ  ğ±ğŸ 
CÃ¡ch xá»­ lÃ½ viá»‡c di chuyá»ƒn ngÆ°á»£c hÆ°á»›ng Ä‘áº¡o 
hÃ m cho ğ±ğŸ vÃ  ğ±ğŸ (Ä‘á»ƒ tÃ¬m ğ‘¥op) khÃ¡c nhau 
hÃ¬nh thÃ nh cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a khÃ¡c 
nhau
ğ±= ğ± âˆ’ ğœ‚ğ
ğğ± ğŸğ±
CÃ¡ch cáº­p nháº­t giÃ¡ trá»‹ x Ä‘Æ¡n giáº£n
Äáº¡o hÃ m táº¡i x
Trá»ng sá»‘
Gradient-based Optimization
AI VIETNAM
All-in-One Course
2
11
ğœƒ value
Cá»© tiáº¿p tá»¥c di chuyá»ƒn ngÆ°á»£c 
hÆ°á»›ng Ä‘áº¡o hÃ m
ğ
ğğœƒğŸğœƒ> ğŸ
Dá»‹ch chuyá»ƒn ğœƒ vá» phÃ­a trÃ¡i
ğ‰ğœƒ
ğœƒ value
Di chuyá»ƒn ğœ½ ngÆ°á»£c hÆ°á»›ng Ä‘áº¡o hÃ m
ğ
ğğœ½f ğœƒ> ğŸ
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a trÃ¡i
ğ‰ğœƒ
Optimization
â– A cue to optimize a function
3
ğ‰ğœƒ
Khá»Ÿi táº¡o giÃ¡ trá»‹ ğœ½
ğ
ğğœ½ğ‰ğœƒ> ğŸ
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a trÃ¡i
ğœƒ value
ğ
ğğœ½ğŸğœƒ> ğŸ
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a trÃ¡i
ğ
ğğœƒğŸğœƒ< ğŸ
Dá»‹ch chuyá»ƒn ğœƒ 
vá» phÃ­a pháº£i
12
Gradient-based Optimization
AI VIETNAM
All-in-One Course
â– Square function
ğ‘“ğ‘¥= ğ‘¥2
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
Gradient-based Optimization
AI VIETNAM
All-in-One Course
â– Square function
Compute 
derivative at x
Move x 
opposite to dx
Initialize x
ğ‘“ğ‘¥= ğ‘¥2
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
Optimization
AI VIETNAM
All-in-One Course
â– Square function
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘“ğ‘¥= ğ‘¥2
ğ‘¥0 = 70.0
ğœ‚= 0.1
ğ‘“â€² ğ‘¥0 = 140.0
ğ‘¥1 = ğ‘¥0 âˆ’ğœ‚ğ‘“â€² ğ‘¥0 = 56.0
f â€² x1 = 112.0
ğ‘¥2 = ğ‘¥1 âˆ’ğœ‚ğ‘“â€² ğ‘¥1 = 44.8
f â€² x2 = 89.6
ğ‘¥3 = ğ‘¥2 âˆ’ğœ‚ğ‘“â€² ğ‘¥2 = 35.84
f â€² x3 = 71.68
ğ‘¥4 = ğ‘¥3 âˆ’ğœ‚ğ‘“â€² ğ‘¥3 = 28.672
Optimization
AI VIETNAM
All-in-One Course
â– Square function
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘“ğ‘¥= ğ‘¥2
ğ‘¥10 = 6.012
ğœ‚= 0.1
ğ‘“â€² ğ‘¥10 = 12.02
ğ‘¥11 = ğ‘¥10 âˆ’ğœ‚ğ‘“â€² ğ‘¥10 = 4.81
f â€² x11 = 9.62
ğ‘¥12 = ğ‘¥11 âˆ’ğœ‚ğ‘“â€² ğ‘¥11 = 3.84
f â€² x12 = 7.69
ğ‘¥13 = ğ‘¥12 âˆ’ğœ‚ğ‘“â€² ğ‘¥12 = 3.078
f â€² x13 = 6.15
ğ‘¥14 = ğ‘¥13 âˆ’ğœ‚ğ‘“â€² ğ‘¥13 = 2.46
Keep doing
Optimization
AI VIETNAM
All-in-One Course
â– Square function
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘“ğ‘¥= ğ‘¥2
ğ‘¥30 = 0.069
ğœ‚= 0.1
ğ‘“â€² ğ‘¥30 = 0.138
ğ‘¥31 = ğ‘¥30 âˆ’ğœ‚ğ‘“â€² ğ‘¥30 = 0.055
f â€² x31 = 0.11
ğ‘¥32 = ğ‘¥31 âˆ’ğœ‚ğ‘“â€² ğ‘¥31 = 0.044
f â€² x32 = 0.88
ğ‘¥33 = ğ‘¥32 âˆ’ğœ‚ğ‘“â€² ğ‘¥32 = 0.035
f â€² x34 = 0.071
ğ‘¥34 = ğ‘¥33 âˆ’ğœ‚ğ‘“â€² ğ‘¥33 = 0.028
Keep doing
Optimization
AI VIETNAM
All-in-One Course
â– Square function
ğ‘“ğ‘¥= ğ‘¥2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘¥1000 â‰ˆ10âˆ’38
Optimized successfully!
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 0.1
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
19
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 0.001
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
Discussion
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 0.8
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
Discussion
Optimization
â– Square function
ğ‘¥0 = 99.0
ğœ‚= 1. 1
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= ğ‘¥2
Discussion
Optimization Algorithms
AI VIETNAM
All-in-One Course
Stochastic gradient descent
23
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
1-sample
m-sample
N-sample
Optimization
AI VIETNAM
All-in-One Course
â– Optimization: 2D function
âˆ’100 â‰¤ğ‘¥, ğ‘¦â‰¤100
ğ‘¥, ğ‘¦âˆˆâ„•
ğ‘“ğ‘¥, ğ‘¦= ğ‘¥2 + ğ‘¦2
7
24
Optimization
AI VIETNAM
All-in-One Course
â– Optimization: 2D function
âˆ’100 â‰¤ğ‘¥, ğ‘¦â‰¤100
ğ‘¥, ğ‘¦âˆˆâ„•
ğ‘“ğ‘¥, ğ‘¦= ğ‘¥2 + ğ‘¦2
ğ‘¥= ğ‘¥âˆ’ğœ‚ğœ•ğ‘“(ğ‘¥, ğ‘¦)
ğœ•ğ‘¥
ğ‘¦= ğ‘¦âˆ’ğœ‚ğœ•ğ‘“(ğ‘¥, ğ‘¦)
ğœ•ğ‘¦
ğ‘¥0 = 6.0
ğœ‚= 0.1
ğ‘¦0 = 9.0
ğœ•ğ‘“(ğ‘¥0, ğ‘¦0)
ğœ•ğ‘¥
= 12
ğœ•ğ‘“(ğ‘¥0, ğ‘¦0)
ğœ•y
= 18
ğ‘¥1 = 4.8
ğ‘¦1 = 7.2
ğœ•ğ‘“(ğ‘¥1, ğ‘¦1)
ğœ•ğ‘¥
= 9.6
ğœ•ğ‘“(ğ‘¥1, ğ‘¦1)
ğœ•y
= 14.4
ğ‘¥2 = 3.84
ğ‘¦2 = 5.75
ğœ•ğ‘“(ğ‘¥2, ğ‘¦2)
ğœ•ğ‘¥
= 7.68
ğœ•ğ‘“(ğ‘¥2, ğ‘¦2)
ğœ•y
= 11.51
ğ‘¥3 = 3.07
ğ‘¦3 = 4.608
ğœ•ğ‘“(ğ‘¥3, ğ‘¦3)
ğœ•ğ‘¥
= 6.14
ğœ•ğ‘“(ğ‘¥3, ğ‘¦3)
ğœ•y
= 9.21
ğ‘¥4 = 2.45
ğ‘¦4 = 3.68
Gradient-based Optimization
AI VIETNAM
All-in-One Course
â– Another Square function
Compute 
derivative at x
Move x 
opposite to dx
Initialize x
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘“ğ‘¥= 9ğ‘¥2
5
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
26
Optimization
AI VIETNAM
All-in-One Course
â– Another Square function
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘“ğ‘¥= 9ğ‘¥2
ğ‘¥0 = 70.0
ğœ‚= 0.1
ğ‘“â€² ğ‘¥0 = 1260.0
ğ‘¥1 = ğ‘¥0 âˆ’ğœ‚ğ‘“â€² ğ‘¥0 = âˆ’56.0
f â€² x1 = âˆ’1008.0
ğ‘¥2 = ğ‘¥1 âˆ’ğœ‚ğ‘“â€² ğ‘¥1 = 44.8
f â€² x2 = 806.4
ğ‘¥3 = ğ‘¥2 âˆ’ğœ‚ğ‘“â€² ğ‘¥2 = âˆ’35.84
f â€² x3 = âˆ’645.12
ğ‘¥4 = ğ‘¥3 âˆ’ğœ‚ğ‘“â€² ğ‘¥3 = 28.672
Optimization
AI VIETNAM
All-in-One Course
â– Another Square function
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘“ğ‘¥= 9ğ‘¥2
Optimization
â– Another Square function
ğ‘¥0 = 99.0
ğœ‚= 0.1
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= 9ğ‘¥2
Observation?
Gradient-based Optimization
AI VIETNAM
All-in-One Course
â– Square function
Compute 
derivative at x
Move x 
opposite to dx
Initialize x
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘“ğ‘¥= 20ğ‘¥2
5
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
31
Optimization
AI VIETNAM
All-in-One Course
â– Square function
âˆ’100 â‰¤ğ‘¥â‰¤100
ğ‘¥âˆˆâ„•
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘“ğ‘¥= 20ğ‘¥2
ğ‘¥0 = 2.0
ğœ‚= 0.1
ğ‘“â€² ğ‘¥0 = 80.0
ğ‘¥1 = ğ‘¥0 âˆ’ğœ‚ğ‘“â€² ğ‘¥0 = âˆ’6.0
f â€² x1 = âˆ’240.0
ğ‘¥2 = ğ‘¥1 âˆ’ğœ‚ğ‘“â€² ğ‘¥1 = 18.0
f â€² x2 = 720.0
ğ‘¥3 = ğ‘¥2 âˆ’ğœ‚ğ‘“â€² ğ‘¥2 = âˆ’54.0
f â€² x3 = âˆ’2160.0
ğ‘¥4 = ğ‘¥3 âˆ’ğœ‚ğ‘“â€² ğ‘¥3 = 162.0
Optimization
â– Square function
ğ‘¥0 = 2.0
ğœ‚= 0.1
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
ğ‘“ğ‘¥= 20ğ‘¥2
Observation?
Gradient-based Optimization
â– For composite function
6
AI VIETNAM
All-in-One Course
34
ğ‘¥
ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘¥
ğ‘“
ğ‘”
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘‘
ğ‘‘ğ‘¥ğ‘”ğ‘“ğ‘¥
=
ğ‘‘
ğ‘‘ğ‘“ğ‘”ğ‘“
âˆ—
ğ‘‘
ğ‘‘ğ‘¥ğ‘“ğ‘¥
ğ‘“ğ‘¥= ğ‘¥2 + 3ğ‘¥
ğ‘”ğ‘¥= ğ‘¥3 + ğ‘¥+ 2
ğ‘”ğ‘“ğ‘¥
Gradient-based Optimization
â– For composite function
AI VIETNAM
All-in-One Course
Select an appropriate 
value for learning rate
ğ‘¥0 = 3.0
ğœ‚= 0.01
ğ‘”â€² ğ‘¥0 = 8757.0
ğ‘¥1 = âˆ’84.0
ğ‘¥0 = 3.0
ğœ‚= 0.0001
ğ‘”â€² ğ‘¥0 = 8757.0
ğ‘¥1 = 2.1243
ğ‘¥0 = 3.0
ğœ‚= 0.001
ğ‘”â€² ğ‘¥0 = 8757.0
ğ‘¥1 = âˆ’5.757
ğ‘¥0 = 3.0
ğœ‚= 0.0005
ğ‘”â€² ğ‘¥0 = 8757.0
ğ‘¥1 = âˆ’1.3785
ğ‘¥0 = 3.0
ğœ‚= 0.0005
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
Optimization
â– Composite function
Observation?
Gradient-based 
Optimization
â– Another function
ğ‘“ğ‘¥= log(ğ‘¥2)
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘¥0 = 9.0
ğœ‚= 10.0
ğ‘“â€² ğ‘¥0 = 0.22
ğ‘¥1 = ğ‘¥0 âˆ’ğœ‚ğ‘“â€² ğ‘¥0 = 6.77
f â€² x1 = 0.295
ğ‘¥2 = ğ‘¥1 âˆ’ğœ‚ğ‘“â€² ğ‘¥1 = 3.82
f â€² x2 = 0.52
ğ‘¥3 = ğ‘¥2 âˆ’ğœ‚ğ‘“â€² ğ‘¥2 = âˆ’1.39
f â€² x3 = âˆ’1.429
ğ‘¥4 = ğ‘¥3 âˆ’ğœ‚ğ‘“â€² ğ‘¥3 = 12.89
ğ‘¥0 = 9.0
ğœ‚= 10.0
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
Optimization
â– Another function
Observation?
â– Another function
Gradient-based Optimization
AI VIETNAM
All-in-One Course
ğ‘¥0 = âˆ’1.5
ğœ‚= 0.2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
â– Another function
Gradient-based Optimization
AI VIETNAM
All-in-One Course
ğ‘¥0 = âˆ’1.5
ğœ‚= 0.2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
â¢Quick Introdution
â¢Stochastic Gradient Descent
â¢Adagrad
â¢RMSProp
â¢Momentum
â¢Adam
â¢Case studies
Outline
Adaptive Learning Rate
Learning rate decay
42
AI VIETNAM
All-in-One Course
Iteration ğ‘–
Period ğ‘
Reduction rate ğ‘˜
ğ‘–ğ‘“ ğ‘–%ğ‘== 0:
ğœ‚= 0.2
ğœ‚= ğœ‚/ğ‘˜
ğ‘¥0 = âˆ’1.5
ğœ‚= 0.2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
Adaptive 
Learning Rate
Learning rate decay
Adaptive Learning Rate
Learning rate decay
ğœ‚= ğœ‚0 Ã— ğœ†
ğ‘ 
ğ‘˜
initial_learning_rate * decay_rate ^ (step / decay_steps)
ğœ‚0 = 0.2
ğ‘˜= 5
ğœ†= 0.9
ğœ‚0 = 0.2
ğ‘˜= 1
ğœ†= 0.9
torch.optim.lr_scheduler
ğ‘¥0 = âˆ’1.5
ğœ‚= 0.2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’ğœ‚ğ‘“â€²(ğ‘¥)
Adaptive 
Learning Rate
Learning rate decay
ğœ‚= ğœ‚0 Ã— ğœ†
ğ‘ 
ğ‘˜
Adaptive Learning Rate
AI VIETNAM
All-in-One Course
Adagrad (one variable functions)
46
ğ‘ ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
Using derivative values
ğ‘ 0 = 0.1
âˆˆ= 10âˆ’7
(or s0 = 0.0)
Adaptive 
Learning 
Rate
Adagrad 
(one variable functions)
ğ‘ 0 = 0.0
ğœ–= 10âˆ’7
ğœ‚= 0.5
Adaptive 
Learning 
Rate
Adagrad 
(one variable functions)
ğ‘ 0 = 0.0
ğœ–= 10âˆ’7
ğœ‚= 0.5
Adaptive Learning Rate
Adagrad (one variable functions)
ğ‘ ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
Using derivative values
ğ‘ 0 = 0.1
ğœ–= 10âˆ’7
ğœ‚= ğœ‚0 Ã— ğœ†
ğ‘ 
ğ‘˜
ğœ‚0 = 0.2
ğ‘˜= 1
ğœ†= 0.9
AI VIETNAM
All-in-One Course
Learning rate decay
Differences?
Adaptive Learning Rate
AI VIETNAM
All-in-One Course
â– Optimization: 2D function
imagine
âˆ’100 â‰¤ğ‘¥, ğ‘¦â‰¤100
ğ‘¥, ğ‘¦âˆˆâ„•
ğ‘“ğ‘¥, ğ‘¦= ğ‘¥2 + ğ‘¦2
ğ‘“ğ‘¥, ğ‘¦
ğ‘“ğ‘¥, ğ‘¦
Adaptive Learning Rate
AI VIETNAM
All-in-One Course
51
ğ‘ ğ‘¡,ğ‘¥= ğ‘ ğ‘¡âˆ’1,ğ‘¥+ ğ‘”ğ‘¡,ğ‘¥
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡,ğ‘¥+ ğœ–ğ‘”ğ‘¡,ğ‘¥
ğ‘”ğ‘¡,ğ‘¥= ğœ•ğ‘“(ğ‘¥, ğ‘¦)
ğœ•ğ‘¥
Adagrad (2D function)
ğ‘ ğ‘¡,ğ‘¦= ğ‘ ğ‘¡âˆ’1,ğ‘¦+ ğ‘”ğ‘¡,ğ‘¦
2
ğ‘¦ğ‘¡= ğ‘¦ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡,ğ‘¦+ ğœ–ğ‘”ğ‘¡,ğ‘¦
ğ‘”ğ‘¡,ğ‘¦= ğœ•ğ‘“(ğ‘¥, ğ‘¦)
ğœ•ğ‘¥
Adagrad
ğ‘ ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
Adaptive 
Learning 
Rate
Adagrad: Limitation
ğ‘ 0 = 0.0
ğœ–= 10âˆ’7
ğœ‚= 0.1
ğ‘ ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
current 
summation
expected 
summation
1.0
-1.0
How to Use historical Data
AI VIETNAM
All-in-One Course
Moving average
53
3
8
6
5
1
9
0
8
4
5.5
7.0
5.5
3.0
8.0
4.5
4.0
6.0
4.0
7
ğ‘˜= 2
ğ‘†ğ‘€ğ´ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘ ğ‘¡âˆ’2 + â‹¯+ ğ‘ ğ‘¡âˆ’ğ‘˜
ğ‘˜
3
8
6
5
1
9
0
8
4
5.5
5.8
5.4
3.2
7.0
3.5
5.8
4.9
5.1
7
3.0
ğ¸ğ‘€ğ´ğ‘¡= ğœŒğ¸ğ‘€ğ´ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘ ğ‘¡
How to Use historical Data
AI VIETNAM
All-in-One Course
Exponentially weighted averages
54
3
8
6
5
1
9
0
8
4
5.5
5.8
5.4
3.2
7.0
3.5
5.8
4.9
5.1
7
3.0
ğ‘‰ğ‘¡= ğœŒğ‘‰ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘ ğ‘¡
ğ‘‰1 = ğœŒğ‘‰0 + (1 âˆ’ğœŒ)ğ‘ 1
ğ‘‰2 = ğœŒğ‘‰1 + (1 âˆ’ğœŒ)ğ‘ 2
ğ‘‰3 = ğœŒğ‘‰2 + (1 âˆ’ğœŒ)ğ‘ 3
ğ‘‰3 = ğœŒğœŒğ‘‰1 + (1 âˆ’ğœŒ)ğ‘ 2 + (1 âˆ’ğœŒ)ğ‘ 3
ğ‘‰3 = ğœŒğœŒğœŒğ‘‰0 + (1 âˆ’ğœŒ)ğ‘ 1 + (1 âˆ’ğœŒ)ğ‘ 2 + (1 âˆ’ğœŒ)ğ‘ 3
Given ğ‘‰0 = 0, we have
ğ‘‰3 = ğœŒğœŒ(1 âˆ’ğœŒ)ğ‘ 1 + (1 âˆ’ğœŒ)ğ‘ 2 + (1 âˆ’ğœŒ)ğ‘ 3
ğ‘‰3 = ğœŒ2 (1 âˆ’ğœŒ)ğ‘ 1 + ğœŒ(1 âˆ’ğœŒ)ğ‘ 2 + (1 âˆ’ğœŒ)ğ‘ 3
How to Use historical Data
AI VIETNAM
All-in-One Course
Exponentially weighted averages
ğ‘‰3 = ğœŒ2 (1 âˆ’ğœŒ)ğ‘ 1 + ğœŒ(1 âˆ’ğœŒ)ğ‘ 2 + (1 âˆ’ğœŒ)ğ‘ 3
With ğœŒ= 0.9
ğ‘‰3 = 0.081ğ‘ 1 + 0.09ğ‘ 2 + 0.1ğ‘ 3
With ğœŒ= 0.98
ğ‘‰3 = 0.0392ğ‘ 1 + 0.0196ğ‘ 2 + 0.02ğ‘ 3
With ğœŒ= 0.5
ğ‘‰3 = 0.125ğ‘ 1 + 0.25ğ‘ 2 + 0.5ğ‘ 3
prediction
Actual
ğ‘¡
ğ‘¦
3
8
6
5
1
9
0
8
4
5.5
5.8
5.4
3.2
7.0
3.5
5.8
4.9
5.1
7
3.0
ğ‘‰ğ‘¡= ğœŒğ‘‰ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘ ğ‘¡
data
EWA
Example
Optimization Algorithms
AI VIETNAM
All-in-One Course
56
ğœŒ= 0.9
Exponentially weighted averages
ğœŒ= 0.9
Optimization Algorithms
Exponentially weighted averages
ğœŒ= 0.5
ğœŒ= 0.98
ğ‘‰ğ‘¡= ğœŒğ‘‰ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘ ğ‘¡
Back to Adaptive Learning Rate
AI VIETNAM
All-in-One Course
Adagrad
ğ‘ ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
ğ‘‰ğ‘¡= ğœŒğ‘‰ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘ ğ‘¡
Exponentially weighted averages
How to apply to adagrad
expected 
summation
-1.0
1.0
Back to Adaptive Learning Rate
AI VIETNAM
All-in-One Course
Adagrad
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
How to apply to adagrad
expected 
summation
RMSProp
-1.0
1.0
ğœ‚= 0.1
Back to Adaptive Learning Rate
AI VIETNAM
All-in-One Course
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡âˆ’1
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
For square function
RMSProp
ğ‘“ğ‘¥= ğ‘¥2
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
For square function
RMSProp
ğ‘¥0 = 9.0
ğœ‚= 0.1
ğ‘“â€² ğ‘¥0 = 18.0
ğ‘¥1 = 9.0 âˆ’
0.1
ğ‘ 1 +âˆˆğ‘“â€² ğ‘¥0 = 8.6837
ğ‘“ğ‘¥= ğ‘¥2
ğœŒ= 0.9
âˆˆ= 10âˆ’7
ğ‘ 1 = 32.4
ğ‘ 0 = 0.0
ğ‘“â€² ğ‘¥1 = 17.36
ğ‘¥2 = 8.458
ğ‘ 2 = 59.3
ğ‘“â€² ğ‘¥2 = 16.916
ğ‘¥2 = 8.27
ğ‘ 3 = 82.0
61
Back to Adaptive Learning Rate
AI VIETNAM
All-in-One Course
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
For square function
RMSProp
ğ‘¥0 = 9.0
ğœ‚= 0.4
ğœŒ= 0.9
ğœ–= 10âˆ’7
ğ‘ 0 = 0.0
Back to Adaptive Learning Rate
AI VIETNAM
All-in-One Course
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
For another function
RMSProp
ğ‘¥0 = âˆ’3.5
ğœ‚= 0.1
ğœŒ= 0.9
ğœ–= 10âˆ’7
ğ‘ 0 = 0.0
-1.0
1.0
Adaptive Learning Rate
AI VIETNAM
All-in-One Course
64
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡,ğ‘–
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
Generalization
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğ‘¥ğ‘¡= ğ‘¥ğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ‘“â€²(ğ‘¥ğ‘¡âˆ’1)
RMSProp (One variable Function)
RMSProp (Multi-variable Function)
â¢Quick Introdution
â¢Stochastic Gradient Descent
â¢Adagrad
â¢RMSProp
â¢Momentum
â¢Adam
â¢Case studies
Outline
A common limitation so far
ğ‘ ğ‘¡= ğ‘ ğ‘¡âˆ’1 + ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+âˆˆğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
Adaptive 
Learning 
Rate
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
Using Momentum
SGD
ğ‘£ğ‘¡= ğœŒğ‘£ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ›¼ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğ‘£ğ‘¡
Genevieve B. Orr
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
SGD + Momentum
ğ‘£ğ‘¡= ğ‘šğ‘£ğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
ğ‘“ğ‘¥= ğ‘¥2
SGD + Momentum
ğ‘£ğ‘¡= ğ‘šğ‘£ğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
ğ‘“ğ‘¥= ğ‘¥2
ğ‘¥0 = 90.0
ğœ‚= 0.1
ğ‘“â€² ğ‘¥0 = 180.0
ğ‘¥1 = ğ‘¥0 + ğ‘£1 = 72.0
f â€² x1 = 144.0
ğ‘¥2 = ğ‘¥1 + ğ‘£2 = 41.4
f â€² x2 = 82.8
ğ‘¥3 = ğ‘¥2 + ğ‘£3 = 5.58
f â€² x3 = 11.16
ğ‘¥4 = ğ‘¥3 + ğ‘£4 = âˆ’27.77
ğ‘š= 0.9
ğ‘£0 = 0.0
ğ‘£1 = âˆ’18.0
ğ‘£2 = âˆ’30.59
ğ‘£3 = âˆ’35.82
ğ‘£4 = âˆ’33.354
f â€² x4 = âˆ’55.54
ğ‘¥5 = ğ‘¥4 + ğ‘£5 = âˆ’52.23
ğ‘£5 = âˆ’24.46
SGD + 
Momentum
ğ‘£ğ‘¡= ğ‘šğ‘£ğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
ğ‘¥0 = 90.0
ğœ‚= 0.03
ğ‘š= 0.0
ğ‘£0 = 0.0
SGD + 
Momentum
ğ‘£ğ‘¡= ğ‘šğ‘£ğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
ğ‘¥0 = 90.0
ğœ‚= 0.01
ğ‘š= 0.9
ğ‘£0 = 0.0
SGD + Momentum
AI VIETNAM
All-in-One Course
Mimic a ball rolling
SGD + 
Momentum
ğ‘£ğ‘¡= ğ‘šğ‘£ğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
ğ‘¥0 = âˆ’3.2
ğœ‚= 0.1
ğ‘š= 0.9
ğ‘£0 = 0.0
What about 
RSMProp+Momentum
SGD + Momentum
ğ‘£ğ‘¡= ğ‘šğ‘£ğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
SGD
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğ‘ ğ‘¡= ğœŒğ‘ ğ‘¡âˆ’1 + (1 âˆ’ğœŒ)ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘ ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
RMSProp
ğ‘£ğ‘¡= ğœŒğ‘£ğ‘¡âˆ’1 âˆ’(1 âˆ’ğœŒ)ğ›¼ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘£ğ‘¡
?
What about 
RSMProp+Momentum
SGD + Momentum
ğ‘šğ‘¡= ğœŒğ‘šğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘šğ‘¡
ğ‘£ğ‘¡= ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ğ›½2)ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘£ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
RMSProp
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘£ğ‘¡+ ğœ–ğ‘šğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
ğ‘šğ‘¡= ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ğ›½1)ğ‘”ğ‘¡
ğ‘£ğ‘¡= ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ğ›½2)ğ‘”ğ‘¡
2
Simpler version of Adam
ğ‘šğ‘¡= ğœŒğ‘šğ‘¡âˆ’1 âˆ’(1 âˆ’ğœŒ)ğ›¼ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘šğ‘¡
ğ‘šğ‘¡= ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ğ›½1)ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğœ‚ğ‘šğ‘¡
idea equivalence
What about 
RSMProp+Momentum
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘£ğ‘¡
1 âˆ’ğ›½2
ğ‘¡+ ğœ–
Ã—
ğ‘šğ‘¡
1 âˆ’ğ›½1
ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
ğ‘šğ‘¡= ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ğ›½1)ğ‘”ğ‘¡
ğ‘£ğ‘¡= ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ğ›½2)ğ‘”ğ‘¡
2
Adam
ğ‘£ğ‘¡= ğ›½2ğ‘£ğ‘¡âˆ’1 + (1 âˆ’ğ›½2)ğ‘”ğ‘¡
2
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’
ğœ‚
ğ‘£ğ‘¡+ ğœ–ğ‘”ğ‘¡
ğ‘”ğ‘¡= ğ›»ğœƒğ¿
RMSProp
SGD + Momentum
ğ‘šğ‘¡= ğœŒğ‘šğ‘¡âˆ’1 âˆ’ğœ‚ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘šğ‘¡
ğ‘šğ‘¡= ğœŒğ‘šğ‘¡âˆ’1 âˆ’(1 âˆ’ğœŒ)ğ›¼ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 + ğ‘šğ‘¡
ğ‘šğ‘¡= ğ›½1ğ‘šğ‘¡âˆ’1 + (1 âˆ’ğ›½1)ğ›»ğœƒğ¿
ğœƒğ‘¡= ğœƒğ‘¡âˆ’1 âˆ’ğœ‚ğ‘šğ‘¡
idea equivalence
Further reading
https://optimization.cbe.cornell.edu/index.php?title=Adam
â¢Quick Introdution
â¢Stochastic Gradient Descent
â¢Adagrad
â¢RMSProp
â¢Momentum
â¢Adam
â¢Case studies
Outline
T-shirt
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle
Boot
Image Data
Fashion-MNIST dataset
Grayscale images
Resolution=28x28
Training set: 60000 samples
Testing set: 10000 samples
784 Nodes
Input layer
1
ğ‘§1
ğ‘§10
Softmax 
activation
Fully 
connect
10 Nodes
Output layer
28
28
784
flatten data
. . .
. . .
Fully 
connect
128 Nodes
Hidden layer
1
. . .
Fully 
connect
128 Nodes
Hidden layer
1
. . .
Case study 1
AI VIETNAM
All-in-One Course
Fashion-MNIST
SGD
AdaGrad
Adam
AdamW
76
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
Image Data
Cifar-10 dataset
Color images
Resolution=32x32
Training set: 50000 samples
Testing set: 10000 samples
3072 Nodes
Input layer
1
ğ‘§1
ğ‘§10
Softmax 
activation
Fully 
connect
10 Nodes
Output layer
32
32
3072
flatten data
. . .
. . .
Fully 
connect
128 Nodes
Hidden layer
1
. . .
Fully 
connect
128 Nodes
Hidden layer
1
. . .
Case study 2
AI VIETNAM
All-in-One Course
Cifar10-MNIST
SGD
AdamW
Case Studies
AI VIETNAM
All-in-One Course
Results
http://manjeetdahiya.com/posts/exponential-weighted-average/
https://towardsdatascience.com/complete-guide-to-adam-optimization-1e5f29532c3d
Bias Correction
Adam
Further Reading
80
https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-
a-bias-correction-term-for-the-adam-optimizer-for
https://optimization.cbe.cornell.edu/index.php?title=Adam
