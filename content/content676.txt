<<<<<<< HEAD
Node ID: 79b0f394-3e61-4142-9c8a-72d350a6f24b
Text: AI VIETNAM aivietnam.edu.vn 4 # Định nghĩa hàm compute_metrics
để tính các độ đo hiệu suất (metrics) cho việc đánh giá model. 5 def
compute_metrics(eval_pred): 6 predictions, labels = eval_pred 7 8 #
Lấy chỉ số của lớp có xác suất cao nhất trong predictions. 9
predictions = np.argmax(predictions, axis=1) 10 11 # Sử dụng module
"accuracy" để tính...
=======
AI VIETNAM aivietnam.edu.vn
4 # Định nghĩa hàm compute_metrics để tính các độ đo hiệu suất (metrics)
cho việc đánh giá model.
5 def compute_metrics(eval_pred):
6 predictions, labels = eval_pred
7
8 # Lấy chỉ số của lớp có xác suất cao nhất trong predictions.
9 predictions = np.argmax(predictions, axis=1)
10
11 # Sử dụng module "accuracy" để tính độ chính xác dựa trên
predictions và labels.
12 return accuracy.compute(predictions=predictions, references=labels)
(f) Train model:Sau khi đã chuẩn bị xong dataset, ta sẽ tiến hành setup một số tham số trong
quá trình train và tiến hành train model.
• Trước hết, ta sẽ định nghĩa một số hyper-parameter mà ta sẽ sử dụng để train model:
1 # Định nghĩa tên project để log thông tin quá trình train trên wandb
2 # os.environ["WANDB_PROJECT"] = "mamba_tutorial"
3
4 # Định nghĩa các tham số train trong class TrainingArguments.
5 # Cụ thể hơn về các tham số hỗ trợ có thể tham khảo: https://
huggingface.co/docs/transformers/main_classes/trainer
6 training_args = TrainingArguments(
7 output_dir="mamba_text_classification", # Tên folder output
8 learning_rate=5e-5,
9 per_device_train_batch_size=4, # Số lượng train sample trên mỗi
device
10 per_device_eval_batch_size=16, # Số lượng eval sample trên mỗi
device
11 num_train_epochs=1, # Số epoch train
12 warmup_ratio=0.01, # Tỉ lệ tăng dần lr trong giai đoạn warmup
13 lr_scheduler_type="cosine", # Loại scheduler để giảm lr
14 report_to="none", # "wandb" nếu muốn log kết quả
15 evaluation_strategy="steps", # Xác định metric đánh giá sau mỗi
số bước
16 eval_steps=0.1, # Số bước giữa các đợt đánh giá
17 save_strategy="steps", # Xác định khi nào lưu checkpoint
18 save_steps=0.1, # Số bước giữa các lần lưu checkpoint
19 logging_strategy="steps", # Xác định khi nào in thông tin log
20 logging_steps=1, # Số bước giữa các lần in thông tin log
21 push_to_hub=True, # Đẩy kết quả lên Hub
22 load_best_model_at_end=True, # Load model có kết quả evaluation
tốt nhất trong quá trình train
23 )
• Sau đó ta sẽ khởi tạo classMambaTrainer kế thừa từ classTrainer. Đầu tiên, ta sẽ tạo hàm
compute_loss() để định nghĩa hàm loss sử dụng trong quá trình train. 
>>>>>>> HEAD@{1}
