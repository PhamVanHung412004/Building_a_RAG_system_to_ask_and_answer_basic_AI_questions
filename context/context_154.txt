Model Generalization
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
Cifar-10 dataset
(complex dataset)
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
Network Training
Color images
Resolution=32x32
Training set: 50000 samples
Testing set: 10000 samples
Model Generalization
Cifar-10   
 Dataset
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
input
(3,32,32)
(32,16,16)
(64,8,8)
(256,2,2)
output
(128,4,4)
Data Normalization
(convert to 0-mean 
and 1-deviation)
à´¤ğ‘‹= ğ‘‹âˆ’ğœ‡
ğœ
ğœ‡= 1
ğ‘›à·
ğ‘–
ğ‘‹ğ‘–
ğœ=
1
ğ‘›à·
ğ‘–
ğ‘‹ğ‘–âˆ’ğœ‡2
Aim to reduce this gap 
Adam lr=1e-3 ; He Init
2
Training
Testing
1
2
3
4
Robustly fit
Overfit
3
Model Generalization
â– Trick 1: â€˜Learn hard â€™ â€“ randomly add noise to training data
AI VIETNAM
All-in-One Course
4
Model Generalization
â– Trick 1: â€˜Learn hard â€™ â€“ randomly add noise to training data
AI VIETNAM
All-in-One Course
Speed-limit 
sign detection
5
Model Generalization
â– Trick 1: â€˜Learn hard â€™ â€“ randomly add noise to training data
AI VIETNAM
All-in-One Course
In PyTorch
Add noise
6
Model Generalization
â– Trick 1: â€˜Learn hard â€™ â€“ randomly add noise to training data
AI VIETNAM
All-in-One Course
In PyTorch
Add noise
val_accuracy increases 
from ~78% to ~80%
7
Network Training
â– Solution 2: Batch normalization
AI VIETNAM
AI-in-One Course
ğ‘‹= ğ‘‹1, â€¦ , ğ‘‹ğ‘š
ğ‘š is mini-batch size
ğœ‡= 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–
ğœ2 = 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–âˆ’ğœ‡2
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
ğ‘Œğ‘–= ğ›¾à· ğ‘‹ğ‘–+ Î²
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ğ‘‹ğ‘–
Scale and shift à· ğ‘‹ğ‘–
ğœ– is a very small value
ğ›¾ and Î² are two learning parameters
Batch Normalization
Do not need bias when using BN
ğœ‡ and ğœ are updated in forward pass
ğ›¾ and Î² are updated in backward pass
8
Network Training
â– Trick 2: Batch normalization
AI VIETNAM
All-in-One Course
ğ‘‹= ğ‘‹1, â€¦ , ğ‘‹ğ‘š
ğ‘š is mini-batch size
ğœ‡= 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–
ğœ2 = 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–âˆ’ğœ‡2
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
ğ‘Œğ‘–= ğ›¾à· ğ‘‹ğ‘–+ Î²
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ğ‘‹ğ‘–
Scale and shift à· ğ‘‹ğ‘–
ğœ– is a very small value
ğ›¾ and Î² are two learning parameters
Batch Normalization
ğ›¾=
ğœ2 + ğœ– and Î² = ğœ‡
What if
8
Network Training
Trick 2: Batch normalization
ğ‘‹= ğ‘‹1, â€¦ , ğ‘‹ğ‘š
ğ‘š is mini-batch size
ğœ‡= 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–
ğœ2 = 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–âˆ’ğœ‡2
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
ğ‘Œğ‘–= ğ›¾à· ğ‘‹ğ‘–+ Î²
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ğ‘‹ğ‘–
Scale and shift à· ğ‘‹ğ‘–
ğœ– is a very small value
ğ›¾ and Î² are two learning parameters
ğ‘‹ğ‘ˆ=
1
3
9
4
6
7
ğ‘‹ğ‘‰=
6
4
7
5
6
2
ğœ‡ğ‘‰= 5.0
ğœğ‘‰
2 = 1.63
ğœ–= 10âˆ’5
ğ›¾ğ‘‰= 1.0
Î²ğ‘‰= 0.0
ğœ‡ğ‘ˆ= 5.0
ğœğ‘ˆ
2 = 2.64
ğ›¾ğ‘ˆ= 1.0
Î²ğ‘ˆ= 0.0
à·¡ğ‘ˆ=
âˆ’1.51
âˆ’0.75
1.51
âˆ’0.37
0.37
0.75
à· ğ‘‰=
0.61
âˆ’0.61
1.22
0.0
0.61
âˆ’1.83
ğ‘Œğ‘ˆ=
âˆ’1.51
âˆ’0.75
1.51
âˆ’0.37
0.37
0.75
ğ‘Œğ‘‰=
0.61
âˆ’0.61
1.22
0.0
0.61
âˆ’1.83
ğ›¾ and Î² are updated in training process
10
Network Training
W
H
C
â€¦
B
â– Trick 2: Batch normalization for 2D data
Compute C means of H*W*B values
Compute C variances of H*W*B values
AI VIETNAM
All-in-One Course
11
Network Training
ğœ–= 10âˆ’5
AI VIETNAM
All-in-One Course
ğœ‡= [2.0, 3.0]
ğœ2 = [4.0, 3.7]
ğ‘‹=  
, 
à· ğ‘Œ= â€¦
batch-size = 2
sample_shape = (2, 2, 2)
sample 1 sample 2
6 4
5 2
0 5
3 0
2 3
0 2
1 4
3 0
à· ğ‘‹=  
, 
sample 1 sample 2
1.6 
0.5
1.1 
âˆ’0.5
1.6 
âˆ’1.1
âˆ’1.1 
0.5
âˆ’0.5 1.1
0.5 
âˆ’1.1
âˆ’0.5 
0.0
âˆ’1.6 
âˆ’0.5
ğ›¾= 1.0
Î² = 0.0
Batch-Norm Layer
12
Network Training
â– Trick 2: Batch normalization
AI VIETNAM
All-in-One Course
ğ‘‹= ğ‘‹1, â€¦ , ğ‘‹ğ‘š
ğ‘š is mini-batch size
ğœ‡= 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–
ğœ2 = 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–âˆ’ğœ‡2
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
ğ‘Œğ‘–= ğ›¾à· ğ‘‹ğ‘–+ Î²
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ğ‘‹ğ‘–
Scale and shift à· ğ‘‹ğ‘–
ğœ– is a very small value
ğ›¾ and Î² are two learning parameters
Backward
ğ‘‹ğ‘–
ğœ‡
ğœ2
à· ğ‘‹ğ‘–
ğ‘Œğ‘–
ğ›½
ğ›¾
ğœ•ğ¿
ğœ•ğ‘Œğ‘–
13
ğ‘‹ğ‘–
ğœ‡
ğœ2
à· ğ‘‹ğ‘–
ğ‘Œğ‘–
ğ›½
ğ›¾
ğœ‡= 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–
ğœ2 = 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–âˆ’ğœ‡2
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
ğ‘Œğ‘–= ğ›¾à· ğ‘‹ğ‘–+ Î²
ğœ•ğ¿
ğœ•ğ›¾= à·
ğ‘–=1
ğ‘šğœ•ğ¿
ğœ•ğ‘Œğ‘–
à· ğ‘‹ğ‘–
ğœ•ğ¿
ğœ•Î² = à·
ğ‘–=1
ğ‘šğœ•ğ¿
ğœ•ğ‘Œğ‘–
ğœ•ğ¿
ğœ•à· ğ‘‹ğ‘–
= ğœ•ğ¿
ğœ•ğ‘Œğ‘–
ğ›¾
ğœ•ğ¿
ğœ•ğœ2 = à·
ğ‘–=1
ğ‘šğœ•ğ¿
ğœ•à· ğ‘‹ğ‘–
ğœ•à· ğ‘‹ğ‘–
ğœ•ğœ2 = à·
ğ‘–=1
ğ‘šğœ•ğ¿
ğœ•à· ğ‘‹ğ‘–
ğ‘‹ğ‘–âˆ’ğœ‡âˆ’1
2
ğœ2 + ğœ–
âˆ’3
2
ğœ•ğ¿
ğœ•ğœ‡= à·
ğ‘–=1
ğ‘šğœ•ğ¿
ğœ•à· ğ‘‹ğ‘–
âˆ’1
ğœ2 + ğœ–
âˆ’ğœ•ğ¿
ğœ•ğœ2
1
ğ‘šà·
ğ‘–=1
ğ‘š
2 ğ‘‹ğ‘–âˆ’ğœ‡
ğœ•ğ¿
ğœ•ğ‘‹ğ‘–
= ğœ•ğ¿
ğœ•à· ğ‘‹ğ‘–
ğœ•à· ğ‘‹ğ‘–
ğœ•ğ‘‹ğ‘–
+ ğœ•ğ¿
ğœ•ğœ‡
ğœ•ğœ‡
ğœ•ğ‘‹ğ‘–
+ ğœ•ğ¿
ğœ•ğœ2
ğœ•ğœ2
ğœ•ğ‘‹ğ‘–
ğœ•à· ğ‘‹ğ‘–
ğœ•ğ‘‹ğ‘–
=
1
ğœ2 + ğœ–
ğœ•ğœ‡
ğœ•ğ‘‹ğ‘–
= 1
ğ‘š
ğœ•ğœ2
ğœ•ğ‘‹ğ‘–
= 2 ğ‘‹ğ‘–âˆ’ğœ‡
ğ‘š
ğœ•ğ¿
ğœ•ğ‘Œğ‘–
14
Model Generalization
â– Trick 2: Batch normalization
AI VIETNAM
All-in-One Course
ğ‘‹= ğ‘‹1, â€¦ , ğ‘‹ğ‘š
ğ‘š is mini-batch size
ğœ‡= 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–
ğœ2 = 1
ğ‘šà·
ğ‘–=1
ğ‘š
ğ‘‹ğ‘–âˆ’ğœ‡2
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
ğ‘Œğ‘–= ğ›¾à· ğ‘‹ğ‘–+ Î²
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ğ‘‹ğ‘–
Scale and shift à· ğ‘‹ğ‘–
ğœ– is a very small value
ğ›¾ and Î² are two learning parameters
mini-batch 1
mini-batch 2
ğœ‡1, ğœ1  â‰ 
ğœ‡2, ğœ2
very
likely
Add noise to the output of BN layers
15
Model Generalization
â– Trick 2: Batch normalization
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
Batch 
normalization
input
(3,32,32)
(32,16,16)
(64,8,8)
(128,4,4)
(256,2,2)
output
val_accuracy increases from ~80.9% to ~84%
16
Model Generalization
â– Trick 3: Dropout
AI VIETNAM
All-in-One Course
layer ğ‘–
layer ğ‘—
Apply dropout 50% to layer ğ‘–
~50% nodes randomly selected in the ğ‘–ğ‘¡â„ layer are 
set to zeros (kind of noise adding)
17
Model Generalization
â– Trick 3: Dropout
AI VIETNAM
All-in-One Course
Apply dropout 50% to layer ğ‘–
~50% nodes randomly selected in 
the ğ‘–ğ‘¡â„ layer are set to zeros
ğ‘= ğ·â¨€ğœğ‘
ğœ•ğ¿
ğœ•ğœ= ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ‘
ğœ•ğœ= ğœ•ğ¿
ğœ•ğ‘Ã— ğ·
18
Overfitting
AI VIETNAM
All-in-One Course
Dropout
Given a dropping rate r
Randomly sets input units to 0 with 
a frequency of r
Only applying in training mode
ğ‘ ğ‘ğ‘ğ‘™ğ‘’=
1
1 âˆ’ğ‘Ÿ
19
Model Generalization
â– Trick 3: Dropout
AI VIETNAM
All-in-One Course
Apply dropout 50% to layer ğ‘–
~50% nodes randomly selected in 
the ğ‘–ğ‘¡â„ layer are set to zeros
https://deepnotes.io/dropout
20
Model Generalization
â– Trick 3: Dropout
AI VIETNAM
All-in-One Course
Dropout
val_accuracy 
increases from 
~84% to ~86.6%
input
(3,32,32)
(32,16,16)
(64,8,8)
(128,4,4)
(256,2,2)
output
dropout 30%
dropout 30%
dropout 30%
dropout 30%
Model Generalization
â– Trick 4: Kernel regularization
AI VIETNAM
All-in-One Course
ğ¿= ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ + ğœ†ğ‘Š2
ğ¿2regularization
Prevent network from 
focusing on specific features
In PyTorch
Smaller weights 
â†’ simpler models
22
Model Generalization
â– Trick 4: Kernel regularizer
AI VIETNAM
All-in-One Course
val_accuracy 
increases from 
~86.6% to ~87.5%
Dropout
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU +
kernel regularization
input
(3,32,32)
(32,16,16)
(64,8,8)
(128,4,4)
(256,2,2)
output
dropout 30%
dropout 30%
dropout 30%
dropout 30%
23
Model Generalization
â– Trick 5: Data augmentation
AI VIETNAM
All-in-One Course
Data distribution
Training data
Testing data
Image
Image
A perfect case: Have unlimited training
A normal case
Training data cover the whole distribution
But, impractical!!!
Model Generalization
â– Trick 5: Data augmentation
AI VIETNAM
All-in-One Course
horizontal 
flip
rotate
crop and 
resize
Data distribution
Training data
Testing data
Image
Augmented training data
Increase data by altering the training data
25
Model Generalization
â– Trick 5: Data augmentation
AI VIETNAM
All-in-One Course
Horizontal flip + crop-and-resize
val_accuracy reaches to ~90.6%
26
Model Generalization
â– What we have
AI VIETNAM
All-in-One Course
Horizontal flip + crop-and-resize
val_accuracy reaches to ~90.6%
Batch normalization
Dropout
Kernel regularization
Data augmentation
train_accuracy reaches to ~92%
Idea: try to increase train_accuracy, 
expect val_accuracy increases too
Increase model capacity
27
Optimization
â– Learning rate
28
Optimization
â– Learning rate
29
Model Generalization
â– Trick 6: Reduce learning rate
AI VIETNAM
All-in-One Course
ğœ‚= ğœ‚0 Ã— ğ›¾ğ‘’ğ‘ğ‘œğ‘â„
30
Model Generalization
â– Trick 6: Reduce learning rate
AI VIETNAM
All-in-One Course
val_accuracy reaches to ~90.6%
train_accuracy reaches to ~98%
31
Model Generalization
â– Discussion: Predict training and test accuracy when using more data augmentation 
AI VIETNAM
All-in-One Course
horizontal 
flip
rotate
crop and 
resize
Add noise
32
Model Generalization
â– Trick 7: Increase model capacity (and use more data augmentation)
AI VIETNAM
All-in-One Course
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
(512,2,2)
output
val_accuracy reaches to ~93%
train_accuracy reaches to ~96%
33
Model Generalization
â– Trick 8: Using skip-connection
AI VIETNAM
All-in-One Course
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
(512,2,2)
output
+
+
+
+
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
(3x3) Convolution
padding=â€˜sameâ€™
stride=2 + ReLU
34
Model Generalization
â– Trick 8: Using skip-connection
AI VIETNAM
All-in-One Course
val_accuracy reaches to ~93%
train_accuracy reaches to ~97%
35
Model Generalization
â– Increase model capacity once more 
AI VIETNAM
All-in-One Course
input
(3,32,32)
(128,16,16)
(256,8,8)
(512,4,4)
(1024,2,2)
output
+
+
+
+
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
(3x3) Convolution
padding=â€˜sameâ€™
stride=2 + ReLU
36
Model Generalization
â– Increase model capacity once more 
AI VIETNAM
All-in-One Course
val_accuracy reaches to ~94.5%
train_accuracy reaches to ~98.3%
37
Summary
â– How to increase validation accuracy
AI VIETNAM
All-in-One Course
Add noise
Trick 1: â€˜Learn hard â€™ â€“ randomly 
add noise to training data
à· ğ‘‹ğ‘–= ğ‘‹ğ‘–âˆ’ğœ‡
ğœ2 + ğœ–
Trick 2: 
Using Batch Normalization
Trick 3: Using Dropout
ğ¿= ğ¶ğ¸ + ğœ†ğ‘Š2
ğ¿2regularization
Trick 4: Kernel regularization
Trick 5: Data augmentation
(*) Use Pretrained Models
