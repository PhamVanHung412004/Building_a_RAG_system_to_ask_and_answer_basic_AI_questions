Machine Learning
K-NEAREST NEIGHBORS
DECISION TREE
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ K-Nearest Neighbors (KNN)
(2) â€“ KNN Applications
(3) â€“ Decision Tree
(4) â€“ Summary
1 â€“ K Nearest Neighbors
What is the KNN Algorithm? 
!
3
Ã˜ KNN is one of the simplest supervised machine learning algorithms
Ã˜ Lazy Learning:
q Does not â€œLEARNâ€ until the test example is given
q A new data is predicted based on K-Nearest Neighbors from the training data
1 â€“ K Nearest Neighbors
What is the KNN Algorithm? 
!
4
Memory-based Learning
Training Phrase
Test Data
Search
K-Nearest Neighbors
Testing Phrase
Training Data
Similarity Measure
1 â€“ K Nearest Neighbors
What is the KNN Algorithm? 
!
5
Memory-based Learning
Training Phrase
Test Data
Search
K-Nearest Neighbors
Testing Phrase
Training Data
Predict
Similarity Measure
1 â€“ K Nearest Neighbors
What is the KNN Algorithm? 
!
6
Memory-based Learning
Training Phrase
Test Data
Search
K-Nearest Neighbors
Testing Phrase
Training Data
Predict
Non-Parametric
Similarity Measure
1 â€“ K Nearest Neighbors
What is the KNN Algorithm? 
!
7
Memory-based Learning
Training Phrase
Test Data
Search
K-Nearest Neighbors
Testing Phrase
Training Data
Predict
Solving
Regression
Classfication
1 â€“ K Nearest Neighbors
What is the KNN Algorithm? 
!
Regression
Classification
What will be the 
temperature tomorrow?
8
Will it be hot or 
cold tomorrow?
Ã˜ Predict a continuous value based
on the input variables
Ã˜ Classify input variables to identify
discrete output variables (labels,
categories)
1 â€“ K Nearest Neighbors
KNN: Classification Approach
!
Classification
9
Will it be hot or 
cold tomorrow?
Ã˜ Classify input variables to identify
discrete output variables (labels,
categories)
Test Data
Training Data
Feature
Label
â€¦
â€¦
?
1 â€“ K Nearest Neighbors
KNN: Classification Approach
!
Step 1: Look at the data
10
Petal_Length
Petal_Width
Label
1.4
0.2
0
1.3
0.4
0
4
1
1
4.7
1.4
1
2.4
0.8
Test Data
Training Data
New data
1 â€“ K Nearest Neighbors
KNN: Classification Approach
!
Step 2: Calculate distances
11
Petal_Length
Petal_Width
Label
Distance
1.4
0.2
0
1.166
1.3
0.4
0
1.170
4
1
1
1.612
4.7
1.4
1
2.376
2.4
0.8
Test Data
Training Data
ğ‘‘(x, y) =
(
!"#
$
x! âˆ’y! %
Euclidean Distance
1.166
1.170
2.376
1.612
1 â€“ K Nearest Neighbors
KNN: Classification Approach
!
Step 3: Find neighbours
12
Petal_Length
Petal_Width
Label
Distance
1.4
0.2
0
1.166
1.3
0.4
0
1.170
4
1
1
1.612
4.7
1.4
1
2.376
2.4
0.8
Test Data
Training Data
1 st
2 nd
3 rd
4 th
Find the nearest neighbours by ranking 
points by increasing distance
Ranking points
1 â€“ K Nearest Neighbors
KNN: Classification Approach
!
Step 4: Vote on labels
13
Petal_Length
Petal_Width
Label
Distance
1.4
0.2
0
1.166
1.3
0.4
0
1.170
4
1
1
1.612
4.7
1.4
1
2.376
2.4
0.8
Test Data
Training Data
1 st
2 nd
3 rd
Vote on the predicted class labels based 
on the class of the k nearest neighbors
K=3 Nearest neighbours
2
1
# of votes
0
1 â€“ K Nearest Neighbors
KNN: Regression Approach
!
Step 1: Look at the data
14
Length
Width
Price
2.0
2.0
2.0
3.0
3.0
2.5
4.0
3.0
3.5
5.0
2.0
5.0
3.0
2.0
Test Data
Training Data
New data
1 â€“ K Nearest Neighbors
KNN: Regression Approach
!
Step 2: Calculate distances
15
Length
Width
Price
Distance
2.0
2.0
2.0
1.0
3.0
3.0
2.5
1.0
4.0
3.0
3.5
1.4
5.0
2.0
5.0
2.0
3.0
2.0
Test Data
Training Data
ğ‘‘(x, y) =
(
!"#
$
x! âˆ’y! %
Euclidean Distance
1.0
2.0
1.0
1.4
1 â€“ K Nearest Neighbors
KNN: Regression Approach
!
Step 3: Find neighbours
16
Length
Width
Price
Distance
2.0
2.0
2.0
1.0
3.0
3.0
2.5
1.0
4.0
3.0
3.5
1.4
5.0
2.0
5.0
2.0
3.0
2.0
Test Data
Training Data
1 st
2 nd
3 rd
4 th
Find the nearest neighbours by ranking 
points by increasing distance
Ranking points
1 â€“ K Nearest Neighbors
KNN: Regression Approach
!
Step 4: Vote on labels
17
Length
Width
Price
Distance
2.0
2.0
2.0
1.0
3.0
3.0
2.5
1.0
4.0
3.0
3.5
1.4
5.0
2.0
5.0
2.0
3.0
2.0
Test Data
Training Data
1 st
2 nd
3 rd
Compute the mean value of the 
k nearest neighbors
K=4 Nearest neighbours
3.25
4 th
Y&'() = 1
k (
*âˆˆ,-
y*
Y&'()
= 1
4 2.0 + 2.5 + 3.5 + 5.0
=3.25
1 â€“ K Nearest Neighbors
KNN: Summary
!
18
Classification
Regression
1 â€“ K Nearest Neighbors
Geometry Distance Functions
!
19
ğ‘‘(x, y) =
(
!"#
$
x! âˆ’y! %
Ã˜ Euclidean (p=2)
ğ‘‘(x, y) =
(
."#
/
ğ‘¥. âˆ’ğ‘¦. 0
#
0
Ã˜ Minkowski (p-norm)
ğ‘‘(x, y) = (
."#
/
ğ‘¥. âˆ’ğ‘¦.
Ã˜ Manhattan (p=1)
ğ‘‘x, y = lim
0â†’2 (
."#
/
ğ‘¥. âˆ’ğ‘¦. 0
#
0
= max
.
ğ‘¥. âˆ’ğ‘¦.
Ã˜ Chebyshev (p=)
1 â€“ K Nearest Neighbors
Feature Scaling (Normalization)
!
20
Ã˜ Standardize the range of independent variables (feature of data)
Petal_Length
L_Distance
Petal_Width
W_Distance
Label
Distance
1.4
1.8
0.2
0.4
0
1.844
1.3
1.9
0.4
0.2
0
1.910
4
0.8
1
0.4
1
0.894
4.7
1.3
1.4
0.8
1
1.526
3.2
Test Data
Training Data
0.6
Strong Influence
Rank
3
4
1
2
1 â€“ K Nearest Neighbors
Feature Scaling (Normalization)
!
21
Ã˜ Standardize the range of independent variables (feature of data)
Petal_Length
L_Distance
Petal_Width
W_Distance
Label
Distance
0.03
0.53
0.00
0.33
0
0.624
0.00
0.56
0.17
0.16
0
0.582
0.79
0.23
0.66
0.33
1
0.402
1.00
0.44
1.00
0.67
1
0.801
0.56
Test Data
Training Data
0.33
MinMaxScaler Normalization
Rank
3
2
1
4
1 â€“ K Nearest Neighbors
Searching in KNN
!
22
KD Tree
O[NDlog(N)]
Ball Tree
1
Ã˜ Training dataset: N samples in D dimensions
Ã˜ Brute Force: NaÃ¯ve neighbor search â€“ O[DN2]
Source: https://varshasaini.in/kd-tree-and-ball-tree-knn-algorithm/
2
1 â€“ K Nearest Neighbors
How to find the optimal value of K in KNN?
!
23
Ã˜ Choose K based on the evaluation on the validation set (Accuracy, Error, F-Score,â€¦)
K=3
K=5
Validation Error
Training Error
2
Error
K Value
2 â€“ KNN Applications
KNN for Regression: â€œDiabetesâ€ Dataset
!
24
Ã˜ Sample: 442
Ã˜ Features: 10
Ã˜ Target: 25 â€“ 346
Ã˜ R2-Score (Validation)
2 â€“ KNN Applications
KNN for Classification: â€œIrisâ€ Dataset
!
25
Ã˜ Sample: 150
Ã˜ Features: 4
Ã˜ Classes: 3 (50 per class)
Ã˜ Accuracy (Validation)
2 â€“ KNN Applications
KNN for Text Classification: â€œIMDBâ€ Dataset
!
26
Ã˜ Sample: 50.000 movie review
Ã˜ Classes: 2 â€“ Positive and Negative (25.000 per class)
Ã˜ Accuracy, F1-Score
https://paperswithcode.com/sota/text-classification-on-imdb
2 â€“ KNN Applications
KNN for Text Classification: â€œIMDBâ€ Dataset
!
27
Feature
Text
I like this movie
So bad
Boring
Love it so much
1
0
0
1
Label
Convert text 
to feature
KNN 
Classifier
BoW
2 â€“ KNN Applications
KNN for Text Classification: â€œIMDBâ€ Dataset
!
28
Ã˜ Bag of Words (BoW)
Ã˜ Document-Level: Consider text as a bag (collection) of words
Ã˜ Represented by a V-dimensional
Use: the number of occurrences of the word in the document
[dog, bites, man]
[man, bites, dog]
[dog, eats, meat]
[man, eats, food]
[dog, bites, man]
Vocabulary
IDX
0
1
2
3
4
5
Token
bites
dog
eats
food
man
meat
1
1
0
0
1
0
Counter
2 â€“ KNN Applications
KNN for Text Classification: â€œIMDBâ€ Dataset
!
29
Ã˜ Bag of Words (BoW)
Ã˜ Document-Level: Consider text as a bag (collection) of words
Ã˜ Represented by a V-dimensional
[man, bites, dog]
1
1
0
0
1
0
[dog, eats, meat]
0
1
1
0
0
1
[man, eats, food]
0
0
0
1
1
0
[dog, bites, man]
1
1
0
0
1
0
2 â€“ KNN Applications
KNN for Text Classification: â€œIMDBâ€ Dataset
!
30
Ã˜ Bag of Words (BoW)
Ã˜ Out of vocabulary (OOV)
[dog, bites, man]
Vocabulary
IDX
0
1
2
3
4
5
Token
bites
dog
eats
food
man
meat
1
1
0
0
1
0
Counter
[dog, and, dog, are, friends]
0
2
0
0
0
0
2 â€“ KNN Applications
KNN for Text Classification: â€œIMDBâ€ Dataset
!
31
Ã˜ Bag of Words (BoW)
Ã˜ Representation without considering frequency (Binary Representation)
Vocabulary
IDX
0
1
2
3
4
5
Token
bites
dog
eats
food
man
meat
Convert to binary representation 
[dog, and, dog, are, friends]
0
2
0
0
0
0
[dog, and, dog, are, friends]
0
1
0
0
0
0
Counter
Value 12
3 â€“ Decision Tree
Decision Tree
!
32
Ã˜ A possible decision tree for the data
Ã˜ Described by IF-ELSE sets
Feature 1
Value 13
Value 11
Feature 2
Feature 3
Yes
Value 21
Value 22
Value 31
Value 32
Yes
No
No
Yes
3 â€“ Decision Tree
Decision Tree
!
33
Ã˜ Each internal node: attribute X (feature)
Ã˜ Each branch from a node: value of X
Ã˜ Each leaf node: predict value
Value 12
Feature 1
Value 13
Value 11
Feature 2
Feature 3
Yes
Value 21
Value 22
Value 31
Value 32
Yes
No
No
Yes
3 â€“ Decision Tree
Decision Tree
!
34
Ã˜ Example: Play Tennis dataset
3 â€“ Decision Tree
Decision Tree
!
35
Ã˜ Example: Play Tennis dataset
Overcast
Outlook
Rain
Sunny
Humidity
Wind
Yes
High
Normal
Strong
Weak
Yes
No
No
Yes
What prediction would we make for
<outlook=Sunny, temperature=Hot, humidity=High, wind=Weak> ?
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
36
Value 12
Feature 1
Value 13
Value 11
Feature 2
Feature 3
Yes
Value 21
Value 22
Value 31
Value 32
Yes
No
No
Yes
Why does it make 
more sense to test the 
feature 1 first?
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
37
Value 12
Feature 1
Value 13
Value 11
Feature 2
Feature 3
Yes
Value 21
Value 22
Value 31
Value 32
Yes
No
No
Yes
Top-Down
Information Gain 
Entropy
Ã˜ Iterative Dichotomiser 3
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
38
Ã˜ Entropy is a measure of randomness/uncertainty of a set
Ã˜ Data: a set S of examples with C many classes
Ã˜ Probability vector a = [p1, p2,â€¦, pc] is the class distribution of the set S
Ã˜ Entropy of the set S:
ğ¸S = âˆ’%
!âˆˆ#
p! log$ p!
Ã˜ If a set S of examples has
Some dominant classes => small entropy of the class distribution
Equiprobable classes => high entropy of the class distribution
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
39
Ã˜ Entropy of the set S:
ğ¸S = âˆ’%
!âˆˆ#
p! log$ p!
Ã˜ Example:
S: 14 examples (9: class c1, 5: class c2)
=> E(S) = -(9/14).log2(9/14) â€“(5/14).log2(5/14) = 0.94
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
40
Ã˜ Entropy = 0
 
=> all samples class 1 (or 2)
Ã˜ Entropy = 1
=> num samples class 1 = class 2
Ã˜ Entropy âˆˆ(0,1)
=> num samples class 1 â‰ class 2
0
1
1
E(S)
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
41
Ã˜ Information Gain (IG) on knowing the value of the feature F in S:
Gain S, F = E(S) âˆ’%
%âˆˆ&
S%
S E(S%)
Ã˜ Sf donates the subset of elements of S for which feature F has value f
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
42
Ã˜ Information Gain (IG)
Ã˜ Gain(S, Wind) with Wind: Weak or Strong
S = {9: Yes, 5: No}
Sweak = {6: Yes, 2: No}
Wstrong = {3: Yes, 3: No}
=> Gain S, Wind
= E S âˆ’8
14 E S'()* âˆ’6
14 E S+,-./0
= 0.94 âˆ’1
23 âˆ—0.81 âˆ’4
23 âˆ—1 = 0.048
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
43
Ã˜ Information Gain (IG)
Gain(S, Outlook) = 0.246
Gain(S, Temperature) = 0.029
Gain(S, Humidity) = 0.151
Gain(S, Wind) = 0.048
Overcast
Outlook
Rain
Sunny
Node 1
Mode 2
Yes
Choose Outlook with 
the highest Gain score
S = {9+, 5-}
Ssunny = {2+, 3-}
Srain = {3+, 2-}
Sovercast = {2+, 0-}
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
44
Ã˜ Information Gain (IG)
Fine â€œNode 1â€: {Temperature, Humidity, Wind?
Gain(Ssunny, Temperature) = 0.57
Gain(Ssunny, Humidity) = 0.57
Gain(Ssunny, Wind) = 0.57
Overcast
Outlook
Rain
Sunny
Node 1
Mode 2
Yes
S = {9+, 5-}
Ssunny = {2+, 3-}
Srain = {3+, 2-}
Sovercast = {2+, 0-}
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
45
Ã˜ Information Gain (IG)
Fine â€œNode 1â€: {Temperature, Humidity, Wind?
Gain(Ssunny, Temperature) = 0.57
Gain(Ssunny, Humidity) = 0.57
Gain(Ssunny, Wind) = 0.57
Overcast
Outlook
Rain
Sunny
Humidity
Mode 2
Yes
S = {9+, 5-}
Ssunny = {2+, 3-}
Srain = {3+, 2-}
Humidity
High
Normal
Yes
No
{0+, 3-}
{2+, 0-}
Sovercast = {2+, 0-}
3 â€“ Decision Tree
Constructing Decision Tree: ID3
!
46
Ã˜ Information Gain (IG)
Overcast
Outlook
Rain
Sunny
Humidity
Wind
Yes
High
Normal
Strong
Weak
Yes
No
No
Yes
Test = <outlook=Sunny, temperature=Hot, humidity=High, wind=Weak> => No
3 â€“ Decision Tree
Overfitting in Decision Trees
!
47
Ã˜ Desired: a DT that is not too big in size, yet fits the training data reasonably
Source
3 â€“ Decision Tree
Overfitting in Decision Trees
!
48
Ã˜ Mainly two approaches
q Prune while building the tree (Stopping Early)
q
Prune after building the tree (Post-Pruning)
Ã˜ Criteria for judging which nodes could potentially be pruned: evaluate validation set
3 â€“ Decision Tree
Decision Tree for Iris Dataset
!
49
50
SUMMARY
KNN
Decision Tree
Ã˜ Predicted based on K-Nearest Neighbors
from the training data through Geometry 
Distance Functions
Ã˜ Build a decision tree (ID3)
Ã˜ Defined by a hierarchy of rules (in form of
a tree)
Thanks!
Any questions?
51
