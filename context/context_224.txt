NLP Course
Parameter-Efficient Fine-Tuning
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
CONTENT
1
Background
2
Adapter Tuning
3
Prefix Tuning
4
Prompt Tuning
5
Low-rank Adaptation
2
3
1 â€“ Background
!
Fine-Tuning
Ã˜ Pretrain a language model on task
4
1 â€“ Background
!
Fine-Tuning
Ã˜ Pretrain a language model on task
Ã˜ Attach a small task specific layer
5
1 â€“ Background
!
Fine-Tuning
Ã˜ Pretrain a language model on task
Ã˜ Attach a small task specific layer
Ã˜ Fine-tune the weights of full NN by 
propagating gradients on a downstream 
task
6
1 â€“ Background
!
In-context Learning
Ã˜ Pretrain a language model based on â€œpromptâ€ â€“ demonstrates NLP tasks
Ã˜ No need to update the model weights at all
7
1 â€“ Background
!
Model sizes are still growing?
Source
8
1 â€“ Background
!
Model sizes are still growing?
Ã˜ Model size scales almost two orders of magnitude quicker than single-GPU memory
9
1 â€“ Background
!
Parameter-Efficient Fine-Tuning
Ã˜ Standard fine-tuning: make a new copy of the model for each task
Ã˜ Parameter-Efficiency: fine tuned a subset of the parameters for each task
10
1 â€“ Background
!
Parameter-Efficient Fine-Tuning
Source
Ã˜ Add a layer to adapt for downstream tasks
11
2 â€“ Adapter Fine Tuning
!
Adapter Layers
LMs
LMs
LMs
Task 1
Task 2
Task N
Ã˜ Add a layer to adapt for downstream tasks
12
2 â€“ Adapter Fine Tuning
!
Adapter Layers
+ Adapter 1
+ Adapter 2
+ Adapter N
Task 1
Task 2
Task N
LMs
Ã˜ Add adapter layers in between the 
transformer layers of a large model
13
2 â€“ Adapter Fine Tuning
!
Adapter Layers
Source
Ã˜ Add adapter layers in between the 
transformer layers of a large model
14
2 â€“ Adapter Fine Tuning
!
Adapter Layers
Source
Ã˜ Add adapter layers in between the 
transformer layers of a large model
Ã˜ During fine-tuning, fix the original 
model parameters and only tune the 
adapter layers
15
2 â€“ Adapter Fine Tuning
!
Adapter Layers
Source
Ã˜ Add adapter layers in between the 
transformer layers of a large model
Ã˜ During fine-tuning, fix the original 
model parameters and only tune the 
adapter layers
Ã˜ 3.6 % of parameters needed
16
2 â€“ Adapter Fine Tuning
!
Adapter Layers
Source
17
2 â€“ Adapter Fine Tuning
!
Results on GLEU Benchmark
Source
Ã˜ For prompt design, the discrete prompts is optimized manually
Ã˜ Optimization in discrete space is hard!
18
3 â€“ Prefix Tuning
!
Prompt Design
Ã˜ Optimization in the continuous 
embedding space
Ã˜ Learn an optimal prefix for each task
19
3 â€“ Prefix Tuning
!
Prefix-Tuning: Optimizing Continuous Prompts for Generation
Source
Ã˜ Optimization in the continuous 
embedding space
Ã˜ Learn an optimal prefix for each task
Ã˜ Only 0.1% of parameters need to be 
tuned
20
3 â€“ Prefix Tuning
!
250K 
parameters
Prefix-Tuning: Optimizing Continuous Prompts for Generation
Source
Ã˜ Prefix-Tuning using an autoregressive LM
21
3 â€“ Prefix Tuning
!
Prefix-Tuning: Optimizing Contiunous Prompts for Generation
Source
Ã˜ Prefix-Tuning using an encoder-decoder model
22
3 â€“ Prefix Tuning
!
Prefix-Tuning: Optimizing Continuous Prompts for Generation
Source
Ã˜ As the tunable prefix-length increases, performance increases, with diminishing returns
Ã˜ Optimal length for table to text is 10 tokens, for summarization it seems closer to 200 
tokens
23
3 â€“ Prefix Tuning
!
Prefix Length
Source
Ã˜ Instead of tuning the prefix, tune a portion at the end of the input and before the output
Ã˜ Infix tuning is worse than prefix tuning, since input embeddings cannot attend to infix
24
3 â€“ Prefix Tuning
!
Prefix Tuning and Infix Tuning
Source
Ã˜ Prefix-tuning learn a sequence of prefixes 
(are prepended at every transformer layer)
Ã˜ Prompt-tuning uses a single prompt 
representation is prepended to the 
embedded input
25
4 - Prompt Tuning
From prefix-tuning to prompt-tuning
!
Ã˜ Prepend virtual tokens to input
Ã˜ Pre-trained
ğ‘ƒ!! ğ‘Œğ‘‹
Ã˜ Fine-tuned
ğ‘ƒ!!;!#(ğ‘Œ|[ğ‘ƒ; ğ‘‹])
26
4 - Prompt Tuning
Prompt Tuning
!
fixed
learnable
27
4 - Prompt Tuning
Design Decision
!
Source
Ã˜ Prompt initialization method
Ã˜ Prompt length
Ã˜ Pre-training method
Ã˜ LM adaptation steps
28
4 - Prompt Tuning
Design Decision: Prompt Initilization
!
Source
Ã˜ Random initialization
Ã˜ Sampled vocabulary: initialize each 
prompt token to an embedding drawn 
from the modelâ€™s vocabulary
Ã˜ Class label: initialize the prompt with 
embeddings that enumerate the output 
classes
29
4 - Prompt Tuning
Design Decision: Prompt Length
!
Source
Ã˜ The shorter the prompt, the fewer 
parameters must be tuned
30
4 - Prompt Tuning
Design Decision: Pre-training Method
!
Source
Ã˜ Span Corruption: reconstructing 
masked span in the input text
Ã˜ Span Corruption + Sentinel: prepend all 
downstream targets with a sentinel
Ã˜ â€ LM Adaptationâ€: as T5 objective 
function
31
4 - Prompt Tuning
Design Decision: Pre-training Method
!
Source
Ã˜ Longer adaptation provides additional 
gains, up to 100K steps
Ã˜ At the largest model size, the gains 
from adaptation are quite modest
32
4 - Prompt Tuning
Comparing: model-tuning, prompt-tuning, prompt-design
!
Source
33
4 - Prompt Tuning
Comparing: model-tuning, prompt-tuning, prompt-design
!
Source
34
5 â€“ Low-Rank Adaptation
Adapter Tuning and Prefix Tuning
!
Ã˜ Adapter Tuning: High-quality, but adds 
latency
Ã˜ No latency, but suboptimal quality
35
5 â€“ Low-Rank Adaptation
Low-Rank Adaptation (LoRA)
!
Pretrained 
Weight
ğ‘Šâˆˆğ‘…"Ã—"
h = Wx
x
d
h
Ã˜ Freezes the pretrained model weights and injects trainable rank decomposition 
matrices into each layer of the Transformer architecture
36
5 â€“ Low-Rank Adaptation
Low-Rank Adaptation (LoRA)
!
Pretrained 
Weight
ğ‘Šâˆˆğ‘…"Ã—"
h = Wx
x
d
h
+
ğ´= ğ’©(0, ğœ!)
ğµ= 0
r
Ã˜ Freezes the pretrained model weights and injects trainable rank decomposition 
matrices into each layer of the Transformer architecture
Update 
Weight
âˆ†ğ‘Šâˆˆğ‘…"Ã—"
37
5 â€“ Low-Rank Adaptation
Low-Rank Adaptation (LoRA)
!
Pretrained 
Weight
ğ‘Šâˆˆğ‘…"Ã—"
h = Wx
x
d
h
+
ğ´= ğ’©(0, ğœ!)
ğµ= 0
r
Ã˜ Freezes the pretrained model weights and injects trainable rank decomposition 
matrices into each layer of the Transformer architecture
Update 
Weight
âˆ†ğ‘Šâˆˆğ‘…"Ã—"
38
5 â€“ Low-Rank Adaptation
Low-Rank Adaptation (LoRA)
!
Pretrained 
Weight
ğ‘Šâˆˆğ‘…"Ã—"
h = Wx
x
d
h
+
ğ´= ğ’©(0, ğœ!)
ğµ= 0
r
Ã˜ Freezes the pretrained model weights and injects trainable rank decomposition 
matrices into each layer of the Transformer architecture
After Training
Merge 
Weight
âˆ†ğ‘Šğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’âˆˆğ‘…!Ã—!
ğ‘Šğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’= ğ‘Š+ ğµğ´
39
5 â€“ Low-Rank Adaptation
Low-Rank Adaptation (LoRA)
!
Ã˜ Adapts the attention weights (query and value) of the Transformer self-attention sub-
layer with LoRA
40
5 â€“ Low-Rank Adaptation
Result on the GLEU benchmark
!
6 - Experiment
Source code
!
Reference
Ã˜ COS 597G (Fall 2022): Understanding Large Language Models
Ã˜ https://adapterhub.ml/blog/2022/09/updates-in-adapter-transformers-v3-1/
Ã˜ https://arxiv.org/pdf/2303.15647.pdf
Thanks!
Any questions?
43
