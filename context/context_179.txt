Extra Class
Advanced CNN
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Review: CNN
(2) â€“ Batch Normalization
(3) â€“ Dropout
(4) â€“ Skip Connection
1 â€“ Review: CNN
!
3
Convolutional Layer
Input: M x N
Kernel: K x O
Bias
1
0
0
0
0
0
0
0
0
3
1
1
0
0
3
1
2
0
0
0
3
4
2
3
0
0
3
0
0
2
0
0
0
0
0
0
0
0
3
1
1
3
1
2
0
3
4
2
3
3
0
0
2
1
1
1
1
1
1
0
1
0
Shape: (M+2P) x (N+2Q)
Padding: (P, Q)
Stride: (S, T)
7
8
15
13
M + 2P âˆ’K
S
+ 1 x N + 2Q âˆ’O
T
+ 1
1 â€“ Review: CNN
!
4
Pooling Layer
Input: 6 x 6
Output: 3 x 3
Kernel Size: 2
Stride: 2
3
2
1
0
0
3
0
3
3
1
1
0
3
1
4
1
1
0
2
4
1
1
0
4
1
0
3
0
3
0
3
4
4
3
3
4
3
3
3
4
4
4
4
4
4
Input: 6 x 6
Output: 2 x 3
Kernel Size: (3, 2)
Stride: 2
3
2
1
0
0
3
0
3
3
1
1
0
3
1
4
1
1
0
2
4
1
1
0
4
1
0
3
0
3
0
3
4
4
3
3
4
2
1.7
0.8
1.8
1.6
1.3
â–Max Pooling
â–Average Pooling
1 â€“ Review: CNN
!
5
Multiple Input â€“ Output Channels
8
13
8
12
2
2
1
4
1
0
0
4
0
3
3
4
0
4
1
2
1
0
1
0
1
0
0
1
0
1
1
0
1
0
1
1
1
0
12
21
12
21
Bias
1
Input Channel #1
Kernel Channel #1
Kernel Channel #2
Bias
1
0
0
2
1
4
1
3
1
4
3
1
4
2
4
2
0
Input Channel #2
1
1
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Kernel Channel #1
15
19
23
18
6
5
14
5
12
14
17
15
25
36
30
37
+ 
= 
+ 
= 
Kernel Channel #2
1 â€“ Review: CNN
!
6
LeNet Model
1 â€“ Review: CNN
!
7
TextCNN Model
2 â€“ Batch Normalization
!
8
Data Normalization
1
1
2
4
â–Compute mean and variance
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
â–Normalize Xi
+X! = X! âˆ’ğœ‡
ğœ%
ğœ‡= 2
ğœ% = 1.5
- 0.817
ğœ= 1.224
0
X& = 1 âˆ’2
1.224
2 â€“ Batch Normalization
!
9
Data Normalization
1
1
2
4
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ%
ğœ‡= 2
ğœ% = 1.5
- 0.817
- 0.817
0.000
ğœ= 1.224
0
X& = 2 âˆ’2
1.224
â–Compute mean and variance
â–Normalize Xi
2 â€“ Batch Normalization
!
10
Data Normalization
1
1
2
4
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ%
ğœ‡= 2
ğœ% = 1.5
- 0.817
- 0.817
0.000
1.634
ğœ= 1.224
0
X& = 4 âˆ’2
1.224
â–Compute mean and variance
â–Normalize Xi
2 â€“ Batch Normalization
!
11
Batch Normalization
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
2
1
4
1
0
0
4
0
3
3
4
ğœ–= 1e'(
ğ›¾= 1
ğ›½= 0
2 â€“ Batch Normalization
!
12
Batch Normalization
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
2
1
4
1
0
0
4
0
3
3
4
2.25
2.5
1.25
1.479
1.118
1.639
mean
variance
ğœ–= 1e'(
ğ›¾= 1
ğ›½= 0
2 â€“ Batch Normalization
!
13
Batch Normalization
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
4
0
3
- 0.169
1.183
-1.521
0.507
.XğŸ=
2 âˆ’2.25
1.479" + 1e#$
2
2
1
4
1
0
0
4
0
3
3
4
2.25
2.5
1.25
1.479
1.118
1.639
mean
variance
ğœ–= 1e'(
ğ›¾= 1
ğ›½= 0
2 â€“ Batch Normalization
!
14
Batch Normalization
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
4
0
3
- 0.169
1.183
-1.521
0.507
- 0.169
1.183
-1.521
0.507
.XğŸ=
2 âˆ’2.25
1.479" + 1e#$
Y% = 1 âˆ—âˆ’0.169 + 0
2
2
1
4
1
0
0
4
0
3
3
4
2.25
2.5
1.25
1.479
1.118
1.639
mean
variance
ğœ–= 1e'(
ğ›¾= 1
ğ›½= 0
2 â€“ Batch Normalization
!
15
Batch Normalization
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
2
1
4
1
0
0
4
0
3
3
4
2.25
2.5
1.25
1.479
1.118
1.639
mean
variance
- 0.169 - 0.447 - 0.153
1.183
- 1.342 - 0.763
-1.521
- 1.342 - 0.763
0.507
0.447
1.677
- 0.169 - 0.447 - 0.153
1.183
- 1.342 - 0.763
-1.521
- 1.342 - 0.763
0.507
0.447
1.677
+X!
Y!
ğœ–= 1e'(
ğ›¾= 1
ğ›½= 0
2 â€“ Batch Normalization
!
16
Batch Normalization - Demo
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2 â€“ Batch Normalization
!
17
Batch Normalization - Demo
2 â€“ Batch Normalization
!
18
Normalization during Inference
ğœ‡= 1
m %
!"#
$
X!
ğœ% = 1
m %
!"#
$
(X!âˆ’ğœ‡)%
+X! = X! âˆ’ğœ‡
ğœ% + ğœ–
â–Compute mean and variance
â–Normalize Xi
â–Get batch data (m: batch size)
X = {X#, X%, â€¦ , X$}
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
2
1
How to compute mean and 
variance with a sample?
ğœ‡)*): estimated mean of the studied population
ğœ%)*): estimated standard-deviation of the studied
population
computed using all the (ğœ‡_batch , Ïƒ_batch) determined 
during training
2 â€“ Batch Normalization
!
19
Normalization during Inference
+X! =
X! âˆ’ğœ‡)*)
ğœ%)*) + ğœ–
â–Normalize Xi
â–Scale and shift +X!
Y! = ğ›¾+X! + ğ›½
ğœ–is a very small 
value (1e-05)
ğ›¾and ğ›½are two 
learning parameters
2
2
1
How to compute mean and 
variance with a sample?
2.25
2.5
1.25
1.479
1.118
1.639
ğœ‡)*)
ğœ)*)
- 0.169
- 0.447
- 0.153
- 0.035
- 0.129
- 0.027
+X!
Y!
ğ›¾= 0.5
ğ›½= 0.05
2 â€“ Batch Normalization
!
20
MNIST using Batch Normalization
â–MNIST dataset
Ã˜ Images: 70.000
Ã˜ Class: 10
Ã˜ Image Size: 28 x 28
2 â€“ Batch Normalization
!
21
MNIST using Batch Normalization - Demo
â–MNIST dataset
Ã˜ Preprocessing
2 â€“ Batch Normalization
!
22
MNIST using Batch Normalization - Demo
â–MNIST dataset
Ã˜ Base Model
2 â€“ Batch Normalization
!
23
MNIST using Batch Normalization - Demo
â–MNIST dataset
Ã˜ Model with BatchNorm Layer
2 â€“ Batch Normalization
!
24
MNIST using Batch Normalization
â–MNIST dataset
Ã˜ Training
Ã˜ Evaluation
w/o BN: 97.53
w BN: 97.85
3 â€“ Dropout
!
25
Dropout
â–Removing units at random during the forward pass and putting them all back during test
â–Probability of an element to be zeroed (P)
Hidden Layer
Output Layer
Input Layer
Without Dropout
With Dropout
3 â€“ Dropout
!
26
Dropout - Demo
Initial
Updated
3 â€“ Dropout
!
27
MNIST using Dropout - Demo
â–MNIST dataset
Ã˜ Model with Dropout Layer
3 â€“ Dropout
!
28
MNIST using Dropout
â–MNIST dataset
Ã˜ Training
Ã˜ Evaluation
w/o Dropout: 97.53
w Dropout: 97.58
3 â€“ Dropout
!
29
MNIST using Dropout - Demo
â–Probability of an element to be zeroed (P)
3 â€“ Dropout
!
30
MNIST using Dropout
â–Probability of an element to be zeroed (P)
4 â€“ Skip Connection
!
31
Degradation Problem
The deeper model doesnâ€™t perform as well as the shallow one
https://arxiv.org/pdf/1512.03385.pdf
4 â€“ Skip Connection
!
32
Degradation Problem
input
Loss
target
output
4 â€“ Skip Connection
!
33
Skip Connection
input
Loss
target
output
block 1
block 2
block 3
block 4
+
+
+
+
4 â€“ Skip Connection
!
34
Skip Connection Example
Input
140
0
5
8
47
175
133
203
217
-12.1794
-2.7855
6.7117
-14.4943
-105.878 -17.4401
-67.1033
-84.1202 29.6049
0 0
6.7117
0 0
0
0 0
29.6049
0.2087
1.9920
-1.4108
0.2087
2.4680
4.0437
0.2087
8.0747
-6.9349
Conv2d
ReLU
Conv2d
140.2087
1.9920
3.5892
8.2087
49.4680
179.0437
133.2087
211.0747 210.0651
+
ReLU
140.2087
1.9920
3.5892
8.2087
49.4680
179.0437
133.2087
211.0747 210.0651
4 â€“ Skip Connection
!
35
Skip Connection Variants
+
Identity Shortcut
Conv2d
in_channels=3
out_channels=10
kernel_size=3
stride=1
padding=1
ReLU
Projection Shortcut
input
feature
map
ReLU
[128, 3, 64, 64]
[128, 10, 64, 64]
[128, 10, 64, 64]
Conv2d
in_channels=10
out_channels=3
kernel_size=3
stride=1
padding=1
[128, 3, 64, 64]
+
Conv2d
in_channels=3
out_channels=10
kernel_size=3
stride=1
padding=1
ReLU
input
feature
map
ReLU
[128, 10, 64, 64]
[128, 10, 64, 64]
Conv2d
in_channels=10
out_channels=3
kernel_size=3
stride=2
padding=1
[128, 3, 32, 32]
Conv2d
in_channels=3
out_channels=3
kernel_size=3
stride=2
padding=1
[128, 3, 64, 64]
[128, 3, 64, 64]
[128, 3, 32, 32]
[128, 3, 32, 32]
[128, 3, 64, 64]
[128, 3, 32, 32]
4 â€“ Skip Connection
!
36
Skip Connection Variants
+
+
Short Skip Connection 
(ResNet, â€¦)
Long Skip Connection 
(UNet, â€¦)
4 â€“ Skip Connection
!
37
Implementation â€“ No Skip Connection
Conv2d
ReLU
Conv2d
input
feature
map
ReLU
4 â€“ Skip Connection
!
38
Implementation â€“ Skip Connection
Conv2d
ReLU
Conv2d
input
feature
map
ReLU
+
4 â€“ Skip Connection
!
39
MNIST using Skip Connection - Demo
Thanks!
Any questions?
40
