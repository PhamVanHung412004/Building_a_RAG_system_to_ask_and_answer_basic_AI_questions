Machine Learning
Linear Regression
Logistic Regression
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) – Linear Regression
(2) – Logistic Regression
(3) – Code
1 – Linear Regression
Linear Regression
!
3
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Level
Salary
3.5
???
10
???
Data
Learning
Prediction
1 – Linear Regression
Linear Regression
!
4
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Visualization
y = 6x + 7
y = f(x): linear function
1 – Linear Regression
Linear Regression
!
5
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Visualization
y = 6x + 7
y = f(x): linear function
Modeling
y = wx + b
Find w and b to fit 
the data
1 – Linear Regression
Linear Regression using Gradient Descent
!
6
Modeling
y = wx + b
Visualization
y = 2x + 2
y = 2x + 2
Init 𝜃
lr = 0.1
Data 
y = 2x + 2 = 6
x = [1  2]
y = 18 
Loss
Difference between 
predicted and actual value
L = (6 – 18)2 = 144
L’ = −24
−48
k = -24
𝜃= 4.4
6.8
1 – Linear Regression
!
7
Modeling
y = wx + b
Visualization
y = 2x + 2 
y = 2x + 2
Linear Regression using Gradient Descent
y = 6.8x + 4.4
y = 6.8x + 4.4 
Updated 
1 – Linear Regression
!
8
Implement for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output &𝑦
3) Compute loss
"𝑦= 𝑤𝑥+ 𝑏
𝐿= ("𝑦−𝑦)!
𝜕𝐿
𝜕𝑤= 2𝑥("𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2("𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
"𝑦= 𝜽"𝒙= 𝒙"𝜽
𝐿= ("𝑦−𝑦)!
∇#L = 2𝒙("𝑦−𝑦)
𝜽= 𝜽−𝜂∇#L
𝜂is learning rate
Traditional
Basic Python
Vectorized
Numpy
1 – Linear Regression
!
9
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output &𝑦
3) Compute loss
"𝑦= 𝑤𝑥+ 𝑏
𝐿= ("𝑦−𝑦)!
𝜕𝐿
𝜕𝑤= 2𝑥("𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2("𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
1 – Linear Regression
!
10
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output &𝑦
3) Compute loss
"𝑦= 𝑤𝑥+ 𝑏
𝐿= ("𝑦−𝑦)!
𝜕𝐿
𝜕𝑤= 2𝑥("𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2("𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
1 – Linear Regression
!
11
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output &𝑦
3) Compute loss
"𝑦= 𝑤𝑥+ 𝑏
𝐿= ("𝑦−𝑦)!
𝜕𝐿
𝜕𝑤= 2𝑥("𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2("𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
1 – Linear Regression
!
12
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output &𝑦
3) Compute loss
"𝑦= 𝑤𝑥+ 𝑏
𝐿= ("𝑦−𝑦)!
𝜕𝐿
𝜕𝑤= 2𝑥("𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2("𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
1 – Linear Regression
!
13
Implement using Basic Python for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output &𝑦
3) Compute loss
"𝑦= 𝑤𝑥+ 𝑏
𝐿= ("𝑦−𝑦)!
𝜕𝐿
𝜕𝑤= 2𝑥("𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2("𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
1 – Linear Regression
!
14
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
"𝑦= 𝜽"𝒙= 𝒙"𝜽
𝐿= ("𝑦−𝑦)!
∇#L = 2𝒙("𝑦−𝑦)
𝜽= 𝜽−𝜂∇#L
𝜂is learning rate
1 – Linear Regression
!
15
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
"𝑦= 𝜽"𝒙= 𝒙"𝜽
𝐿= ("𝑦−𝑦)!
∇#L = 2𝒙("𝑦−𝑦)
𝜽= 𝜽−𝜂∇#L
𝜂is learning rate
1 – Linear Regression
!
16
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
"𝑦= 𝜽"𝒙= 𝒙"𝜽
𝐿= ("𝑦−𝑦)!
∇#L = 2𝒙("𝑦−𝑦)
𝜽= 𝜽−𝜂∇#L
𝜂is learning rate
1 – Linear Regression
!
17
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
"𝑦= 𝜽"𝒙= 𝒙"𝜽
𝐿= ("𝑦−𝑦)!
∇#L = 2𝒙("𝑦−𝑦)
𝜽= 𝜽−𝜂∇#L
𝜂is learning rate
1 – Linear Regression
!
18
Implement using Numpy for One Sample 
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
"𝑦= 𝜽"𝒙= 𝒙"𝜽
𝐿= ("𝑦−𝑦)!
∇#L = 2𝒙("𝑦−𝑦)
𝜽= 𝜽−𝜂∇#L
𝜂is learning rate
1 – Linear Regression
!
19
Pratice
Modeling
y = wx + b
y = 2x + 2
Init 𝜃
lr = 0.1
Data 
y = ?
x = [1  3]
y = 22 
L = ?
L’ = ?
𝜃= ?
1 – Linear Regression
!
20
Pratice
Modeling
y = wx + b
y = 2x + 2
Init 𝜃
lr = 0.1
Data 
y = 2x + 2 = 8
x = [1  3]
y = 22 
L = (6 – 18)2 = 196
L’ = −28
−84
𝜃= 4.8
10.4
y = 2x + 2
y = 10.4x + 4.8
1 – Linear Regression
!
21
Problem
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Data #1
Any value
Values: [0, 1]
Discrete values:{0, 1} 
y = ax + b
Need flexible model?
2 – Logistic Regresion
!
22
Problem
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Hours
Pass
0.25
???
4.5
???
Learning
Prediction
2 – Logistic Regresion
!
23
Problem
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Visualization
Modeling
y = f(x)
Find a function to 
fit the data
Sigmoid function
2 – Logistic Regresion
!
24
Sigmoid Function
𝜎(𝑢) =
1
1 + 𝑒$%
𝑧∈−∞
+ ∞
𝜎(𝑢) ∈0
1
∀𝑧!𝑧" ∈𝑎𝑏and 𝑧! ≤𝑧"
→𝜎(𝑧!) ≤𝑧(𝑢!)
𝑧
+∞
−∞
𝑧!
𝑧"
𝜎
𝜎!
𝜎"
Sigmoid function
Property
2 – Logistic Regresion
!
25
Sigmoid Function
𝜎(𝑧) =
1
1 + 𝑒$%
𝑧= 𝜽"𝒙
𝑧= 𝜽"𝒙
𝑧∈−∞
+ ∞
𝜎(𝑧) ∈0
1
𝑥
𝑧
𝑥
𝑧
𝑧
𝜎
𝑧
𝜎
2 – Logistic Regresion
!
26
Logistic Regression using Gradient Descent
𝜽! = [𝑏
𝑤]
𝒙! = [1
0.5]
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
𝜽! = [0.1
0.1]
𝒚= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂is learning rate
𝑧= 𝜽!𝒙
𝜂= 0.1
2 – Logistic Regresion
!
27
Logistic Regression using Gradient Descent
𝒙! = [1
0.5]
𝜽! = [0.1
0.1]
𝒚= 0
Model
0.1
0.1
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝑧= 0.15
)𝑦= 0.54
𝜂= 0.1
is learning rate
2 – Logistic Regresion
!
28
Logistic Regression using Gradient Descent
𝒙! = [1
0.5]
𝜽! = [0.1
0.1]
𝒚= 0
Model
Loss
0.1
𝑦
0.1
𝑦= 0
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝑧= 0.15
)𝑦= 0.54
L = 0.771
𝜂= 0.1
is learning rate
2 – Logistic Regresion
!
29
Logistic Regression using Gradient Descent
𝒙! = [1
0.5]
𝜽! = [0.1
0.1]
𝒚= 0
Model
Loss
0.1
𝑦
0.1
𝑦= 0
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝑧= 0.15
)𝑦= 0.54
L = 0.771
𝛻!𝐿= 𝐱()y −𝑦)
=
1
0.5
0.54
= 0.54
0.27
= 𝐿"
#
𝐿$
#
𝜂= 0.1
is learning rate
2 – Logistic Regresion
!
30
Logistic Regression using Gradient Descent
𝒙! = [1
0.5]
𝜽! = [0.1
0.1]
𝒚= 0
Model
Loss
0.1
𝑦
0.1
𝑦= 0
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝑧= 0.15
)𝑦= 0.54
L = 0.771
𝛻!𝐿= 𝐱()y −𝑦)
=
1
0.5
0.54
= 0.54
0.27
= 𝐿"
#
𝐿$
#
𝑏= 0.1 −𝜂0.54
=0.046
𝑤= 0.1 −𝜂0.27
=0.073
𝜂= 0.1
is learning rate
2 – Logistic Regresion
!
31
Logistic Regression using Gradient Descent
𝒙! = [1
0.5]
𝜽! = [0.046
0.073]
𝒚= 0
Model
Loss
0.046
𝑦
0.073
𝑦= 0
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝑧= 0.0825
)𝑦= 0.52
L = 0.735
𝑏= 0.1 −𝜂0.54
=0.046
𝑤= 0.1 −𝜂0.27
=0.073
𝜂= 0.1
Previous L = 0.777
is learning rate
2 – Logistic Regresion
!
32
Logistic Regression using Gradient Descent
𝜽! = [𝑏
𝑤]
𝒙! = [1
1.0]
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
𝜽! = [0.046
0.073]
𝒚= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝜂= 0.1
is learning rate
2 – Logistic Regresion
!
33
Logistic Regression using Gradient Descent
𝒙! = [1
1.0]
𝜽! = [0.046 0.073]
𝒚= 0
Model
Loss
𝑦
𝑦= 0
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝑧= 0.119
)𝑦= 0.53
L = 0.755
∇%L = 𝐱()y −𝑦)
=
1
1.0
0.53
= 0.53
0.53
= 𝐿"
#
𝐿$
#
𝑏= 0.046 −𝜂0.53
= −0.007
𝑤= 0.073 −𝜂0.53
=0.02
𝜂= 0.1
0.046
0.073
is learning rate
2 – Logistic Regresion
!
34
Logistic Regression using Gradient Descent
𝒙! = [1
1.0]
𝜽! = [−0.007 0.02]
𝒚= 0
Model
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂
𝑧= 𝜽!𝒙
𝜂= 0.1
−0.007
0.02
is learning rate
2 – Logistic Regresion
!
35
Prediction
Data #1
Hours
Pass
0.5
0
1.0
0
1.5
0
2.0
0
2.5
1
3.0
1
3.5
1
4.0
1
Hours
Pass
0.25
???
4.5
???
Learning
Prediction
𝜽! = [0.046 0.073]
𝑏
𝑤
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−0.007
0.02
Prediction
2 – Logistic Regresion
!
36
Prediction
Hours
Pass
0.25
???
4.5
???
Prediction
Prediction
𝑏
𝑤
&𝑦= 𝜎𝑧=
1
1 + 𝑒"# = 0.499
𝑧= 𝑤𝑥+ 𝑏= −0.002
−0.007
0.02
Hours
Pass
0.25
???
4.5
???
Prediction
Prediction
𝑏
𝑤
&𝑦= 𝜎𝑧=
1
1 + 𝑒"# = 0.52
𝑧= 𝑤𝑥+ 𝑏= 0.083
−0.007
0.02
Thresholds = 0.5
𝑦&'(): 0
Thresholds = 0.5
𝑦&'(): 1
2 – Logistic Regresion
!
37
Multivariable Logistic Regression
Data #2
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
Day
Hours
Pass
2
0.25
???
1
4.5
???
Learning
Prediction
2 – Logistic Regresion
!
38
Multivariable Logistic Regression using Gradient Descent
𝜽! = [𝑏
𝑤% 𝑤&]
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
𝒚= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂is learning rate
𝑧= 𝜽!𝒙
𝜂= 0.1
Data #2
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
2 – Logistic Regresion
!
39
Multivariable Logistic Regression using Gradient Descent
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
40
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
Model
𝑏
𝑤G
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
𝑧= 0.35
)𝑦= 0.59
𝜂= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
0.1
𝑤H
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
41
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
𝜂= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
42
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
Model
Loss
𝑦
𝑦= 0
𝑏
𝑤G
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
𝑧= 0.35
)𝑦= 0.59
L = 0.883
𝜂= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
0.1
𝑤H
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
43
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
𝜂= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
44
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
Model
Loss
𝑦
𝑦= 0
𝑏
𝑤G
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
𝑧= 0.35
)𝑦= 0.59
L = 0.883
∇%L = 𝐱()y −𝑦)
=
1
1.0
0.5
0.59 =
0.59
0.59
0.295
=
𝐿"
#
𝐿&#
#
𝐿&$
#
𝜂= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
0.1
𝑤H
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
45
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
𝜂= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
46
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
Model
Loss
𝑦
𝑦= 0
𝑏
𝑤G
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
−ylog"y−(1−y)log(1−"y )
𝑧= 0.35
)𝑦= 0.59
L = 0.883
∇%L = 𝐱()y −𝑦)
=
1
1.0
0.5
0.59 =
0.59
0.59
0.295
=
𝐿"
#
𝐿&#
#
𝐿&$
#
𝑏= 0.1 −𝜂0.59 = 0.041
𝑤'= 0.2 −𝜂0.59 =0.141
𝜂= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
0.1
𝑤H
𝑤(= 0.1 −𝜂0.295
=0.0706
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
47
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
𝜂= 0.1
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.1
0.2 0.1]
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
48
Multivariable Logistic Regression using Gradient Descent
𝒚= 0
Model
𝑏
𝑤G
!𝑦= 𝜎(𝑧) =
1
1 + 𝑒!"
𝑧= 𝑤𝑥+ 𝑏
𝑏= 0.1 −𝜂0.59 = 0.041
𝑤'= 0.2 −𝜂0.59 =0.141
𝜂= 0.1
0.041
0.141
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from 
training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜂is learning rate
𝑧= 𝜽!𝒙
𝒙! = [1.
1.0 0.5]
𝜽! = [0.041
0.141 0.0706]
0.0706
𝑤H
𝑤(= 0.1 −𝜂0.295
=0.0705
𝜽= 𝜽−𝜂∇$L
2 – Logistic Regresion
!
49
Prediction
Prediction
Prediction
&𝑦= 𝜎𝑧=
1
1 + 𝑒"# = 0.58
𝑧= 𝑤𝑥+ 𝑏= 0.3417
Prediction
Thresholds = 0.5
𝑦&'(): 1
Day
Hours
Pass
2
0.25
???
1
4.5
???
Day
Hours
Pass
2
0.25
???
1
4.5
???
Model
𝑏
𝑤G
0.041
0.141
0.0706
𝑤H
Prediction
&𝑦= 𝜎𝑧=
1
1 + 𝑒"# = 0.622
𝑧= 𝑤𝑥+ 𝑏= 0.5
Thresholds = 0.5
𝑦&'(): 1
Model
𝑏
𝑤G
0.041
0.141
0.0706
𝑤H
3 – Code Demo
50
Summary
51
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output &𝑦
3) Compute loss
&𝑦= 𝜎(𝑧) =
1
1 + 𝑒"#
𝐿(𝜽) = −ylog&y−(1−y)log(1−&y )
∇$L = 𝐱(&y −𝑦)
𝜽= 𝜽−𝜂∇$L
𝜂is learning rate
𝑧= 𝜽!𝒙
Data #2
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
Thanks!
Any questions?
52
