CNN Training
How to increase training accuracy?
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
‚û¢Network Architectures
‚û¢Network Training
‚û¢Case Study
‚û¢Problem-Solving Approach
Outline
LeNet
1994
AlexNet
2012
Network-in-Network
2013
VGG
2014
Inception
2014
ResNet
2015
SqueezeNet
2016
MobileNets
2017
U-Net
2015
DenseNet
2016
EfficientNet
2019
ConvNext
2022
VGG16
https://neurohive.io/en/popular-networks/vgg16/
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer
+ ReLU
(4096) 
Dense Layer
+ Softmax
(1000) 
2
CNN Architectures
AI VIETNAM
All-in-One Course
‚ùñ VGG16 for ImageNet
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer
+ ReLU
(4096) 
Dense Layer
+ Softmax
(1000) 
Block-1
Block-2
Block-3
Block-4
Block-5
Dense Block
input
(3,224,224)
(64,112,112)
(256,56,56)
(512,7,7)
(512,28,28)
(512,14,14)
output
3
CNN Architectures
AI VIETNAM
All-in-One Course
‚ùñ VGG16-like for Cifar-10
Block-1
Block-2
Block-3
Block-4
Block-5
Dense Block
input
(3,32,32)
(64,16,16)
(256,8,8)
(512,1,1)
(512,4,4)
(512,2,2)
output
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer
+ ReLU
(256) 
Dense Layer
+ Softmax
(10) 
5
‚û¢Network Architectures
‚û¢Network Training
‚û¢Case Study
‚û¢Problem-Solving Approach
Outline
Fashion-MNIST dataset
T-shirt
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle
Boot
Image Data
Grayscale images
Resolution=28x28
Training set: 60000 samples
Testing set: 10000 samples
Network Training
‚ùñ Fashion-MNIST dataset
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
input
(1,28,28)
(64,14,14)
(128,7,7)
output
Flatten
(7,7,128)
6272
8
Network Training
‚ùñ Fashion-MNIST dataset
AI VIETNAM
All-in-One Course
X-data format
(batch, channel, height, width)
Data normalization [0,1]
(3x3) Convolution with 64 filters, 
stride=1, padding=‚Äòsame‚Äô
+ Sigmoid activation 
+ glorot_uniform initialization
 
Adam optimizer and Cross-entropy loss
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Network Training
‚ùñ Fashion-MNIST dataset
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
input
(1,28,28)
(64,14,14)
(128,7,7)
output
Flatten
10
Cifar-10 dataset
(more complex dataset)
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
Network Training
Color images
Resolution=32x32
Training set: 50000 samples
Testing set: 10000 samples
Network Training
‚ùñ Cifar-10 dataset
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
input
(3,32,32)
(64,16,16)
(128,8,8)
output
Flatten
Data normalization [0,1]
Glorot uniform initialization 
Adam optimizer with lr=1e-3
AI VIETNAM
All-in-One Course
12
Network Training
‚ùñ Cifar-10 dataset
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
input
(3,32,32)
(64,16,16)
(128,8,8)
output
Flatten
(8,8,128)
8192
Accuracy: 69.3% - Val_accuracy: 64.5%
Network Training
‚ùñ Cifar-10 dataset: 
‚ùñAdding more layers
input
(3,32,32)
(64,16,16)
(128,8,8)
output
(256,4,4)
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Data normalization [0,1]
Glorot uniform initialization 
Adam optimizer with lr=1e-3
14
Network Training
‚ùñ Cifar-10 dataset: 
‚ùñAdding more layers
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
input
(3,32,32)
(64,16,16)
(128,8,8)
output
(256,4,4)
Good news: Network accuracy 
increases about 25%
Accuracy: 93.8% - Val_accuracy: 68.7%
15
Network 
Training
Cifar-10 dataset: 
‚ùñ Keep adding more layers
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
output
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Data normalization [0,1]
Glorot uniform initialization 
Adam optimizer with lr=1e-3
16
Network Training
‚ùñ Cifar-10 dataset: 
‚ùñ Keep adding more layers
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ Sigmoid
The network does  
not learn
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
output
17
Network Training
‚ùñ Cifar-10 dataset: 
‚ùñ Keep adding more layers
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + Sigmoid
Dense Layer-512
+ Sigmoid
sigmoid ùë•=
1
1 + ùëí‚àíùë•
Values are too small
Vanishing Problem
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
output
18
Network Training
‚ùñ Cifar-10 dataset: 
‚ùñ Keep adding more layers
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
Dense Layer-512
+ ReLU
ReLU ùë•= ·âä0 
if ùë•< 0
ùë• 
if ùë•‚â•0
nn.Conv2D(...), nn.Sigmoid()
nn.Conv2D(...), nn.ReLU()
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
output
Network 
Training
‚ùñ Cifar-10 dataset: 
‚ùñ  Use ReLU
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
output
Data normalization [0,1]
Glorot uniform initialization 
Adam optimizer with lr=1e-3
Network Training
‚ùñ Cifar-10 dataset: 
‚ùñ  Use ReLU
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
Training Accuracy 
reaches up to 99%
Adding more layers; Hope reach to 100%
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
output
21
Network Training
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
Use ReLU and add more layers
input
(3,32,32)
(64,16,16)
(128,8,8)
(512,2,2)
output
(256,4,4)
Data normalization [0,1]
Glorot uniform initialization 
Adam optimizer with lr=1e-3
22
Implementation
Network Training
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
Use ReLU and add more layers
Network does not learn again
input
(3,32,32)
(64,16,16)
(128,8,8)
(512,2,2)
output
(256,4,4)
24
Network Training
‚ùñ Summary of the current network
AI VIETNAM
All-in-One Course
Data Normalization
(scale to [0,1])
Network Construction
(Convs , ReLU, max 
pooling, Dense layers)
Parameter 
Initialization
(Glorot uniform)
Cifar-10
Dataset
Training 
(Adam and cross-
entropy loss)
Network does 
not learn
25
Network Training
‚ùñ Solution 1: Observation
AI VIETNAM
All-in-One Course
Data Normalization
(scale to [0,1])
Network Construction
(Convs , ReLU, max 
pooling, Dense layers)
Parameter 
Initialization
(Glorot uniform)
Training 
(Adam and cross-
entropy loss)
MNIST
Dataset
The current network 
performs excellently
for MNIST dataset
26
Network Training
‚ùñ Solution 1: Idea
AI VIETNAM
All-in-One Course
MNIST
Dataset
Cifar-10
Dataset
<
complex
Current 
Network
Current 
Network
Train (failed)
Train (ok)
How to reduce  the complexity 
of the Cifar-10 dataset
Data Normalization
(scale to [0,1])
0
255
0
1
Data Normalization
(convert to 0-mean 
and 1-deviation)
ùëã=
ùëã= ùëã‚àíùúá
ùúé
ùúá= 1
ùëõ‡∑ç
ùëñ
ùëãùëñ
ùúé=
1
ùëõ‡∑ç
ùëñ
ùëãùëñ‚àíùúá2
27
Network Training
AI VIETNAM
All-in-One Course
ùëå= ùëéùëã+ ùëè
‡¥§ùëå= ùëå‚àíùúáùëå
ùúéùëå
=
ùëéùëã+ ùëè‚àí1
ùëõœÉùëñùëéùëãùëñ+ ùëè
1
ùëõœÉùëñ
ùëéùëãùëñ+ ùëè‚àí1
ùëõœÉùëñùëéùëãùëñ+ ùëè
2
=
ùëéùëã‚àí1
ùëõœÉùëñùëéùëãùëñ
1
ùëõœÉùëñùëéùëãùëñ‚àí1
ùëõœÉùëóùëéùëãùëó
2
=
ùëã‚àí1
ùëõœÉùëñùëãùëñ
1
ùëõœÉùëñùëãùëñ‚àí1
ùëõœÉùëóùëãùëó
2
=
ùëã‚àíùúáùëã
1
ùëõœÉùëñùëãùëñ‚àíùúáùëã2
= ‡¥§ùëã
‡¥§ùëã= ùëã‚àíùúá
ùúé
ùúá= 1
ùëõ‡∑ç
ùëñ
ùëãùëñ
ùúé=
1
ùëõ‡∑ç
ùëñ
ùëãùëñ‚àíùúá2
This normalization helps 
network to be invariant to 
linear transformation
ùëå= ùëéùëã+ ùëè
‡¥§ùëå= ùëå‚àíùúáùëå
ùúéùëå
= ‡¥§ùëã
‚ùñ Solution 1: Idea
28
Network Training
Solution 1: 0-mean and unit-
deviation normalization
Data Normalization
(convert to 0-mean 
and 1-deviation)
ùëã= ùëã‚àíùúáùëë
ùúéùëë
ùúáùëë is the mean of dataset
ùúéùëë is the deviation for the whole dataset
Network Training
‚ùñ Solution 1: 0-mean and unit-deviation normalization
AI VIETNAM
All-in-One Course
Data Normalization
(convert to 0-mean 
and 1-deviation)
ùëã= ùëã‚àíùúáùëë
ùúéùëë
ùúáùëë is the mean of dataset
ùúéùëë is the deviation for the whole dataset
Normalize each channel separately
30
Network Training
‚ùñ Solution 1 (extension): 
    Normalize to [-1, 1]
Normalize each channel separately
epoch
epoch
31
Network Training
‚ùñ Solution 2
AI VIETNAM
All-in-One Course
Data Normalization
(convert to 0-mean 
and 1-deviation)
Network Construction
(Convs , ReLU, max 
pooling, Dense layers)
Parameter 
Initialization
(Glorot uniform)
Training 
(Adam and cross-
entropy loss)
MNIST
Dataset
ùëã= ùëã‚àíùúáùëë
ùúéùëë
How to use the idea (from solution 1) to integrate to network
Batch Normalization
32
Network Training
‚ùñ Solution 2: Batch normalization
AI VIETNAM
All-in-One Course
ùëã= ùëã1, ‚Ä¶ , ùëãùëö
ùëö is mini-batch size
ùúá= 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ
ùúé2 = 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ‚àíùúá2
‡∑†ùëãùëñ= ùëãùëñ‚àíùúá
ùúé2 + ùúñ
ùëåùëñ= ùõæ‡∑†ùëãùëñ+ Œ≤
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ùëãùëñ
Scale and shift ‡∑†ùëãùëñ
ùúñ is a very small value
ùõæ and Œ≤ are two learning parameters
Batch Normalization
Do not need bias when using BN*
ùúá and ùúé are updated in forward pass
ùõæ and Œ≤ are updated in backward pass
33
Network Training
‚ùñ Solution 2: Batch normalization
AI VIETNAM
All-in-One Course
ùëã= ùëã1, ‚Ä¶ , ùëãùëö
ùëö is mini-batch size
ùúá= 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ
ùúé2 = 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ‚àíùúá2
‡∑†ùëãùëñ= ùëãùëñ‚àíùúá
ùúé2 + ùúñ
ùëåùëñ= ùõæ‡∑†ùëãùëñ+ Œ≤
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ùëãùëñ
Scale and shift ‡∑†ùëãùëñ
ùúñ is a very small value
ùõæ and Œ≤ are two learning parameters
Batch Normalization
ùõæ=
ùúé2 + ùúñ and Œ≤ = ùúá
What if
34
Network Training
Solution 2: Batch normalization
ùëã= ùëã1, ‚Ä¶ , ùëãùëö
ùëö is mini-batch size
ùúá= 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ
ùúé2 = 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ‚àíùúá2
‡∑†ùëãùëñ= ùëãùëñ‚àíùúá
ùúé2 + ùúñ
ùëåùëñ= ùõæ‡∑†ùëãùëñ+ Œ≤
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ùëãùëñ
Scale and shift ‡∑†ùëãùëñ
ùúñ is a very small value
ùõæ and Œ≤ are two learning parameters
ùúáùëà= 5.0
ùúéùëà= 2.64
ùõæùëà= 1.0
Œ≤ùëà= 0.0
‡∑°ùëà=
‚àí1.51
‚àí0.75
1.51
‚àí0.37
0.37
0.75
ùëãùëà=
1
3
9
4
6
7
ùëãùëâ=
6
4
7
5
6
2
ùúáùëâ= 5.0
ùúéùë£= 1.63
ùúñ= 10‚àí5
ùõæùëâ= 1.0
Œ≤ùëâ= 0.0
‡∑†ùëâ=
0.61
‚àí0.61
1.22
0.0
0.61
‚àí1.83
ùëåùëà=
‚àí1.51
‚àí0.75
1.51
‚àí0.37
0.37
0.75
ùëåùëâ=
0.61
‚àí0.61
1.22
0.0
0.61
‚àí1.83
ùõæ and Œ≤ are updated in training process
35
Batch Normalization
ùúá= 2.5
ùúé2 = 6.58
‡∑†ùëã=
1.75 
0.97
‚àí0.97 
0.58
‚àí0.97 
1.75
0.19 
‚àí0.58
‚àí0.19 ‚àí0.97
‚àí0.97 ‚àí0.58
ùëã=
7 5
0 4 , 0 7
3 1 , 2 0
0 1
‡∑†ùëå=
1.75 
0.97
‚àí0.97 
0.58
‚àí0.97 
1.75
0.19 
‚àí0.58
‚àí0.19 ‚àí0.97
‚àí0.97 ‚àí0.58
sample 1
sample 3
sample 2
batch-size = 3
AI VIETNAM
All-in-One Course
ùúñ= 10‚àí5
ùõæ= 1.0
Œ≤ = 0.0
ùúéùëê=
1
ùëÅ√ó ùêª√ó ùëä‡∑ç
ùëñ=1
ùëÅ
‡∑ç
ùëó=1
ùêª
‡∑ç
ùëò=1
ùëä
ùêπùëñùëóùëò‚àíùúáùëê
2
ùúáùëê=
1
ùëÅ√ó ùêª√ó ùëä‡∑ç
ùëñ=1
ùëÅ
‡∑ç
ùëó=1
ùêª
‡∑ç
ùëò=1
ùëä
ùêπùëñùëóùëò
https://arxiv.org/pdf/
1803.08494.pdf
36
input_shape = (BS=3, C=1, H=2, W=2)
Network Training
ùúá= [2.0, 3.0]
ùúé2 = [6.0, 8.67]
‡∑†ùëã=
‚àí0.94 
1.41
0.47 
‚àí0.94
1.56 
‚àí0.39
‚àí1.17 
0
ùëã=  
‡∑†ùëå=
‚àí0.94 
1.41
0.47 
‚àí0.94
1.56 
‚àí0.39
‚àí1.17 
0
batch-size = 1
sample_shape = (BS=1, C=2, H=2, W=2)
7 2
0 3
0 5
3 0
sample 1
AI VIETNAM
All-in-One Course
ùõæ= 1.0
Œ≤ = 0.0
ùúñ= 10‚àí5
37
Network Training
ùúñ= 10‚àí5
AI VIETNAM
All-in-One Course
ùúá= [2.0, 3.0]
ùúé2 = [4.0, 3.7]
ùëã=  
, 
‡∑†ùëå= ‚Ä¶
batch-size = 2
sample_shape = (BS=2 , C=2, H=2, W=2)
sample 1 sample 2
6 4
5 2
0 5
3 0
2 3
0 2
1 4
3 0
‡∑†ùëã=  
, 
sample 1 sample 2
1.6 
0.5
1.1 
‚àí0.5
1.6 
‚àí1.1
‚àí1.1 
0.5
‚àí0.5 1.1
0.5 
‚àí1.1
‚àí0.5 
0.0
‚àí1.6 
‚àí0.5
ùõæ= 1.0
Œ≤ = 0.0
Batch-Norm Layer
38
Network Training
‚ùñ Solution 2: Batch normalization
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
Batch 
normalization
torch.nn.BatchNorm2d(num_features)
num_features (int): C from an expected input of 
                                size (N, C, H, W)
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
(512,2,2)
output
39
Network Training
‚ùñ Solution 2: Batch normalization
AI VIETNAM
All-in-One Course
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
Batch 
normalization
conv = nn.Sequential(nn.Conv2d(3, 64, 3),
 
      nn.ReLU(),
 
      nn.BatchNorm2d(64))
input
(3,32,32)
(64,16,16)
(128,8,8)
(256,4,4)
(512,2,2)
output
40
Network Training
‚ùñ Solution 2: Batch normalization
AI VIETNAM
All-in-One Course
ùëã= ùëã1, ‚Ä¶ , ùëãùëö
ùëö is mini-batch size
ùúá= 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ
ùúé2 = 1
ùëö‡∑ç
ùëñ=1
ùëö
ùëãùëñ‚àíùúá2
‡∑†ùëãùëñ= ùëãùëñ‚àíùúá
ùúé2 + ùúñ
ùëåùëñ= ùõæ‡∑†ùëãùëñ+ Œ≤
Input data for a node in batch normalization layer
Compute mean and variance
Normalize ùëãùëñ
Scale and shift ‡∑†ùëãùëñ
ùúñ is a very small value
ùõæ and Œ≤ are two learning parameters
Speed up training
Reduce the dependence on initial weights
Model Generalization
41
Network Training
‚ùñ Solution 3: Use more robust initialization
AI VIETNAM
All-in-One Course
Data Normalization
(convert to 0-mean 
and 1-deviation)
Network Construction
(Convs , ReLU, max 
pooling, Dense layers)
Parameter 
Initialization
(Glorot uniform)
Training 
(Adam and cross-
entropy loss)
MNIST
Dataset
Glorot uniform initialization (2010)
Understanding the difficulty of training deep feedforward neural networks
http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
He initialization (2015)
Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification
https://arxiv.org/pdf/1502.01852.pdf
42
Network Training
‚ùñ Solution 3: He Initialization
AI VIETNAM
All-in-One Course
Glorot initialization (2010)
n‚±º is #inputs in layer j
Assuming activation functions are linear
He initialization (2015)
Taking activation function into account
Adapt to ReLU activation
ùëä~ùí©0, 1
ùëõùëó
ùëä~ùí©0, 2
ùëõùëó
Data normalization [0,1]
He normal initialization 
Adam optimizer with lr=1e-3
43
Network Training
‚ùñ Solution 3: He Initialization
AI VIETNAM
All-in-One Course
Glorot initialization (2010)
n‚±º is #inputs in layer j
Assuming activation functions are linear
He initialization (2015)
Taking activation function into account
Adapt to ReLU activation
ùëä~ùí©0, 1
ùëõùëó
ùëä~ùí©0, 2
ùëõùëó
Data normalization [0,1]
He normal initialization 
Adam optimizer with lr=1e-3
44
Network Training
‚ùñ Solution 4: Using advanced activation
AI VIETNAM
All-in-One Course
Data Normalization
(convert to 0-mean 
and 1-deviation)
Network Construction
(Convs , ReLU, max 
pooling, Dense layers)
Parameter 
Initialization
(Glorot uniform)
Training 
(Adam and cross-
entropy loss)
MNIST
Dataset
swish x = ùë•‚àó
1
1 + ùëí‚àíùë•
2017
ReLU ùë•= ·âä0 
if ùë•< 0
ùë• 
if ùë•‚â•0
2010
Sigmoid Linear Unit (SiLU)
45
https://arxiv.org/pdf/
1702.03118.pdf
Network Training
‚ùñ Solution 4: 
    Using advanced activation
swish x = ùë•‚àó
1
1 + ùëí‚àíùë•
2017
Sigmoid Linear Unit (SiLU)
epoch
epoch
46
https://arxiv.org/pdf/1702.03118.pdf
Network Training
‚ùñ Solution 5: Skip connection
AI VIETNAM
All-in-One Course
Data Normalization
(convert to 0-mean 
and 1-deviation)
Network Construction
(Convs , ReLU, max 
pooling, Dense layers)
Parameter 
Initialization
(Glorot uniform)
Training 
(Adam and cross-
entropy loss)
MNIST
Dataset
X
Conv
Conv
Y
+
X
Conv
Conv
Y
+
Conv
Skip connection
Improve gradient flow 
in backward pass
47
‚ùñ Solution 5: Skip connection
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max 
pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=2 + ReLU
Network Training
AI VIETNAM
All-in-One Course
input
(3,32,32)
(64,16,16)
(128,8,8)
(512,2,2)
output
(256,4,4)
+
+
+
+
48
Network 
Training
‚ùñ Solution 5: 
    Skip connection
X
Conv
Conv
Conv
Max
pooling
+
Conv
There are several variants that use fully skip 
connection, concatenation, long skip connection
49
Network Training
‚ùñ Solution 5: Skip connection
AI VIETNAM
All-in-One Course
X
Conv
Conv
Y
+
X
Conv
Conv
Y
+
Conv
Weight Layer 
(Conv or Dense)
Weight Layer 
(Conv or Dense)
relu
+
F(x)
x
x
F(x) + x
relu
50
Network Training
‚ùñ Solution 5: Skip connection
AI VIETNAM
All-in-One Course
Weight Layer 
(Conv or Dense)
Weight Layer 
(Conv or Dense)
relu
+
F(x)
x
x
F(x) + x
relu
Weight Layer + BN 
(Conv or Dense)
Weight Layer + BN 
(Conv or Dense)
relu
+
F(x)
x
x
F(x) + x
relu
Weight Layer
(Conv or Dense)
Weight Layer 
(Conv or Dense)
BN + relu
+
F(x)
x
x
F(x) + x
relu
BN + relu
51
Network Training
‚ùñ Solution 5: Skip connection
https://arxiv.org/pdf/1608.06993v5.pdf
52
Network Training
AI VIETNAM
All-in-One Course
‚ùñ Solution 6: Reduce learning rate
input
(3,32,32)
(64,16,16)
(128,8,8)
(512,2,2)
output
(256,4,4)
(3x3) Convolution
padding=‚Äòsame‚Äô
stride=1 + ReLU
(2x2) max pooling
Dense Layer-10
+ Softmax
Flatten
Dense Layer-512
+ ReLU
From ‚ÄúMachine Learning Simplified‚Äù
53
Network 
Training
Reduce learning rate
input
(3,32,32)
(64,16,16)
(128,8,8)
(512,2,2)
output
(256,4,4)
epoch
epoch
54
Further Reading
AI VIETNAM
All-in-One Course
https://theaisummer.com/skip-connections/
Skip connection
http://karpathy.github.io/2019/04/25/recipe/
Trying to overfit Data 
https://arxiv.org/pdf/1608.06993v5.pdf
DenseNet
55
Summary
‚ùñ Train a CNN model
‚ùñ Try to overfit data
AI VIETNAM
All-in-One Course
Data Normalization
(convert to 0-mean 
and 1-deviation)
Network Construction
(ReLU or better)
Parameter 
Initialization
(He Init. or better)
Training 
(Adam or better)
Dataset
ùëã= ùëã‚àíùúáùëë
ùúéùëë
X
Conv
Conv
Y
+
Skip connection
·àòùëçùëñ= ùëçùëñ‚àíùúá
ùúé2 + ùúñ
Batch normalization
56
