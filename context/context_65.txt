AI VIET NAM – COURSE 2024
Decision Tree
Trung-Trực Trần
Ngày 17 tháng 2 năm 2024
1
Giới thiệu.
Cây quyết định (Decision Tree) là một trong những thuật toán máy học phổbiến nhất. Nó
là một công cụmạnh mẽđược sửdụng cho việc phân loại và dựđoán trong học máy và khai
phá dữliệu.
Cây quyết định thực hiện quá trình học bằng cách phân chia tập dữliệu thành các phần
nhỏhơn và xây dựng một cây quyết định dựa trên các quy tắc. Mỗi nút trên cây biểu diễn
một thuộc tính (hoặc biến độc lập), mỗi cạnh đi từnút cha đến các nút con biểu diễn các
quy tắc quyết định, và mỗi lá trên cây biểu diễn một lớp hoặc một giá trịdựđoán.
2
Công thức cây quyết định
2.1
Gini Impurity
Công thức Gini Impurity được sửdụng trong thuật toán cây quyết định đểđo lường độ
không chính xác của một dựđoán khi phân loại một tập dữliệu.
cần lưu ý là Gini Impurity càng nhỏ(gần 0) thì tập dữliệu đó càng "thuần khiết", nghĩa
là các mẫu trong cùng một nhóm có xu hướng thuộc vào cùng một lớp. Ngược lại, nếu Gini
Impurity cao (gần 1), thì việc phân loại các mẫu trong nhóm đó trởnên không chắc chắn.
Giảsửbạn đang xem xét một tập dữliệu chia thành K nhóm, mỗi nhóm chứa một phần
tỷlệpi với i=1,2,...,K.
Công thức được biểu diễn như sau:
IG = 1 −
K
X
i=1
p2
i
(1)
Trong đó:
• IG là Gini Impurity.
• pi là tỷlệcác mẫu thuộc vào lớp i.
1
Khi xây dựng cây quyết định, chúng ta cần chọn thuộc tính và giá trịphân chia sao cho
Gini Impurity sau phân chia là nhỏnhất, tức là mức độ"thuần khiết" cho dữliệu thuộc về
từng nhóm con được tạo ra.
Đối với tệp dữliệu gồm 2 labels (i=2) thì chỉsốIG sẽđạt ngưỡng lớn nhất là bằng 0.5
(Dữliệu sẽbịnhiễu lớn nhất).
Hình 1: Ví dụ1 với 2 label xanh và đen
Với ví dụởhình 1, ta có 2 label là xanh và đen. Với 5 viên bi thuộc xanh và 3 viên bi
thuộc đen ta có thểtính được xác suất suất hiện của lần lượt 2 tập là 5
8 và 3
8. Áp dụng công
thức Gini Impurity ta sẽtính được:
IG = 1 −5
8
2 −3
8
2 = 0.46875.
Vì ví dụchỉcó 2 label là xanh và đen nên kết quả0.46 có thểcoi là tính không thuần
khiết tiệm cận mức cao nhất (Max=0.5).
2.2
Entropy
Entropy trong cây quyết định là một khái niệm được sửdụng đểđo lường sựkhông chắc chắn
trong dữliệu (Trung bình surprise). Trong ngữcảnh của cây quyết định, entropy thường
được sửdụng đểđo lường mức độkhông chắc chắn của phân phối lớp trong tập dữliệu.
Entropy được tính bằng công thức sau:
Entropy(S) = −
c
X
i=1
pi log2(pi)
(2)
Trong đó:
• S là tập dữliệu.
• c là sốlớp trong tập dữliệu.
2
• pi là tỷlệcủa lớp i trong tập dữliệu.
Entropy càng cao khi tỷlệcủa các lớp trong tập dữliệu gần bằng nhau, và càng thấp
khi một lớp chiếm đa số.
Khi xây dựng cây quyết định, chúng ta cốgắng chia tập dữliệu sao cho entropy sau khi
chia là thấp nhất có thể. Điều này giúp cây quyết định có thểhọc được các quy tắc quyết
định hiệu quảtừdữliệu.
Quyết định vềcách chia tập dữliệu dựa trên entropy thường được thực hiện bằng cách
so sánh entropy trước và sau khi chia, và chọn cách chia mà giảm entropy nhiều nhất.
2.2.1
Diễn giải công thức Entropy
Đểhiểu rõ hơn vềcông thức Entropy chúng ta hãy cùng sơ lược lại cái yếu tốchính và cách
công thức hình thành.
Hình 2: Ví dụ2 gồm 10 viên bi xanh và đỏ
Với ví dụởhình 2 chúng ta có thểthấy đây là một tập dataset gồm 10 viên bi và có 2
label (xanh-đỏ). Với 8 viên bi là xanh và 2 viên bi là đỏta có thểtính xác suất của từng
trường hợp là P(A)= 8
10= 4
5 và P(B) 2
10= 1
5
Từxác suất trên chúng ta có thểtính được độngạc nhiên khi xảy ra trường hợp đó
surprise= 1
P . Vậy độngạc nhiên của trường hợp bi xanh và bi đỏlà surprise(A)= 1
4
5 =1.25 và
surprise(B)= 1
1
1
5
=5.
Công thức surprise:
surprise =
1
P(E)
(3)
• Với P(E) là xác suất xảy ra sựkiện E.
3
Hình 3: Minh hoạđồthịtương quan giữa surprise và log
Như ta có thểthấy ởhình 3, với surprise=
1
P(E) thì surprise ∈[1,∞]. Nếu xác suất là 0%
thì mức độngạc nhiên sẽlà dương vô cùng và với xác suất là 100% thì kết quảlại trảvề
mức độngạc nhiên là 1. Điều này khác vô lý ởvế2 vì với xác suất 100% thì mức độngạc
nhiên nên trảvề0 (Không bất ngờ).
Vì vậy nên chúng ta sửdụng hàm Log đểcó thểđổi cận của surprise ∈[1,∞] thành
surprise ∈[0,∞].
Công thức hàm log khi áp dụng vào surprise:
log(
1
P(E)) = −log(P(E))
(4)
Điều này đã giúp cho công thức trởnên hợp lý hơn khi xác suất xảy ra bằng 100% thì
mức độngạc nhiên sẽtrảvề0.
Từcác suy luận trên ta có công thức trung bình của mức độngạc nhiên:
Entropy = −
c
X
i=1
pi log2(pi)
(5)
4
2.3
Ví dụthực tếvềDecision Tree dùng Entropy (Classify)
2.3.1
Data có biến rời rạc
Ta có một dataset vềchơi tenis, với 2 labels là yes (có chơi tenis) và no(không chơi tenis)
với 14 samples và các cột (outlook, temperature, humidity, wind) là các Features.
Bảng 1: Dữliệu
STT
Outlook
Temperature
Humidity
Wind
PlayTennis
1
Sunny
Hot
High
Weak
No
2
Sunny
Hot
High
Strong
No
3
Overcast
Hot
High
Weak
Yes
4
Rainy
Mild
High
Weak
Yes
5
Rainy
Cool
Normal
Weak
Yes
6
Rainy
Cool
Normal
Strong
No
7
Overcast
Cool
Normal
Strong
Yes
8
Sunny
Mild
High
Weak
No
9
Sunny
Cool
Normal
Weak
Yes
10
Rainy
Mild
Normal
Weak
Yes
11
Sunny
Mild
Normal
Strong
Yes
12
Overcast
Mild
High
Strong
Yes
13
Overcast
Hot
Normal
Weak
Yes
14
Rainy
Mild
High
Strong
No
Chúng ta cần tính Entropy của các Features và tính information gain theo công thức sau.
Hình 4: Công thức tính entropy và gain
Nếu entropy nhánh giảm càng nhiều chúng ta sẽnhận được Gain càng lớn nên ta ưu tiên
chọn Features có Gain lớn nhất.
Với data có [9 yes, 5 no]: Entropy(data)= −5
14.log2( 5
14) −9
14.log2( 9
14) = 0.94.
Tính gain cho feature Outlook:
• Entropy(Sunny)= −2
5.log2(2
5) −3
5.log2(3
5) = 0.971.
• Entropy(Rainy)= −3
5.log2(3
5) −2
5.log2(2
5) = 0.971.
• Entropy(Overcast)= −4
4.log2(4
4) −0
4.log2(0
4) = 0.
5
• Gain(Outlook)= 0.94 −0.971. 5
14 −0.971. 5
14 −0= 0.246.
Tính gain cho feature Temperature:
• Entropy(Hot)= −2
4.log2(2
4) −2
4.log2(2
4) = 1.
• Entropy(Mild)= −4
6.log2(4
6) −2
6.log2(2
6) = 0.918.
• Entropy(Cool)= −3
4.log2(3
4) −1
4.log2(1
4) = 0.811.
• Gain(Outlook)= 0.94 −1. 4
14 −0.918. 6
14 −0.811. 4
14= 0.029.
Tính gain cho feature Humidity:
• Entropy(High)= −3
7.log2(3
7) −4
7.log2(4
7) = 0.985.
• Entropy(Normal)= −6
7.log2(6
7) −1
7.log2(1
7) = 0.592.
• Gain(Humidity)= 0.94 −0.985. 7
14 −0.592. 7
14= 0.151.
Tính gain cho feature Wind:
• Entropy(Weak)= −6
8.log2(6
8) −2
8.log2(2
6) = 0.811.
• Entropy(Strong)= −3
6.log2(3
6) −3
6.log2(3
6) = 1.
• Gain(Wind)= 0.94 −0.811. 8
14 −1. 6
14= 0.048.
Sau khi tính Gain, ta chọn Outlook làm gốc vì Outlook có chỉsốGain lớn nhất (Nghĩa
là entropy giảm nhiều nhất) ta được sơ đồnhư sau:
Hình 5: Nút gốc của cây
ta có thểthấy với Outlook(Overcast) thì tất cảsamples đều là yes thì Entropy=0 ta có
thểtrảvềyes cho Overcast.
6
Đối với 2 thuộc tính Sunny [2 yes, 3 no], Rainy [3 yes, 2 no] ta phải tính thêm các điều
kiện của các features. Bằng cách tính gain lần lượt của Temperature, Humidity và Wind.
Nhưng với điều kiện Outlook phải là Sunny hoặc Rainy. Từdữliệu trên ta có được dataset
ởbảng 2.
Bảng 2: Dữliệu thu gọn
STT
Outlook
Temperature
Humidity
Wind
PlayTennis
1
Sunny
Hot
High
Weak
No
2
Sunny
Hot
High
Strong
No
3
Rainy
Mild
High
Weak
Yes
4
Rainy
Cool
Normal
Weak
Yes
5
Rainy
Cool
Normal
Strong
No
6
Sunny
Mild
High
Weak
No
7
Sunny
Cool
Normal
Weak
Yes
8
Rainy
Mild
Normal
Weak
Yes
9
Sunny
Mild
Normal
Strong
Yes
10
Rainy
Mild
High
Strong
No
Bây giờchúng ta lặp lại các bước tính gain của các Features với điều kiện là Out-
look=Sunny và Outlook=Rainy.
Đối với Outlook là Sunny
Bảng 3: Dữliệu Outlook(Sunny)
STT
Outlook
Temperature
Humidity
Wind
PlayTennis
1
Sunny
Hot
High
Weak
No
2
Sunny
Hot
High
Strong
No
3
Sunny
Mild
High
Weak
No
4
Sunny
Cool
Normal
Weak
Yes
5
Sunny
Mild
Normal
Strong
Yes
Ởđây Entropy tổng sẽlà Entropy của Sunny:
Entropy(Sunny)= −2
5.log2(2
5) −3
5.log2(3
5) = 0.971.
Tính Gain cho Temperature
• Entropy(Cool)= 0.
• Entropy(Mild)= 1.
• Entropy(Hot)= 0.
7
• Gain(Temperature)= 0.571.
Tính Gain cho Humidity
• Entropy(High)= 0.
• Entropy(Normal)= 0.
• Gain(Humidity)= 0.971.
Tính Gain cho Wind
• Entropy(Weak)= 0.918.
• Entropy(Strong)= 1.
• Gain(Wind)= 0.1977.
Sau khi tính Gain, ta chọn Humidity làm gốc vì Humidity có chỉsốGain lớn nhất (Nghĩa
là entropy giảm nhiều nhất).
Hình 6: Tree
ta có thểthấy với Humidity(High) thì tất cảsamples đều là no thì Entropy=0 ta có thể
trảvềno cho High. Tương tựvới Normal sẽtrảvềyes.
8
Bảng 4: Dữliệu Outlook(Rainy)
STT
Outlook
Temperature
Humidity
Wind
PlayTennis
1
Rainy
Mild
High
Weak
Yes
2
Rainy
Cool
Normal
Weak
Yes
3
Rainy
Cool
Normal
Strong
No
4
Rainy
Mild
Normal
Weak
Yes
5
Rainy
Mild
High
Strong
No
Đối với Outlook là Rainy
Ởđây Entropy tổng sẽlà Entropy của Rainy:
Entropy(Rainy)= −3
5.log2(3
5) −2
5.log2(2
5) = 0.971.
Tương tựnhư trên, chúng ta tính được gain lần lượt cho Temperature, Humidity và Wind
là: 0.0202, 0.0202, 0.971
Sau khi tính Gain, ta chọn Wind làm gốc vì Wind có chỉsốGain lớn nhất (Nghĩa là
entropy giảm nhiều nhất).
Hình 7: Final Tree
9
ta có thểthấy với Wind(Strong) thì tất cảsamples đều là no thì Entropy=0 ta có thể
trảvềno cho Strong. Tương tựvới Weak sẽtrảvềyes.
2.3.2
Data có biến liên tục
Ta có 1 tệp data với các biến là độdài của cách hoa với 2 label là 0 và 1.
Bảng 5: Dữliệu liên tục
STT
Petal_Length
Label
1
1
0
2
1.3
0
3
0.9
0
4
1.7
1
5
1.8
1
6
1.2
1
Đầu tiên chúng ta phải sắp xếp dữliệu của tệp data trên từbé đến lớn hoặc từlớn đến
bé, từđó chúng ta có 1 dataset mới như sau. Sau đó chúng ta tiếp tục tính trung bình cộng
của từng đôi.
Ta tính gain của các thuộc tính Trung bình đểtìm gain lớn nhất, với cách tính là xét
các giá trịbé hơn trung bình đang xét và các giá trịlớn hơn trung bình.
Hình 8: Minh hoạTree Data liên tục
Như vậy ta tính gain lần lượt của 0.95, 1.1, 1.25, 1.5, 1.75 và có kết quảlà gain của 1.1
sẽlà lớn nhất, ta chọn điều kiện <=1.1 làm nút gốc.
Tính tương tựnhư ví dụtính cây bằng biến rời rạc, ta được một sơ đồcây cho biến liên
tục như sau.
10
Bảng 6: Dữliệu liên tục (sắp xếp từbé đến lớn theo Petal_Length) và trung bình cộng
STT
Petal_Length
Label
Trung bình
3
0.9
0
1
1
0
0.95
2
1.2
1
1.1
6
1.3
0
1.25
4
1.7
1
1.5
5
1.8
1
1.75
Hình 9: Minh hoạDecison Tree với Data liên tục
11
2.4
Ví dụvềDecision Tree dùng Mean và Variance (Regression)
Chúng ta có một table thểhiện sựtương quan giữa năm kinh nghiệm (Exp) và mức lương
tương ứng (Salary) với năm kinh nghiệm là feature và lương là label.
Bảng 7: Dữliệu liên tục
STT
Experience
Salary
1
1
0
2
1.5
0
3
2
0
4
2.5
0
5
3
60
6
3.5
64
7
4
55
8
4.5
61
9
5
66
10
5.5
83
11
6
93
12
6.5
91
13
7
98
14
7.5
101
Đểcó thểhoàn thành bài toán dựđoán Decision Tree, chúng ta cần tính lần lượt
Mean(data), Variance(Data).
Với Mean và Variance được biểu diễn như sau:
Mean = 1
S
n
X
i=1
Si
(6)
• Mean là giá trịtrung bình.
• S là sốlượng các phần tửtrong tập hợp dữliệu.
• Si là sốlượng các phần tửtrong tập hợp dữliệu.
mse = 1
S
n
X
i=1
(Si −Mean)2
(7)
• mse là phương sai.
• Mean là giá trịtrung bình.
• S là sốlượng các phần tửtrong tập hợp dữliệu.
• Si là sốlượng các phần tửtrong tập hợp dữliệu.
12
Chúng ta lần lượt tách dataset theo hàng và phân chia thành dataset-right và dataset-left
đểcó thểtính amse Mean và mse của left và right, ưu tiên lựa chọn amse có giá trịnhỏnhất.
Sau khi áp dụng công thức ta có được dataset-left và dataset-right với chỉsốamse nhỏ
nhất như sau.
STT
Experience
Salary
1
1
0
2
1.5
0
3
2
0
4
2.5
0
Bảng 8: Dataset-left
STT
Experience
Salary
1
3
60
2
3.5
64
3
4
55
4
4.5
61
5
5
66
6
5.5
83
7
6
93
8
6.5
91
9
7
98
10
7.5
101
Bảng 9: Dataset-right
Lần lượt các chỉsốcủa Mean và Variance của data-left và data-right là: 0,0 và 77.2,
282.35.
Hình 10: Minh hoạcách tính amse
• với L là sốlượng data ởdata-left.
• với R là sốlượng data ởdata-right.
• với S là sốlượng data ởdata gốc.
Lặp lại việc tách và tìm min(amse) chúng ta có được một sơ đồcây với Depth=3 như
sau:
13
Hình 11: Ví dụDecision Tree Regression
Chúng ta có thểtiếp tục tách data đến khi Squared_error đạt 0.0 nhưng vì đểtránh bị
Overfitting nên mô hình dừng ởDepth=3 với chỉsốSquared_error ởmức chấp nhận được.
THAT’S IT, THANK YOU FOR READING ♡♡♡
14
