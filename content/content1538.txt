<<<<<<< HEAD
Node ID: 4aa6d258-6d52-4d95-a94a-1480f4bb3fe9
Text: mean ()) / ( returns . std () + eps ) 38 39 policy_loss = [] 40
for log_prob , disc_return in zip ( saved_log_probs , returns ): 41
policy_loss . append (- log_prob * disc_return ) 42 policy_loss =
torch . cat ( policy_loss ). sum () 43 44 optimizer . zero_grad () 45
policy_loss . backward () 46 optimizer . step () 47 48 if i_episode %
print_eve...
=======
mean ()) / ( returns . std () + eps )
38
39 policy_loss = []
40 for log_prob , disc_return in zip ( saved_log_probs , returns ):
41 policy_loss . append (- log_prob * disc_return )
42 policy_loss = torch . cat ( policy_loss ). sum ()
43
44 optimizer . zero_grad ()
45 policy_loss . backward ()
46 optimizer . step ()
47
48 if i_episode % print_every == 0:
49 print (" Episode {}\ tAverage Score : {:.2 f}". format ( i_episode , np. mean (
scores_deque )))
50
51 return scores
5. Khai báo policy network và optimizer:
1 device = torch . device (’cuda ’ if torch . cuda . is_available () else ’cpu ’)
2
3 policy = Policy (
4 s_size = state_space ,
5 a_size = action_space ,
6 h_size = h_size ,
7 ).to( device )
8
9 optimizer = optim . Adam ( policy . parameters () , lr=lr)
6. Thực hiện huấn luyện:
1 scores = reinforce (
2 policy ,
13
>>>>>>> HEAD@{1}
