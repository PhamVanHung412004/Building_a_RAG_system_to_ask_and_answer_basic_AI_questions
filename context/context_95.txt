Imbalanced Data
Quang-Vinh Dinh
Ph.D. in Computer Science
Year 2024
AI VIETNAM
All-in-One Course
â¢Introduction
â¢Examples and Discussion
â¢Metrics
â¢Case Study
Outline
Introduction
â–Imbalanced Data vs. Balanced Data
AI VIETNAM
All-in-One Course
Negative
100000
Positive
200
Negative
50100
Postive
50100
Imbalanced Data
Balanced Data
Introduction
â–Cat-Dog dataset
AI VIETNAM
All-in-One Course
cats_and_dogs
train
validation
cat
dog
cat
dog
2
VGG16
https://neurohive.io/en/popular-networks/vgg16/
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU
(2x2) max pooling
Dense Layer
+ ReLU
(4096) 
Dense Layer
+ Softmax
(1000) 
3
Introduction
AI VIETNAM
All-in-One Course
Correct prediction
#ğ‘ğ‘ğ‘¡= 1245
#ğ‘‘ğ‘œğ‘”= 1236
cats_and_dogs
train
validation
cat
dog
cat
dog
6000 images
6000 images
1500 images
1500 images
cats_and_dogs
train
validation
cat
dog
cat
dog
1000 images
11000 images
1500 images
1500 images
Correct prediction
#ğ‘ğ‘ğ‘¡= 1052
#ğ‘‘ğ‘œğ‘”= 1443
approximately
4
Introduction
â–Why?
AI VIETNAM
All-in-One Course
Correct prediction (1)
#ğ‘ğ‘ğ‘¡= 1245
#ğ‘‘ğ‘œğ‘”= 1236
Correct prediction (2)
#ğ‘ğ‘ğ‘¡= 1052
#ğ‘‘ğ‘œğ‘”= 1443
5
â¢Introduction
â¢Examples and Discussion
â¢Metrics
â¢Case Study
Outline
Imbalanced Data 
â–Lie/Truth classification
AI VIETNAM
All-in-One Course
Feature
Label
Output
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
ğ‘ƒ(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ‘ƒ(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
à·œğ‘¦
à·œğ‘¦
L(. ) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
6
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
à·œğ‘¦
à·œğ‘¦
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
Feature
Label
Output
ğ¿(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ¿(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
L(y, à·œğ‘¦) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
Imbalanced Data 
â– Lie/Truth classification
AI VIETNAM
All-in-One Course
Imbalanced Data 
â–Lie/Truth classification
AI VIETNAM
All-in-One Course
Feature
Label
Output
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
ğ‘ƒ(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ‘ƒ(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
à·œğ‘¦
à·œğ‘¦
L(. ) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
After a period of time
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
8
Feature
Label
Output
Imbalanced Data 
â–Lie/Truth classification
AI VIETNAM
All-in-One Course
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
ğ‘ƒ(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ‘ƒ(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
à·œğ‘¦
à·œğ‘¦
L(. ) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
A special context
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
9
Feature
Label
Output
Imbalanced Data 
â–Lie/Truth classification
AI VIETNAM
All-in-One Course
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
ğ‘ƒ(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ‘ƒ(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
à·œğ‘¦
à·œğ‘¦
P = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
a more severe context!
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ
ğ‘™ğ‘œğ‘ ğ‘ ğ‘¦=1 = 2.039 
ğ‘™ğ‘œğ‘ ğ‘ ğ‘¦=0 = 1.204 
idea
Model
input (mini-batch)
#C0
#C1
loss
approximately
optimizer update
For C0
For C1
11
idea
Model
input (mini-batch)
#C0
#C1
optimizer update
loss
For C0
For C1
12
solution 1
Model
input (mini-batch)
#C0
#C1
optimizer update
loss
For C0
For C1
Feature
Label
Output
fixed
fixed
fixed
how?
13
solution 2
Model
input (mini-batch)
#C0
#C1
loss
optimizer update
For C0
For C1
Feature
Label
Output
fixed
fixed
fixed
how?
14
Imbalanced Data
â–Approach 1: Data manipulation
AI VIETNAM
All-in-One Course
Positive 
(1000 samples)
Negative 
(11000 samples)
Original Data
Positive
(1000 samples)
Undersampling Data
Negative
(1000 samples)
Positive
(11000 samples)
Oversampling Data
Negative
(11000 samples)
15
solution 3
Model
input (mini-batch)
#C0
#C1
optimizer update
loss
For C0
For C1
fixed
fixed
fixed
y0 & y1
L(. ) = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
ğ¿(à·œğ‘¦) = âˆ’log(à·œğ‘¦)
if y = 1 
ğ¿(à·œğ‘¦) = âˆ’log(1 âˆ’à·œğ‘¦)
if y = 0 
how?
16
Feature
Label
Output
Imbalanced Data 
â–Lie/Truth classification
AI VIETNAM
All-in-One Course
-log(1-à·œğ‘¦)
with y = 0
-log(à·œğ‘¦)
with y = 1
y
à·œğ‘¦
à·œğ‘¦
y
Signal from 
class 0
Signal from 
class 1
Signal balancing
Balanced Signal 
(class 1)
Balanced Signal 
(class 0)
L(. ) = âˆ’ğ›¼1ğ‘¦logà·œğ‘¦âˆ’ğ›¼2 1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Class-weighted Binary cross-entropy
Imbalanced Data 
â–Imbalance data
AI VIETNAM
All-in-One Course
Imbalance Case: 
- 100000 easy samples vs 100 hard samples
easy samples  
loss=0.1
hard samples
loss=2.3  
Easy samples loss = 100000*0.1 = 10000
Hard samples loss = 100*2.3 = 230
Loss = Easy samples loss + Hard samples loss
Easy samples loss : Hard samples loss = 10000:230 â‰ˆ43
BCE khÃ´ng tá»‘t cho trÆ°á»ng há»£p data bá»‹ 
imbalance náº·ng
How to solve it!!!
18
ğ‘¦= âˆ’ğ‘™ğ‘œğ‘”(ğ‘ğ‘¡)
ğ‘¦= (1 âˆ’ğ‘ğ‘¡)0
ğ‘¦= (1 âˆ’ğ‘ğ‘¡)1
ğ‘¦= (1 âˆ’ğ‘ğ‘¡)2
ğ‘¦= (1 âˆ’ğ‘ğ‘¡)3
ğ‘¦= (1 âˆ’ğ‘ğ‘¡)4
Designing a Function
f(à·œğ‘¦) = -log(1-à·œğ‘¦)
with y = 0
loss
à·œğ‘¦
f(à·œğ‘¦) = -log(à·œğ‘¦)
with y = 1
à·œğ‘¦
loss
(a)
(b)
Given 0 â‰¤ğ‘˜â‰¤1
if f à·œğ‘¦âˆ—k
where k approaches 1
â†’f à·œğ‘¦âˆ—k reduces slightly
if f à·œğ‘¦âˆ—k
where k approaches 0
â†’f à·œğ‘¦âˆ—k reduces 
                  significantly
f à·œğ‘¦= 1 âˆ’à·œğ‘¦ğ›¾
f à·œğ‘¦= à·œğ‘¦ğ›¾
Designing a 
Function
f(à·œğ‘¦) = -log(1-à·œğ‘¦)
with y = 0
loss
à·œğ‘¦
f(à·œğ‘¦) = -log(à·œğ‘¦)
with y = 1
à·œğ‘¦
loss
(a)
(b)
Given 0 â‰¤ğ‘˜â‰¤1
if f à·œğ‘¦âˆ—k
where k approaches 1
â†’f à·œğ‘¦âˆ—k reduces slightly
if f à·œğ‘¦âˆ—k
where k approaches 0
â†’f à·œğ‘¦âˆ—k reduces 
                  significantly
Reducing significantly for 
the correct part
Reducing slightly for the 
incorrect part
f à·œğ‘¦= 1 âˆ’à·œğ‘¦ğ›¾
f à·œğ‘¦= à·œğ‘¦ğ›¾
ğ‘“à·œğ‘¦= âˆ’à·œğ‘¦ğ›¾log 1 âˆ’à·œğ‘¦
ğ‘“à·œğ‘¦= âˆ’1 âˆ’à·œğ‘¦ğ›¾logà·œğ‘¦
f(à·œğ‘¦) = -log(1-à·œğ‘¦)
with y = 0
f à·œğ‘¦= à·œğ‘¦ğ›¾
f(à·œğ‘¦) = -log(à·œğ‘¦)
with y = 1
f à·œğ‘¦= 1 âˆ’à·œğ‘¦ğ›¾
ğ‘™ğ‘œğ‘ ğ‘ ğ‘¦=1:  2.039 
0.458 
0.111 
0.028 
0.007 
Applying to our Problem
L(y, à·œğ‘¦, ğ›¾) = âˆ’ğ‘¦1 âˆ’à·œğ‘¦ğ›¾logà·œğ‘¦âˆ’1 âˆ’ğ‘¦à·œğ‘¦ğ›¾log 1 âˆ’à·œğ‘¦
Focal Loss for Dense Object Detection, ICCV, 2017.
ğ‘™ğ‘œğ‘ ğ‘ ğ‘¦=1:  2.039 
0.458 
0.111 
0.028 
0.007 
Combine with Class Weight
L(. ) = âˆ’ğ›¼1ğ‘¦1 âˆ’à·œğ‘¦ğ›¾logà·œğ‘¦âˆ’ğ›¼2 1 âˆ’ğ‘¦à·œğ‘¦ğ›¾log 1 âˆ’à·œğ‘¦
Focal Loss for Dense Object Detection, ICCV, 2017.
solution 5
Parameter 
Space
Optimal Region
solution 5
convergence 1
solution 5
convergence 2
solution 5
Using
Pretrained
Model
solution 5
Model
input (mini-batch)
#C0
#C1
optimizer update
loss
For C0
For C1
Super Big 
Dataset
pretraining (~transfer learning)
fixed
fixed
fixed
29
Exploitation of Pretrained Models
â–As Initialization
AI VIETNAM
All-in-One Course
ImageNet dataset 
(1.2 million images 
of 1000 categories)
Feature 
Extraction
Classifier
Large 
dataset
Feature 
Extraction
New 
Classifier
Small 
dataset
Copy FE and its 
pretrained weights
Pretrained Model
Our Model
Train the whole new model using a small dataset
30
Will be trained with the small dataset
Exploitation of Pretrained Models
â–Use the pretrained weights 
as an initialization
AI VIETNAM
All-in-One Course
31
Imbalanced Data
â–Approach 2: Loss Functions
AI VIETNAM
All-in-One Course
Total loss
Negative loss
Positive loss
Total loss
Pay more attention to samples from 
an under-represented class
A higher loss â†’ higher optimization
ğ‘¤ğ‘= ğ‘
2ğ‘ğ‘
Class weight 
Focal loss
FL(pt) = âˆ’ğ›¼ğ‘¡(1 âˆ’ğ‘ğ‘¡)ğ›¾log(pt)
ğ›¼ğ‘¡balances losses using a number of
samples in a class 
ğ›¾helps the loss funcion focus on 
â€˜hardâ€™  samples 
32
Imbalanced Data
â–Solution 6
AI VIETNAM
All-in-One Course
AutoBalance: Optimized Loss Functions for Imbalanced Data, 2022 
https://arxiv.org/pdf/2201.01212.pdf
33
â¢Introduction
â¢Examples and Discussion
â¢Metrics
â¢Case Study
Outline
Metrics
â–Confusion matrix
AI VIETNAM
All-in-One Course
Predictions
Wrong
Predictions
Correct
Predictions
Model
Positive data
Negative data
Found Data
Unfound Data
Wrongly
Discovered Data
Correctly
Undiscovered Data
34
Predictions
Wrong
Predictions
Correct
Predictions
Model
Positive data
Negative data
Found Data
Unfound Data
Wrongly
Discovered Data
Correctly
Undiscovered Data
Precision
Found 
Data
Unfound 
Data
Recall
Precision=1?
Recall=1?
1
2
Wrong
Predictions
Correct
Predictions
Metrics
â–Precision and recall
Prediction
Result
True Positive (TP): A correct positive detection 
False Positive (FP): A wrong positive detection 
False Negative (FN): A wrong negative detection
True Negative (TN): A correct negative detection
Label
Precision =
TP
TP + FP =
Sá»‘ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c
Tá»•ng sá»‘ láº§n dá»± Ä‘oÃ¡n positive
Recall =
TP
TP + FN =
Sá»‘ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c
Tá»•ng sá»‘ ground truth cho positive
Precision
Found 
Data
Unfound 
Data
Recall
Wrong
Predictions
Correct
Predictions
AI VIETNAM
All-in-One Course
36
Metrics
AI VIETNAM
All-in-One Course
Actual:
NEGATIVE
Actual:
POSITIVE
Predicted:
NEGATIVE
Predicted:
POSITIVE
â– Quiz
Fill TP, FP, TN, FN into appropriate cells
37
Metrics
AI VIETNAM
All-in-One Course
â– Precision
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘ 
â– Recall
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
Accuracy =
TP + TN
TP + TN + FP + FN
â– Accuracy
Metrics
AI VIETNAM
All-in-One Course
Accuracy = 145/165
Precision: When it predicts yes, how often is it 
correct?
TP/predicted yes = 95/105 = 0.9
Recall: When it's actually yes, how often does it 
predict yes?
TP/actual yes = 95/105 = 0.90
n = 165
Predicted:
NEGATIVE
Predicted:
POSITIVE
Actual:
NEGATIVE
TN=50
FP=10
60
Actual:
POSITIVE
FN=10
TP=95
105
60
105
â– Precision
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘ 
â– Recall
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
Accuracy =
TP + TN
TP + TN + FP + FN
â– Accuracy
39
Metrics
AI VIETNAM
All-in-One Course
Accuracy = 145/165
Precision: When it predicts yes, how often is it 
correct?
TP/predicted yes = 85/85 = 1.0
Recall: When it's actually yes, how often does it 
predict yes?
TP/actual yes = 85/105 = 0.81
n = 165
Predicted:
NEGATIVE
Predicted:
POSITIVE
Actual:
NEGATIVE
TN=60
FP=0
60
Actual:
POSITIVE
FN=20
TP=85
105
80
85
â– Precision
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘ 
â– Recall
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
Accuracy =
TP + TN
TP + TN + FP + FN
â– Accuracy
40
Metrics
AI VIETNAM
All-in-One Course
Accuracy = 145/165
Precision: When it predicts yes, how often is it 
correct?
TP/predicted yes = 105/125 = 0.84
Recall: When it's actually yes, how often does it 
predict yes?
TP/actual yes = 105/105 = 1.0
n = 165
Predicted:
NEGATIVE
Predicted:
POSITIVE
Actual:
NEGATIVE
TN=40
FP=20
60
Actual:
POSITIVE
FN=0
TP=105
105
60
105
â– Precision
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘ 
â– Recall
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘=
ğ‘‡ğ‘ƒ
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
Accuracy =
TP + TN
TP + TN + FP + FN
â– Accuracy
41
Metrics
AI VIETNAM
All-in-One Course
â– Combine precision and recall
2
F1
=
1
precision +
1
recall
F1 = precision + recall
2
Metrics
â–F1 Score
Accuracy =
TP + TN
TP + TN + FP + FN
Precision =
TP
TP + FP
Recall =
TP
TP + FN
F1 = 2 âˆ—Precision âˆ—Recall
Precision + Recall
2
F1
=
1
precision +
1
recall
F1 = 2
1
1
precision +
1
recall
= 2 precion â‹…recall
precision + recall
Precision
Recall
F1
1
1
1
0.1
0.1
0.1
0.5
0.5
0.5
1
0.1
0.182
0.3
0.8
0.36
AI VIETNAM
All-in-One Course
Example 
â–Cat-Dog dataset
AI VIETNAM
All-in-One Course
Validation data (3000 samples)
n=3000
Predicted:
NEGATIVE
Predicted:
POSITIVE
Actual:
NEGATIVE
TN=1489
FP=11
1500
Actual:
POSITIVE
FN=679
TP=821
1500
2168
832
=
821 
821 + 679 â‰ˆ0.547
Recall =
TP 
TP + FN
Precision =
TP 
TP + FP =
821 
821 + 11 â‰ˆ0.987
F1 = 2 âˆ—Recall âˆ—Precision
Recall + Precision
= 2 âˆ—0.547 âˆ—0.987 
0.547 âˆ—0.987
â‰ˆ0.704
Correct prediction
#ğ‘ğ‘ğ‘¡ = 821
#ğ‘‘ğ‘œğ‘”= 1489
46
Cat
(1500 samples)
Dog
(1500 samples)
Cat is of the positive class
â¢Introduction
â¢Examples and Discussion
â¢Metrics
â¢Case Study
Outline
Experiments
â–Cat-Dog dataset
AI VIETNAM
All-in-One Course
cats_and_dogs
train
validation
cat
dog
cat
dog
47
Experiments
â–Cat-Dog dataset
AI VIETNAM
All-in-One Course
Cat
(1000 samples)
Imbalanced Data 1
Dog
(11000 samples)
Imbalanced Data 2
Cat
(500 samples)
Dog
(11000 samples)
Balanced Data
Cat
(11000 samples)
Dog
(11000 samples)
Validation data (3000 samples)
48
VGG16
https://neurohive.io/en/popular-networks/vgg16/
(3x3) Convolution
padding=â€˜sameâ€™
stride=1 + ReLU
(2x2) max pooling
Dense Layer
+ ReLU
(4096) 
Dense Layer
+ Softmax
(1000) 
50
Experiments
â–Cat-Dog dataset: Results from the TF codes
AI VIETNAM
All-in-One Course
Balanced Data
Cat
(11000 samples)
Dog
(11000 samples)
Validation data (3000 samples)
ğ¿ğ‘= ğ¿ğ‘+ ğ¿ğ‘‘
ğ¿ğ‘= ğ¿ğ‘+ 100 Ã— ğ¿ğ‘‘
ğ¿ğ‘= ğ¿ğ‘+ 1000 Ã— ğ¿ğ‘‘
Correct prediction
Correct prediction
Correct prediction
#ğ‘ğ‘ğ‘¡= 1445
#ğ‘‘ğ‘œğ‘”= 1436
#ğ‘ğ‘ğ‘¡= 1076
#ğ‘‘ğ‘œğ‘”= 1499
#ğ‘ğ‘ğ‘¡= 670
#ğ‘‘ğ‘œğ‘”= 1498
Balanced Loss
Imbalanced Loss 1
Imbalanced Loss 2
ğ¹1 = 0.96
ğ¹1 = 0.835
ğ¹1 = 0.617
Experiments
â–Cat-Dog dataset
Cat
(1000 samples)
Imbalanced Data 1
Dog
(11000 samples)
Validation data (3000 samples)
ğ¿ğ‘= ğ¿ğ‘+ ğ¿ğ‘‘
ğ¿ğ‘= 6 Ã— ğ¿ğ‘+ 0.55 Ã— ğ¿ğ‘‘
Correct prediction
Correct prediction
#ğ‘ğ‘ğ‘¡= 1082
#ğ‘‘ğ‘œğ‘”= 1483
#ğ‘ğ‘ğ‘¡= 1163
#ğ‘‘ğ‘œğ‘”= 1379
Balanced Loss
Imbalanced Loss
ğ¹1 = 0.833
ğ¹1 = 0.835
Focal loss
#ğ‘ğ‘ğ‘¡= 1210
#ğ‘‘ğ‘œğ‘”= 1447
ğ¹1  = 0.876
Oversampling Data
#ğ‘ğ‘ğ‘¡= 1167
#ğ‘‘ğ‘œğ‘”= 1438
ğ¹1 = 0.855
FL pt = âˆ’ğ›¼ğ‘¡(1 âˆ’ğ‘ğ‘¡)ğ›¾log(pt)
ğ›¾= 2.0
ğ›¼ğ‘¡= 0.5
Results from the TF codes
Experiments
â–Cat-Dog dataset
Validation data (3000 samples)
Imbalanced Data 2
Cat
(500 samples)
Dog
(11000 samples)
ğ¿ğ‘= ğ¿ğ‘+ ğ¿ğ‘‘
Correct prediction
#ğ‘ğ‘ğ‘¡= 821
#ğ‘‘ğ‘œğ‘”= 1489
Balanced Loss
ğ¹1 = 0.704
ğ¿ğ‘= 11.5 Ã— ğ¿ğ‘+ 0.52 Ã— ğ¿ğ‘‘
Correct prediction
#ğ‘ğ‘ğ‘¡= 1123
#ğ‘‘ğ‘œğ‘”= 1309
Imbalanced Loss
ğ¹1 = 0.798
Correct prediction
Focal Loss
#ğ‘ğ‘ğ‘¡= 1210
#ğ‘‘ğ‘œğ‘”= 1447
ğ¹1  = 0.876
FL pt = âˆ’ğ›¼ğ‘¡(1 âˆ’ğ‘ğ‘¡)ğ›¾log(pt)
ğ›¾= 2.0
ğ›¼ğ‘¡= 0.5
Correct prediction
Oversampling
#ğ‘ğ‘ğ‘¡= 1159
#ğ‘‘ğ‘œğ‘”= 1386
ğ¹1 = 0.836
Results from the TF codes
Experiments
â–Cat-Dog dataset: Using Resnet and Pytorch
AI VIETNAM
All-in-One Course
Cat
(1000 samples)
Imbalanced Data 1
Dog
(11000 samples)
Using the pretrained model
ğ¹1 = 0.96
Focal Loss
ğ¹1 =? ? ?
Normal cross-entropy
ğ¹1 = 0.67
Class weight
ğ¹1 = 0.75
54
