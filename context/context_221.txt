From Text Generation
to Machine Translation
Year 2024
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
â¢Text Generation Using Transformer
â¢An improved Approach to Text Generation
â¢Machine Translation Using RNN
â¢Machine Translation Using Transformer
Outline
Self-Supervision Using Text Data
â– Encode the sequential relationship
natural language processing is a branch of artificial intelligence
natural
language
X
y
Text Model
natural
language
language
à·œğ‘¦
ğ‘¦
loss
update
1
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
Ä‘i
há»c
ai
data
ai
Model
Output
Loss
optimizer
Label
Linear
Ä‘i
há»c
input
ai
label
Using all the features
Implementation Using RNN
2
Copus
Vocab
Id
Text
0
Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y
1
CÃ³ chÃ­ thÃ¬ nÃªn
Tokenizer
Ä‚n
quáº£
cÃ¢y
trá»“ng
káº»
nhá»›
cÃ³
chÃ­
thÃ¬
nÃªn
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
Build vocab
<unk>
<pad>
<sos>
Special tokens
Add
Example
vocab_size = 13
Id
Text
0
Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y
1
CÃ³ chÃ­ thÃ¬ nÃªn
Input tokens
Target token
<sos>
Ä‚n
<sos> Ä‚n
quáº£
<sos> Ä‚n quáº£
nhá»›
<sos> Ä‚n quáº£ nhá»›
káº»
<sos> Ä‚n quáº£ nhá»› káº»
trá»“ng
<sos> Ä‚n quáº£ nhá»› káº» trá»“ng
cÃ¢y
<sos> 
CÃ³
<sos> CÃ³
chÃ­
<sos> CÃ³ chÃ­
thÃ¬
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Next token prediction dataset
4
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
vocab_size = 13
Id
Text
0
Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y
1
CÃ³ chÃ­ thÃ¬ nÃªn
Input tokens
Target token
<sos>
Ä‚n
<sos> Ä‚n
quáº£
<sos> Ä‚n quáº£
nhá»›
<sos> Ä‚n quáº£ nhá»›
káº»
<sos> Ä‚n quáº£ nhá»› káº»
trá»“ng
<sos> Ä‚n quáº£ nhá»› káº» trá»“ng
cÃ¢y
<sos> 
CÃ³
<sos> CÃ³
chÃ­
<sos> CÃ³ chÃ­
thÃ¬
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Next token prediction dataset
data_x_ids
data_y_ids
[2, 1, 1, 1, 1, 1]
12
[2, 12, 1, 1, 1, 1]
9
[2, 12, 9, 1, 1, 1]
7
[2, 12, 9, 7, 1, 1]
6
[2, 12, 9, 7, 6, 1]
11
[2, 12, 9, 7, 6, 11]
4
[2, 1, 1, 1, 1, 1]
5
[2, 5, 1, 1, 1, 1]
3
[2, 5, 3, 1, 1, 1]
10
[2, 5, 3, 10, 1, 1]
8
padding
Vocab
Training data
Example
4
sequence_length = 6
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
Token
Id
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
Id
Text
0
Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y
1
CÃ³ chÃ­ thÃ¬ nÃªn
Input tokens
Target token
<sos>
Ä‚n
<sos> Ä‚n
quáº£
<sos> Ä‚n quáº£
nhá»›
<sos> Ä‚n quáº£ nhá»›
káº»
<sos> Ä‚n quáº£ nhá»› káº»
trá»“ng
<sos> Ä‚n quáº£ nhá»› káº» trá»“ng
cÃ¢y
<sos> 
CÃ³
<sos> CÃ³
chÃ­
<sos> CÃ³ chÃ­
thÃ¬
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Next token prediction dataset
data_x_ids
data_y_ids
[2, 1, 1, 1, 1, 1]
12
[2, 12, 1, 1, 1, 1]
9
[2, 12, 9, 1, 1, 1]
7
[2, 12, 9, 7, 1, 1]
6
[2, 12, 9, 7, 6, 1]
11
[2, 12, 9, 7, 6, 11]
4
[2, 1, 1, 1, 1, 1]
5
[2, 5, 1, 1, 1, 1]
3
[2, 5, 3, 1, 1, 1]
10
[2, 5, 3, 10, 1, 1]
8
padding
Vocab
Training data
Example
4
sequence_length = 6
data_x_ids
[2, 1, 1, 1, 1, 1]
Model
(RNN / Transformer)
predictions
[â€¦, â€¦, â€¦ â€¦ â€¦, â€¦ ]
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
vocab_size = 13
sequence_length = 6
?
Example
data_x_ids
[2, 1, 1, 1, 1, 1]
Model
(RNN / Transformer)
predictions
[â€¦, â€¦, â€¦ â€¦ â€¦, â€¦ ]
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
vocab_size = 13
sequence_length = 6
Example
Training
data_x_ids
[2, 1, 1, 1, 1, 1]
[2, 12, 1, 1, 1, 1]
[2, 12, 9, 1, 1, 1]
[2, 12, 9, 7, 1, 1]
[2, 12, 9, 7, 6, 1]
[2, 12, 9, 7, 6, 11]
[2, 1, 1, 1, 1, 1]
[2, 5, 1, 1, 1, 1]
[2, 5, 3, 1, 1, 1]
[2, 5, 3, 10, 1, 1]
Model
(RNN / Transformer)
data_y_ids
12
9
7
6
11
4
5
3
10
8
predicted ids
12
9
7
6
12
4
5
3
9
8
CrossEntropy
Loss
Update params
Example
AI VIETNAM
All-in-One Course
5
Input tokens
Target token
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Input ids
Target ids
[2, 5, 3, 10]
[8]
Embedding 
layer
2
5
3
10
ğ‘‹1 = âˆ’0.7521
1.6487
âˆ’0.3925
âˆ’1.4036
ğ‘‹2 = âˆ’0.7581
1.0783
0.8008
1.6806
ğ‘‹3 = âˆ’0.7279
âˆ’0.5594
âˆ’0.7688
0.7624
ğ‘‹4 = âˆ’0.8371
âˆ’0.9224
1.8113
0.1606
0.3035
âˆ’0.2523
0.2980
0.4578
âˆ’0.1187
0.1524
0.3399
âˆ’0.1687
0.2860
0.1057
âˆ’0.3626
âˆ’0.1773
âˆ’0.3885
âˆ’0.1275
âˆ’0.2669
âˆ’0.4838
ğ‘¾ğ’‰ğ’‰
âˆ’0.2863
0.1249
âˆ’0.0660
âˆ’0.3629
ğ‘ğ‘–â„
ğ‘¾ğ’Šğ’‰
âˆ’0.2982
0.4811
âˆ’0.3363
âˆ’0.2582
âˆ’0.2982
âˆ’0.4126
0.2025
âˆ’0.3409
0.4497
âˆ’0.4959
0.1790
0.2653
0.1666
âˆ’0.3912
0.4155
âˆ’0.2021
0.0117
âˆ’0.3415
âˆ’0.4242
âˆ’0.2753 
ğ‘â„â„
â„0
â„1
â„2
â„3
ğ‘‹1
0
0
0
0
ğ’ƒğ’Šğ’‰
â„4
ğ‘‹4
ğ‘‹3
ğ‘‹2
RNN
ğ’ƒğ’‰ğ’‰
ğ‘¾ğ’Šğ’‰
ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’Šğ’‰
ğ‘¾ğ’Šğ’‰
ğ‘¾ğ’Šğ’‰
ğ’ƒğ’‰ğ’‰
ğ’ƒğ’‰ğ’‰
ğ’ƒğ’‰ğ’‰
â„1 = âˆ’0.7409
âˆ’0.4739
âˆ’0.5055
âˆ’0.6786
â„2 = 0.2170
âˆ’0.9590
0.6681
âˆ’0.6519
â„3 = 0.4735
âˆ’0.2915
âˆ’0.4692
âˆ’0.1583
â„4 = 0.8327
âˆ’0.8839
0.2449
0.6446
Linear (16, 13)
Flatten
ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’‰ğ’‰
ğ‘¾ğ’‰ğ’‰
ğ’ƒğ’Šğ’‰
ğ’ƒğ’Šğ’‰
ğ’ƒğ’Šğ’‰
Feed Forward
6
sequence_length = 4
hidden_dim = 4
FC
Probability
0.1121
0.1334
0.0742
0.0896
0.0648
0.0569
0.0956
0.0744
0.0751
0.0809
0.0636
0.0490
0.0304
Target
0
0
0
0
0
0
0
0
1
0
0
0
0
CrossEntropyLoss (L)
Loss
0.0896
âˆ’0.0079
0.0049
âˆ’0.0619
âˆ’0.0204
âˆ’0.0960
âˆ’0.1445
0.0827
0.0843
0.0819
âˆ’0.0518
âˆ’0.1736
0.0201
âˆ’0.0723
âˆ’0.1252
0.0291
ğ›ğ‘¾ğ’‰ğ’‰ğ‘³
ğ›ğ‘¾ğ’Šğ’‰ğ‘³
0.0187
âˆ’0.1285
âˆ’0.2816
âˆ’0.0437
âˆ’0.1589
0.1600
0.0904
0.1782
âˆ’0.0991
âˆ’0.1825
0.1933
0.2889
âˆ’0.0988
âˆ’0.0411
0.1014
âˆ’0.0192
âˆ’0.0235
0.1788
0.3628
0.0449
ğ›ğ‘â„â„ğ‘³
âˆ’0.0235
0.1788
0.3628
0.0449
ğ›ğ‘ğ‘–â„ğ‘³
2.5884
backward
backward
Back-Propagation
data_x_ids
[2, 1, 1, 1, 1, 1]
Model
(Transformer)
predictions
[â€¦, â€¦, â€¦ â€¦ â€¦, â€¦ ]
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
vocab_size = 13
sequence_length = 6
Example
Embedding
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Masked 
Multi-head 
Attention
Add & Norm
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
N
Ã—
Embedding
ğ·0
ğ·2
ğ·ğ‘›
ğ‘Œ0
ğ‘Œ2
ğ‘Œğ‘›
a word 
vector
â€¦
â€¦
ğ·1
ğ‘Œ1
a word 
vector
a word 
vector
a word 
vector
N
Ã—
Ä‘i
há»c
3
4
index
word
0
[UNK]
1
[pad]
2
ai
3
Ä‘i
4
há»c
â€¦
â€¦
Embedding
index
Embedding
0
[-0.188, â€¦,  0.7013]
1
[1.7840â€¦,  1.3586]
2
[1.0281, â€¦,  0.4211]
3
[-1.308,  â€¦, -0.3680]
4
[0.2293,  â€¦,  2.0501]
â€¦
â€¦
Transformer Encoder
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Encoder 
in Pytorch
(N, 2, 3)
(N, 2, 3)
15
Ä‘i
há»c
3
4
Embedding
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Encoder 
in Pytorch
(N, 2, 3)
(N, 2, 3)
[0.69,  0.72, -1.41]
[0.21,  1.10, -1.31]
[0.97, 0.39, -1.37]
[0.58, 0.82, -1.40]
Transformer Encoder
16
Embedding
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Masked 
Multi-head 
Attention
Add & Norm
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
N
Ã—
Embedding
ğ·0
ğ·2
ğ·ğ‘›
ğ‘Œ0
ğ‘Œ2
ğ‘Œğ‘›
a word 
vector
â€¦
â€¦
ğ·1
ğ‘Œ1
a word 
vector
a word 
vector
a word 
vector
N
Ã—
Ä‘i
há»c
ai
3
4
2
index
word
0
[UNK]
1
[pad]
2
ai
3
Ä‘i
4
há»c
â€¦
â€¦
Embedding
index
Embedding
0
[-0.188, â€¦,  0.7013]
1
[1.7840â€¦,  1.3586]
2
[1.0281, â€¦,  0.4211]
3
[-1.308,  â€¦, -0.3680]
4
[0.2293,  â€¦,  2.0501]
â€¦
â€¦
Transformer Encoder
Masked 
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Masked Encoder 
in Pytorch
(N, 3, 3)
(N, 3, 3)
17
Ä‘i
há»c
ai
3
4
2
Embedding
Masked 
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Masked Encoder 
in Pytorch
(N, 3, 3)
(N, 3, 3)
[0.69,  0.72, -1.41]
[0.21,  1.10, -1.31]
[-0.88,  0.60, -0.31]
[0.97, 0.39, -1.37]
[0.58, 0.82, -1.40]
[-0.85, 1.40, -0.54]
Transformer Encoder
18
ğ‘‹= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
ğ‘Šğ‘„=
âˆ’0.35 
0.51 
0.50
 0.36 âˆ’0.47 âˆ’0.29
âˆ’0.51 âˆ’0.14 âˆ’0.56
ğ‘Šğ¾=
âˆ’0.49 
âˆ’0.68 0.18
âˆ’0.44 
âˆ’0.46 0.18
 0.07 
âˆ’0.10 0.44
ğ‘Šğ‘‰=
âˆ’0.41 
0.39 âˆ’0.65
âˆ’0.40 âˆ’0.07 âˆ’0.34
âˆ’0.55 âˆ’0.13 âˆ’0.29
ğ‘Šğ‘‚=
âˆ’0.36 âˆ’0.08 0.32
 0.27 
0.05 0.15
âˆ’0.05 âˆ’0.28 0.05
ğ‘„= ğ‘‹ğ‘Šğ‘„= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
âˆ’0.35 
0.51 
0.50
 0.36 âˆ’0.47 âˆ’0.29
âˆ’0.51 âˆ’0.14 âˆ’0.56
ğ¾= ğ‘‹ğ‘Šğ¾= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
âˆ’0.49 
âˆ’0.68 
0.18
âˆ’0.44 
âˆ’0.46 
0.18
 0.07 
âˆ’0.10 
0.44
ğ‘‰= ğ‘‹ğ‘Šğ‘‰= âˆ’0.1 
0.1 
0.3
 0.4 âˆ’1.1 âˆ’0.3
âˆ’0.41 
0.39 âˆ’0.65
âˆ’0.40 âˆ’0.07 âˆ’0.34
âˆ’0.55 âˆ’0.13 âˆ’0.29
head = 1
Masked Multi-
head Attention
ğ‘€= 0 
âˆ’âˆ
0 
0
= âˆ’0.08 âˆ’0.14 âˆ’0.24
âˆ’0.39 0.77 0.69
= 0.02 âˆ’0.01 0.13
0.27 0.27 âˆ’0.26
= âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 0.05
12
approximately
Masked Multi-head Attention
AI VIETNAM
All-in-One Course
â– Example 
ğ‘Œ= ğ´ğ‘Šğ‘‚= âˆ’0.16 âˆ’0.08 âˆ’0.05
 0.12 
0.08 
0.06
âˆ’0.36 âˆ’0.08 0.32
 0.27 
0.05 0.15
âˆ’0.05 âˆ’0.28 0.05
=
0.03 0.02 âˆ’0.06
âˆ’0.02 âˆ’0.02 0.05
ğ´= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ğ‘„ğ¾ğ‘‡
ğ‘‘
+ ğ‘€ğ‘‰
= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥
âˆ’0.08 âˆ’0.14 âˆ’0.24
âˆ’0.39 
0.77 
0.69
 0.02 
0.27
âˆ’0.01 
0.27
 0.13 âˆ’0.26
1
ğ‘‘
+ 0 
âˆ’âˆ
0 
0
âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 
0.05
=
1.0 
0.0
0.52 0.48
âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 
0.05 = âˆ’0.16 âˆ’0.08 âˆ’0.05
 0.12 
0.08 0.06
= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥
âˆ’0.019 
0.002
 0.043 
âˆ’0.046 + 0 
âˆ’âˆ
0 
0
âˆ’0.16 âˆ’0.08 âˆ’0.05
âˆ’0.02 âˆ’0.02 
0.05
n
d
n
d
d
d
n
n
n
n
n
ğ‘‘ğ‘
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘‘ğ‘
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘Šğ‘
ğ‘Šğ‘˜
ğ‘Šğ‘£
Q
K
V
ğ‘„ğ¾ğ‘‡
Z
ğ‘‘ğ‘= ğ‘‘ğ‘˜= ğ‘‘ğ‘£
Embedding size
Sequence length
Input
ğ‘‘ğ‘£
0
âˆ’âˆ
âˆ’âˆ
âˆ’âˆ
âˆ’âˆ
0
0
âˆ’âˆ
âˆ’âˆ
âˆ’âˆ
0
0
0
âˆ’âˆ
âˆ’âˆ
0
0
0
0
âˆ’âˆ
0
0
0
0
0
ğ‘€ğ‘ğ‘ ğ‘˜
Softmax
A
n
n
Masked-Attention
Training process
14
Embedding
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Masked 
Multi-head 
Attention
Add & Norm
Multi-head 
Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Softmax
N
Ã—
Embedding
N
Ã—
(N, 2, 3)
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
query
value
key
20
Transformer in PyTorch
AI VIETNAM
All-in-One Course
â– Transformer Encoder 
(N, 2, 3)
Masked 
Multi-head 
Self-Attention
Add & 
Norm
(N, 3, 3)
Masked 
Multi-head 
Self-Attention
query
value
key
query
value
key
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
Masked 
Multi-head 
Self-Attention
Add & 
Norm
(N, 3, 3)
Masked 
Multi-head 
Self-Attention
query
value
key
query
value
key
query
value
key
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Positional 
Embedding
Output Embedding
+
Masked 
Multi-head 
Self-Attention
Add & Norm
Feed Forward
Add & Norm
Linear
Decoder
NÃ—
Text Generation
AI VIETNAM
All-in-One Course
â– Architecture
interesting
Learning AI is
Text Encoding
Softmax
Dataset
(x, y)
19
data_x_ids
[2, 1, 1, 1, 1, 1]
Model
(Transformer)
predictions
[â€¦, â€¦, â€¦ â€¦ â€¦, â€¦ ]
Token
Id
<unk>
0
<pad>
1
<sos>
2
chÃ­
3
cÃ¢y
4
cÃ³
5
káº»
6
nhá»›
7
nÃªn
8
quáº£
9
thÃ¬ 
10
trá»“ng
11
Äƒn
12
vocab_size = 13
sequence_length = 6
Example
Input tokens
Target token
<sos> CÃ³ chÃ­ thÃ¬ 
nÃªn
Input ids
Target ids
[2, 5, 3, 10]
[8]
Embedding layer
ğ‘‹1 = âˆ’0.7521
1.6487
âˆ’0.3925
âˆ’1.4036
ğ‘‹2 = âˆ’0.7581
1.0783
0.8008
1.6806
ğ‘‹3 = âˆ’0.7279
âˆ’0.5594
âˆ’0.7688
0.7624
ğ‘‹4 = âˆ’0.8371
âˆ’0.9224
1.8113
0.1606
Masked Multi-
Head Attention
Feed Forward
Add & Norm
Add & Norm
Linear
2
5
3
10
0.3035
âˆ’0.2523
0.2980
0.4578
âˆ’0.1187
0.1524
0.3399
âˆ’0.1687
0.2860
0.1057
âˆ’0.3626
âˆ’0.1773
âˆ’0.3885
âˆ’0.1275
âˆ’0.2669
âˆ’0.4838
ğ‘¾ğ’Œ_ğ’‘ğ’“ğ’ğ’‹
0.0782
0.0014
âˆ’0.3432
0.4192
âˆ’0.4400
âˆ’0.1861
âˆ’0.2917
âˆ’0.0992
âˆ’0.2154
âˆ’0.0346
âˆ’0.1711
0.4302
âˆ’0.2993
âˆ’0.3388
âˆ’0.3946
0.1558
ğ‘¾ğ’_ğ’‘ğ’“ğ’ğ’‹
âˆ’0.2863
0.0117
âˆ’0.4376
0.1541
0.1249
âˆ’0.3415
âˆ’0.3184
âˆ’0.4663
âˆ’0.0660
âˆ’0.4242
0.4998
âˆ’0.3284
âˆ’0.3629
âˆ’0.2753
0.0944
âˆ’0.1664
ğ‘¾ğ’—_ğ’‘ğ’“ğ’ğ’‹
Linear (fc2)
Feed Forward
Linear (fc1)
ReLU
Transformer
âˆ’0.2982
0.4811
âˆ’0.3363
âˆ’0.2582
âˆ’0.2982
âˆ’0.4126
0.2025
âˆ’0.3409
0.4497
âˆ’0.4959
0.1790
0.2653
0.1666
âˆ’0.3912
0.4155
âˆ’0.2021
ğ‘¾ğ’’_ğ’‘ğ’“ğ’ğ’‹
ğ‘¾ğ’‡ğ’„_ğŸ
0.2792
âˆ’0.3735
0.1161
0.2610
âˆ’0.1278
0.1783
0.2583
0.2628
âˆ’0.2853
0.3870
0.0907
0.1870
âˆ’0.1712
âˆ’0.4707
âˆ’0.1781
âˆ’0.0879
âˆ’0.1324
0.3196
0.0073
âˆ’0.4541
0.0535
0.4297
âˆ’0.0299
âˆ’0.1845
âˆ’0.0883
âˆ’0.0495
0.1202
0.4211
âˆ’0.1490
âˆ’0.1119
0.1401
0.1948
âˆ’0.3125
âˆ’0.2288
âˆ’0.2465
âˆ’0.0035
âˆ’0.2126
0.2581
âˆ’0.3489
âˆ’0.2689
âˆ’0.0558
âˆ’0.1608
âˆ’0.2871
âˆ’0.3029
0.3420
âˆ’0.0709
0.2636
âˆ’0.3307
0.0511
âˆ’0.3517
0.1697
0.1447
âˆ’0.0916
0.2366
0.2975
âˆ’0.1736
0.1463
0.2679
0.1852
âˆ’0.0712
âˆ’0.1347
0.1289
0.0895
âˆ’0.2035
ğ–ğŸğœ_ğŸ
Linear
Linear
Linear
Scaled Dot-Product 
Attention
Linear
V
K
Q
Masked Multi-Head Attention
Decoder
â¢Text Generation Using Transformer
â¢An improved Approach to Text Generation
â¢Machine Translation Using RNN
â¢Machine Translation Using Transformer
Outline
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
há»c
Ä‘i
há»c
input
Ä‘i
há»c
ai
data
há»c
ai
label
ai
há»c
Model
Output
Loss
optimizer
Label
An improved approach
34
*Linear can be added (where?)
RNN Cell
RNN Cell
ho
h1
Wih
Whh
bih
Wih
Whh
bih
h1
h2
bhh
bhh
ai
há»c
Ä‘i
4
3
há»c
[-0.88,  0.81,  0.77]
[-1.27,  0.84,  0.04]
Embedded Input
[0.25,   0.05,  0.17]
[-0.11,  0.35,  0.22]
[0.44, -0.22, -0.42]
[0.38,  0.36, -0.01]
[-0.08,  0.68, -0.08, -0.53]
[-0.30,  0.77, -0.15, -0.58]
Output
[-0.30,  0.77, -0.15, -0.58]
Hidden
[0.38, -0.06,  0.37, -0.19]
[-0.16,  0.42,  0.08, -0.02]
[-0.36, -0.01,  0.17,  0.39]
[-0.43,  0.09,  0.33,  0.03]
Wih
[-0.47,  0.13,  0.46, -0.15]
bih
Whh
[0.43,  0.16,  0.34, -0.39]
bhh
seq_len = 2
An improved approach
*Linear can be added
hidden_dim = vocab_size = 4
RNN Cell
RNN Cell
ho
h1
h2
Wxh
Whh
bxh
Wxh
Whh
bxh
h1
h2
bhh
bhh
Whh
bhh
ai
há»c
Ä‘i
4
3
há»c
RNN Cell
h3
Wxh
bxh
h2
Whh
bhh
<eos>
2
ai
Ä‘i
há»c
ai
<eos>
Training
Loss
ai
há»c
<eos>
label
Add more special tokens
seq_len = 3
36
RNN Cell
RNN Cell
ho
h1
h3
Wxh
Whh
bxh
Wxh
Whh
bxh
h1
h3
bhh
bhh
Whh
bhh
ai
há»c
<sos>
4
1
RNN Cell
h4
Wxh
bxh
h4
Whh
bhh
<eos>
2
ai
RNN Cell
h2
Wxh
bxh
Whh
h2
bhh
Ä‘i
3
há»c
Ä‘i
<sos>
Ä‘i
há»c
ai
<eos>
Training
Loss
ai
<eos>
há»c
Ä‘i
label
seq_len = 4
37
â¢Text Generation Using Transformer
â¢An improved Approach to Text Generation
â¢Machine Translation Using RNN
â¢Machine Translation Using Transformer
Outline
IWSLT'15 English-Vietnamese data
133K
sentences
Train
1K3
sentences
Val
1K3
sentences
Test
Ref: https://nlp.stanford.edu/projects/nmt/
En - sequence
Vi - sequence
Over 15,000 scientists go to San Francisco every year for that .
Má»—i nÄƒm , hÆ¡n 15,000 nhÃ  khoa há»c Ä‘áº¿n San Francisco Ä‘á»ƒ tham dá»± há»™i nghá»‹ nÃ y .
Neural Machine 
Translation
Dataset
(N, 2, 3)
Multi-head 
Self-Attention
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
(N, 2, 3)
Masked 
Multi-head 
Self-Attention
Add & 
Norm
(N, 3, 3)
Masked 
Multi-head 
Self-Attention
query
value
key
query
value
key
query
value
key
Add & 
Norm
Linear
(3, 4)
ReLU
Add & 
Norm
Linear
(4, 3)
Feed Forward
Source sequences
good morning <eos>
ai books <eos>
Target sequences
<sos> chÃ o buá»•i sÃ¡ng <eos>
<sos> sÃ¡ch ai <eos> <pad>
RNN /
Transformer Encoder
Source sequences
good morning <eos>
ai books <eos>
Input encoder
[5, 6, 2]
[3, 4, 2]
en vocab
vi vocab
Target sequences
<sos> chÃ o buá»•i sÃ¡ng <eos>
<sos> sÃ¡ch ai <eos> <pad>
Input decoder
[2, 6, 5, 8]
[2, 7, 3, 4]
Embedding
hidden
state
Encoder
Embedding
RNN /
Transformer Decoder
FC
Decoder
Training phase
Teacher forcing
Decoder
[2]
[7]
Decoder
[2, 7]
Decoder
[2, 6, 4]
[3]
[4]
<sos>
sÃ¡ch
ai
<eos>
<sos> sÃ¡ch
<sos> sÃ¡ch ai
Stop
Inference phase
Label
[6, 5, 8, 3]
[7, 3, 4, 1]
Multi-Head 
Attention
Feed Forward
Add & Norm
Add & Norm
Multi-Head 
Attention
Feed Forward
Add & Norm
Add & Norm
Embedding
Masked Multi-
Head Attention
Add & Norm
Embedding
Input encoder
[5, 6, 2]
[3, 4, 2]
Input decoder
[2, 6, 5, 8]
[2, 7, 3, 4]
Linear (fc2)
Feed Forward
Linear (fc1)
ReLU
Linear
Linear
Linear
Scaled Dot-Product 
Attention
Linear
V
K
Q
Multi-Head Attention
Linear
ğ‘Šğ‘“ğ‘1
ğ‘Šğ‘“ğ‘2
ğ‘Šğ‘_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘˜_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘£_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘œ_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘˜_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘£_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘œ_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘“ğ‘1
ğ‘Šğ‘“ğ‘2
ğ‘Šğ‘_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘˜_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘£_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘œ_ğ‘ğ‘Ÿğ‘œğ‘—
ğ‘Šğ‘ğ‘™ğ‘ 
Predict
[5, 3, 5, 1]
[1, 3, 3, 3]
Label
[6, 5, 8, 3]
[7, 3, 4, 1]
CrossEntropyLoss
2.31
Transformer
(PyTorch)
Decoder FFN
Encoder FFN
Decoder Attention
Decoder Masked Attention
Encoder Attention
Classifer
Source sequences
good morning <eos>
ai books <eos>
Target sequences
<sos> chÃ o buá»•i sÃ¡ng <eos>
<sos> sÃ¡ch ai <eos> <pad>
Embedding
Encoder
â„0
â„1
â„3
â„2
ğ‘¥1
ğ‘¥2
ğ‘¥3
zero_init
RNN
Embedding
â„0
â„1
â„3
â„2
ğ‘¥1
ğ‘¥2
ğ‘¥3
FC
Decoder
RNN
ğ‘¥4
â„4
Input 
encoder
[5, 6, 2]
[3, 4, 2]
Predict
[1, 1, 1, 6]
[1, 1, 1, 1]
Label
[6, 5, 8, 3]
[7, 3, 4, 1]
âˆ’0.2961
âˆ’0.2180
â€¦
âˆ’0.2338
0.1020
â€¦
âˆ’0.3464
âˆ’0.2248
â€¦
0.1258
âˆ’0.3808
â€¦
âˆ’0.1759
âˆ’0.2444
â€¦
âˆ’0.2802
âˆ’0.2382
â€¦
â€¦
âˆ’0.1447
âˆ’0.3950
â€¦
0.0096
âˆ’0.2789
â€¦
0.4081
0.0771
â€¦
0.0638
âˆ’0.3592
â€¦
âˆ’0.0283
âˆ’0.2766
â€¦
0.3423
âˆ’0.0810
ğ‘¾ğ’‰ğ’‰
ğ‘Šâ„â„
ğ‘Šğ‘–â„
ğ‘Šâ„â„
ğ‘Šâ„â„
ğ‘Šğ‘–â„
ğ‘Šğ‘–â„
ğ‘Šğ‘–â„
ğ‘Šğ‘–â„
ğ‘Šğ‘–â„
ğ‘Šğ‘–â„
ğ‘Šâ„â„
ğ‘Šâ„â„
ğ‘Šâ„â„
ğ‘Šâ„â„
0.3195
âˆ’0.2901
â€¦
0.1251
âˆ’0.0851
â€¦
0.3672
0.1360
â€¦
âˆ’0.2746
0.1654
â€¦
0.2166
âˆ’0.1650
â€¦
âˆ’0.2060
0.1245
â€¦
â€¦
0.1259
âˆ’0.1406
. . .
âˆ’0.2435
âˆ’0.2435
. . .
âˆ’0.4049
âˆ’0.3194
â€¦
âˆ’0.2108
âˆ’0.2783
â€¦
0.2335
âˆ’0.3172]
â€¦
0.2433
0.2775
ğ‘¾ğ’Šğ’‰
âˆ’0.3609
âˆ’0.2455
â€¦
0.1689
âˆ’0.1555
â€¦
âˆ’0.4061
0.2732
â€¦
âˆ’0.3316
0.3044
â€¦
âˆ’0.0040
âˆ’0.3105
â€¦
âˆ’0.0822
âˆ’0.2349
â€¦
â€¦
0.0591
âˆ’0.1057
. . .
âˆ’0.1856
âˆ’0.0818
. . .
âˆ’0.2847
âˆ’0.4029
â€¦
0.2139
0.1033
â€¦
0.1671
âˆ’0.2004
â€¦
âˆ’0.2668
0.1354
ğ‘¾ğ’Šğ’‰
âˆ’0.1213
0.2520
â€¦
âˆ’0.1249
âˆ’0.3887
â€¦
0.2917
âˆ’0.3131
â€¦
âˆ’0.0576
âˆ’0.0032
â€¦
0.2534
0.3055
â€¦
0.2255
âˆ’0.4063
â€¦
â€¦
âˆ’0.0720
âˆ’0.1979
. . .
0.2052
0.1853
. . .
0.1515
0.3834
â€¦
0.1959
âˆ’0.4053
â€¦
âˆ’0.3354
0.0918
â€¦
âˆ’0.0357
âˆ’0.2009
ğ‘¾ğ’‰ğ’‰
CrossEntropyLoss
2.33
Input 
decoder
[2, 6, 5, 8]
[2, 7, 3, 4]
RNN
Model
Ref: https://arxiv.org/pdf/1910.10683.pdf
T5: Text-to-Text Transfer Transformer
Original transformer 
Encoder-Decoder model pre-trained on a multi-task mixture of unsupervised 
and supervised tasks
Unsupervised
Supervised
seq2seq 
input-output mapping
Predict masked spans
