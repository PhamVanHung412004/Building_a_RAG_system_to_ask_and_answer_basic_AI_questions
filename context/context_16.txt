Extra Class
Introduction to LSTM
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Recurrent Neural Network
(2) â€“ Text Classification using RNN
(3) â€“ RNN Variants: LSTM
1 â€“ Recurrent Neural Network
!
3
Language Model
â–Estimate the probability of an upcoming words
P(w|h) = P(school|i,go,to)
w: token as word â€œschoolâ€
h: history tokens as â€œi,go,toâ€
ğ‘ƒğ‘¤â„= ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(â„ğ‘¤)
ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(â„)
ğ‘ƒğ‘ ğ‘â„ğ‘œğ‘œğ‘™ğ‘–, ğ‘”ğ‘œ, ğ‘¡ğ‘œ= ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘–, ğ‘”ğ‘œ, ğ‘¡ğ‘œ, ğ‘ ğ‘â„ğ‘œğ‘œğ‘™)
ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘–, ğ‘”ğ‘œ, ğ‘¡ğ‘œ)
!
4
Language Model
â–The probability of a word depends only on some previous words
â–N-gram model with N = {1, 2,â€¦}
ğ‘ƒğ‘¤!:# = $
$%!
#
ğ‘ƒ(ğ‘¤$|ğ‘¤$&'(!:$&!)
ğ‘ƒ(ğ‘¤$| ğ‘¤!:$&!) = ğ‘ƒ(ğ‘¤$| ğ‘¤$&'(!:$&!)
1 â€“ Recurrent Neural Network
1 â€“ Recurrent Neural Network
!
5
Language Model
â–N = 1
â–Unigram Model (1 â€“ gram)
ğ‘ƒğ‘¤!:# = âˆ$%!
#
ğ‘ƒğ‘¤$ ğ‘¤$&'(!:$&! = âˆ$%!
#
ğ‘ƒ(ğ‘¤$)
P(â€œi,go,to,schoolâ€)
= P(i).P(go|i).P(to|i,go).P(school|i,go,to)
= P(i).P(go).P(to).P(school)
1 â€“ Recurrent Neural Network
!
6
From Neural Network to Recurrent Neural Network
â–A neural Probabilistic Language Model
Source
Target
trÄƒm
nÄƒm
â€¦
â€¦
trÄƒm nÄƒm
trong
â€¦
â€¦
trÄƒm nÄƒm trong
cÃµi
â€¦
â€¦
trÄƒm nÄƒm trong cÃµi ngÆ°á»i
ta
â€œtrÄƒm nÄƒm trong cÃµi ngÆ°á»i taâ€
1 â€“ Recurrent Neural Network
!
7
From Neural Network to Recurrent Neural Network
â–Text Classification using Neural Network
X1
X2
XN
Embedding Layer
Dense vector
Flatten
Classifier
Input
1 â€“ Recurrent Neural Network
!
8
From Neural Network to Recurrent Neural Network
â–Text Classification using Neural Network
X1
X2
XN
EmbeddingBag Layer
Classifier
Input
1 â€“ Recurrent Neural Network
!
9
From Neural Network to Recurrent Neural Network
â–Models need to learn the context in which words appear
â–Need better network architecturesâ€¦
RNNs for Sequence
1 â€“ Recurrent Neural Network
!
10
Recurrent Neural Network (RNN)
X1
X2
XN
Embedding Layer
Dense vector
Hidden State
Input
RNN
RNN
RNN
Output Sequence
Last Hidden State
1 â€“ Recurrent Neural Network
!
11
Recurrent Neural Network (RNN)
X1
X2
X3
0
1
0
Dense vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
2
2
x!W"#
$ + b"#
0
1
h%W##
$ + b##
h! = R h%W##
$ + b## + x!W"#
$ + b"#
Initilization
h0
1 â€“ Recurrent Neural Network
!
12
Recurrent Neural Network (RNN)
X1
X2
X3
0
1
0
Dense vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
2
2
x!W"#
$ + b"#
0
1
h%W##
$ + b##
Initilization
h0
2
2
x&W"#
$ + b"#
2
4
h!W##
$ + b##
4
6
h& = R h!W##
$ + b## + x&W"#
$ + b"#
1 â€“ Recurrent Neural Network
!
13
Recurrent Neural Network (RNN)
X1
X2
X3
0
1
0
Dense vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
2
2
x!W"#
$ + b"#
0
1
h%W##
$ + b##
Initilization
h0
2
2
x&W"#
$ + b"#
2
4
h!W##
$ + b##
4
6
1
1
x'W"#
$ + b"#
4
7
h&W##
$ + b##
5
8
h' = R h&W##
$ + b## + x'W"#
$ + b"#
5
8
1 â€“ Recurrent Neural Network
!
14
Recurrent Neural Network (RNN)
X1
X2
X3
0
1
0
Dense vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
Initilization
h0
4
6
5
8
h! = R h!"#W$$
% + b$$ + x!W&$
% + b&$
5
8
Last Hidden State
1 â€“ Recurrent Neural Network
!
15
Loss Function
X1
X2
X3
0
1
0
Dense 
vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
Initilization
h0
4
6
5
8
5
8
Last Hidden State
1 â€“ Recurrent Neural Network
!
16
Loss Function
X1
X2
X3
0
1
0
Dense 
vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
Initilization
h0
4
6
5
8
5
8
Last Hidden State
â–Loss Function for Text Classification
CE Loss
Wxh
Wxh
Wxh
Whh
Whh
Whh
1 â€“ Recurrent Neural Network
!
17
Loss Function
X1
X2
X3
0
1
0
Dense 
vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
Initilization
h0
4
6
5
8
5
8
Last Hidden State
â–Loss Function for Text Classification
CE    Loss
Wxh
Wxh
Wxh
Whh
Whh
Whh
1 â€“ Recurrent Neural Network
!
18
Loss Function
X1
X2
X3
0
1
0
Dense vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
Initilization
h0
4
6
5
8
Last Hidden State
â–Loss Function for Text Generation
CE    Loss
Wxh
Wxh
Wxh
Whh
Whh
Whh
1 â€¦ 0
0 â€¦ 1
0 â€¦ 1
0 â€¦ 1
0 â€¦ 1
1 â€¦ 1
CE    Loss
CE    Loss
One-hot Encoding
Y1
Y2
Y3
FC Layer
1 â€“ Recurrent Neural Network
!
19
Loss Function
X1
X2
X3
0
1
0
Dense vector
Wxh
2
3
0
0
1
0
0
0
R
R
R
1
1
1
1
1
1
0 0
Whh
1
0
0
1
1
1
bxh
0
1
bhh
Initilization
h0
4
6
5
8
Last Hidden State
â–Loss Function for Text Generation
CE    Loss
Wxh
Wxh
Wxh
Whh
Whh
Whh
1 â€¦ 0
0 â€¦ 1
0 â€¦ 1
0 â€¦ 1
0 â€¦ 1
1 â€¦ 1
CE    Loss
CE    Loss
One-hot Encoding
Y1
Y2
Y3
1 â€“ Recurrent Neural Network
!
20
Pytorch - Demo
2 â€“ Text Classification
!
21
NTC-SCV Dataset
Positive Example
Negative Example
MÃ¬nh Ä‘Æ°á»£c 1 cÃ´ báº¡n giá»›i_thiá»‡u Ä‘áº¿n Ä‘Ã¢y , tÃ¬m
Ä‘á»‹a_chá»‰khÃ¡ dá»…. Menu nÆ°á»›c uá»‘ng cháº¥t khá»i nÃ³i
. MÃ¬nh muá»‘n cÅ©ng Ä‘c 8 loáº¡i nÆ°á»›c á»ŸÄ‘Ã¢y , mÃ³n
nÃ o cÅ©ng ngon vÃ  bá»•_dÆ°á»¡ng cáº£.
QuÃ¡n cháº¿_biáº¿n Ä‘á»“_Äƒn lÃ¢u , CÃ¡_Sapa nÆ°á»›ng 
uá»›p ráº¥t dá»Ÿ , sÃ² LÃ´ng ko tÆ°Æ¡i , nÆ°á»›c_cháº¥m ko 
ngon\n TÃ³m_láº¡i sáº½ ko bao_giá» ghÃ© ná»¯a , Äƒn_dá»Ÿ 
mÃ  uá»•ng tiá»n
Má»—i láº§n thÃ¨m trÃ  sá»¯a lÃ  lÃ m 1 ly . QuÃ¡n dá»…
kiáº¿m , khÃ´ng_gian láº¡i rá»™ng_rÃ£i . NhÃ¢n_viÃªn thÃ¬
dá»…_thÆ°Æ¡ng gáº§n_gÅ©i . NÃ³i_chung thÃ¨m trÃ  sá»¯a
lÃ  mÃ¬nh ghÃ© QuÃ¡n á»ŸÄ‘Ã¢y vÃ¬ gáº§n nhÃ  .
QuÃ¡n nÃ y tháº¥y khÃ¡ nhiá»u ngÆ°á»i báº£o mÃ¬nh nÃªn 
mÃ¬nh Ä‘Ã£ Ä‘i Äƒn thá»­ , nhÆ°ng thá»±c_sá»± Äƒn xong 
tháº¥y khÃ´ng Ä‘Æ°á»£c nhÆ° mong_Ä‘á»£i láº¯m .
â–NTC-CSV Dataset
Ã˜ Sentiment Analysis
2 â€“ Text Classification
!
22
Preprocessing
â–Language Detection
Vietnamese Language
Other Language
QuÃ¡n nÃ y tháº¥y khÃ¡ nhiá»u ngÆ°á»i báº£o mÃ¬nh nÃªn mÃ¬nh Ä‘Ã£ Ä‘i Äƒn thá»­
, nhÆ°ng thá»±c_sá»±Äƒn xong tháº¥y khÃ´ng Ä‘Æ°á»£c ngon. ğŸ‘ğŸ‘</p>
MÃ¬nh Ä‘Æ°á»£c 1 cÃ´ báº¡n giá»›i_thiá»‡u Ä‘áº¿n Ä‘Ã¢y , tÃ¬m Ä‘á»‹a_chá»‰khÃ¡ dá»….
Menu nÆ°á»›c uá»‘ng cháº¥t khá»i nÃ³i . https://foody.com
Visiting_Da_Nang frequently but this is the first time I have
found a coffee shop which has a creative design ( korean style )
The room is cheap ! ! ! ! It ' s near the city center . The staff is
so nice : - D ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘\n
Language 
Detector
langid library
2 â€“ Text Classification
!
23
Preprocessing
â–Language Detection
â–Text Cleaning
Vietnamese Language
QuÃ¡n nÃ y tháº¥y khÃ¡ nhiá»u ngÆ°á»i báº£o mÃ¬nh nÃªn mÃ¬nh Ä‘Ã£ Ä‘i Äƒn thá»­
, nhÆ°ng thá»±c_sá»±Äƒn xong tháº¥y khÃ´ng Ä‘Æ°á»£c ngon. ğŸ‘ğŸ‘</p>
MÃ¬nh Ä‘Æ°á»£c 1 cÃ´ báº¡n giá»›i_thiá»‡u Ä‘áº¿n Ä‘Ã¢y , tÃ¬m Ä‘á»‹a_chá»‰khÃ¡ dá»….
Menu nÆ°á»›c uá»‘ng cháº¥t khá»i nÃ³i . https://foody.com
1 â€“ Removal URLs, HTML Tags
2 â€“ Removal punctuations, digits
3 â€“ Removal emoticons, flags,â€¦
4 â€“ Normalize whitespace
5 â€“ Lowercasing
2 â€“ Text Classification
!
24
Index-Based Represenation
[dog, bites, man]
[man, bites, dog]
2
4
3
3
4
2
Input matrix
Index-based Representation
Input shape: N x M
0
0.1
3.1
1
0.5
2.5
2
1.3
0.6
Embedding Matrix
(Lookup Table)
V   Vocabulary
D: Embedding Dim
IDX
Token
0
<pad>
1
<unk>
2
dog
0.6 1.4 0.1
1.3 0.7 0.4
0.4 0.7 1.3
N Samples
M: Sequence Length
Shape: V x D
Output shape: N x M x D
Model
2 â€“ Text Classification
!
25
Modeling
X1
X2
X3
Dense  vector
R
R
R
h0
Classifier
Embedding Layer
RNN Layer
Last Hidden State
2 â€“ Text Classification
!
26
Modeling â€“ Demo
2 â€“ Text Classification
!
27
Training â€“ Demo
3 â€“ Long Short Term Memory
!
28
RNN Drawbacks
x 0.5
x 0.5
x 0.5
x 0.5
input * 0.5n
n: unroll time
x 2
x 2
x 2
x 2
input * 2n
n: unroll time
Vanishing Gradient
Exploding Gradient
3 â€“ Long Short Term Memory
!
29
Prior Knowledge
Sigmoid
ğœğ‘¥=
1
1 + e"'
=
1
1 + 1
e'
=
1
e' + 1
e'
=
e'
e' + 1
0 â‰¤ğœğ‘¥â‰¤1
Tanh
tanh ğ‘¥= e' âˆ’e"'
e' + e"'
âˆ’1 â‰¤ğ‘¡ğ‘ğ‘›â„ğ‘¥â‰¤1
Matrix 
Multiplication
ğ‘#
ğ‘#
ğ‘#
ğ‘‘# â‹…ğ‘(
ğ‘(
ğ‘(
ğ‘‘(
= ğ‘#ğ‘( + ğ‘#ğ‘(
ğ‘#ğ‘( + ğ‘)ğ‘‘(
ğ‘#ğ‘( + ğ‘‘#ğ‘(
ğ‘#ğ‘( + ğ‘‘#ğ‘‘(
3 â€“ Long Short Term Memory
!
30
RNN
ht
xt
ht-1
tanh
gt
Input
Hidden State
3 â€“ Long Short Term Memory
!
31
Filter RNNâ€™s Output
x
ğœ
tanh
gt
it
xt
ht-1
ht
Input
Hidden State
Sigmoid
x
Multiplication
3 â€“ Long Short Term Memory
!
32
Long-Term Memory
x
ğœ
tanh
gt
it
xt
ht-1
ct-1
ct
ht
Input
Hidden State
Sigmoid
x
Multiplication
Cell State
3 â€“ Long Short Term Memory
!
33
Add Information For Long-Term Memory
x
ğœ
tanh
gt
it
xt
ht-1
ct-1
ct
+
ht
Input
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
3 â€“ Long Short Term Memory
!
34
Filter Long-Term Memory
x
ğœ
tanh
gt
it
xt
ht-1
ct-1
ct
x
ğœ
+
ht
ft
Input
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
3 â€“ Long Short Term Memory
!
35
Normalize Shot-Term Memory
+
Previous Memory
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
Input
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
3 â€“ Long Short Term Memory
!
36
Filter The Normalized Short-Term Memory
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Input
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
ot
3 â€“ Long Short Term Memory
!
37
LSTM â€“ Forget Gate
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Input
Hidden State
Sigmoid
ot
ğ‘“! = ğœğ‘Š"#ğ‘¥! + ğ‘"# + ğ‘Š$#â„!%& + ğ‘$#
3 â€“ Long Short Term Memory
!
38
LSTM â€“ Input Gate
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Input
Hidden State
Sigmoid
ot
ğ‘–! = ğœğ‘Š""ğ‘¥! + ğ‘"" + ğ‘Š$"â„!%& + ğ‘$"
3 â€“ Long Short Term Memory
!
39
LSTM â€“ Candidate Memory
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Input
Hidden State
ot
ğ‘”! = ğ‘¡ğ‘ğ‘›â„ğ‘Š"'ğ‘¥! + ğ‘"' + ğ‘Š$'â„!%& + ğ‘$'
3 â€“ Long Short Term Memory
!
40
LSTM â€“ Output Gate
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Input
Hidden State
Sigmoid
ot
ğ‘œ! = ğœğ‘Š"(ğ‘¥! + ğ‘"( + ğ‘Š$(â„!%& + ğ‘$(
3 â€“ Long Short Term Memory
!
41
LSTMâ€“ Current Cell State
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Sigmoid
x
Multiplication
+
Addition
Cell State
ot
ğ‘! = ğ‘“! âŠ™ğ‘)%& + ğ‘–) âŠ™ğ‘”)
3 â€“ Long Short Term Memory
!
42
LSTM - Current Hidden State
+
ct-1
ct
x
x
ğœ
tanh
gt
it
xt
ht-1
ğœ
ft
tanh
ht
ğœ
x
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
ot
â„! = ğ‘œ! âŠ™tanh(ğ‘))
3 â€“ Long Short Term Memory
!
43
Example
x
+
1
ğœ
2
1
ğœ
x
tanh
?
ğœ
tanh
x
?
2.7
1.63
1.65
0.94
-0.19
2
1.41
4.38
1.62
0.62
-0.32
Weight
Bias
Input
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
0
0
0
0.59
0
?
?
?
?
?
?
?
3 â€“ Long Short Term Memory
!
44
Example - Forget Gate
ğ‘Š!"ğ‘¥# + ğ‘!" + ğ‘Š$"â„#%& + ğ‘$"
= 1.63*1+0+2.7*1+1.62
= 5.95
ğœğ‘Š!"ğ‘¥# + ğ‘!" + ğ‘Š$"â„#%& + ğ‘$"
= ğœ(5.95)
= 
!!.#!
!!.#!"#
= 0.997
ğ‘“! = ğœğ‘Š"#ğ‘¥! + ğ‘"# + ğ‘Š$#â„!%& + ğ‘$#
x
+
1
ğœ
2
1
ğœ
x
tanh
?
ğœ
tanh
x
?
1.65
0.94
-0.19
2
1.41
4.38
0.62
-0.32
0
0
0.59
0
2.7
1.63
1.62
0
0.997
?
?
?
?
?
?
Weight
Bias
Input
Hidden State
Sigmoid
3 â€“ Long Short Term Memory
!
45
Example - Input Gate
ğ‘–! = ğœğ‘Š""ğ‘¥! + ğ‘"" + ğ‘Š$"â„!%& + ğ‘$"
x
+
1
ğœ
2
1
ğœ
x
tanh
?
ğœ
tanh
x
?
2.7
1.63
0.94
-0.19
1.41
4.38
1.62
-0.32
0
0
0.59
0
0.997
0.986
?
?
?
1.65
2
0.62
0
ğ‘Š!!ğ‘¥# + ğ‘!! + ğ‘Š$!â„#%& + ğ‘$!
= 1.65*1+0.62+2*1+0
= 4.27
ğœğ‘Š!!ğ‘¥# + ğ‘!! + ğ‘Š$!â„#%& + ğ‘$!
= ğœ(4.27)
= 
!$.%&
!$.%&"#
= 0.986
?
?
Weight
Bias
Input
Hidden State
Sigmoid
2 â€“ Long Short Term Memory
!
46
Example - Candidate Memory
ğ‘”! = ğ‘¡ğ‘ğ‘›â„ğ‘Š"'ğ‘¥! + ğ‘"' + ğ‘Š$'â„!%& + ğ‘$'
x
+
1
ğœ
2
1
ğœ
x
tanh
?
ğœ
tanh
x
?
2.7
1.63
1.65
-0.19
2
4.38
1.62
0.62
0
0
0.59
0
0.966
?
?
0.997
0.986
0.94
1.41
-0.32
0
ğ‘Š!'ğ‘¥# + ğ‘!' + ğ‘Š$'â„#%& + ğ‘$'
= 0.94*1+(-0.32)+1.41*1+0
= 2.03
ğ‘¡ğ‘ğ‘›â„ğ‘Š!'ğ‘¥# + ğ‘!' + ğ‘Š$'â„#%& + ğ‘$'
= ğ‘¡ğ‘ğ‘›â„(2.03)
= 
(!.#$%(%!.#$
(!.#$)(%!.#$
= 0.966
?
?
Weight
Bias
Input
Hidden State
Sigmoid
3 â€“ Long Short Term Memory
!
47
Example - Output Gate
x
+
1
ğœ
2
1
ğœ
x
tanh
?
ğœ
tanh
x
?
2.7
1.63
1.65
0.94
-0.19
2
1.41
4.38
1.62
0.62
-0.32
0
0
0
0.59
0
0.992
?
0.966
0.997
0.986
ğ‘Š!*ğ‘¥# + ğ‘!* + ğ‘Š$*â„#%& + ğ‘$*
= -0.19*1+0+4.38*1+0.59
= 4.78
ğœğ‘Š!*ğ‘¥# + ğ‘!* + ğ‘Š$*â„#%& + ğ‘$*
= ğœ(4.78)
= 
!$.&'
!$.&'"#
= 0.992
ğ‘œ! = ğœğ‘Š"(ğ‘¥! + ğ‘"( + ğ‘Š$(â„!%& + ğ‘$(
?
?
Weight
Bias
Input
Hidden State
Sigmoid
3 â€“ Long Short Term Memory
!
48
Example â€“ Current Cell State
x
+
1
2
1
ğœ
x
tanh
2.946
ğœ
tanh
x
?
2.7
1.63
1.65
0.94
-0.19
2
1.41
4.38
1.62
0.62
-0.32
0
0
0
0.59
0
1.994
0.952
?
0.992
ğ‘! = ğ‘“! âŠ™ğ‘)%& + ğ‘–) âŠ™ğ‘”)
ğœ
0.966
0.997
0.986
ğ‘“# âŠ™ğ‘+%& + ğ‘–+ğ‘”+
= 0.997 * 2 + 0.986 * 0.966
= 1.994 + 0.952
= 2.946
x
Multiplication
+
Addition
Cell State
3 â€“ Long Short Term Memory
!
49
Example - Current Hidden State
â„! = ğ‘œ! âŠ™tanh(ğ‘))
x
+
1
ğœ
2
1
ğœ
x
tanh
ğœ
tanh
x
1.986
2.7
1.63
1.65
0.94
-0.19
2
1.41
4.38
1.62
0.62
-0.32
0
0
0
0.59
0
0.994
1.994
0.952
0.992
0.966
0.997
0.986
2.946
tanh(ğ‘#)
= tanh(2.946)
= 
!%.#$($!)%.#$(
!%.#$("!)%.#$(
= 0.994
ğ‘œ# âŠ™tanh(ğ‘+)
= 0.992 + 0.994
= 1.986
Hidden State
x
Multiplication
3 â€“ Long Short Term Memory
!
50
Example - Final Result
x
+
1
ğœ
2
1
ğœ
x
tanh
ğœ
tanh
x
2.7
1.63
1.65
0.94
-0.19
2
1.41
4.38
1.62
0.62
-0.32
0
0
0
0.59
0
1.986
0.994
0.992
2.946
1.994
0.952
0.966
0.997
0.986
Weight
Bias
Input
Hidden State
Sigmoid
x
Multiplication
+
Addition
Cell State
Thanks!
Any questions?
51
