AI VIETNAM aivietnam.edu.vn
Paper này sử dụng Latent Diffusion Model (LDM, [6]) đã được pretrained như một mô hình text-to-
image. Trong đó LDM sử dụng kiến trúc U-Net, sử dụng nhiều khối Convolution 2D và khối transformer,
với từng khối transformer sử dụng các layer self-attention, cross-attention và feed-forward. Để sử dụng
các khối này cho tác vụ video, nghiên cứu này có một số chỉnh sửa cho khối U-Net. Đầu tiên, khối
Convolution 2D được thay thế bằng khối Convolution 3D giả, với các filter 3x3 được thay thế bằng các
layer 1x3x3. Về khối attention, một khối temporal self-attention (T-Attn) được thêm vào ở cuối cùng
ở mối khối transformer để mô tả thông tin về thời gian. Để tăng độ mạch lạc về thời gian cho video,
paper này sử dụng một khối spatial-temporal attention thay cho khối self-attention. Trong đó, để giảm
tài nguyên cần cho tính toán, attention chỉ được tính giữa frame hiện tại, frame liền trước nó và frame
đầu tiên, trong đó, thông tin về feature giữa frame đầu tiên và frame liền trước được concat lại với
nhau. Quá trình này được mô tả trong hình 20.
Hình 20: Minh họa Spatial-Temporal Attention
Tổng kết lại, bài báo giới thiệu một task mới có tên one-shot video-tuning cho việc tạo video từ văn
bản (T2V), mà trong đó việc huấn luyện mô hình tạo sinh T2V chỉ sử dụng một cặp văn bản-video
duy nhất với sự hỗ trợ của các mô hình chuyển đổi từ văn bản sang hình ảnh (T2I) đã được huấn luyện
từ trước. Công cụ được giới thiệu, Tune-A-Video, hỗ trợ việc tạo và chỉnh sửa video dựa trên văn bản
thông qua một chiến lược tuning đặc biệt và sự đảo ngược cấu trúc, đảm bảo tính nhất quán về thời
gian trong các video được tạo ra. 