Insight into 
Logistic Regression
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
â¢Vectorization
â¢Optimiztion for 1+ samples
â¢Logistic Regression â€“ Mini-batch
â¢Logistic Regression â€“ Batch
â¢BCE and MSE Loss Functions
â¢Sigmoid and Tanh Function (Optional)
Outline
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Implementation - One Sample
Feature
Label
if #features changes, which functions are affected?
1
demo
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤ğ‘–= ğ‘¤ğ‘–âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
2
demo
How to solve the problem?
Vector/Matrix Operations
Multiply with a number
ğ›¼ğ‘¢= ğ›¼
ğ‘¢1
â€¦
ğ‘¢ğ‘›
=
ğ›¼ğ‘¢1
â€¦
ğ›¼ğ‘¢ğ‘›
=
data 
1
2
3
2
*
result 
2
4
6
1
3
2
4
T
1
2
3
4
A =
ğ‘11 â€¦  ğ‘1ğ‘›
â€¦ â€¦ . . .
ğ‘ğ‘š1 â€¦  ğ‘ğ‘šğ‘›
Ağ‘‡=
ğ‘11 â€¦  ğ‘ğ‘š1
â€¦ â€¦ â€¦
ğ‘1ğ‘› â€¦  ğ‘ğ‘šğ‘›
Ô¦ğ‘£=
ğ‘£1
â€¦
ğ‘£ğ‘›
Ô¦ğ‘£ğ‘‡= ğ‘£1 â€¦ ğ‘£ğ‘›
1
2
T
1
2
Transpose
3
Vector/Matrix Operations
Dot product
Ô¦ğ‘£=
ğ‘£1
â€¦
ğ‘£ğ‘›
ğ‘¢=
ğ‘¢1
â€¦
ğ‘¢ğ‘›
Ô¦ğ‘£âˆ™ğ‘¢= ğ‘£1 Ã— ğ‘¢1 + â‹¯+ ğ‘£ğ‘›Ã— ğ‘¢ğ‘›
v 
1
2
2
3
w 
=
result 
8
4
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Vectorization
AI VIETNAM
All-in-One Course
ğ‘§= ğ‘¤ğ‘¥+ ğ‘1 = ğ‘ ğ‘¤
1
ğ‘¥= ğœ½ğ‘‡ğ’™
dot product
Traditional
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ½= ğ‘
ğ‘¤
â†’ğœ½ğ‘‡= ğ‘ ğ‘¤
Feature
Label
ğ‘¥
ğ‘¦
5
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§= ğœ½ğ‘‡ğ’™
ğ¿à·œğ‘¦, ğ‘¦= âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
numbers
What will we do?
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
6
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Traditional
Vectorization
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦âˆ’ğ‘¦Ã— 1
à·œğ‘¦âˆ’ğ‘¦Ã— ğ‘¥ =
à·œğ‘¦âˆ’ğ‘¦
1 
ğ‘¥ =
à·œğ‘¦âˆ’ğ‘¦ğ’™=
ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ¿
ğœ•ğ‘¤
= ğ›»ğœ½ğ¿
common factor
â†’
ğ›»ğœ½ğ¿= ğ’™( Æ¸ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥à·œğ‘¦âˆ’ğ‘¦=
à·œğ‘¦âˆ’ğ‘¦Ã— ğ‘¥
ğœ•ğ¿
ğœ•ğ‘=
à·œğ‘¦âˆ’ğ‘¦ =
à·œğ‘¦âˆ’ğ‘¦Ã— 1
7
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
ğ’™= 1
ğ‘¥
ğœ½= ğ‘
ğ‘¤
ğ‘§= ğœ½ğ‘‡ğ’™
ğ‘¤ =  ğ‘¤ âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘ =  ğ‘ âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğ›»ğœ½ğ¿=
ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ¿
ğœ•ğ‘¤
ğœ½
ğœ½
ğ›»ğœ½ğ¿
â†’
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
8
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output à·œğ‘¦
3) Compute loss
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğœ•ğ¿
ğœ•ğ‘¤= ğ‘¥(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= (à·œğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ±, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ’™(à·œğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
Traditional
Vectorized
Vectorization
AI VIETNAM
All-in-One Course
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
9
Vectorization
AI VIETNAM
All-in-One Course
â– Implementation (using Numpy)
# Given X and y
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ’™, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ’™(à·œğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
10
ğ’™=
1
ğ‘¥1
ğ‘¥2
=
1
1.4
0.2
Given ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
ğœ‚= 0.01
1
Model
Input
Label
Loss
ğ’™
ğœ½=
0.1
0.5
âˆ’0.1
à·œğ‘¦= ğœğœ½ğ‘‡ğ’™ = 0.6856
ğ¿= 1.1573
ğ‘¦= 0
3
Dataset
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ’™, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
ğ¿(à·œğ‘¦, ğ‘¦) = âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ’™(à·œğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ›»ğœ½ğ¿= ğ’™à·œğ‘¦âˆ’ğ‘¦=
1
1.4
0.2
0.6856 =
0.6856
0.9599
0.1371
=
ğ¿ğ‘
â€²
ğ¿ğ‘¤1
â€²
ğ¿ğ‘¤2
â€²
4
ğ›‰âˆ’Î·Lğ›‰
â€² =
0.1
0.5
âˆ’0.1
âˆ’Î·
0.6856
0.9599
0.1371
=
0.093
0.499
âˆ’0.101
5
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ’™, ğ‘¦) from training data   
2) Compute output à·œğ‘¦
3) Compute loss
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ¿ğœ½= âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ›»ğœ½ğ¿= ğ±(à·œy âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
ğ‘§= ğœ½ğ‘‡ğ’™
Demo
AI VIETNAM
All-in-One Course
ğ’™ =
1
1.4
0.2
ğ’š= 0
Dataset
12
â¢Vectorization
â¢Optimiztion for 1+ samples
â¢Logistic Regression â€“ Mini-batch
â¢Logistic Regression â€“ Batch
â¢BCE and MSE Loss Functions
â¢Sigmoid and Tanh Function (Optional)
Outline
Optimization for One+  Samples 
â– Equations for partial gradients
AI VIETNAM
All-in-One Course
ğ‘‘ğ‘“
ğ‘‘ğ‘= ğ‘¥
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2ğ‘¥ğ‘“âˆ’ğ‘¦
ğ‘‘ğ‘“
ğ‘‘ğ‘= 1
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2 ğ‘“âˆ’ğ‘¦
ğ‘‘ğ‘”
ğ‘‘ğ‘“= 2 ğ‘“âˆ’ğ‘¦
During looking for optimal a 
and b, at a given time, a and b 
have concrete values
ğ‘“ğ‘¥ğ‘–= ğ‘ğ‘¥ğ‘–+ ğ‘
ğ‘”ğ‘“ğ‘–= ğ‘“ğ‘–âˆ’ğ‘¦ğ‘–2
(ğ‘¥2=2, ğ‘¦2=7)
(ğ‘¥1=1, ğ‘¦1=5)
ğ‘
ğ‘“ğ‘–
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘
ğ‘
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘
ğ‘¥ğ‘–
ğ‘”ğ‘–
ğ‘‘ğ‘”ğ‘–
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘”ğ‘–
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘
ğ‘¦ğ‘–
ğ‘‘ğ‘”ğ‘–
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘“ğ‘–
ğ‘‘ğ‘
13
illustration
ğ‘
ğ‘“1
ğ‘‘ğ‘“1
ğ‘‘ğ‘
ğ‘
ğ‘‘ğ‘“1
ğ‘‘ğ‘
ğ‘¥1
ğ‘”1
ğ‘‘ğ‘”1
ğ‘‘ğ‘“1
ğ‘‘ğ‘”1
ğ‘‘ğ‘“1
ğ‘‘ğ‘“1
ğ‘‘ğ‘
ğ‘¦1
ğ‘‘ğ‘”1
ğ‘‘ğ‘“1
ğ‘‘ğ‘“1
ğ‘‘ğ‘
ğ‘
ğ‘“2
ğ‘‘ğ‘“2
ğ‘‘ğ‘
ğ‘
ğ‘‘ğ‘“2
ğ‘‘ğ‘
ğ‘¥2
ğ‘”2
ğ‘‘ğ‘”2
ğ‘‘ğ‘“2
ğ‘‘ğ‘”2
ğ‘‘ğ‘“2
ğ‘‘ğ‘“2
ğ‘‘ğ‘
ğ‘¦2
ğ‘‘ğ‘”2
ğ‘‘ğ‘“2
ğ‘‘ğ‘“2
ğ‘‘ğ‘
â– Optimization for a composite function
à·
ğ‘–
ğ‘‘ğ‘”ğ‘–
ğ‘‘ğ‘= ğ‘‘ğ‘”1
ğ‘‘ğ‘“1
ğ‘‘ğ‘“1
ğ‘‘ğ‘+ ğ‘‘ğ‘”2
ğ‘‘ğ‘“2
ğ‘‘ğ‘“2
ğ‘‘ğ‘
à·
ğ‘–
ğ‘‘ğ‘”ğ‘–
ğ‘‘ğ‘= ğ‘‘ğ‘”1
ğ‘‘ğ‘“1
ğ‘‘ğ‘“1
ğ‘‘ğ‘+ ğ‘‘ğ‘”2
ğ‘‘ğ‘“2
ğ‘‘ğ‘“2
ğ‘‘ğ‘
Find a and b so that g(f(x)) is minimum
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2ğ‘¥ğ‘“âˆ’ğ‘¦
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2 ğ‘“âˆ’ğ‘¦
Partial derivative functions
ğ‘“ğ‘¥ğ‘–= ğ‘ğ‘¥ğ‘–+ ğ‘
ğ‘”ğ‘“ğ‘–= ğ‘“ğ‘–âˆ’ğ‘¦ğ‘–2
(ğ‘¥2=2,ğ‘¦2=7)
(ğ‘¥1=1,ğ‘¦1=5)
Optimization
â– How to use gradient information
AI VIETNAM
All-in-One Course
info 1
info 2
1
update at 
time t
update at 
time t+1
info 1
info 2
2
update
Compute partial 
gradient at a, b
Move a, b  
opposite to db, db
Initialize a, b
ğœ‚= 0.01
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2ğ‘¥ğ‘“âˆ’ğ‘¦
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2 ğ‘“âˆ’ğ‘¦
info 1
info 2
1
Summary
16
Compute partial 
gradient at a, b
Move a, b  
opposite to db, db
Initialize a, b
ğœ‚= 0.01
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2ğ‘¥ğ‘“âˆ’ğ‘¦
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2 ğ‘“âˆ’ğ‘¦
Summary
info 1
info 2
2
update
17
Compute partial 
gradient at a, b
Move a, b  
opposite to db, db
Initialize a, b
ğœ‚= 0.001
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2ğ‘¥ğ‘“âˆ’ğ‘¦
ğ‘‘ğ‘”
ğ‘‘ğ‘= ğ‘‘ğ‘”
ğ‘‘ğ‘“
ğ‘‘ğ‘“
ğ‘‘ğ‘= 2 ğ‘“âˆ’ğ‘¦
Summary
info 1
info 2
2
update
18
â¢Vectorization
â¢Optimiztion for 1+ samples
â¢Logistic Regression â€“ Mini-batch
â¢Logistic Regression â€“ Batch
â¢BCE and MSE Loss Functions
â¢Sigmoid and Tanh Function (Optional)
Outline
Linear Regression (m-samples)
AI VIETNAM
All-in-One Course
ğ’™ = 1 1.5 0.2
1 4.1 1.3
ğ’š= 0
1
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
â– Construct formulas
ğ’›= ğ‘§(1)
ğ‘§(2) = ğ‘¤1ğ‘¥1
(1) + ğ‘¤2ğ‘¥2
(1) + ğ‘
ğ‘¤1ğ‘¥1
(2) + ğ‘¤2ğ‘¥2
(2) + ğ‘
ğ’™ = 1 1.5 0.2
1 4.1 1.3 = 1 ğ‘¥1
(1) ğ‘¥2
(1)
1 ğ‘¥1
(2) ğ‘¥2
(2)
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
2) Compute output à·œğ‘¦
Dataset
= 1 ğ‘¥1
(1) ğ‘¥2
(1)
1 ğ‘¥1
(2) ğ‘¥2
(2)
ğ‘
ğ‘¤1
ğ‘¤2
= ğ’™ğœ½= 0.83
2.02
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ’›= ğœ½ğ‘‡ğ’™
19
Linear Regression (m-samples)
AI VIETNAM
All-in-One Course
ğ’™ = 1 1.5 0.2
1 4.1 1.3
ğ’š= 0
1
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
â– Construct formulas
à·ğ’š= ğœğ’›= à·œğ‘¦1
à·œğ‘¦2
=
1
1 + ğ‘’âˆ’ğ‘§(1)
1
1 + ğ‘’âˆ’ğ‘§(2)
=
1
1 + ğ‘’âˆ’ğ’›= 0.69
0.88
20
ğ’™ = 1 1.5 0.2
1 4.1 1.3 = 1 ğ‘¥1
(1) ğ‘¥2
(1)
1 ğ‘¥1
(2) ğ‘¥2
(2)
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
2) Compute output à·œğ‘¦
Dataset
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ’›= ğœ½ğ‘‡ğ’™
ğ’›= ğ’™ğœ½= 0.83
2.02
Numpy perspective
Dataset
Linear Regression (m-samples)
AI VIETNAM
All-in-One Course
â– Construct formulas
21
3) Compute loss
ğ¿(à·ğ’š, ğ’š) = ğ¿(1) à·œğ‘¦(1), ğ‘¦(1) + ğ¿(2) à·œğ‘¦(2), ğ‘¦(2)
m
ğ’™ = 1 1.5 0.2
1 4.1 1.3
ğ’š= 0
1
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
ğ¿(à·ğ’š, ğ’š) = 1
m âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ¿(1) à·œğ‘¦(1), ğ‘¦(1) = âˆ’ğ‘¦(1)logà·œğ‘¦(1) âˆ’(1âˆ’ğ‘¦(1))log(1âˆ’à·œğ‘¦(1))
ğ¿(2) à·œğ‘¦(2), ğ‘¦(2) = âˆ’ğ‘¦(2)logà·œğ‘¦(2) âˆ’(1âˆ’ğ‘¦(2))log(1âˆ’à·œğ‘¦(2))
+
ğ²Tlogà·œğ²
(1âˆ’y)Tlog(1âˆ’à·œğ² )
22
4) Compute derivative
ğœ•ğ¿(1)
ğœ•ğ‘
= (à·œğ‘¦(1) âˆ’ğ‘¦(1))
ğœ•ğ¿(1)
ğœ•ğ‘¤1
= ğ‘¥1
(1)(à·œğ‘¦(1) âˆ’ğ‘¦(1))
ğœ•ğ¿(1)
ğœ•ğ‘¤2
= ğ‘¥2
(1)(à·œğ‘¦(1) âˆ’ğ‘¦(1))
ğœ•ğ¿(2)
ğœ•ğ‘
= (à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğœ•ğ¿(2)
ğœ•ğ‘¤1
= ğ‘¥1
(2)(à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğœ•ğ¿(2)
ğœ•ğ‘¤2
= ğ‘¥2
(2)(à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğœ•ğ¿
ğœ•ğ‘=
ğœ•ğ¿(1)
ğœ•ğ‘+ ğœ•ğ¿(2)
ğœ•ğ‘
ğ‘š
= (à·œğ‘¦(1) âˆ’ğ‘¦(1)) + (à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğ‘š
sample 1
sample 2
= 1
ğ‘š1 âˆ—(à·œğ‘¦(1) âˆ’ğ‘¦(1)) + 1 âˆ—(à·œğ‘¦(2) âˆ’ğ‘¦(2))
= 1
ğ‘šğ‘¥0
(1) 
ğ‘¥0
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
ğœ•ğ¿
ğœ•ğ‘¤1
=
ğœ•ğ¿(1)
ğœ•ğ‘¤1 + ğœ•ğ¿(2)
ğœ•ğ‘¤1
ğ‘š
= ğ‘¥1
(1)(à·œğ‘¦(1) âˆ’ğ‘¦(1)) + ğ‘¥1
(2)(à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğ‘š
= 1
ğ‘šğ‘¥1
(1) 
ğ‘¥1
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
ğœ•ğ¿
ğœ•ğ‘¤2
=
ğœ•ğ¿(1)
ğœ•ğ‘¤2 + ğœ•ğ¿(2)
ğœ•ğ‘¤2
ğ‘š
= ğ‘¥2
(1)(à·œğ‘¦(1) âˆ’ğ‘¦(1)) + ğ‘¥2
(2)(à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğ‘š
= 1
ğ‘šğ‘¥2
(1) 
ğ‘¥2
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
ğ‘¥0
(1) = 1
ğ‘¥0
(2) = 1
23
4) Compute derivative
ğœ•ğ¿(1)
ğœ•ğ‘
= (à·œğ‘¦(1) âˆ’ğ‘¦(1))
ğœ•ğ¿(1)
ğœ•ğ‘¤1
= ğ‘¥1
(1)(à·œğ‘¦(1) âˆ’ğ‘¦(1))
ğœ•ğ¿(1)
ğœ•ğ‘¤2
= ğ‘¥2
(1)(à·œğ‘¦(1) âˆ’ğ‘¦(1))
ğœ•ğ¿(2)
ğœ•ğ‘
= (à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğœ•ğ¿(2)
ğœ•ğ‘¤1
= ğ‘¥1
(2)(à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğœ•ğ¿(2)
ğœ•ğ‘¤2
= ğ‘¥2
(2)(à·œğ‘¦(2) âˆ’ğ‘¦(2))
ğœ•ğ¿
ğœ•ğ‘= 1
ğ‘šğ‘¥0
(1) 
ğ‘¥0
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
sample 1
sample 2
ğœ•ğ¿
ğœ•ğ‘¤1
= 1
ğ‘šğ‘¥1
(1) 
ğ‘¥1
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
ğœ•ğ¿
ğœ•ğ‘¤2
= 1
ğ‘šğ‘¥2
(1) 
ğ‘¥2
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
ğ›»ğœ½ğ‘³=
ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ¿
ğœ•ğ‘¤1
ğœ•ğ¿
ğœ•ğ‘¤2
= 1
ğ‘š
ğ‘¥0
(1) 
ğ‘¥0
(2)
ğ‘¥1
(1) 
ğ‘¥1
(2)
ğ‘¥2
(1) 
ğ‘¥2
(2)
à·œğ‘¦(1) âˆ’ğ‘¦(1)
à·œğ‘¦(2) âˆ’ğ‘¦(2)
ğ’™(1) ğ‘‡
ğ’™(2) ğ‘‡
à·œğ²
ğ²
ğ›»ğœ½ğ¿= 1
ğ‘šğ’™ğ‘‡(à·œğ²âˆ’ğ’š)
24
5) Update parameters
ğ›»ğœ½ğ‘³=
ğœ•ğ¿
ğœ•ğ‘
ğœ•ğ¿
ğœ•ğ‘¤1
ğœ•ğ¿
ğœ•ğ‘¤2
= 1
ğ‘šğ’™ğ‘‡(à·œğ²âˆ’ğ’š)
ğ’™ = 1 1.5 0.2
1 4.1 1.3
ğ’š= 0
1
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
=
0.1
0.5
âˆ’0.1
Dataset
ğœ½
ğœ½
ğ›»ğœ½ğ‘³
ğ‘=  ğ‘ âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘ 
ğ‘¤1 = ğ‘¤1 âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘¤1
ğ‘¤2 = ğ‘¤2 âˆ’ ğœ‚ğœ•ğ¿
ğœ•ğ‘¤2
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
Logistic Regression - Minibatch
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
m âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
m ğ±T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
ğ’›= ğ’™ğœ½
AI VIETNAM
All-in-One Course
Model
Input
Label
Loss
ğ’™
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
à·ğ’š= ğœğ’™ğœ½ 
ğ¿(ğœ½) =â€¦
ğ’š
Mini-batch m=2
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
ğ’™ = 1 ğ‘¥1
(1) ğ‘¥2
(1)
1 ğ‘¥1
(2) ğ‘¥2
(2)
25
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
m âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
m ğ±T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ’›= ğ’™ğœ½
Model
Loss
ğ’™= 1 1.5 0.2
1 4.1 1.3
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½ = 0.6963
0.8828
ğ¿(ğœ½) = â€¦
ğ’š= 0
1
Mini-batch m=2
Dataset
2
= ğœ
0.83
2.02
= 0.6963
0.8828
à·ğ’š= ğœğ’™ğœ½= ğœ
1 1.5 0.2
1 4.1 1.3
0.1
0.5
âˆ’0.1
26
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
m âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
m ğ±T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ’›= ğ’™ğœ½
Model
Loss
ğ’™= 1 1.5 0.2
1 4.1 1.3
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½ = 0.6963
0.8828
ğ¿(ğœ½) = 0.65815
ğ’š= 0
1
Mini-batch m=2
3
ğ¿(ğœ½) = 1
m âˆ’0 1
log0.6963
log0.8828 âˆ’1 0
log(1 âˆ’0.6963)
log(1 âˆ’0.8828)
= 1
m âˆ’log0.8828âˆ’log(1 âˆ’0.6963)
= 0.1246 + 1.1917
m
= 0.65815
ğ‘¦= log(ğ‘¥)
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
m âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
m ğ±T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ’›= ğ’™ğœ½
Model
Loss
ğ’™= 1 1.5 0.2
1 4.1 1.3
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½ = 0.6963
0.8828
ğ¿(ğœ½) = 0.65815
ğ’š= 0
1
Mini-batch m=2
Dataset
4
ğ›»ğœ½ğ¿= 1
m
1 
1 
1.5 
4.1
0.2 
1.3
0.6963
0.8828 âˆ’0
1
= 1
m
1 
1 
1.5 
4.1
0.2 
1.3
0.6963
âˆ’0.1171 =
0.28961
0.28217
âˆ’0.0064
3
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
m âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
m ğ±T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ’›= ğ’™ğœ½
Model
Loss
ğ’™= 1 1.5 0.2
1 4.1 1.3
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½ = 0.6963
0.8828
ğ¿(ğœ½) = 0.65815
ğ’š= 0
1
Mini-batch m=2
Dataset
4
ğ›»ğœ½ğ¿=
0.28961
0.28217
âˆ’0.0064
ğ›‰âˆ’Î·Lğ›‰
â€² =
0.1
0.5
âˆ’0.1
âˆ’Î·
0.28961
0.28217
âˆ’0.0064
=
0.0971
0.4971
âˆ’0.099
5
3
29
â¢Vectorization
â¢Optimiztion for 1+ samples
â¢Logistic Regression â€“ Mini-batch
â¢Logistic Regression â€“ Batch
â¢BCE and MSE Loss Functions
â¢Sigmoid and Tanh Function (Optional)
Outline
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·œğ‘¦
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğœ‚is learning rate
ğ‘§= ğ’™ğœ½
Logistic Regression - Batch
Model
Input
Label
Loss
ğ’™
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
à·ğ’š= ğœğœ½ğ‘‡ğ’™ 
ğ¿(ğœ½) =â€¦
ğ’š
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
ğ’™ =
1 ğ‘¥1
(1) ğ‘¥2
(1)
1 ğ‘¥1
(2) ğ‘¥2
(2)
1 ğ‘¥1
(3) ğ‘¥2
(3)
1 ğ‘¥1
(4) ğ‘¥2
(4)
30
Dataset
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ‘§= ğ’™ğœ½
ğ’™ =
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
ğ’š=
0
0
1
1
Model
Input
Label
Loss
ğ’™
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
à·ğ’š= ğœğ’™ğœ½ 
ğ¿(ğœ½) =â€¦
ğ’š
ğœ½=
ğ‘
ğ‘¤1
ğ‘¤2
Logistic Regression - Batch
31
Model
Loss
ğ’™
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½=
1
1 + ğ‘’âˆ’ğ’›
ğ¿(ğœ½) = â€¦
ğ’š
2
= ğœ
0.78
0.83
1.49
2.02
=
0.6856
0.6963
0.8160
0.8828
à·ğ’š= ğœğ’™ğœ½= ğœ
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
0.1
0.5
âˆ’0.1
Dataset
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ‘§= ğ’™ğœ½
ğ’™ =
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
32
3
ğ¿(ğœ½) = 1
N
âˆ’
0
0
1
1
T
log
0.6856
0.6963
0.8160
0.8828
âˆ’
1
1
1
1
âˆ’
0
0
1
1
ğ‘‡
log
1âˆ’
0.6856
0.6963
0.8160
0.8828
ğ‘¦= log(ğ‘¥)
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ‘§= ğ’™ğœ½
Model
Loss
ğ’™
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½=
1
1 + ğ‘’âˆ’ğ’›
ğ¿(ğœ½) = â€¦
ğ’š
ğ’š=
0
0
1
1
à·ğ’š=
0.6856
0.6963
0.8160
0.8828
33
3
ğ¿(ğœ½) = 1
N
âˆ’
0
0
1
1
T
log
0.6856
0.6963
0.8160
0.8828
âˆ’
1
1
0
0
ğ‘‡
log
0.3144
0.3037
0.1840
0.1172
= 1
N âˆ’log0.8160âˆ’log0.8828 âˆ’log0.3144âˆ’log0.3037
ğ‘¦= log(ğ‘¥)
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ‘§= ğ’™ğœ½
= 0.6691
Model
Loss
ğ’™
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½=
1
1 + ğ‘’âˆ’ğ’›
ğ¿(ğœ½) = 0.6691
ğ’š
ğ’š=
0
0
1
1
à·ğ’š=
0.6856
0.6963
0.8160
0.8828
34
4
ğ›»ğœ½ğ¿= 1
N
1 
1 
1 
1 
1.4 1.5 3.0 4.1
0.2 0.2 1.1 1.3
0.6856
0.6963
0.8160
0.8828
âˆ’
0
0
1
1
= 1
N
1 
1 
1 
1 
1.4 1.5 3.0 4.1
0.2 0.2 1.1 1.3
0.6856
0.6963
âˆ’0.184
âˆ’0.117
=
0.2702
0.2431
âˆ’0.019
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ‘§= ğ’™ğœ½
ğ’™ =
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
à·ğ’š=
0.6856
0.6963
0.8160
0.8828
Model
Loss
ğ’™
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½=
1
1 + ğ‘’âˆ’ğ’›
ğ¿(ğœ½) = 0.6691
ğ’š
ğ’š=
0
0
1
1
3
Model
Loss
ğœ½=
0.1
0.5
âˆ’0.1
à·ğ’š= ğœğ’™ğœ½=
1
1 + ğ‘’âˆ’ğ’›
ğ¿(ğœ½) = 0.6691
4
ğ›»ğœ½ğ¿=
0.2702
0.2431
âˆ’0.019
ğ›‰âˆ’Î·Lğ›‰
â€² =
0.1
0.5
âˆ’0.1
âˆ’Î·
0.2702
0.2431
âˆ’0.019
=
0.0971
0.4971
âˆ’0.099
5
3
Dataset
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(à·œğ², ğ’š) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ›»ğœ½ğ¿
ğ‘§= ğ’™ğœ½
ğ’™
ğ’š
â¢Vectorization
â¢Optimiztion for 1+ samples
â¢Logistic Regression â€“ Mini-batch
â¢Logistic Regression â€“ Batch
â¢BCE and MSE Loss Functions
â¢Sigmoid and Tanh Function (Optional)
Outline
Hessian Matrices
â– Definition
AI VIETNAM
All-in-One Course
The Hessian matrix or Hessian is a
square matrix of second-order partial
derivatives of a scalar-valued function
https://en.wikipedia.org/wiki/Hessian_matrix
Given ğ‘“(ğ‘¥, ğ‘¦)
ğ‘“: ğ‘…2 â†’ğ‘…
ğ»ğ‘“=
ğœ•2ğ‘“
ğœ•ğ‘¥2 
ğœ•2ğ‘“
ğœ•ğ‘¥ğœ•ğ‘¦
ğœ•2ğ‘“
ğœ•ğ‘¥ğœ•ğ‘¦ 
ğœ•2ğ‘“
ğœ•ğ‘¦2
Given ğ‘“ğ‘¥, ğ‘¦= ğ‘¥2 + 2ğ‘¥2ğ‘¦+ ğ‘¦3
ğ»ğ‘“= 2 + 4ğ‘¦ 
4ğ‘¥
4ğ‘¥ 
6ğ‘¦
ğœ•ğ‘“
ğœ•ğ‘¥= 2ğ‘¥+ 4ğ‘¥ğ‘¦
ğœ•ğ‘“
ğœ•ğ‘¦= 2ğ‘¥2 + 3ğ‘¦2
37
Binary Cross-entropy
â– Convex function
L = âˆ’ğ‘¦log à·œğ‘¦âˆ’(1 âˆ’ğ‘¦)log 1 âˆ’à·œğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™
Model and Loss
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğœ•ğ¿
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘§
ğœ•ğ‘§
ğœ•ğœƒğ‘–
ğœ•à·œğ‘¦
ğœ•ğ‘§= à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ‘§
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
Derivative
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•à·œğ‘¦= âˆ’ğ‘¦
à·œğ‘¦+ 1 âˆ’ğ‘¦
1 âˆ’à·œğ‘¦=
à·œğ‘¦âˆ’ğ‘¦
à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)
ğœ•2ğ¿
ğœ•ğœƒğ‘–
2 = ğœ•
ğœ•ğœƒğ‘–
ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦) = ğ‘¥ğ‘–
2(à·œğ‘¦âˆ’à·œğ‘¦2) â‰¥0
ğ‘¥ğ‘–
2 â‰¥0
à·œğ‘¦âˆ’à·œğ‘¦2 âˆˆ0, 1
4
Logistic Regression-MSE
L = (à·œğ‘¦âˆ’ğ‘¦)2
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
Model and Loss
ğœ•ğ¿
ğœ•ğœƒğ‘–
= 2ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğœ•ğ¿
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘§
ğœ•ğ‘§
ğœ•ğœƒğ‘–
ğœ•à·œğ‘¦
ğœ•ğ‘§= à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ‘§
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
Derivative
ğœ•ğ¿
ğœ•à·œğ‘¦= 2(à·œğ‘¦âˆ’ğ‘¦)
â– Construct loss
AI VIETNAM
All-in-One Course
39
Mean Squared Error
L = (à·œğ‘¦âˆ’ğ‘¦)2
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
Model and Loss
ğœ•ğ¿
ğœ•ğœƒğ‘–
= 2ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğœ•ğ¿
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘§
ğœ•ğ‘§
ğœ•ğœƒğ‘–
ğœ•à·œğ‘¦
ğœ•ğ‘§= à·œğ‘¦(1 âˆ’à·œğ‘¦)
ğœ•ğ‘§
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
Derivative
ğœ•ğ¿
ğœ•à·œğ‘¦= 2(à·œğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğœƒğ‘–= 2ğ‘¥ğ‘–(à·œğ‘¦âˆ’ğ‘¦)à·œğ‘¦(1 âˆ’à·œğ‘¦) = 2ğ‘¥ğ‘–âˆ’à·œğ‘¦3 + à·œğ‘¦2 âˆ’ğ‘¦à·œğ‘¦+ ğ‘¦à·œğ‘¦2
ğœ•2ğ¿
ğœ•ğœƒğ‘–
2 = ğœ•
ğœ•ğœƒğ‘–
2ğ‘¥ğ‘–âˆ’à·œğ‘¦3 + à·œğ‘¦2 âˆ’ğ‘¦à·œğ‘¦+ ğ‘¦à·œğ‘¦2
= 2ğ‘¥ğ‘–âˆ’3à·œğ‘¦2ğ‘¥ğ‘–à·œğ‘¦1 âˆ’à·œğ‘¦+ 2ğ‘¥ğ‘–à·œğ‘¦à·œğ‘¦1 âˆ’à·œğ‘¦âˆ’ğ‘¦ğ‘¥ğ‘–à·œğ‘¦(1 âˆ’à·œğ‘¦) + 2ğ‘¥ğ‘–ğ‘¦à·œğ‘¦à·œğ‘¦1 âˆ’à·œğ‘¦
= 2ğ‘¥ğ‘–
2à·œğ‘¦(1 âˆ’à·œğ‘¦) âˆ’3à·œğ‘¦2 + 2à·œğ‘¦âˆ’ğ‘¦+ 2ğ‘¦à·œğ‘¦
40
Mean Squared Error
ğœ•2ğ¿
ğœ•ğœƒğ‘–
2 = 2ğ‘¥ğ‘–
2 à·œğ‘¦(1 âˆ’à·œğ‘¦) âˆ’3à·œğ‘¦2 + 2à·œğ‘¦âˆ’ğ‘¦+ 2ğ‘¦à·œğ‘¦
ğ‘¥ğ‘–
2 â‰¥0
à·œğ‘¦(1 âˆ’à·œğ‘¦) âˆˆ0, 1
4
ğ‘¦= 0
ğ‘“(à·œğ‘¦) = âˆ’3à·œğ‘¦2 + 2à·œğ‘¦
ğ‘¦= 1
ğ‘“à·œğ‘¦= âˆ’3à·œğ‘¦2 + 4à·œğ‘¦âˆ’1
ğ‘“(à·œğ‘¦) = âˆ’3à·œğ‘¦2 + 2à·œğ‘¦
ğ‘“à·œğ‘¦= âˆ’3à·œğ‘¦2 + 4à·œğ‘¦âˆ’1
41
MSE and BCE
AI VIETNAM
All-in-One Course
â– Visualization
Mean Squared Error
Binary Cross-Entropy
42
Sigmoid and Tanh Functions
AI VIETNAM
All-in-One Course
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥
sigmoid ğ‘¥=
1
1 + ğ‘’âˆ’ğ‘¥
35
1
2
3
4
Sigmoid and Tanh Functions
AI VIETNAM
All-in-One Course
tanh ğ‘¥= 2 Ã—
1
1 + ğ‘’âˆ’2ğ‘¥âˆ’1
sigmoid 2ğ‘¥=
1
1 + ğ‘’âˆ’2ğ‘¥
tanh ğ‘¥= 2 Ã— sigmoid 2ğ‘¥âˆ’1
â¢Vectorization
â¢Optimiztion for 1+ samples
â¢Logistic Regression â€“ Mini-batch
â¢Logistic Regression â€“ Batch
â¢BCE and MSE Loss Functions
â¢Sigmoid and Tanh Function (Optional)
Outline
Tanh function
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥= 1 âˆ’ğ‘’âˆ’2ğ‘¥
1 + ğ‘’âˆ’2ğ‘¥
= âˆ’ğ‘’âˆ’2ğ‘¥âˆ’1
ğ‘’âˆ’2ğ‘¥+ 1 =
2
ğ‘’âˆ’2ğ‘¥+ 1 âˆ’1
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥= ğ‘’2ğ‘¥âˆ’1
ğ‘’2ğ‘¥+ 1
= 1 âˆ’
2
ğ‘’2ğ‘¥+ 1
AI VIETNAM
All-in-One Course
Tanh function
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥= ğ‘’2ğ‘¥âˆ’1
ğ‘’2ğ‘¥+ 1 = 1 âˆ’
2
ğ‘’2ğ‘¥+ 1
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥= 1 âˆ’ğ‘’âˆ’2ğ‘¥
1 + ğ‘’âˆ’2ğ‘¥= âˆ’ğ‘’âˆ’2ğ‘¥âˆ’1
ğ‘’âˆ’2ğ‘¥+ 1 =
2
ğ‘’âˆ’2ğ‘¥+ 1 âˆ’1
ğ‘¡ğ‘ğ‘›â„â€²(ğ‘¥) =
ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥
â€²
= ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥âˆ’ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥2
= ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥2 âˆ’ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥2
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥2
= 1 âˆ’ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥
2
= 1 âˆ’ğ‘¡ğ‘ğ‘›â„2(ğ‘¥)
AI VIETNAM
All-in-One Course
Tanh function
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥= ğ‘’2ğ‘¥âˆ’1
ğ‘’2ğ‘¥+ 1 = 1 âˆ’
2
ğ‘’2ğ‘¥+ 1
tanh ğ‘¥= ğ‘’ğ‘¥âˆ’ğ‘’âˆ’ğ‘¥
ğ‘’ğ‘¥+ ğ‘’âˆ’ğ‘¥= 1 âˆ’ğ‘’âˆ’2ğ‘¥
1 + ğ‘’âˆ’2ğ‘¥= âˆ’ğ‘’âˆ’2ğ‘¥âˆ’1
ğ‘’âˆ’2ğ‘¥+ 1 =
2
ğ‘’âˆ’2ğ‘¥+ 1 âˆ’1
ğ‘¡ğ‘ğ‘›â„â€²(ğ‘¥) =
2
ğ‘’âˆ’2ğ‘¥+ 1 âˆ’1 
â€²
=
4ğ‘’âˆ’2ğ‘¥
ğ‘’âˆ’2ğ‘¥+ 1 2 = 4 ğ‘’âˆ’2ğ‘¥+ 1 âˆ’1
ğ‘’âˆ’2ğ‘¥+ 1 2  
= 4
1
ğ‘’âˆ’2ğ‘¥+ 1 âˆ’
1
ğ‘’âˆ’2ğ‘¥+ 1 2
= âˆ’
4
ğ‘’âˆ’2ğ‘¥+ 1 2 âˆ’
4
ğ‘’âˆ’2ğ‘¥+ 1
= âˆ’
4
ğ‘’âˆ’2ğ‘¥+ 1 2 âˆ’
4
ğ‘’âˆ’2ğ‘¥+ 1 + 1 âˆ’1
= 1 âˆ’
2
ğ‘’âˆ’2ğ‘¥+ 1 âˆ’1
2
= 1 âˆ’ğ‘¡ğ‘ğ‘›â„2(ğ‘¥)
AI VIETNAM
All-in-One Course
Logistic Regression
Tanh
â– Construct loss
tanh ğ‘¥=
2
1 + ğ‘’âˆ’2ğ‘¥âˆ’1
L = âˆ’ğ‘¦log à·œğ‘¦ğ‘ âˆ’(1 âˆ’ğ‘¦)log 1 âˆ’à·œğ‘¦ğ‘ 
à·œğ‘¦= ğ‘¡ğ‘ğ‘›â„(ğ‘§) = ğ‘’ğ‘§âˆ’ğ‘’âˆ’ğ‘§
ğ‘’ğ‘§+ ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
Model and Loss
à·œğ‘¦ğ‘ = à·œğ‘¦+ 1
2
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğœ•ğ¿
ğœ•à·œğ‘¦ğ‘ 
ğœ•à·œğ‘¦ğ‘ 
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘§
ğœ•ğ‘§
ğœ•ğœƒğ‘–
ğœ•à·œğ‘¦
ğœ•ğ‘§= 1 âˆ’à·œğ‘¦2
ğœ•ğ‘§
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
Derivative
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
(à·œğ‘¦ğ‘ âˆ’ğ‘¦)(1 âˆ’à·œğ‘¦2)
2à·œğ‘¦ğ‘ (1 âˆ’à·œğ‘¦ğ‘ )
ğœ•ğ¿
ğœ•à·œğ‘¦ğ‘ 
= âˆ’ğ‘¦
à·œğ‘¦ğ‘ 
+ 1 âˆ’ğ‘¦
1 âˆ’à·œğ‘¦ğ‘ 
=
à·œğ‘¦ğ‘ âˆ’ğ‘¦
à·œğ‘¦ğ‘ (1 âˆ’à·œğ‘¦ğ‘ )
ğœ•à·œğ‘¦ğ‘ 
ğœ•à·œğ‘¦= 1
2
Logistic Regression
Tanh
â– Construct loss
tanh ğ‘¥=
2
1 + ğ‘’âˆ’2ğ‘¥âˆ’1
L = âˆ’ğ‘¦log à·œğ‘¦ğ‘ âˆ’(1 âˆ’ğ‘¦)log 1 âˆ’à·œğ‘¦ğ‘ 
à·œğ‘¦= ğ‘¡ğ‘ğ‘›â„(ğ‘§) = ğ‘’ğ‘§âˆ’ğ‘’âˆ’ğ‘§
ğ‘’ğ‘§+ ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
Model and Loss
à·œğ‘¦ğ‘ = à·œğ‘¦+ 1
2
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğœ•ğ¿
ğœ•à·œğ‘¦ğ‘ 
ğœ•à·œğ‘¦ğ‘ 
ğœ•à·œğ‘¦
ğœ•à·œğ‘¦
ğœ•ğ‘§
ğœ•ğ‘§
ğœ•ğœƒğ‘–
ğœ•à·œğ‘¦
ğœ•ğ‘§= 1 âˆ’à·œğ‘¦2
ğœ•ğ‘§
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
Derivative
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
(à·œğ‘¦ğ‘ âˆ’ğ‘¦)(1 âˆ’à·œğ‘¦2)
2à·œğ‘¦ğ‘ (1 âˆ’à·œğ‘¦ğ‘ )
ğœ•ğ¿
ğœ•à·œğ‘¦ğ‘ 
= âˆ’ğ‘¦
à·œğ‘¦ğ‘ 
+ 1 âˆ’ğ‘¦
1 âˆ’à·œğ‘¦ğ‘ 
=
à·œğ‘¦ğ‘ âˆ’ğ‘¦
à·œğ‘¦ğ‘ (1 âˆ’à·œğ‘¦ğ‘ )
ğœ•à·œğ‘¦ğ‘ 
ğœ•à·œğ‘¦= 1
2
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
(à·œğ‘¦+ 1
2
âˆ’ğ‘¦)(1 âˆ’à·œğ‘¦2)
2 à·œğ‘¦+ 1
2
(1 âˆ’à·œğ‘¦+ 1
2
)
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–
(à·œğ‘¦+ 1 âˆ’2ğ‘¦)(1 âˆ’à·œğ‘¦2)
(à·œğ‘¦+ 1)(1 âˆ’à·œğ‘¦)
ğœ•ğ¿
ğœ•ğœƒğ‘–
= ğ‘¥ğ‘–(à·œğ‘¦+ 1 âˆ’2ğ‘¦)
Summary
ğ‘¥
+âˆ
âˆ’âˆ
ğ‘¥1
ğ‘¥2
ğ’š
ğ‘¦1
ğ‘¦2
ğ‘¦=
1
1 + ğ‘’âˆ’ğ‘¥
Sigmoid function
4) Compute derivative
5) Update parameters
1) Pick all the samples from training data   
2) Compute output à·ğ’š
3) Compute loss (binary cross-entropy)
à·ğ’š= ğœ(ğ’›) =
1
1 + ğ‘’âˆ’ğ’›
ğ¿(ğœ½) = 1
N âˆ’ğ²Tlogà·œğ²âˆ’(1âˆ’y)Tlog(1âˆ’à·œğ² )
ğ›»ğœ½ğ¿= 1
N ğ’™T(à·œğ²âˆ’ğ’š)
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
â€²
ğœ‚is learning rate
ğ’›= ğ’™ğœ½
