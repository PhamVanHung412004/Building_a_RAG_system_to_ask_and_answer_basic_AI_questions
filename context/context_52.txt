Chá»§ Ä‘á»:
Há»c giÃ¡m sÃ¡t (Supervised Learning)
Má»¥c Ä‘Ã­ch buá»•i há»c
â—Há»c viÃªn Ä‘Æ°á»£c tiáº¿p cáº­n cÃ¡c khÃ¡i niá»‡m, ká»¹ thuáº­t vÃ  mÃ´ hÃ¬nh 
há»c mÃ¡y cÆ¡ báº£n vÃ  phá»• dá»¥ng vá»›i bÃ i toÃ¡n há»c giÃ¡m sÃ¡t 
(Supervised Learning)
â—‹BÃ i toÃ¡n há»“i quy (Regression)
â—‹BÃ i toÃ¡n phÃ¢n loáº¡i (Classification)
â—Há»c viÃªn Ä‘Æ°á»£c tiáº¿p cáº­n vá»›i cÃ¡c á»©ng dá»¥ng bÃ i toÃ¡n há»c giÃ¡m 
sÃ¡t vá»›i nhiá»u vÃ­ dá»¥ minh há»a, dá»¯ liá»‡u tá»« cÃ¡c bÃ i toÃ¡n thá»±c 
táº¿.
â—‹Dá»± Ä‘oÃ¡n/ dá»± bÃ¡o giÃ¡ cáº£
â—‹PhÃ¢n loáº¡i thÃ´ng tin
Ná»™i dung chÃ­nh
Há»c giÃ¡m sÃ¡t (Supervised Learning)
Classification
Regression
Tá»•ng quan vá» há»c mÃ¡y (Machine Learning)
â—Má»™t thuáº­t toÃ¡n machine learning lÃ  má»™t thuáº­t toÃ¡n cÃ³ kháº£ 
nÄƒng há»c tá»« dá»¯ liá»‡u.
â—Äá»‹nh nghÄ©a há»c (chÆ°Æ¡ng trÃ¬nh mÃ¡y tÃ­nh)
â—‹â€œMá»™t chÆ°Æ¡ng trÃ¬nh mÃ¡y tÃ­nh Ä‘Æ°á»£c gá»i lÃ  há»c tá»« kinh 
nghiá»‡m E Ä‘á»ƒ hoÃ n thÃ nh nhiá»‡m vá»¥ T, vá»›i hiá»‡u quáº£ Ä‘Æ°á»£c 
Ä‘o báº±ng phÃ©p Ä‘Ã¡nh giÃ¡ P, náº¿u hiá»‡u quáº£ cá»§a nÃ³ khi thá»±c 
hiá»‡n nhiá»‡m vá»¥ T, khi Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ bá»Ÿi P, cáº£i thiá»‡n theo 
kinh nghiá»‡m E.â€ - Tom Mitchell 1997
â—NhÆ° váº­y má»™t bÃ i toÃ¡n há»c mÃ¡y cÃ³ thá»ƒ biá»ƒu diá»…n báº±ng 1 bá»™ (T, 
P, E)
â—‹T: má»™t cÃ´ng viá»‡c (nhiá»‡m vá»¥)
â—‹P: tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng
â—‹E: kinh nghiá»‡m
Tá»•ng quan vá» há»c mÃ¡y (Machine Learning) - VÃ­ dá»¥
â—Lá»c thÆ° rÃ¡c (email spam filtering)
â—‹T: Dá»± Ä‘oÃ¡n (Ä‘á»ƒ lá»c) nhá»¯ng thÆ° 
Ä‘iá»‡n tá»­ nÃ o lÃ  thÆ° rÃ¡c (spam 
email)
â—‹P: sá»‘ lÆ°á»£ng thÆ° Ä‘iá»‡n tá»­ gá»­i Ä‘áº¿n 
Ä‘Æ°á»£c phÃ¢n loáº¡i chÃ­nh xÃ¡c
â—‹E: Má»™t táº­p cÃ¡c thÆ° Ä‘iá»‡n tá»­ (emails) 
máº«u, má»—i thÆ° Ä‘iá»‡n tá»­ Ä‘Æ°á»£c biá»ƒu 
diá»…n báº±ng má»™t táº­p thuá»™c tÃ­nh (vd: 
táº­p tá»« khÃ³a) vÃ  nhÃ£n lá»›p (thÆ° 
thÆ°á»ng/thÆ° rÃ¡c) tÆ°Æ¡ng á»©ng
â—Nháº­n dáº¡ng chá»¯ viáº¿t tay
â—‹T: Nháº­n dáº¡ng vÃ  phÃ¢n loáº¡i cÃ¡c 
tá»« trong cÃ¡c áº£nh chá»¯ viáº¿t
â—‹P: Tá»· lá»‡ (%) cÃ¡c tá»« Ä‘Æ°á»£c nháº­n 
dáº¡ng vÃ  phÃ¢n loáº¡i Ä‘Ãºng
â—‹E: Má»™t táº­p cÃ¡c áº£nh chá»¯ viáº¿t, 
trong Ä‘Ã³ má»—i áº£nh Ä‘Æ°á»£c gáº¯n vá»›i 
má»™t Ä‘á»‹nh danh cá»§a má»™t tá»«
Tá»•ng quan vá» há»c mÃ¡y (Machine Learning) - VÃ­ dá»¥
â—GÃ¡n nhÃ£n áº£nh
â—‹T: Ä‘Æ°a ra má»™t vÃ i mÃ´ táº£ Ã½ nghÄ©a cá»§a 1 bá»©c áº£nh
â—‹P: ?
â—‹E: Má»™t táº­p cÃ¡c bá»©c áº£nh, trong Ä‘Ã³ má»—i áº£nh Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n má»™t 
táº­p cÃ¡c tá»« mÃ´ táº£ Ã½ nghÄ©a cá»§a chÃºng
Tá»•ng quan vá» há»c mÃ¡y (Machine Learning) - VÃ­ dá»¥
PhÃ¢n loáº¡i cÃ¡c bÃ i toÃ¡n há»c mÃ¡y
â—Há»c giÃ¡m sÃ¡t (Supervised Learning):
â—‹Dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n cho cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ
â—‹Dá»± Ä‘oÃ¡n Ä‘áº§u ra cá»§a má»™t hoáº·c nhiá»u dá»¯ liá»‡u má»›i dá»±a trÃªn cÃ¡c 
cáº·p (Ä‘áº§u vÃ o, Ä‘áº§u ra) Ä‘Ã£ biáº¿t tá»« trÆ°á»›c.
â—Há»c khÃ´ng giÃ¡m sÃ¡t (Unsupervised Learning):
â—‹Dá»¯ liá»‡u Ä‘áº§u vÃ o khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n
â—‹TÃ¬m kiáº¿m thÃ´ng tin, cÃ¡c Ä‘áº·c trÆ°ng cá»§a bá»™ dá»¯ liá»‡u
â—Há»c tÄƒng cÆ°á»ng (Reinforcement Learning):
â—‹NghiÃªn cá»©u cÃ¡ch thá»©c má»™t agent trong má»™t mÃ´i trÆ°á»ng nÃªn 
chá»n thá»±c hiá»‡n cÃ¡c hÃ nh Ä‘á»™ng nÃ o Ä‘á»ƒ cá»±c Ä‘áº¡i hÃ³a má»™t khoáº£n 
thÆ°á»Ÿng (reward) nÃ o Ä‘Ã³ vá» lÃ¢u dÃ i.
â—‹KhÃ´ng cÃ³ cáº·p dá»¯ liá»‡u vÃ o/káº¿t quáº£ Ä‘Ãºng, cÃ¡c hÃ nh Ä‘á»™ng gáº§n 
tá»‘i Æ°u cÅ©ng khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ Ä‘Ãºng sai tÆ°á»ng minh
KhÃ¡i niá»‡m chung há»c giÃ¡m sÃ¡t (Supervised Learning)
â—Há»c cÃ³ giÃ¡m sÃ¡t lÃ  má»™t kÄ© thuáº­t cá»§a ngÃ nh há»c mÃ¡y Ä‘á»ƒ xÃ¢y 
dá»±ng má»™t hÃ m tá»« dá»¯ liá»‡u huáº¥n luyá»‡n. Dá»¯ liá»‡u huáº¥n luyá»‡n bao 
gá»“m cÃ¡c cáº·p gá»“m Ä‘á»‘i tÆ°á»£ng Ä‘áº§u vÃ o, vÃ  Ä‘áº§u ra mong muá»‘n. 
â—‹Äáº§u ra cá»§a hÃ m dá»± Ä‘oÃ¡n lÃ  má»™t giÃ¡ trá»‹ liÃªn tá»¥c
=> BÃ i toÃ¡n há»“i quy
â—‹Dá»± Ä‘oÃ¡n má»™t nhÃ£n phÃ¢n loáº¡i cho má»™t Ä‘á»‘i tÆ°á»£ng Ä‘áº§u vÃ o 
=> BÃ i toÃ¡n phÃ¢n loáº¡i
KhÃ¡i niá»‡m chung há»c giÃ¡m sÃ¡t (Supervised Learning)
VÃ­ dá»¥ cho bÃ i toÃ¡n phÃ¢n loáº¡i
â—PhÃ¢n loáº¡i thÆ° rÃ¡c:
â—‹Xem thÆ° nháº­n Ä‘Æ°á»£c lÃ  thÆ° rÃ¡c hay khÃ´ng 
â—‹NhÃ£n: thÆ° rÃ¡c hoáº·c thÆ° thÃ´ng thÆ°á»ng
VÃ­ dá»¥ há»“i quy
â—Dá»± Ä‘oÃ¡n giÃ¡ xe:
â—‹ÄoÃ¡n giÃ¡ xe dá»±a trÃªn táº­p há»£p cÃ¡c Ä‘áº·c trÆ°ng: sá»‘ cÃ¢y sá»‘ Ä‘Ã£ Ä‘i, 
tuá»•i xe, hÃ£ng xe, Ä‘Ã£ gáº·p tai náº¡n chÆ°aâ€¦
â—‹NhÃ£n: thÃ´ng tin Ä‘áº·c trÆ°ng cá»§a cÃ¡c máº«u xe khÃ¡c nhau cÃ¹ng giÃ¡ 
cá»§a chÃºng
CÃ³ thá»ƒ Ã¡p dá»¥ng thuáº­t toÃ¡n há»“i quy cho phÃ¢n loáº¡i
â—Há»“i quy logistic: giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng lÃ  xÃ¡c suáº¥t máº«u thuá»™c 1 lá»›p 
nháº¥t Ä‘á»‹nh
Quy trÃ¬nh há»c giÃ¡m sÃ¡t
Classification
Supervised Classification
Ã tÆ°á»Ÿng:
Äi tÃ¬m ranh giá»›i phÃ¢n cÃ¡ch (decision boundary) giá»¯a cÃ¡c lá»›p dá»¯ liá»‡u
Supervised Classification
BÃ i toÃ¡n:
Cho cÃ¡c cáº·p máº«u quan sÃ¡t vÃ  Ä‘áº§u ra tÆ°Æ¡ng á»©ng (x1,
y1),
(x2,
y2),
...,
(xn,
yn). Cáº§n tÃ¬m hÃ m xáº¥p xá»‰ 
f(x) Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ Ä‘áº§u ra y khi cho dá»¯ liá»‡u Ä‘áº§u vÃ o x
â—Äáº§u ra - y: dáº¡ng nhÃ£n (category)
â—Äáº§u vÃ o - x: CÃ³ thá»ƒ lÃ  dá»¯ liá»‡u nhiá»u chiá»u (multi-dimensions)
â—‹Má»—i chiá»u tÆ°Æ¡ng á»©ng vá»›i má»™t thuá»™c tÃ­nh (attribute)
Supervised Classification - VÃ­ dá»¥
PhÃ¢n lá»›p hoa hoa DiÃªn vÄ© (Iris)
â—3 loÃ i hoa, má»—i loÃ i gá»“m 50 cÃ¡ thá»ƒ:
â—‹Iris setosa; Iris virginica, Iris versicolor 
â—4 Ä‘áº·c trÆ°ng:
â—‹Äá»™ dÃ i cá»§a lÃ¡
â—‹Äá»™ rá»™ng cá»§a lÃ¡
â—‹Äá»™ dÃ i cá»§a cÃ¡nh hoa 
â—‹Äá»™ rá»™ng cá»§a cÃ¡nh hoa
XÃ¢y dá»±ng chÆ°Æ¡ng trÃ¬nh tá»± Ä‘á»™ng phÃ¢n lá»›p hoa
â—Cho má»™t bÃ´ng hoa Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi 4 Ä‘áº·c trÆ°ng
â—ÄÆ°a bÃ´ng hoa Ä‘Ã³ vÃ o 1 trong 3 lá»›p
Supervised Classification - VÃ­ dá»¥
Táº­p dá»¯ liá»‡u IRIS
Supervised Classification - VÃ­ dá»¥
Kiá»ƒm tra:
Dá»±a vÃ o cÃ¡c thÃ´ng tin thuá»™c tÃ­nh cá»§a loÃ i hoa, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n 
luyá»‡n Ä‘á»ƒ Ä‘Æ°a ra káº¿t quáº£ phÃ¢n loáº¡i.
CÃ¡c thuáº­t toÃ¡n phÃ¢n loáº¡i (classification algorithms)
â—NaÃ¯ve Bayes classifier
â—k-Nearest Neighbors
â—Decision Tree
â—Random Forest
â—Support vector machine
â—â€¦
NaÃ¯ve Bayes classifier
â—LÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p há»c phÃ¢n lá»›p cÃ³ giÃ¡m sÃ¡t vÃ  dá»±a trÃªn 
xÃ¡c suáº¥t
â—Dá»±a trÃªn má»™t mÃ´ hÃ¬nh (hÃ m) xÃ¡c suáº¥t
â—Viá»‡c phÃ¢n loáº¡i dá»±a trÃªn cÃ¡c giÃ¡ trá»‹ xÃ¡c suáº¥t cá»§a cÃ¡c kháº£ nÄƒng 
xáº£y ra cá»§a cÃ¡c giáº£ thiáº¿t
â—LÃ  má»™t trong cÃ¡c phÆ°Æ¡ng phÃ¡p há»c mÃ¡y thÆ°á»ng Ä‘Æ°á»£c sá»­ 
dá»¥ng trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿
â—Dá»±a trÃªn Ä‘á»‹nh lÃ½ Bayes (Bayes theorem)
Äá»‹nh lÃ½ Bayes
â—P(h): XÃ¡c suáº¥t trÆ°á»›c (tiÃªn nghiá»‡m) cá»§a giáº£ thiáº¿t h
â—P(D): XÃ¡c suáº¥t trÆ°á»›c (tiÃªn nghiá»‡m) cá»§a viá»‡c quan sÃ¡t Ä‘Æ°á»£c dá»¯ 
liá»‡u D
â—P(D|h): XÃ¡c suáº¥t (cÃ³ Ä‘iá»u kiá»‡n) cá»§a viá»‡c quan sÃ¡t Ä‘Æ°á»£c dá»¯ 
liá»‡u D, náº¿u biáº¿t giáº£ thiáº¿t h lÃ  Ä‘Ãºng. (likelihood)
â—P(h|D): XÃ¡c suáº¥t (háº­u nghiá»‡m) cá»§a giáº£ thiáº¿t h lÃ  Ä‘Ãºng, náº¿u 
quan sÃ¡t Ä‘Æ°á»£c dá»¯ liá»‡u D
â”Nhiá»u phÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i dá»±a trÃªn xÃ¡c suáº¥t sáº½ sá»­ 
dá»¥ng xÃ¡c suáº¥t háº­u nghiá»‡m (posterior probability) nÃ y!
Äá»‹nh lÃ½ Bayes - VÃ­ dá»¥
Giáº£ sá»­ chÃºng ta cÃ³ táº­p dá»¯ liá»‡u sau (dá»± Ä‘oÃ¡n 1 ngÆ°á»i cÃ³ chÆ¡i tennis)?
Äá»‹nh lÃ½ Bayes - VÃ­ dá»¥
â—Dá»¯ liá»‡u D. NgoÃ i trá»i lÃ  náº¯ng vÃ  GiÃ³ lÃ  máº¡nh
â—Giáº£ thiáº¿t (phÃ¢n loáº¡i) h. Anh ta chÆ¡i tennis
â—XÃ¡c suáº¥t trÆ°á»›c P(h). XÃ¡c suáº¥t ráº±ng anh ta chÆ¡i tennis (báº¥t ká»ƒ 
NgoÃ i trá»i nhÆ° tháº¿ nÃ o vÃ  GiÃ³ ra sao)
â—XÃ¡c suáº¥t trÆ°á»›c P(D). XÃ¡c suáº¥t ráº±ng NgoÃ i trá»i lÃ  náº¯ng vÃ  GiÃ³ lÃ  
máº¡nh
â—P(D|h). XÃ¡c suáº¥t NgoÃ i trá»i lÃ  náº¯ng vÃ  GiÃ³ lÃ  máº¡nh, náº¿u biáº¿t ráº±ng 
anh ta chÆ¡i tennis
â—P(h|D). XÃ¡c suáº¥t anh ta chÆ¡i tennis, náº¿u biáº¿t ráº±ng NgoÃ i trá»i lÃ  
náº¯ng vÃ  GiÃ³ lÃ  máº¡nh
NaÃ¯ve Bayes classifier
â—Biá»ƒu diá»…n bÃ i toÃ¡n phÃ¢n loáº¡i (classification problem)
â—‹Má»™t táº­p há»c D_train, trong Ä‘Ã³ má»—i vÃ­ dá»¥ há»c x Ä‘Æ°á»£c biá»ƒu 
diá»…n lÃ  má»™t vectÆ¡ n chiá»u: (x1, x2, ..., xn)
â—‹Má»™t táº­p xÃ¡c Ä‘á»‹nh cÃ¡c nhÃ£n lá»›p: C={c1, c2, ..., cm}
â—‹Vá»›i má»™t vÃ­ dá»¥ (má»›i) z, thÃ¬ z sáº½ Ä‘Æ°á»£c phÃ¢n vÃ o lá»›p nÃ o?
â—Má»¥c tiÃªu: XÃ¡c Ä‘á»‹nh phÃ¢n lá»›p cÃ³ thá»ƒ (phÃ¹ há»£p) nháº¥t Ä‘á»‘i vá»›i z
NaÃ¯ve Bayes classifier
â—Äá»ƒ tÃ¬m Ä‘Æ°á»£c phÃ¢n lá»›p cÃ³ thá»ƒ nháº¥t Ä‘á»‘i vá»›i zâ€¦
â—Giáº£ thuyáº¿t (assumption) trong phÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i NaÃ¯ve 
Bayes: CÃ¡c thuá»™c tÃ­nh lÃ  Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n (conditionally 
independent) Ä‘á»‘i vá»›i cÃ¡c lá»›p
â—PhÃ¢n loáº¡i NaÃ¯ve Bayes tÃ¬m phÃ¢n lá»›p cÃ³ thá»ƒ nháº¥t Ä‘á»‘i vá»›i z
NaÃ¯ve Bayes classifier
â—Giai Ä‘oáº¡n há»c (training phase), sá»­ dá»¥ng má»™t táº­p há»c
â—‹Äá»‘i vá»›i má»—i phÃ¢n lá»›p cÃ³ thá»ƒ (má»—i nhÃ£n lá»›p) ci âˆˆC
â– TÃ­nh giÃ¡ trá»‹ xÃ¡c suáº¥t tiÃªn nghiá»‡m: P(ci)
â– Äá»‘i vá»›i má»—i giÃ¡ trá»‹ thuá»™c tÃ­nh xj, tÃ­nh giÃ¡ trá»‹ xÃ¡c suáº¥t xáº£y ra 
cá»§a giÃ¡ trá»‹ thuá»™c tÃ­nh Ä‘Ã³ Ä‘á»‘i vá»›i má»™t phÃ¢n lá»›p cz: P(xj|ci)
â—Giai Ä‘oáº¡n phÃ¢n lá»›p (classification phase), Ä‘á»‘i vá»›i má»™t vÃ­ dá»¥ má»›i
â—‹Äá»‘i vá»›i má»—i phÃ¢n lá»›p ci âˆˆC, tÃ­nh giÃ¡ trá»‹ cá»§a biá»ƒu thá»©c:
â—‹XÃ¡c Ä‘á»‹nh phÃ¢n lá»›p cá»§a z lÃ  lá»›p cÃ³ thá»ƒ nháº¥t c*
NaÃ¯ve Bayes classifier - VÃ­ dá»¥
Má»™t sinh viÃªn tráº» vá»›i thu nháº­p trung bÃ¬nh vÃ  má»©c Ä‘Ã¡nh giÃ¡ tÃ­n dá»¥ng bÃ¬nh 
thÆ°á»ng sáº½ mua má»™t cÃ¡i mÃ¡y tÃ­nh?
NaÃ¯ve Bayes classifier - VÃ­ dá»¥
Biá»ƒu diá»…n bÃ i toÃ¡n phÃ¢n loáº¡i
â—z = (Age=Young, Income=Medium, Student=True, Credit_Rating=Fair)
â—CÃ³ 2 phÃ¢n lá»›p cÃ³ thá»ƒ: 
â—‹c1 (â€œMua mÃ¡y tÃ­nhâ€)
â—‹c2 (â€œKhÃ´ng mua mÃ¡y tÃ­nhâ€)
â—TÃ­nh giÃ¡ trá»‹ xÃ¡c suáº¥t trÆ°á»›c cho má»—i phÃ¢n lá»›p
â—‹P(c1) = 9/14
â—‹P(c2) = 5/14
â—TÃ­nh giÃ¡ trá»‹ xÃ¡c suáº¥t cá»§a má»—i giÃ¡ trá»‹ thuá»™c tÃ­nh Ä‘á»‘i vá»›i má»—i phÃ¢n lá»›p
â—‹P(Age=Young|c1) = 2/9
â—‹P(Age=Young|c2) = 3/5
â—‹P(Income=Medium|c1) = 4/9
â—‹P(Income=Medium|c2) = 2/5
â—‹P(Student=Yes|c1) = 6/9
â—‹P(Student=Yes|c2) = 1/5
â—‹P(Credit_Rating=Fair|c1) = 6/9
â—‹P(Credit_Rating=Fair|c2) = 2/5
NaÃ¯ve Bayes classifier - VÃ­ dá»¥
TÃ­nh toÃ¡n xÃ¡c suáº¥t cÃ³ thá»ƒ xáº£y ra (likelihood) cá»§a vÃ­ dá»¥ z Ä‘á»‘i vá»›i má»—i phÃ¢n 
lá»›p
â—Äá»‘i vá»›i phÃ¢n lá»›p c1
P(z|c1) = P(Age=Young|c1).P(Income=Medium|c1).P(Student=Yes|c1).
P(Credit_Rating=Fair|c1) = (2/9).(4/9).(6/9).(6/9) = 0.044
â—Äá»‘i vá»›i phÃ¢n lá»›p c2
P(z|c2) = P(Age=Young|c2).P(Income=Medium|c2).P(Student=Yes|c2).
P(Credit_Rating=Fair|c2) = (3/5).(2/5).(1/5).(2/5) = 0.019
XÃ¡c Ä‘á»‹nh phÃ¢n lá»›p cÃ³ thá»ƒ nháº¥t (the most probable class)
â—Äá»‘i vá»›i phÃ¢n lá»›p c1
P(c1).P(z|c1) = (9/14).(0.044) = 0.028
â—Äá»‘i vá»›i phÃ¢n lá»›p c2
P(c2).P(z|c2) = (5/14).(0.019) = 0.007
â†’ Káº¿t luáº­n: Anh ta (z) sáº½ mua má»™t mÃ¡y tÃ­nh!
k-Nearest Neighbors
â—K-nearest neighbors (k-NN) lÃ  má»™t trong sá»‘ cÃ¡c phÆ°Æ¡ng phÃ¡p 
phá»• biáº¿n trong há»c mÃ¡y. VÃ i tÃªn gá»i khÃ¡c nhÆ°:
â—‹Instance-based learning 
â—‹Lazy learning 
â—‹Memory-based learning
â—Ã tÆ°á»Ÿng cá»§a phÆ°Æ¡ng phÃ¡p
â—‹KhÃ´ng xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh (mÃ´ táº£) rÃµ rÃ ng cho hÃ m 
má»¥c tiÃªu cáº§n há»c.
â—‹QuÃ¡ trÃ¬nh há»c chá»‰ lÆ°u láº¡i cÃ¡c dá»¯ liá»‡u huáº¥n luyá»‡n.
â—‹Viá»‡c dá»± Ä‘oÃ¡n cho má»™t quan sÃ¡t má»›i sáº½ dá»±a vÃ o cÃ¡c hÃ ng 
xÃ³m gáº§n nháº¥t trong táº­p há»c.
â—Do Ä‘Ã³ k-NN lÃ  má»™t phÆ°Æ¡ng phÃ¡p phi tham sá»‘ (nonparametric 
methods)
â—Hai thÃ nh pháº§n chÃ­nh:
â—‹Äá»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng (similarity measure/distance) giá»¯a cÃ¡c 
Ä‘á»‘i tÆ°á»£ng.
â—‹CÃ¡c hÃ ng xÃ³m sáº½ dÃ¹ng vÃ o viá»‡c phÃ¡n Ä‘oÃ¡n
k-Nearest Neighbors
â—XÃ©t 1 lÃ¡ng giá»ng gáº§n nháº¥t
â†’ GÃ¡n z vÃ o lá»›p c2
â—XÃ©t 3 lÃ¡ng giá»ng gáº§n nháº¥t
â†’ GÃ¡n z vÃ o lá»›p c1
â—XÃ©t 5 lÃ¡ng giá»ng gáº§n nháº¥t
â†’ GÃ¡n z vÃ o lá»›p c1
k-Nearest Neighbors
â—Má»—i vÃ­ dá»¥ há»c x Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi 2 thÃ nh pháº§n:
â—‹MÃ´ táº£ cá»§a vÃ­ dá»¥: x = (x1, x2, x3, ..., xd), trong Ä‘Ã³ xi âˆˆR
â—‹NhÃ£n lá»›p c âˆˆC, vá»›i C lÃ  táº­p nhÃ£n cho trÆ°á»›c
â—Giai Ä‘oáº¡n há»c
â—‹ÄÆ¡n giáº£n lÃ  lÆ°u láº¡i cÃ¡c vÃ­ dá»¥ há»c trong táº­p há»c: D
â—Giai Ä‘oáº¡n phÃ¢n lá»›p: Äá»ƒ phÃ¢n lá»›p cho má»™t vÃ­ dá»¥ (má»›i) z
â—‹Vá»›i má»—i vÃ­ dá»¥ há»c x âˆˆD, tÃ­nh khoáº£ng cÃ¡ch giá»¯a x vÃ  z
â—‹XÃ¡c Ä‘á»‹nh táº­p NB(z) - cÃ¡c lÃ¡ng giá»ng gáº§n nháº¥t cá»§a z
â†’Gá»“m k vÃ­ dá»¥ há»c trong D gáº§n nháº¥t vá»›i z tÃ­nh theo má»™t 
hÃ m khoáº£ng cÃ¡ch d
â—‹PhÃ¢n z vÃ o lá»›p chiáº¿m sá»‘ Ä‘Ã´ng (the majority class) trong 
sá»‘ cÃ¡c lá»›p vÃ­ dá»¥ trong NB(z)
k-Nearest Neighbors - CÃ¡c váº¥n Ä‘á» cá»‘t lÃµi
â—Chá»n táº­p lÃ¡ng giá»ng ğ‘ğµ(z)
â—‹Chá»n bao nhiÃªu lÃ¡ng giá»ng?
â—‹Giá»›i háº¡n chá»n theo vÃ¹ng?
k-Nearest Neighbors - CÃ¡c váº¥n Ä‘á» cá»‘t lÃµi
â—Vá» lÃ½ thuyáº¿t thÃ¬ 1-NN cÅ©ng cÃ³ thá»ƒ lÃ  má»™t trong sá»‘ cÃ¡c phÆ°Æ¡ng 
phÃ¡p tá»‘i Æ°u.
â—Trong thá»±c tiá»…n ta nÃªn láº¥y nhiá»u hÃ ng xÃ³m (k > 1) khi cáº§n 
phÃ¢n lá»›p/dá»± Ä‘oÃ¡n, nhÆ°ng khÃ´ng quÃ¡ nhiá»u. LÃ½ do:
â—‹TrÃ¡nh áº£nh hÆ°á»Ÿng cá»§a lá»—i/nhiá»…u náº¿u chá»‰ dÃ¹ng 1 hÃ ng xÃ³m.
â—‹Náº¿u quÃ¡ nhiá»u hÃ ng xÃ³m thÃ¬ sáº½ phÃ¡ vá»¡ cáº¥u trÃºc tiá»m áº©n 
trong dá»¯ liá»‡u.
HÃ m tÃ­nh khoáº£ng cÃ¡ch
â—HÃ m tÃ­nh khoáº£ng cÃ¡ch d
â—‹ÄÃ³ng vai trÃ² ráº¥t quan trá»ng trong phÆ°Æ¡ng phÃ¡p há»c dá»±a trÃªn cÃ¡c 
lÃ¡ng giá»ng gáº§n nháº¥t
â—‹ThÆ°á»ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c, vÃ  khÃ´ng thay Ä‘á»•i trong suá»‘t quÃ¡ 
trÃ¬nh há»c vÃ  phÃ¢n loáº¡i/dá»± Ä‘oÃ¡n
â—Lá»±a chá»n hÃ m khoáº£ng cÃ¡ch d
â—‹CÃ¡c hÃ m khoáº£ng cÃ¡ch hÃ¬nh há»c: CÃ³ thá»ƒ phÃ¹ há»£p vá»›i cÃ¡c bÃ i 
toÃ¡n cÃ³ cÃ¡c thuá»™c tÃ­nh Ä‘áº§u vÃ o lÃ  kiá»ƒu sá»‘ thá»±c (xi âˆˆR)
â—‹HÃ m khoáº£ng cÃ¡ch Hamming: CÃ³ thá»ƒ phÃ¹ há»£p vá»›i cÃ¡c bÃ i toÃ¡n 
cÃ³ cÃ¡c thuá»™c tÃ­nh Ä‘áº§u vÃ o lÃ  kiá»ƒu nhá»‹ phÃ¢n (xi âˆˆ{0, 1})
HÃ m tÃ­nh khoáº£ng cÃ¡ch hÃ¬nh há»c (Geometry distance functions)
â—HÃ m Minkowski (p-norm):
â—HÃ m Euclide (p = 2):
â—HÃ m Manhattan (p = 1):
HÃ m tÃ­nh khoáº£ng cÃ¡ch hÃ¬nh há»c (Geometry distance functions)
â—CÃ³ thá»ƒ chuáº©n hÃ³a miá»n giÃ¡ trá»‹ (Ä‘Æ°a vá» cÃ¹ng má»™t khoáº£ng giÃ¡ trá»‹)
â—CÃ¡c thuá»™c tÃ­nh khÃ¡c nhau cÃ³ thá»ƒ (nÃªn) cÃ³ má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng 
khÃ¡c nhau Ä‘á»‘i vá»›i giÃ¡ trá»‹ khoáº£ng cÃ¡ch
â—‹LÃ m sao Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c giÃ¡ trá»‹ trá»ng sá»‘ cá»§a cÃ¡c thuá»™c tÃ­nh?
â—‹Dá»±a trÃªn cÃ¡c tri thá»©c cá»¥ thá»ƒ cá»§a bÃ i toÃ¡n (vd: Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh bá»Ÿi 
cÃ¡c chuyÃªn gia trong lÄ©nh vá»±c cá»§a bÃ i toÃ¡n Ä‘ang xÃ©t)
â—‹Báº±ng má»™t quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a cÃ¡c giÃ¡ trá»‹ trá»ng sá»‘ (vd: sá»­ dá»¥ng 
má»™t táº­p há»c Ä‘á»ƒ há»c má»™t bá»™ cÃ¡c giÃ¡ trá»‹ trá»ng sá»‘ tá»‘i Æ°u)
â—CÃ¡c lÃ¡ng giá»ng khÃ¡c nhau cÃ³ áº£nh hÆ°á»Ÿng khÃ¡c nhau Ä‘á»‘i vá»›i viá»‡c 
phÃ¢n lá»›p/dá»± Ä‘oÃ¡n cho z
â—‹CÃ³ thá»ƒ gÃ¡n cÃ¡c má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng (Ä‘Ã³ng gÃ³p) cá»§a má»—i lÃ¡ng 
giá»ng gáº§n nháº¥t tÃ¹y theo khoáº£ng cÃ¡ch cá»§a nÃ³ Ä‘áº¿n z
â—‹Má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cao hÆ¡n cho cÃ¡c lÃ¡ng giá»ng gáº§n hÆ¡n!
k-NN: Æ¯u nhÆ°á»£c Ä‘iá»ƒm
â—CÃ¡c Æ°u Ä‘iá»ƒm
â—‹Chi phÃ­ tháº¥p cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n (chá»‰ viá»‡c lÆ°u láº¡i cÃ¡c vÃ­ dá»¥ 
há»c)
â—‹Hoáº¡t Ä‘á»™ng tá»‘t vá»›i cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i gá»“m nhiá»u lá»›p â†’ 
KhÃ´ng cáº§n pháº£i há»c C bá»™ phÃ¢n loáº¡i cho C lá»›p
â—Vá» máº·t lÃ½ thuyáº¿t thÃ¬ k-NN cÃ³ thá»ƒ Ä‘áº¡t kháº£ nÄƒng phÃ¡n Ä‘oÃ¡n tá»‘i Æ°u khi 
gáº·p má»™t sá»‘ Ä‘iá»u kiá»‡n.
â—Ráº¥t linh Ä‘á»™ng trong viá»‡c chá»n hÃ m khoáº£ng cÃ¡ch.
â—‹CÃ³ thá»ƒ dÃ¹ng Ä‘á»™ tÆ°Æ¡ng tá»± (similarity): cosine
â—‹CÃ³ thá»ƒ dÃ¹ng Ä‘á»™ Ä‘o khÃ¡c, cháº³ng háº¡n Kullback-Leibler 
divergence, Bregman divergence, ...
â—CÃ¡c nhÆ°á»£c Ä‘iá»ƒm
â—‹Pháº£i lá»±a chá»n hÃ m tÃ­nh khoáº£ng cÃ¡ch (sá»± khÃ¡c biá»‡t) thÃ­ch há»£p 
vá»›i bÃ i toÃ¡n
â—‹Chi phÃ­ tÃ­nh toÃ¡n (thá»i gian, bá»™ nhá»›) cao táº¡i thá»i Ä‘iá»ƒm phÃ¢n 
loáº¡i/dá»± Ä‘oÃ¡n
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh)
BÃ i toÃ¡n: quyáº¿t Ä‘á»‹nh cÃ³ Ä‘á»£i 1 bÃ n á»Ÿ quÃ¡n Äƒn khÃ´ng, dá»±a trÃªn cÃ¡c thÃ´ng 
tin sau:
1. Lá»±a chá»n khÃ¡c: cÃ³ quÃ¡n Äƒn nÃ o khÃ¡c gáº§n Ä‘Ã³ khÃ´ng?
2. QuÃ¡n rÆ°á»£u: cÃ³ khu vá»±c phá»¥c vá»¥ Ä‘á»“ uá»‘ng gáº§n Ä‘Ã³ khÃ´ng?
3. Fri/Sat: hÃ´m nay lÃ  thá»© sÃ¡u hay thá»© báº£y?
4. ÄÃ³i: chÃºng ta Ä‘Ã£ Ä‘Ã³i chÆ°a?
5. KhÃ¡ch hÃ ng: sá»‘ khÃ¡ch trong quÃ¡n (khÃ´ng cÃ³, vÃ i ngÆ°á»i,
Ä‘áº§y)
6. GiÃ¡ cáº£: khoáº£ng giÃ¡ ($,$$,$$$)
7. MÆ°a: ngoÃ i trá»i cÃ³ mÆ°a khÃ´ng?
8. Äáº·t chá»—: chÃºng ta Ä‘Ã£ Ä‘áº·t trÆ°á»›c chÆ°a?
9. Loáº¡i: loáº¡i quÃ¡n Äƒn (PhÃ¡p, Ã, ThÃ¡i, quÃ¡n Äƒn nhanh)
10. Thá»i gian Ä‘á»£i: 0-10, 10-30, 30-60, >60
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh)
â—CÃ¡c máº«u Ä‘Æ°á»£c miÃªu táº£ dÆ°á»›i dáº¡ng cÃ¡c giÃ¡ trá»‹ thuá»™c tÃ­nh (logic, rá»i 
ráº¡c, liÃªn tá»¥c)
â—VÃ­ dá»¥,  tÃ¬nh huá»‘ng khi Ä‘á»£i 1 bÃ n Äƒn
CÃ¡c loáº¡i (lá»›p) cá»§a máº«u lÃ  kháº³ng Ä‘á»‹nh (T) hoáº·c phá»§ Ä‘á»‹nh (F)
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh)
Patrons, WaitEstimates, Alternative, Hungry, Rain
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh)
CÃ¢y quyáº¿t Ä‘á»‹nh lÃ  cÃ¡ch biá»ƒu diá»…n cÃ¡c giáº£ thiáº¿t. 
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh)
KhÃ´ng gian giáº£ thiáº¿t:
Khi cÃ³ n thuá»™c tÃ­nh Boolean, sá»‘ lÆ°á»£ng cÃ¡c cÃ¢y quyáº¿t Ä‘á»‹nh lÃ ? 
= sá»‘ cÃ¡c hÃ m Boolean 
= sá»‘ cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau trong báº£ng vÃ­ dá»¥ máº«u vá»›i 2n hÃ ng 
= 2 ^ 2n
VÃ­ dá»¥, vá»›i 6 thuá»™c tÃ­nh Boolean, cÃ³ 
18,446,744,073,709,551,616 cÃ¢y
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh) - ID3
â—ID3 (Iterative Dichotomiser 3) thá»±c hiá»‡n tÃ¬m kiáº¿m tham lam trÃªn 
khÃ´ng gian cÃ¡c cÃ¢y quyáº¿t Ä‘á»‹nh (do Ross Quinlan Ä‘á» xuáº¥t nÄƒm 
1986).
â—Giáº£ thuyáº¿t má»—i quan sÃ¡t x Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi d thuá»™c tÃ­nh 
â—‹x = (x1,x2,...,xd)
â—‹Má»—i xi lÃ  má»™t thuá»™c tÃ­nh rá»i ráº¡c (discrete) hoáº·c Ä‘á»‹nh danh 
(categorical)
â—Má»—i quan sÃ¡t trong táº­p há»c cÃ³ má»™t nhÃ£n lá»›p tÆ°Æ¡ng á»©ng.
â—XÃ¢y dá»±ng (há»c) má»™t cÃ¢y quyáº¿t Ä‘á»‹nh má»™t cÃ¡ch quy náº¡p, theo 
chiáº¿n lÆ°á»£c top-down, báº¯t Ä‘áº§u tá»« nÃºt gá»‘c á»©ng vá»›i táº¥t cáº£ táº­p há»c.
Decision Tree (CÃ¢y quyáº¿t Ä‘á»‹nh) - ID3
â—á» má»—i nÃºt tiáº¿p theo, chá»n thuá»™c tÃ­nh kiá»ƒm tra (lÃ  thuá»™c tÃ­nh cÃ³ kháº£ 
nÄƒng phÃ¢n loáº¡i tá»‘t nháº¥t Ä‘á»‘i vá»›i cÃ¡c vÃ­ dá»¥ há»c gáº¯n vá»›i nÃºt Ä‘Ã³).
â—Táº¡o má»›i má»™t cÃ¢y con cá»§a nÃºt hiá»‡n táº¡i cho má»—i giÃ¡ trá»‹ cá»§a thuá»™c tÃ­nh 
kiá»ƒm tra, vÃ  táº­p há»c sáº½ Ä‘Æ°á»£c tÃ¡ch ra (thÃ nh cÃ¡c táº­p con) tÆ°Æ¡ng á»©ng 
vá»›i cÃ¢y con vá»«a táº¡o. 
â—QuÃ¡ trÃ¬nh phÃ¡t triá»ƒn cÃ¢y quyáº¿t Ä‘á»‹nh sáº½ tiáº¿p tá»¥c cho Ä‘áº¿n khi:
â—‹Táº¥t cáº£ cÃ¡c máº«u hoáº·c thuá»™c tÃ­nh Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng vÃ  biá»ƒu quyáº¿t 
theo sá»‘ Ä‘Ã´ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ gÃ¡n node lÃ¡
â—‹Táº¥t cáº£ cÃ¡c máº«u thuá»™c cÃ¹ng má»™t lá»›p
â—ChÃº Ã½: Má»—i thuá»™c tÃ­nh chá»‰ Ä‘Æ°á»£c phÃ©p xuáº¥t hiá»‡n tá»‘i Ä‘a 1 láº§n Ä‘á»‘i vá»›i 
báº¥t ká»³ má»™t Ä‘Æ°á»ng Ä‘i nÃ o trong cÃ¢y.
ID3 - VÃ­ dá»¥
ID3 - VÃ­ dá»¥
ID3 - VÃ­ dá»¥
ID3 - VÃ­ dá»¥
ID3 - VÃ­ dá»¥
ID3 - VÃ­ dá»¥
ID3 - VÃ­ dá»¥
Overfitting
â—HÃ m h Ä‘Æ°á»£c gá»i lÃ  overfitting náº¿u tá»“n táº¡i hÃ m g mÃ :  
â—‹g cÃ³ thá»ƒ tá»“i hÆ¡n h Ä‘á»‘i vá»›i táº­p huáº¥n luyá»‡n
â—‹nhÆ°ng g tá»‘t hÆ¡n h Ä‘á»‘i vá»›i dá»¯ liá»‡u tÆ°Æ¡ng lai.
â—Model há»c thuá»™c dá»¯ liá»‡u trÃªn táº­p huáº¥n luyá»‡n
â—‹khÃ´ng cÃ³ tÃ­nh khÃ¡i quÃ¡t hÃ³a
â—‹khÃ´ng hoÃ n thÃ nh tá»‘t khi Ä‘em vÃ o validate/test
â—VÃ i nguyÃªn nhÃ¢n gÃ¢y ra Overfitting:
â—‹HÃ m h quÃ¡ phá»©c táº¡p
â—‹Lá»—i (nhiá»…u) trong táº­p huáº¥n luyá»‡n (do quÃ¡ trÃ¬nh thu 
tháº­p/xÃ¢y dá»±ng táº­p dá»¯ liá»‡u)
â—‹Sá»‘ lÆ°á»£ng cÃ¡c vÃ­ dá»¥ há»c quÃ¡ nhá», khÃ´ng Ä‘áº¡i diá»‡n cho 
toÃ n bá»™ táº­p (phÃ¢n bá»‘) cá»§a cÃ¡c vÃ­ dá»¥ cá»§a bÃ i toÃ¡n há»c
Overfitting
Overfitting
â—Khi tÄƒng cá»¡ lá»›n cá»§a má»™t CÃ¢y quyáº¿t Ä‘á»‹nh thÃ¬ cháº¥t lÆ°á»£ng 
phÃ¡n Ä‘oÃ¡n cá»§a nÃ³ cÃ³ thá»ƒ giáº£m dáº§n, máº·c dÃ¹ Ä‘á»™ chÃ­nh xÃ¡c 
trÃªn táº­p huáº¥n luyá»‡n tÄƒng dáº§n
Overfitting
â—Nhiá»u nhÃ¡nh cá»§a cÃ¢y quyáº¿t Ä‘á»‹nh pháº£n Ã¡nh sá»± báº¥t thÆ°á»ng 
trong dá»¯ liá»‡u há»c (cÃ³ thá»ƒ do nhiá»…u hoáº·c giÃ¡ trá»‹ ngoáº¡i lai)
â—Äá»™ chÃ­nh xÃ¡c tháº¥p Ä‘á»‘i vá»›i cÃ¡c máº«u má»›i (kiá»ƒm tra) 
â—Giáº£i phÃ¡p: Cáº¯t tá»‰a (Pruning)
â—‹Loáº¡i bá» nhá»¯ng nhÃ¡nh Ã­t tin cáº­y
Random Forest
â—Má»¥c tiÃªu: trÃ¡nh overfitting
â—Ã tÆ°á»Ÿng: chia thÃ nh nhiá»u decision tree, dá»¯ liá»‡u á»Ÿ má»—i cÃ¢y 
Ä‘Æ°á»£c láº¥y random tá»« táº­p training, káº¿t quáº£ sáº½ Ä‘Æ°á»£c láº¥y trung 
bÃ¬nh hoáº·c chá»n tá»« Ä‘a sá»‘ Ä‘á»ƒ phá»¥c vá»¥ cho phÃ¢n loáº¡i vÃ  há»“i quy.
â—Äáº·c Ä‘iá»ƒm quan trá»ng:
â—‹Tá»«ng cÃ¢y chá»‰ cÃ³ sá»‘ lÆ°á»£ng 
feature nháº¥t Ä‘á»‹nh, vÃ  khÃ´ng 
cÃ¢y nÃ o giá»‘ng nhau
â—‹Má»—i cÃ¢y Ä‘Æ°á»£c táº¡o tÃ¡ch biá»‡t 
vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n 
song song => giáº£m thá»i 
gian tÃ­nh toÃ¡n
â—‹TÃ­nh á»•n Ä‘á»‹nh cao do káº¿t quáº£ 
dá»±a trÃªn Ä‘a sá»‘/trung bÃ¬nh
So sÃ¡nh Decision Tree vÃ  Random Forest
Decision Tree
Random Forest
â—Dá»… bá»‹ overfit náº¿u khÃ´ng bá»‹ 
kiá»ƒm soÃ¡t
â—TÃ­nh toÃ¡n nhanh hÆ¡n
â—Khi má»™t bá»™ dá»¯ liá»‡u vá»›i cÃ¡c 
thuá»™c tÃ­nh Ä‘Æ°á»£c Ä‘Æ°a vÃ o 
decision tree, nÃ³ sáº½ hÃ¬nh thÃ nh 
má»™t sá»‘ quy táº¯c/hÃ m Ä‘á»ƒ tiáº¿n 
hÃ nh dá»± Ä‘oÃ¡n 
â—ÄÆ°á»£c táº¡o nÃªn tá»« cÃ¡c táº­p há»£p 
con cá»§a dá»¯ liá»‡u, káº¿t quáº£ cuá»‘i 
láº¥y tá»« trung bÃ¬nh, hoáº·c xÃ©t Ä‘a 
sá»‘ nÃªn giáº£i quyáº¿t Ä‘Æ°á»£c overfit
â—Random forest chá»n ngáº«u nhiÃªn, 
dá»±ng cÃ¡c decision tree vÃ  láº¥y káº¿t 
quáº£ trung bÃ¬nh. NÃ³ khÃ´ng sá»­ 
dá»¥ng báº¥t cá»© bá»™ cÃ´ng thá»©c nÃ o
â—TÃ­nh toÃ¡n cháº­m
SVM - Support vector machine
â—MÃ¡y vectÆ¡ há»— trá»£ (Support vector machine - SVM)
â—‹V. Vapnik vÃ  cÃ¡c Ä‘á»“ng nghiá»‡p cá»§a Ã´ng vÃ o nhá»¯ng nÄƒm 1970s á»Ÿ 
Nga 
â—‹Trá»Ÿ nÃªn hoÃ n chá»‰nh vÃ  phá»• biáº¿n vÃ o nhá»¯ng nÄƒm 1990s
â—SVM lÃ  má»™t phÆ°Æ¡ng phÃ¡p phÃ¢n lá»›p tuyáº¿n tÃ­nh (linear classifier), 
vá»›i má»¥c Ä‘Ã­ch xÃ¡c Ä‘á»‹nh má»™t siÃªu pháº³ng (hyperplane) Ä‘á»ƒ phÃ¢n tÃ¡ch 
hai lá»›p cá»§a dá»¯ liá»‡u.
â—‹VÃ­ dá»¥: lá»›p cÃ³ nhÃ£n dÆ°Æ¡ng (positive) vÃ  lá»›p cÃ³ nhÃ£n Ã¢m 
(negative)
â—CÃ¡c hÃ m nhÃ¢n (kernel functions), cÅ©ng Ä‘Æ°á»£c gá»i lÃ  cÃ¡c hÃ m biáº¿n 
Ä‘á»•i (transformation functions), Ä‘Æ°á»£c dÃ¹ng cho cÃ¡c trÆ°á»ng há»£p phÃ¢n 
lá»›p phi tuyáº¿n
SVM - Support vector machine
â—SVM lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n phÃ¢n lá»›p phá»• biáº¿n vÃ  hiá»‡u quáº£. 
Ã tÆ°á»Ÿng khÃ¡ Ä‘Æ¡n giáº£n, vÃ  Ä‘á»ƒ sá»­ dá»¥ng ta cáº§n má»™t chÃºt kiáº¿n thá»©c vá» 
tá»‘i Æ°u.
â—Giáº£ sá»­ ráº±ng cÃ³ 2 lá»›p dá»¯ liá»‡u mÃ´ táº£ bá»Ÿi cÃ¡c Ä‘iá»ƒm (feature vector) 
trong khÃ´ng gian nhiá»u chiá»u, vÃ  tá»“n táº¡i siÃªu pháº³ng phÃ¢n chia chÃ­nh 
xÃ¡c 2 lá»›p Ä‘Ã³.
SVM - Support vector machine
â—SiÃªu pháº³ng phÃ¢n tÃ¡ch cÃ¡c quan sÃ¡t thuá»™c lá»›p dÆ°Æ¡ng vÃ  cÃ¡c quan 
sÃ¡t thuá»™c lá»›p Ã¢m: w x x + b = 0
â—CÃ²n Ä‘Æ°á»£c gá»i lÃ  ranh giá»›i (bá» máº·t) quyáº¿t Ä‘á»‹nh
â—Tá»“n táº¡i nhiá»u siÃªu pháº³ng phÃ¢n tÃ¡ch. Chá»n cÃ¡i nÃ o?
SVM - Support vector machine
â—SVM giáº£i quyáº¿t cÃ¢u há»i nÃ y dá»±a báº±ng cÃ¡ch xÃ¢y dá»±ng Ä‘á»‹nh nghÄ©a vá» 
lá» vÃ  Ä‘Æ°a ra thuáº­t toÃ¡n tá»‘i Æ°u nÃ³.
â—Lá» (margin) cá»§a má»™t lá»›p Ä‘Æ°á»£c Ä‘á»‹nh 
nghÄ©a lÃ  khoáº£ng cÃ¡ch tá»« cÃ¡c Ä‘iá»ƒm 
gáº§n nháº¥t cá»§a lá»›p Ä‘Ã³ tá»›i máº·t phÃ¢n 
chia.
â—Trong Ä‘Ã³:
â—‹||w|| lÃ  Ä‘á»™ dÃ i cá»§a w:
â—‹w = [w1, w2, â€¦, wd]T
â—‹x0 = [x10, x20, â€¦, wd0]T
SVM - Support vector machine
â—SVM lá»±a chá»n máº·t siÃªu pháº³ng phÃ¢n tÃ¡ch cÃ³ lá» (margin) lá»›n nháº¥t
â—LÃ½ thuyáº¿t há»c mÃ¡y Ä‘Ã£ chá»‰ ra ráº±ng má»™t máº·t siÃªu pháº³ng phÃ¢n tÃ¡ch 
nhÆ° tháº¿ sáº½ tá»‘i thiá»ƒu hÃ³a giá»›i háº¡n lá»—i (phÃ¢n lá»›p) máº¯c pháº£i (so vá»›i má»i 
siÃªu pháº³ng khÃ¡c)
SVM - Support vector machine
â—Há»c SVM tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i giáº£i quyáº¿t bÃ i toÃ¡n tá»‘i Æ°u báº­c 2 sau Ä‘Ã¢y
â—TÃ¬m w vÃ  b sao cho:                             Ä‘áº¡t cá»±c Ä‘áº¡i
â—Vá»›i Ä‘iá»u kiá»‡n:
vá»›i má»i vÃ­ dá»¥ huáº¥n luyá»‡n xi (i=1..r)
SVM - Support vector machine
Regression
Regression
â—Classification: dá»± Ä‘oÃ¡n loáº¡i (category) 
â—Regression: dá»± Ä‘oÃ¡n giÃ¡ trá»‹ sá»‘
â—‹NhÆ° nhiá»‡t Ä‘á»™ trong tuáº§n tá»›i sáº½ nhÆ° tháº¿ nÃ o?
â—‹GiÃ¡ nhÃ  Ä‘áº¥t khu vá»±c ná»™i thÃ nh lÃ  bao nhiÃªu?
â—‹TÆ°Æ¡ng tá»± nhÆ° classification
â– Äáº§u vÃ o cho cÃ¡c mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ lÃ  cáº£ sá»‘ vÃ  phÃ¢n 
loáº¡i
â– Äáº§u ra dá»± Ä‘oÃ¡n lÃ  má»™t sá»‘
Há»“i quy tuyáº¿n tÃ­nh (linear regression)
â—BÃ i toÃ¡n há»“i quy: cáº§n há»c má»™t hÃ m y = f(x) tá»« má»™t táº­p há»c 
cho trÆ°á»›c D = {(x1, y1), (x2, y2), ..., (xM, yM)} trong Ä‘Ã³ yi â‰ˆ f(xi)
vá»›i má»i i.
â—‹Má»—i quan sÃ¡t Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t vÃ©ctÆ¡ n chiá»u, 
cháº³ng háº¡n xi = (xi1, xi2, ..., xin)T
â—‹Má»—i chiá»u biá»ƒu diá»…n má»™t thuá»™c tÃ­nh (attribute/feature)
â—MÃ´ hÃ¬nh tuyáº¿n tÃ­nh: náº¿u giáº£ thuyáº¿t hÃ m y = f(x) lÃ  hÃ m cÃ³ 
dáº¡ng tuyáº¿n tÃ­nh
f(x) = w0 + w1x1 + ... + wnxn
â—Há»c má»™t hÃ m há»“i quy tuyáº¿n tÃ­nh thÃ¬ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c há»c 
vÃ©ctÆ¡ trá»ng sá»‘ w = (w0, w1, ..., wn)T
Há»“i quy tuyáº¿n tÃ­nh (linear regression)
â—HÃ m tuyáº¿n tÃ­nh f(x) nÃ o phÃ¹ há»£p?
Há»“i quy tuyáº¿n tÃ­nh (linear regression)
â—Äá»‘i vá»›i má»—i quan sÃ¡t x = (x1, x2, ..., xn)T
â—‹GiÃ¡ trá»‹ Ä‘áº§u ra mong muá»‘n cx
(KhÃ´ng biáº¿t trÆ°á»›c Ä‘á»‘i vá»›i cÃ¡c quan sÃ¡t trong tÆ°Æ¡ng lai)
â—‹GiÃ¡ trá»‹ phÃ¡n Ä‘oÃ¡n (bá»Ÿi há»‡ thá»‘ng)
yx = w0 + w1x1 + ... + wnxn
â—‹Ta thÆ°á»ng mong muá»‘n yx xáº¥p xá»‰ tá»‘t cx
â—PhÃ¡n Ä‘oÃ¡n cho quan sÃ¡t tÆ°Æ¡ng lai z = (z1, z2, ..., zn)T
â—‹Cáº§n dá»± Ä‘oÃ¡n giÃ¡ trá»‹ Ä‘áº§u ra, báº±ng cÃ¡ch Ã¡p dá»¥ng hÃ m má»¥c 
tiÃªu Ä‘Ã£ há»c Ä‘Æ°á»£c f:
f(z) = w0 + w1z1 + ... + wnzn
Há»“i quy tuyáº¿n tÃ­nh (linear regression)
Má»¥c tiÃªu há»c: há»c má»™t hÃ m f* sao cho kháº£ nÄƒng phÃ¡n Ä‘oÃ¡n trong 
tÆ°Æ¡ng lai lÃ  tá»‘t nháº¥t.
â—Tá»©c lÃ  sai sá»‘ |cz â€“ f(z)| lÃ  nhá» nháº¥t cho cÃ¡c quan sÃ¡t tÆ°Æ¡ng lai z.
â—Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a (generalization) lÃ  tá»‘t nháº¥t.
Váº¥n Ä‘á»: CÃ³ vÃ´ háº¡n hÃ m tuyáº¿n tÃ­nh!! 
â—LÃ m sao Ä‘á»ƒ há»c? Quy táº¯c nÃ o?
DÃ¹ng má»™t tiÃªu chuáº©n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.
â—TiÃªu chuáº©n thÆ°á»ng dÃ¹ng lÃ  hÃ m 
máº¥t mÃ¡t (generalization error, 
loss function, ...)
Há»“i quy tuyáº¿n tÃ­nh (linear regression) - HÃ m máº¥t mÃ¡t
â—Äá»‹nh nghÄ©a hÃ m máº¥t mÃ¡t E
â—‹Lá»—i (error/loss) phÃ¡n Ä‘oÃ¡n cho quan sÃ¡t x = (x1, x2, ..., xn)T
r(x) = [cx â€“ f*(x)]2 = (cx â€“ w0 â€“ w1x1 -... - wnxn)2
â—Lá»—i cá»§a há»‡ thá»‘ng trÃªn toÃ n bá»™ khÃ´ng gian cá»§a x:
E = Ex[r(x)] = Ex[cx â€“ f*(x)]2
â—Má»¥c tiÃªu há»c lÃ  tÃ¬m hÃ m f* mÃ  E lÃ  nhá» nháº¥t:
â—‹Trong Ä‘Ã³ H lÃ  khÃ´ng gian cá»§a hÃ m f.
â—NhÆ°ng: trong quÃ¡ trÃ¬nh há»c ta khÃ´ng thá»ƒ lÃ m viá»‡c Ä‘Æ°á»£c vá»›i bÃ i toÃ¡n 
nÃ y.
Há»“i quy tuyáº¿n tÃ­nh (linear regression) - HÃ m máº¥t mÃ¡t
â—Ta chá»‰ quan sÃ¡t Ä‘Æ°á»£c má»™t táº­p D = {(x1, y1), (x2, y2), ..., (xM, yM)}. 
Cáº§n há»c hÃ m f tá»« D.
â—Lá»—i thá»±c nghiá»‡m (empirical loss; residual sum of squares)
â—‹RSS/M lÃ  má»™t xáº¥p xá»‰ cá»§a Ex[r(x)] trÃªn táº­p há»c D
â—Lá»—i tá»•ng quÃ¡t hÃ³a (generalization error) cá»§a hÃ m f lÃ 
BÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu (Ordinary Least Squares - OLS)
â—Cho trÆ°á»›c D, ta Ä‘i tÃ¬m hÃ m f mÃ  cÃ³ RSS nhá» nháº¥t.
ÄÃ¢y Ä‘Æ°á»£c gá»i lÃ  bÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu (least squares).
â—TÃ¬m nghiá»‡m w* báº±ng cÃ¡ch láº¥y Ä‘áº¡o hÃ m cá»§a RSS vÃ  giáº£i phÆ°Æ¡ng 
trÃ¬nh RSSâ€™ = 0. Thu Ä‘Æ°á»£c:
w* = (ATA)-1ATy
Trong Ä‘Ã³ A lÃ  ma tráº­n dá»¯ liá»‡u cá»¡ Mx(n+1) mÃ  hÃ ng thá»© i lÃ  
Ai = (1, xi1, xi2, ..., xin); B-1 lÃ  ma tráº­n nghá»‹ch Ä‘áº£o; y = (y1, y2, ..., yM) .
ChÃº Ã½: giáº£ thuyáº¿t ATA tá»“n táº¡i nghá»‹ch Ä‘áº£o.
BÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu (Ordinary Least Squares - OLS)
â—Input: D = {(x1, y1), (x2, y2), ..., (xM, yM)}
â—Output: w*
â—Há»c w* báº±ng cÃ¡ch tÃ­nh:
w* = (ATA)-1ATy
Trong Ä‘Ã³:
âA lÃ  ma tráº­n dá»¯ liá»‡u cá»¡ Mx(n+1) mÃ  hÃ ng thá»© i lÃ  Ai = (1, xi1, xi2, ..., 
xin); 
âB-1 lÃ  ma tráº­n nghá»‹ch Ä‘áº£o
ây = (y1, y2, ..., yM)
ChÃº Ã½: giáº£ thuyáº¿t ATA tá»“n táº¡i nghá»‹ch Ä‘áº£o.
â—PhÃ¡n Ä‘oÃ¡n cho quan sÃ¡t má»›i x:
BÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu - VÃ­ dá»¥
Káº¿t quáº£ há»c báº±ng bÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu
BÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu - NhÆ°á»£c Ä‘iá»ƒm
â—Náº¿u ATA khÃ´ng tá»“n táº¡i nghá»‹ch Ä‘áº£o thÃ¬ khÃ´ng há»c Ä‘Æ°á»£c.
â—‹Náº¿u cÃ¡c thuá»™c tÃ­nh (cá»™t cá»§a A) cÃ³ phá»¥ thuá»™c láº«n nhau.
â—Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n lá»›n do pháº£i tÃ­nh ma tráº­n nghá»‹ch Ä‘áº£o.
â†’ KhÃ´ng lÃ m viá»‡c Ä‘Æ°á»£c náº¿u sá»‘ chiá»u n lá»›n.
â—Kháº£ nÄƒng overfitting cao vÃ¬ viá»‡c há»c hÃ m f chá»‰ quan tÃ¢m tá»‘i thiá»ƒu 
lá»—i Ä‘á»‘i vá»›i táº­p há»c Ä‘ang cÃ³.
â—Cho trÆ°á»›c D = {(x1, y1), (x2, y2), â€¦, (xM, yM)}, ta Ä‘i giáº£i bÃ i toÃ¡n:
Há»“i quy Ridge
â—Trong Ä‘Ã³ Ai = (1, xi1, xi2, â€¦, xin) Ä‘Æ°á»£c táº¡o ra tá»« xi; Î» lÃ  má»™t háº±ng 
sá»‘ pháº¡t (Î»> 0).
Há»“i quy Ridge
â€¢ TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c giáº£i bÃ i toÃ¡n sau:
t lÃ  má»™t háº±ng sá»‘ nÃ o Ä‘Ã³.
â€¢ Äáº¡i lÆ°á»£ng hiá»‡u chá»‰nh (pháº¡t) ğœ†
ğ’˜2
2
â€¢ CÃ³ vai trÃ² háº¡n cháº¿ Ä‘á»™ lá»›n cá»§a w* (háº¡n cháº¿
khÃ´ng gian hÃ m 
f).
â€¢ ÄÃ¡nh Ä‘á»•i cháº¥t lÆ°á»£ng cá»§a hÃ m f Ä‘á»‘i vá»›i táº­p há»c D, Ä‘á»ƒ cÃ³ kháº£ 
nÄƒng phÃ¡n  Ä‘oÃ¡n tá»‘t hÆ¡n vá»›i quan sÃ¡t tÆ°Æ¡ng lai.
Há»“i quy Ridge
â€¢ Trong Ä‘Ã³ A lÃ  ma tráº­n dá»¯ liá»‡u cá»¡ Mx(n+1) mÃ  hÃ ng thá»© i lÃ  (1, xi1, 
xi2, â€¦, xin); y = (y1, y2, â€¦, yM)T; In+1 lÃ  ma tráº­n Ä‘Æ¡n vá»‹ cá»¡ n+1.
â€¢ So sÃ¡nh vá»›i phÆ°Æ¡ng phÃ¡p bÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu:
â€¢ TrÃ¡nh Ä‘Æ°á»£c trÆ°á»ng há»£p ma tráº­n dá»¯ liá»‡u suy biáº¿n. Há»“i quy Ridge 
luÃ´n
lÃ m viá»‡c Ä‘Æ°á»£c.
â€¢ Kháº£ nÄƒng overfitting thÆ°á»ng Ã­t hÆ¡n.
â€¢ Lá»—i trÃªn táº­p há»c cÃ³ thá»ƒ nhiá»u hÆ¡n.
â€¢ ChÃº Ã½: cháº¥t lÆ°á»£ng cá»§a phÆ°Æ¡ng phÃ¡p phá»¥ thuá»™c ráº¥t nhiá»u vÃ o sá»± lá»±a 
chá»n cá»§a tham sá»‘ Î».
â€¢ TÃ¬m nghiá»‡m w* báº±ng cÃ¡ch láº¥y Ä‘áº¡o hÃ m cá»§a RSS vÃ  giáº£i phÆ°Æ¡ng
trÃ¬nh RSSâ€™ = 0. Thu Ä‘Æ°á»£c:
Há»“i quy Ridge: thuáº­t toÃ¡n
â– Input: D = {(x1, y1), (x2, y2), â€¦, (xM, yM)}, háº±ng sá»‘ Î»>0
â– Output: w*
â€¢ Há»c w* báº±ng cÃ¡ch tÃ­nh:
â€¢ Trong Ä‘Ã³ A lÃ  ma tráº­n dá»¯ liá»‡u cá»¡ Mx(n+1) mÃ  hÃ ng thá»© i lÃ  Ai 
= (1, xi1,  xi2, â€¦, xin); B-1 lÃ  ma tráº­n nghá»‹ch Ä‘áº£o; y = (y1, y2, â€¦, 
yM)T.
â€¢ PhÃ¡n Ä‘oÃ¡n cho quan sÃ¡t má»›i x:
â€¢ ChÃº Ã½: Ä‘á»ƒ trÃ¡nh vÃ i áº£nh hÆ°á»Ÿng xáº¥u tá»« Ä‘á»™ lá»›n cá»§a y, ta nÃªn loáº¡i  bá» 
thÃ nh pháº§n w0 trong Ä‘áº¡i lÆ°á»£ng pháº¡t á»Ÿ cÃ´ng thá»©c (2). Khi Ä‘Ã³  nghiá»‡m 
w* sáº½ thay Ä‘á»•i má»™t chÃºt.
Há»“i quy Ridge
â– XÃ©t táº­p dá»¯ liá»‡u Prostate gá»“m 67 quan sÃ¡t dÃ¹ng Ä‘á»ƒ há»c, vÃ  31 quan 
sÃ¡t dÃ¹ng  Ä‘á»ƒ kiá»ƒm thá»­. Dá»¯ liá»‡u gá»“m 8 thuá»™c tÃ­nh.
w
Least 
squares
Ridge
0
2.465
2.452
lcavol
0.680
0.420
lweight
0.263
0.238
age
âˆ’0.141
âˆ’0.046
lbph
0.210
0.162
svi
0.305
0.227
lcp
âˆ’0.288
0.000
gleason
âˆ’0.021
0.040
pgg45
0.267
0.133
Test RSS
0.521
0.492
Há»“i quy Ridge
â– W* = (w0, S1, S2, S3, S4, S5, S6, AGE, SEX, BMI, BP) thay Ä‘á»•i khi cho Î» 
thay  Ä‘á»•i.
Há»“i quy Lasso
â€¢ Há»“i quy Ridge sá»­ dá»¥ng chuáº©n L2 cho Ä‘áº¡i lÆ°á»£ng hiá»‡u 
chá»‰nh:
â€¢ Thay L2 báº±ng L1 thÃ¬ ta sáº½ thu Ä‘Æ°á»£c phÆ°Æ¡ng phÃ¡p LASSO:
â€¢ Hoáº·c cÃ³ thá»ƒ viáº¿t láº¡i:
â€¢ HÃ m má»¥c tiÃªu cá»§a bÃ i toÃ¡n lÃ  khÃ´ng trÆ¡n. Do Ä‘Ã³ viá»‡c giáº£i nÃ³ cÃ³ thá»ƒ khÃ³
hÆ¡n há»“i quy Ridge.
Há»“i quy Lasso: Ä‘áº¡i lÆ°á»£ng hiá»‡u chá»‰nh
â€¢ CÃ¡c kiá»ƒu hiá»‡u chá»‰nh khÃ¡c nhau sáº½ táº¡o ra cÃ¡c miá»n khÃ¡c nhau cho
w.
â€¢ LASSO thÆ°á»ng táº¡o ra nghiá»‡m thÆ°a, tá»©c lÃ  nhiá»u thÃ nh pháº§n cá»§a w
cÃ³ giÃ¡ trá»‹ lÃ  0.
â€¢ VÃ¬ tháº¿ LASSO thá»±c hiá»‡n Ä‘á»“ng thá»i viá»‡c háº¡n cháº¿ vÃ  lá»±a chá»n Ä‘áº·c 
trÆ°ng
Figure by Nicoguaro - Own work, CC BY 4.0,  
https://commons.wikimedia.org/w/index.php?curid=5825896
6
OLS, Ridge, LASSO
â– XÃ©t táº­p dá»¯ liá»‡u Prostate gá»“m 67 quan sÃ¡t dÃ¹ng Ä‘á»ƒ há»c, vÃ  31 quan 
sÃ¡t dÃ¹ng  Ä‘á»ƒ kiá»ƒm thá»­. Dá»¯ liá»‡u gá»“m 8 thuá»™c tÃ­nh.
w
Ordinary Least
Squares
Ridge
LASSO
0
2.465
2.452
2.468
lcavol
0.680
0.420
0.533
lweight
0.263
0.238
0.169
age
âˆ’0.141
âˆ’0.046
lbph
0.210
0.162
0.002
svi
0.305
0.227
0.094
lcp
âˆ’0.288
0.000
gleason
âˆ’0.021
0.040
pgg45
0.267
0.133
Test RSS
0.521
0.492
0.479
Má»™t sá»‘ trá»ng 
sá»‘ lÃ  0 
â†’ ChÃºng cÃ³ 
thá»ƒ khÃ´ng 
quan trá»ng
Logistic Regression
â—Náº¿u Linear Regression xÃ¡c Ä‘á»‹nh má»™t giÃ¡ trá»‹ liÃªn tá»¥c nhÆ° nhiá»‡t 
Ä‘á»™, kÃ­ch thÆ°á»›c thÃ¬ Logistic regression xÃ¡c Ä‘á»‹nh ráº±ng má»™t viá»‡c lÃ  
True hay False, Pass hay Fail, 1 hay 0
â—Sá»­ dá»¥ng cÃ¡c hÃ m 
kÃ­ch hoáº¡t Ä‘áº·c biá»‡t
â—TÃ­nh cháº¥t quan trá»ng cá»§a hÃ m:
â—‹LÃ  cÃ¡c hÃ m sá»‘ liÃªn tá»¥c nháº­n giÃ¡ trá»‹ thá»±c trong khoáº£ng (0, 1).
â—‹Náº¿u coi Ä‘iá»ƒm cÃ³ tung Ä‘á»™ 0.5 lÃ  ngÆ°á»¡ng, cÃ¡c Ä‘iá»ƒm cÃ ng xa 
ngÆ°á»¡ng vá» phÃ­a bÃªn trÃ¡i cÃ³ giÃ¡ trá»‹ cÃ ng gáº§n khÃ´ng, cÃ²n Ä‘iá»ƒm cÃ ng 
xa ngÆ°á»¡ng vá» bÃªn pháº£i thÃ¬ cÃ ng gáº§n 1.
â—‹CÃ³ Ä‘áº¡o hÃ m á»Ÿ má»i nÆ¡i, vÃ¬ váº­y cÃ³ thá»ƒ Ä‘Æ°á»£c lá»£i trong viá»‡c tá»‘i Æ°u
â—Ta cÃ³ thá»ƒ tháº¥y Ä‘Æ°á»ng mÃ u lá»¥c vÃ  mÃ u lam phÃ¹ há»£p vá»›i tÃ­nh cháº¥t trÃªn
â—HÃ m sá»‘ nÃ y nháº­n giÃ¡ trá»‹ trong khoáº£ng (-1, 1)
HÃ m sigmoid
â—Trong sá»‘ cÃ¡c hÃ m cÃ³ tÃ­nh cháº¥t trÃªn, cÃ³ hÃ m sigmoid Ä‘Æ°á»£c sá»­ 
dá»¥ng nhiá»u nháº¥t, vÃ¬ nÃ³ chÄƒn trong khoáº£ng (0, 1).
â—NgoÃ i ra, hÃ m tanh cÅ©ng hay Ä‘Æ°á»£c sá»­ dá»¥ng:
â—CÃ¡c thuáº­t toÃ¡n há»c giÃ¡m sÃ¡t cÆ¡ báº£n
â—‹BÃ i toÃ¡n phÃ¢n loáº¡i
â– Naive Bayes
â– kNN
â– Decision Tree
â– Random Forest
â– SVM
â—‹BÃ i toÃ¡n há»“i quy
â– Linear Regression
â– Ridge Regression
â– Lasso Regression
â—Ã tÆ°á»Ÿng, cÃ¡ch tiáº¿p cáº­n, á»©ng dá»¥ng cho tá»«ng dáº¡ng Ä‘áº§u ra
Tá»•ng káº¿t buá»•i há»c
