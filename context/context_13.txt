NLP Course
Large Language Models
Prompting Techniques
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
CONTENT
1
Large Language Models
2
Prompting Techniques
3
Learning from Human Feedback
4
Experiment
2
Ã˜ Transformer
3
1 â€“ Large Language Models
!
Review
Ã˜ Decoder-only models (GPT-x models)
4
1 â€“ Large Language Models
!
Review
Ã˜ Encoder-only models (BERT, RoBERTa, ELECTRA)
5
1 â€“ Large Language Models
!
Input BERT
Review
Ã˜ Encoder-Decoder models (T5, BART)
6
1 â€“ Large Language Models
Review
!
BART model
7
1 â€“ Large Language Models
Why need Large Language Models?
!
Ã˜ Pre-training: trained on huge amounts of unlabeled text using â€œself-supervisedâ€ 
training objective
Ã˜ Adaptation: how to use a pre-trained model for downstream task?
8
1 â€“ Large Language Models
Why need Large Language Models?
!
Ã˜ The promise: one single model to solve 
many NLP tasks
Ã˜ Emergent properties in LLMs
Ã˜ Prompts involve instructions and context passed to a language model to achieve a
desired task
Ã˜ Prompt engineering is the practice of developing and optimizing prompts to
efficiently use language models (LMs) for a variety of applications
9
1 â€“ Large Language Models
Prompt
!
10
1 â€“ Large Language Models
Elements of a Prompt
!
Prompt
Response
Language
Model
TASK DESCRIPTION
CURRENT INPUT
OUTPUT INDICATOR
EXAMPLE 1
EXAMPLE 2
One-shot
Few-shot
Ã˜ Three setting for In-Content-Learning
11
1 â€“ Large Language Models
Prompt Examples
!
Ã˜ Medium-sized models: BERT/RoBERTa models (100M or 300M), T5 models 
(220M, 700M, 3B, 11B)
Ã˜ â€œVeryâ€ large LMs: models of 100+ billion parameters
GPT3 (175B), BLOOM (176B), PaLM (540B), GLaM (1200B)â€¦
Ã˜ Larger model sizes => Larger compute, more expensive during inference
12
1 â€“ Large Language Models
Parameter
!
Ã˜ Data scale: usually in the order of trillions of tokens
GPT3 (0.5 trillion tokens), LLaMA (1.4 trillion tokens), â€¦
Ã˜ Training data: Low-quality data 
13
1 â€“ Large Language Models
Dataset
!
Ã˜ Data bottleneck for training
14
1 â€“ Large Language Models
Dataset
!
15
1 â€“ Large Language Models
A timeline of existing large language models
!
16
1 â€“ Large Language Models
T5 Model
!
Ã˜ Text-to-Text Transfer Transformer
17
1 â€“ Large Language Models
T5 Model
!
Ã˜ Every task, one format!
Ã˜ [â€œTask-specific prefix]: [Input text]â€ => â€œ[Output text]â€
18
1 â€“ Large Language Models
T5 Model
!
Ã˜ Workflow
19
1 â€“ Large Language Models
T5 Model
!
Ã˜ Baseline Objective
20
1 â€“ Large Language Models
T5 Model
!
Ã˜ Different Attention Mask Patterns
21
1 â€“ Large Language Models
T5 Model
!
Ã˜ Transformer architecture variants
22
1 â€“ Large Language Models
T5 Model
!
Ã˜ Transformer architecture variants
23
1 â€“ Large Language Models
T5 Model
!
Ã˜ Different Unsupervised Objectives
24
1 â€“ Large Language Models
T5 Model
!
Ã˜ Different Unsupervised Objectives
25
1 â€“ Large Language Models
T5 Model
!
Ã˜ Different Unsupervised Objectives
26
1 â€“ Large Language Models
T5 Model
!
Ã˜ Different Unsupervised Objectives
27
1 â€“ Large Language Models
T5 Model
!
Ã˜ Different Unsupervised Objectives
28
1 â€“ Large Language Models
T5 Model
!
Ã˜ Multi-task
29
1 â€“ Large Language Models
T5 Model
!
Ã˜ Multi-task
30
1 â€“ Large Language Models
T5 Model
!
Ã˜ Multi-task
31
1 â€“ Large Language Models
T5 Model
!
Ã˜ Multi-task
32
1 â€“ Large Language Models
T5 Model
!
Ã˜ Multi-task
33
1 â€“ Large Language Models
T5 Model
!
Ã˜ Model Variants
34
1 â€“ Large Language Models
BLOOM
!
Ã˜ BLOOM is a decoder-only Transformer language model
Ã˜ Training data: ROOTs Corpus â€“ 46 natural and 13 programming language 
35
1 â€“ Large Language Models
BLOOM
!
Ã˜ Two architectural deviations in BLOOM
q ALiBi Positional Embeddings
q Embedding LayerNorm
The khead slope parameters for ALiBi
are taken as 2âˆ’8i/n with n the number 
of heads and i âˆˆ1, 2, ..., n.
Original 
Transformer-Decoder
36
1 â€“ Large Language Models
BLOOM
!
Ã˜ Prompts (Based on PromptSource)
37
1 â€“ Large Language Models
BLOOM
!
Ã˜ Prompts (Based on PromptSource)
Label
Template
Input
Prompt
38
1 â€“ Large Language Models
BLOOM
!
Ã˜ Prompts (Based on PromptSource)
Label
Template
Input
Prompt
Ã˜ Basic Prompting: Zero-shot, Few-shot
Ã˜ Chain-of-Thought (CoT)
Ã˜ Self-Consistency Sampling
Ã˜ Automatic Prompt Engineer
Ã˜ Prompt as Parameter-Efficient Fine-Tuning
39
2 â€“ Prompting Techniques
Advanced Prompting Techniques
!
https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
https://learnprompting.org/
Ã˜ Three setting for In-Content-Learning
40
2 â€“ Prompting Techniques
2.1. Basic Prompting
!
Ã˜ Example Selection, Ordering
41
2 â€“ Prompting Techniques
2.1. Basic Prompting
!
q Choose examples that are semantically similar to the test example using k-NN 
clustering in the embedding space
Source: What makes good In-content Examples for GPT-3
Ã˜ Example Selection, Ordering
42
2 â€“ Prompting Techniques
2.1. Basic Prompting
!
q Based on contrastive learning
Source: Learning To Retrieve Prompts for In-Context Learning
Ã˜ Generates a sequence of short sentences to describe reasoning logics step by step
Ã˜ Helpful for complicated reasoning tasks
43
2 â€“ Prompting Techniques
2.2. Chain-of-Thought Prompting
!
Source: Chain-of-Thought Prompting Elicits Reasoning in LLMs
Ã˜ Few-shot-CoT
44
2 â€“ Prompting Techniques
2.2. Chain-of-Thought Prompting
!
Source: Large Language Models are Zero-Shot Reasoners
Ã˜ Zero-shot-CoT
45
2 â€“ Prompting Techniques
2.2. Chain-of-Thought Prompting
!
Source: Large Language Models are Zero-Shot Reasoners
Ã˜ Zero-shot-CoT
46
2 â€“ Prompting Techniques
2.2. Chain-of-Thought Prompting
!
Source: Large Language Models are Zero-Shot Reasoners
Ã˜ To sample multiple outputs with beam search, temperature > 0 and then selecting the
best one out of these candidates
47
2 â€“ Prompting Techniques
2.3. Self-Consistency Sampling
!
Source: Chain-of-Thought Prompting Elicits Reasoning in LLMs
Ã˜ LLMs as inference model to generate demonstrations and scoring model
48
2 â€“ Prompting Techniques
2.3. Automatic Prompt Engineer
!
Source
Ã˜ LLMs as inference model to generate demonstrations and scoring model
49
2 â€“ Prompting Techniques
2.4. Prompt as Parameter-Efficient Fine-Tuning
!
Prefix-Tuning
Prompt-Tuning
3 â€“ Learning from HF (RLHF)
Overview: InstructGPT
!
3 â€“ Learning from HF (RLHF)
Pre-training LLMs
!
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ Goal: optimize the LLM to generate the response that users are 
looking for
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ A large collections of prompts:
q Labeler-written prompts
Plain
Labelers to come up with  an arbitrary task, while 
ensuring diversity of tasks
Few-shot
Labelers to come up with an instruction and multiple 
query/response pairs for that instruction
â€œGiven the sentiment for a tweetâ€
User-based
Collect use-cases stated in applications to the 
OpenAI API. Labelers to come up with prompts 
correspponding to these use-cases
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ A large collections of prompts:
q Labeler-written prompts
q API user prompts (From OpenAI GPT3 Playground)
- 200 prompts / per organization
- 10 use cases
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ A large collections of prompts:
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ A large collections of prompts:
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ Fine tune the model, call this model SFT Model
q Initialized with pretrained GPT3 175B model
q Trained for 16 epochs on demonstration data
q Notation:
ğœ‹!"#
3 â€“ Learning from HF (RLHF)
3.1. SFT Model
!
Ã˜ Task: language modeling
Ã˜ Training data: high-quality in the format of (prompt, response)
Ã˜ Data scale: 10,000 â€“ 100,000 (prompt, response) pairs
Ã˜ Model: LLMs
- Input: prompt
- Output: response for this prompt
Ã˜ Loss: cross entropy
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ Training a model to output a score on a given input (a pair of 
prompt â€“ response)
Ã˜ A classification or regression task
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ Given K = 4 to 9 outputs to rank for each prompt
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ Given K = 4 to 9 outputs to rank for each prompt
Ã˜ For 4 ranked responses:  D > C > A = B
=> 5 ranked pairs: (D > C), (D > A), (D > B), (C > A), (C > B)
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
x: the prompt, yw: the better completion, yl: the worse completion
loss ğœƒ= E %,'!,'" ~) log ğœr* x, y+ âˆ’r* x, y,
Reward on better 
completion
Reward on worse 
completion
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
Ã˜ Overfitting problem
Each prompt has K completions => K choose 2 pairs to compare
Each completion can appear in K-1 gradient updates
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
Ã˜ Overfitting problem
Each prompt has K completions => K choose 2 pairs to compare
Each completion can appear in K-1 gradient updates
Ã˜ Solution: train on all comparisons from each prompt as a single 
batch element
Ã˜ Normalization in loss with -1/(K choose 2):
loss ğœƒ= âˆ’1
K
2
E %,'!,'" ~) log ğœr* x, y+ âˆ’r* x, y,
3 â€“ Learning from HF (RLHF)
3.2. Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
Ã˜ Training data: high-quality data
x: the prompt, yw: the better completion, yl: the worse completion
Ã˜ Data scale: 100K â€“ 1M examples
InstructGPT: 50,000 prompts (each prompt: 4 to 9 responses) => 
300K to 1.8M training examples
Ã˜ Training sample (x, yw, yl )
loss ğœƒ= âˆ’1
K
2
E %,'!,'" ~) log ğœr* x, y+ âˆ’r* x, y,
3 â€“ Learning from HF (RLHF)
3.3. Reinforcement learning (RL)
!
Ã˜ Goal: train the SFT model to generate output responses that will 
maximize the scores by the RM model
Ã˜ Training data: randomly selected prompts
Ã˜ Data sacle: 10,000 â€“ 100,0000 prompts
3 â€“ Learning from HF (RLHF)
3.3. Reinforcement learning (RL)
!
ML Task: Reinforcement Learning
Ã˜ Action space: the vocabulary of tokens the LLM uses. Taking 
action means choosing a token to generate
Ã˜ Observation space: the distribution over all possible prompts
Ã˜ Policy: the probability distribution over all actions to take (all 
tokens to generate) given an observation (a  prompt)
Ã˜ Reward function: the reward model from stage 2
3 â€“ Learning from HF (RLHF)
3.3. Reinforcement learning (RL)
!
Ã˜ DRL: the distribution of prompts used for RL model
Ã˜ LLM ğœ‹-
./: the model being trained with RL, parameterized by ğœ™
Ã˜ For each x from DRL: y: ğœ‹-
./(x)
objective0 x, y; ğœ™= ğ’“ğœ½(x, y)
Ã˜ For all training data DRL
objective0 x, y; ğœ™= E(2,')~4#$./ğ’“ğœ½(x, y)
3 â€“ Learning from HF (RLHF)
3.3. Reinforcement learning (RL)
!
Ã˜ Worse reward esimates: as RLHF is updated, its outputs become 
very different from what the RM was trained on
Ã˜ Solution: add a KL penalty that makes sure PPO model output 
does not deviate too far from SFT model
objective0 x, y; ğœ™= E %,5 ~4#$67 ğ’“ğœ½x, y âˆ’ğ›½log
ğœ‹-
./ ğ‘¦ğ‘¥
ğœ‹89: ğ‘¦ğ‘¥
3 â€“ Learning from HF (RLHF)
3.3. Reinforcement learning (RL)
!
Ã˜ Just using RL objective leads to performance degradation 
on many NLP tasks
Ã˜ Solution: add a auxiliary LM ojective on the pretraining data. 
Call this variant PPO-ptx
Ã˜ Dpretrain: the distribution of the pretraining data for the pretrain 
model
objective; x<=>?=@AB; ğœ™= ğ›¾E2~4%&'(&)*+ log ğ…ğ“
./ x
3 â€“ Learning from HF (RLHF)
3.3. Reinforcement learning (RL)
!
Ã˜ Maximize the objective function in RL training:
objective ğ“= objective0 x, y; ğœ™+ objective; x<=>?=@AB; ğœ™
objective ğ“= E %,5 ~4#$67 ğ’“ğœ½x, y âˆ’ğ›½log
ğœ‹-
./ ğ‘¦ğ‘¥
ğœ‹89: ğ‘¦ğ‘¥
+ ğ›¾E2~4%&'(&)*+ log ğ…ğ“
./ x
3 â€“ Learning from HF (RLHF)
Summary
!
4 - Experiment
Source code: https://github.com/thainq107/text_classification.git
!
Reference
Ã˜ COS 597G (Fall 2022): Understanding Large Language Models
Ã˜ RLHF: Reinforcement Learning from Human Feedback, HuyenChip
Ã˜ https://github.com/ethanyanjiali/minChatGPT
Ã˜ https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
Ã˜ https://learnprompting.org/
Thanks!
Any questions?
75
