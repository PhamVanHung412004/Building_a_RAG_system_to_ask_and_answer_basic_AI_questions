1
AI VIETNAM
All-in-One Course
1
AI VIETNAM
All-in-One Course
Support Vector Machine
(Another View)
Year 2023
Vinh Dinh Nguyen
PhD in Computer Science
AI VIETNAM
All-in-One Course
2
AI VIETNAM
All-in-One Course
3
AI VIETNAM
All-in-One Course
3
Vinh Dinh Nguyen- PhD in Computer Science
√ò Linear Regression to Logistic Regression
√ò Logistic Regression to Support Vector Machine
√ò Example
Outline
4
AI VIETNAM
All-in-One Course
4
Vinh Dinh Nguyen- PhD in Computer Science
√ò Linear Regression to Logistic Regression
√ò Logistic Regression to Support Vector Machine
√ò Example
Outline
5
AI VIETNAM
All-in-One Course
5
Vinh Dinh Nguyen- PhD in Computer Science
Review ‚Äì Linear SVM
min !
" W "
"+ C ‚àë#
$ ùúÄ# such that Y * (ùë§%X+ b) ‚â•1- ùúÄ#
ùúÄ! ‚â•0
Primal Problem
6
AI VIETNAM
All-in-One Course
6
Vinh Dinh Nguyen- PhD in Computer Science
min !
" W "
"+ C ‚àë#
$ ùúÄ# such that Y * (ùë§%X+ b) ‚â•1- ùúÄ#
ùúÄ! ‚â•0
Primal Problem
How bout Non-Linear Case?
Another form:
The HyperParameter C is also called 
as Regularization Constant.
If k = 1 , then the loss is named 
as Hinge Loss and if k =2 then its 
called Quadratic Loss
Review- Linear SVM
7
AI VIETNAM
All-in-One Course
7
Vinh Dinh Nguyen- PhD in Computer Science
min !
" W "
"+ C ‚àë#
$ ùúÄ# such that Y * (ùë§%X+ b) ‚â•1- ùúÄ#
ùúÄ! ‚â•0
Primal Problem
However, this way we won‚Äôt be able use the objective function to solve for non-linear cases. Hence, we will find an equivalent problem 
named Dual Problem and solve that using Lagrange Multipliers.
Another form:
The HyperParameter C is also called 
as Regularization Constant.
If k = 1 , then the loss is named 
as Hinge Loss and if k =2 then its 
called Quadratic Loss
Review- Linear SVM
8
AI VIETNAM
All-in-One Course
8
Vinh Dinh Nguyen- PhD in Computer Science
Dualily
9
AI VIETNAM
All-in-One Course
9
Vinh Dinh Nguyen- PhD in Computer Science
Dual Problem (B√†i to√°n ƒë·ªëi ng·∫´u_
10
AI VIETNAM
All-in-One Course
10
Vinh Dinh Nguyen- PhD in Computer Science
Warm-up
Primal Problem
Primal Problem is helpful in solving Linear SVM using SGD. Remember we need 
to optimize D+1 ( where D is the dimension ) parameters in Primal Problem.
11
AI VIETNAM
All-in-One Course
11
Vinh Dinh Nguyen- PhD in Computer Science
Lagrange Multiplier
Primal Problem
Final Result: 
12
AI VIETNAM
All-in-One Course
12
Vinh Dinh Nguyen- PhD in Computer Science
Assume x has D dimensions, then there will be 
total D+1 unknowns for the following function,
If you have n constraints then there will be total D+n unknowns.
Inequality Constraint
However, in order to find the solution for the variables, it wont
be enough only to take the gradients and set those to zero
due to the Inequality Constraints.
These above five conditions are
called as KKT (Karush‚ÄìKuhn‚Äì
Tucker)
conditions
and
they must be met for strong
duality, i.e for P‚àó‚àíD‚àó=0 to be
true.
13
AI VIETNAM
All-in-One Course
13
Vinh Dinh Nguyen- PhD in Computer Science
Duality: Hard Margin Classifier
Change Hard Margin Classifier‚Äôs Objective Function from Primal Problem to Dual 
Problem using Lagrange Multiplier and KKT Conditions.
KKT (Karush‚Äì
Kuhn‚ÄìTucker) 
conditions
14
AI VIETNAM
All-in-One Course
14
Vinh Dinh Nguyen- PhD in Computer Science
Duality: Hard Margin Classifier
Change Hard Margin Classifier‚Äôs Objective Function from Primal Problem to Dual 
Problem using Lagrange Multiplier and KKT Conditions.
Dual Lagrangian Objective Function
15
AI VIETNAM
All-in-One Course
15
Vinh Dinh Nguyen- PhD in Computer Science
Duality: Hard Margin Classifier
16
AI VIETNAM
All-in-One Course
16
Vinh Dinh Nguyen- PhD in Computer Science
Another View of the SVM
You
SVM
SVM
You
17
AI VIETNAM
All-in-One Course
17
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression
Hypothesis:
Parameters:
Cost Function:
Goal:
Di·ªán t√≠ch (m2)
Gi√° (VND)
Develop a program to predict the price of the house based on the area of the house. Can we use Linear Regression to solve this problem? 
h" x  = [‚àí‚àû, +‚àû]
18
AI VIETNAM
All-in-One Course
18
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression: Cost Function
0
1
2
3
0
1
2
3
y
x
(for fixed     , this is a function of x)
(function of the parameter      )
0
1
2
3
-0,5
0
0,5
1
1,5
2
2,5
h& x = Œ∏!x
Œ∏# = 1
Œ∏# = 0.5
0.58
1.31
0.25
Œ∏# = 0.25
Œ∏# = 1.5
Œ∏# = 2
Convex function
19
AI VIETNAM
All-in-One Course
19
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression: Cost Function
20
AI VIETNAM
All-in-One Course
20
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression: Cost Function
q1
q0
J(q0,q1)
Hypothesis
Gradient Descent
21
AI VIETNAM
All-in-One Course
21
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression: Cost Function
q0
q1
J(q0,q1)
Gradient Descent
Hypothesis
22
AI VIETNAM
All-in-One Course
22
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression for Classification
Hypothesis:
Parameters:
Cost Function:
Goal:
K√≠ch th∆∞·ªõc kh·ªëi u 
√Åc t√≠nh
L√†nh t√≠nh 0
1
Develop a program to classify whether a tumor is malignant or not based on its size. Can we use Linear Regression to solve this problem? 
h" x  = [‚àí‚àû, +‚àû]
h$ x must be 0 or 1
23
AI VIETNAM
All-in-One Course
23
Vinh Dinh Nguyen- PhD in Computer Science
Linear Regression for Classification
Hypothesis:
Parameters:
Cost Function:
Goal:
K√≠ch th∆∞·ªõc kh·ªëi u 
√Åc t√≠nh
L√†nh t√≠nh 0
1
Develop a program to classify whether a tumor is malignant or not based on its size. Can we use Linear Regression to solve this problem? 
h" x  = [‚àí‚àû, +‚àû]
h" x  = [‚àí‚àû, +‚àû]
h" x  = [0,1]
If h! x ‚â•0.5 predict 1 √°c t√≠nh
If h! x < 0.5 predict 0 (l√†nh t√≠nh)
Logistic Regression
Logistic function
24
AI VIETNAM
All-in-One Course
24
Vinh Dinh Nguyen- PhD in Computer Science
Logistic Regression for Classification 
Hypothesis:
Parameters:
Cost Function:
Goal:
K√≠ch th∆∞·ªõc kh·ªëi u 
√Åc t√≠nh
L√†nh t√≠nh 0
1
h" x  = [‚àí‚àû, +‚àû]
h" x  = [0,1]
If h! x ‚â•0.5 predict 1 √°c t√≠nh
If h! x < 0.5 predict 0 (l√†nh t√≠nh)
Logistic Regression
Logistic function
0.5
25
AI VIETNAM
All-in-One Course
25
Vinh Dinh Nguyen- PhD in Computer Science
Sigmoid Function
Sigmoid functions are general mathematical functions that share similar properties: have S-shaped curves, just as the figure
below shows.
Members of Sigmoid Functions Family, from Wikipedia
26
AI VIETNAM
All-in-One Course
26
Vinh Dinh Nguyen- PhD in Computer Science
Logistic Function
The most common sigmoid function used in machine learning is Logistic Function, as the formula below.
The Curve of a Logistic Function, from Wikipedia
27
AI VIETNAM
All-in-One Course
27
Vinh Dinh Nguyen- PhD in Computer Science
z
1
Hypothesis:
predict ‚Äú         ‚Äú if
predict ‚Äú          ‚Äú  if
Z = ùúÉ%x
g z ‚â•0.5 when Z = ùúÉ%x ‚â•0
g z < 0.5 when Z = ùúÉ%x < 0
Logistic Regression
ùúÉ= ùúÉ&
ùúÉ#
x= x&
x#
28
AI VIETNAM
All-in-One Course
28
Vinh Dinh Nguyen- PhD in Computer Science
Cost Function
‚ÄúNon-convex Function‚Äù
‚ÄúConvex Function‚Äù
Linear Regression
Logistic Regression
29
AI VIETNAM
All-in-One Course
29
Vinh Dinh Nguyen- PhD in Computer Science
Logarithm Function 
30
AI VIETNAM
All-in-One Course
30
Vinh Dinh Nguyen- PhD in Computer Science
Logistic Regression: Cost Function
If y = 1
1
0
If y = 0
1
0
Cost = 0 if y = 0, h" x = 0
Cost = 0 if y = 1, h" x = 1
31
AI VIETNAM
All-in-One Course
31
Vinh Dinh Nguyen- PhD in Computer Science
Logistic Regression: Cost Function
Cost(h" x , y) = ‚àíylog h" x
‚àí(1 ‚àíy)log 1 ‚àíh" x
If y = 1: Cost(h$ x , y) = ‚àílog h$ x
If y = 0: Cost(h$ x , y) = ‚àílog 1 ‚àíh" x
Prevent Overfitting
32
AI VIETNAM
All-in-One Course
32
Vinh Dinh Nguyen- PhD in Computer Science
'( "
'"! = ‚àí
#
) ‚àë!*#
)
ùë¶(!)√ó
#
-" . # √ó
'( -" . #
'"!
+
#
) ‚àë!*#
)
1 ‚àíùë¶(!) √ó
#
#/-" . #
√ó
'( #/-" . #
'"!
 
'( "
'"! = ‚àí
#
) ‚àë!*#
)
ùë¶(!)√ó
#
-" . # √ó(ùúé(ùëß)(1 ‚àíùúéùëß)√ó
'( 0
'"!
+
#
) ‚àë!*#
) N
O
1 ‚àíùë¶(!) √ó
#
#/-" . #
√ó
(‚àíùúé(ùëß)(1 ‚àíùúéùëß)√ó
'( 0
'"!  
ùúé=
#
#12$%
ùúé‚Ä≤= ùúé(x) √ó(1- ùúé(x))
ùëß= ùúÉ3ùë•
'( "
'"! = ‚àí
#
) ‚àë!*#
)
ùë¶(!)√ó
#
-" . # √ó(‚Ñé" ùë•!
(1 ‚àí‚Ñé" ùë•!
)√óùë•4
!) +
#
) ‚àë!*#
) S
T
1 ‚àíùë¶(!) √ó
#
#/-" . #
√ó(‚àí‚Ñé" ùë•!
(1 ‚àí‚Ñé" ùë•!
)√óùë•4
!  
Behind The Scene
33
AI VIETNAM
All-in-One Course
33
Vinh Dinh Nguyen- PhD in Computer Science
ùúé=
#
#12$%
ùúé‚Ä≤= ùúé(x) √ó(1- ùúé(x))
ùëß= ùúÉ3ùë•
'( "
'"! = ‚àí
#
) ‚àë!*#
)
ùë¶(!)√ó (1 ‚àí‚Ñé" ùë•!
√óùë•4
! ‚àí1 ‚àíùë¶(!) √ó‚Ñé" ùë•!
√óùë•4
!  
Behind The Scene
'( "
'"! = ‚àí
#
) ‚àë!*#
)
ùë¶(!) ‚àíùë¶! √ó‚Ñé" ùë•!
‚àí‚Ñé" ùë•!
+ ùë¶! √ó‚Ñé" ùë•!
√óùë•4
! 
'( "
'"! = ‚àí
#
) ‚àë!*#
)
ùë¶(!) ‚àí‚Ñé" ùë•!
√óùë•4
! 
The gradient descent is the same as Linear Regression
34
AI VIETNAM
All-in-One Course
34
Vinh Dinh Nguyen- PhD in Computer Science
√ò Linear Regression to Logistic Regression
√ò Logistic Regression to Support Vector Machine
√ò Example
Outline
35
AI VIETNAM
All-in-One Course
35
Vinh Dinh Nguyen- PhD in Computer Science
Another View of Logistic Regression
If y = 1, we want h" x ‚âà1, Z = ùúÉ%x ‚â´0
z
1
Hypothesis:
Z = ùúÉ%x
‚àílog
1
1 + e!"
If y = 0, we want h" x ‚âà0, Z = ùúÉ%x ‚â™0
‚àílog (1 ‚àí
#
#$%!")
Cost(h! x , y) = ‚àíylog h! x
‚àí(1 ‚àíy)log 1 ‚àíh! x
Cost5*#(z)
Cost5*&(z)
Cost5*#(ùúÉ%xi)
Cost5*&(ùúÉ%xi)
36
AI VIETNAM
All-in-One Course
36
Vinh Dinh Nguyen- PhD in Computer Science
Logistic Regression to 
Support Vector Machine
Logistic Regression
Cost5*#(ùúÉ%xi)
Cost5*&(ùúÉ%xi)
S·∫Øp x·ªâ
S·∫Øp x·ªâ
A
B
min (A +ŒªB)
Support Vector Machine
min C \
6*#
7
‚àíyiCost5*#(ùúÉ%xi) + ( 1 ‚àíy)Cost5*&(ùúÉ%xi) + 1
2 \
8*#
7
ùúÉ8
9
min (CA + B)
A
B
C = 1 / Œª
ùúÉ
37
AI VIETNAM
All-in-One Course
37
Vinh Dinh Nguyen- PhD in Computer Science
Support Vector Machine
If y = 1, we want h& x ‚âà1, Z = ùúÉ'x ‚â•1
‚àílog
1
1 + e!"
Cost5*#(z)
‚àílog (1 ‚àí
#
#$%!")
Cost5*&(z)
min C %
^_ `
a
‚àíyiCostb_`(ùúÉcxi) + ( 1 ‚àíy)Costb_d(ùúÉcxi) + 1
2 %
e_`
a
ùúÉe
f
ùúÉ
If y = 1, we want h& x ‚âà1, Z = ùúÉ'x > 0
Logistic Regression
Support Vector Machine
If y = 1, we want h& x ‚âà1, Z = ùúÉ'x ‚â§‚àí1
If y = 1, we want h& x ‚âà0, Z = ùúÉ'x < 0
Logistic Regression
Support Vector Machine
Z = ùúÉ!x ‚â•1
Z = ùúÉ!x ‚â§‚àí1
Supposing that C is very very 
large (100,000)
Does it affect to the optimization?
38
AI VIETNAM
All-in-One Course
38
Vinh Dinh Nguyen- PhD in Computer Science
SVM Decision Boundary
min C %
^_`
a
‚àíyiCostb_`(ùúÉcxi) + ( 1 ‚àíy)Costb_d(ùúÉcxi) + 1
2 %
e_`
a
ùúÉe
f
Supposing that C is very very 
large (100,000)
This term should be ‚Ä¶.
min C 1
2 %
e_`
a
ùúÉe
f
1
2
3
Margin
Linear separable case
Outlier
This line presents for C ‚Ä¶?
This line presents for C ‚Ä¶ ? ‚Ä¶
39
AI VIETNAM
All-in-One Course
39
Vinh Dinh Nguyen- PhD in Computer Science
Vector Innder Product
u
u1
v2
v1
u2
p1
u =
u1
u2 
v =
v1
v2 
p1: Length of projection of vector v on vector u
u = u#
9 + u9
9
Length of vector u
u%v = p1. u
= u1v1+ u2v2
u
u1
v1
u2
p2
v
v
u%v = p2 . u
= u1v1+ u2v2
What are the value of p in two cases?
a.
p1 > 0 & p2 < 0
b.
p1 < 0 & p2 > 0
c.
p1 > 0 & p2 > 0
d.
p1 < 0 & p2 < 0
40
AI VIETNAM
All-in-One Course
40
Vinh Dinh Nguyen- PhD in Computer Science
SVM Decision Boundary
min C `
f ‚àëe_`
a ùúÉe
f = `
f ùúÉd
f + ùúÉ`
f + ùúÉf
f  =`
f
ùúÉd
f + ùúÉ`
f + ùúÉf
f
f
=`
f ùúÉf
Subjective to 
ùúÉ%x(i) ‚â•1  
If y(i)  = 1
ùúÉ%x(i) ‚â§‚àí1
If y(i)  = 0
ùúÉ=
ùúÉ&
ùúÉ#
ùúÉ9
ùúÉ& = 0 
Simplication:
n = 2
p(i)
x(i)
x(i) =
x&
(6)
x#
(6)
x9
(6)
ùúÉ
ùúÉ#
ùúÉ9
ùúÉ%x(i) = p(i) ùúÉ
= ùúÉ#x#
(6) + ùúÉ9x9
(6)
x#
(6)
x9
(6)
x&
(6) = 1
41
AI VIETNAM
All-in-One Course
41
Vinh Dinh Nguyen- PhD in Computer Science
min C `
f ‚àëe_`
a ùúÉe
f = `
f ùúÉd
f + ùúÉ`
f + ùúÉf
f  =`
f
ùúÉd
f + ùúÉ`
f + ùúÉf
f
f
=`
f ùúÉf
p(i) ùúÉ ‚â•1  
If y(i)  = 1
p(i) ùúÉ
‚â§‚àí1
If y(i)  = 0
ùúÉ=
ùúÉ&
ùúÉ#
ùúÉ9
ùúÉ& = 0 
Simplication:
n = 2
x(i) =
x&
(6)
x#
(6)
x9
(6)
x&
(6) = 1
p(i) is the projection of x(i)  of onto the vector ùúÉ  
Subjective to 
ùúÉ
x(#)
x(%)
x(&)
x(')
x(()
x())
p(1)
p(2)
p(i) ùúÉ ‚â•1  
ùúÉ is large
Violate (1)
(1)
p(i) ùúÉ
‚â§‚àí1
ùúÉ is large
Violate (1)
ùúÉ
x(#)
x(%)
x(&)
x(')
x(()
x())
p(i) ùúÉ ‚â•1  
ùúÉ is small
Satisfy (1)
p(i) ùúÉ
‚â§‚àí1
ùúÉ is small
Satisfy (1)
p(1)
p(2)
Maximize Margin
Margin
SVM Decision Boundary
42
AI VIETNAM
All-in-One Course
42
Vinh Dinh Nguyen- PhD in Computer Science
Non-Linear Decision Boundary
h$ x  = l
1 z ‚â•1
0 z ‚â§‚àí1
z = ùúÉ& + ùúÉ#x# + ùúÉ9x9 + ùúÉ:x#x9 + ùúÉ;x#
9+ùúÉ<x9
9 + ‚ãØ
z = ùúÉ& + ùúÉ#f# + ùúÉ9f9 + ùúÉ9f: + ùúÉ;f; ‚Ä¶
f#= x#, f9= x9, f:= x#x9, f;=x#
9, f<= x9
9, ‚Ä¶
C√≥ ph∆∞∆°ng ph√°p n√†o ƒë·ªÉ x√¢y d·ª±ng features f#, f9, f:, 
 t·ªë h∆°n kh√¥ng kh√¥ng ?
h$ x  = l
1 z ‚â•1
0 z ‚â§‚àí1
z = ùúÉ& + ùúÉ#x# + ùúÉ9x9 + ùúÉ:x: + ùúÉ:x;+ùúÉ<x< + ‚ãØ
Linear case
Polynomial case
Polynomial case
43
AI VIETNAM
All-in-One Course
43
Vinh Dinh Nguyen- PhD in Computer Science
Kernel and Similarity
Given sample x x#, x9 , compute new feature depending
on proximity to landmarks l(#), l(9), l(:)
x#
x9
l(:)
l(9)
l(#)
f#=similarity x, l(#) = e /
&$'()) +
+,+
f9=similarity x, l(9) = e /
&$'(+) +
+,+
f:=similarity x, l(:) = e /
&$'(-) +
+,+
Kernel
Gausian kernel
If x is near to ‚âàl(#):
x
f# = e / =/>()) +
9?+
‚âà1
If is far from l(#):
f# = e / =/>()) +
9?+
‚âà0
X and l(-) Relationship
Generate new feature
44
AI VIETNAM
All-in-One Course
44
Vinh Dinh Nguyen- PhD in Computer Science
Landmark Selection
x#
x9
l(:)
l(9)
l(#)
x
Given samples: 
x(#), y(#) , x(9), y(9) , ‚Ä¶ , x(7), y(7)
Choose l(#) = x(#), l(9) = x(9), ‚Ä¶, l(7) = x(7) 
Given sample x:
f#
(6) =similarity x(6), l(#) = e /
&(.)$'()) +
+,+
f9
(6) =similarity x(6), l(9) = e /
&(.)$'(+) +
+,+
f7
(6) =similarity x(6), l(7) = e /
&(.)$'(/) +
+,+
‚Ä¶
x@AB
(6)
= f =
f#
(6)
f9
(6)
f:
(6)
‚Ä¶
f7
(6)
xC>D
(6) =
x#
x9
Generate new features
Old feature
New feature
45
AI VIETNAM
All-in-One Course
45
Vinh Dinh Nguyen- PhD in Computer Science
Other Kernel
https://medium.com/geekculture/kernel-methods-in-support-vector-machines-bb9409342c49
46
AI VIETNAM
All-in-One Course
46
Vinh Dinh Nguyen- PhD in Computer Science
Multiple Class Classification
47
AI VIETNAM
All-in-One Course
47
Vinh Dinh Nguyen- PhD in Computer Science
√ò Linear Regression to Logistic Regression
√ò Logistic Regression to Support Vector Machine
√ò Example
Outline
48
AI VIETNAM
All-in-One Course
48
Vinh Dinh Nguyen- PhD in Computer Science
Email Spam Classification - 1
Normal Email
Spam Email
49
AI VIETNAM
All-in-One Course
49
Vinh Dinh Nguyen- PhD in Computer Science
Email Spam Classification - 1
Normal Email
Spam Email
Text cleanup
Sample Code
50
AI VIETNAM
All-in-One Course
50
Vinh Dinh Nguyen- PhD in Computer Science
Email Spam Classification - 1
Normal Email
Spam Email
Text cleanup
Build dictionary
51
AI VIETNAM
All-in-One Course
51
Vinh Dinh Nguyen- PhD in Computer Science
Email Spam Classification - 1
Normal Email
Spam Email
Text cleanup
Build dictionary
Encoding data
52
AI VIETNAM
All-in-One Course
52
Vinh Dinh Nguyen- PhD in Computer Science
Email Spam Classification - 1
Normal Email
Spam Email
Text cleanup
Build dictionary
Encoding data
Preparing Train Set
53
AI VIETNAM
All-in-One Course
53
Vinh Dinh Nguyen- PhD in Computer Science
Email Spam Classification - 1
Normal Email
Spam Email
Text cleanup
Build dictionary
Encoding data
Preparing Train Set
SVM in sklearn
54
AI VIETNAM
All-in-One Course
54
Vinh Dinh Nguyen- PhD in Computer Science
Product Classfication - 2
Dataset
Feature 1
Feature 2
Category
1.9643
4.5957
1
2.2753
3.8589
1
2.9781
4.5651
1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶.
3.3814 
3.4291
0
Visualization
55
AI VIETNAM
All-in-One Course
55
Vinh Dinh Nguyen- PhD in Computer Science
Product Classfication - 2
Dataset
Feature 1
Feature 2
Category
1.9643
4.5957
1
2.2753
3.8589
1
2.9781
4.5651
1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶.
3.3814 
3.4291
0
Visualization
56
AI VIETNAM
All-in-One Course
56
Vinh Dinh Nguyen- PhD in Computer Science
Product Classfication - 2
Dataset
Feature 1
Feature 2
Category
1.9643
4.5957
1
2.2753
3.8589
1
2.9781
4.5651
1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶.
3.3814 
3.4291
0
Visualization (C = 100)
57
AI VIETNAM
All-in-One Course
57
Vinh Dinh Nguyen- PhD in Computer Science
Product Classfication - 2
Dataset
Feature 1
Feature 2
Category
-1.58986e-01
4.23977e-01
1
-3.47926e-01
4.70760e-01
1
-5.04608e-01
3.53801e-01
1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶.
-5.96774e-01
-5.96774e-01
0
Visualization
58
AI VIETNAM
All-in-One Course
58
Vinh Dinh Nguyen- PhD in Computer Science
Product Classfication - 2
Dataset
Feature 1
Feature 2
Category
-1.58986e-01
4.23977e-01
1
-3.47926e-01
4.70760e-01
1
-5.04608e-01
3.53801e-01
1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶.
-5.96774e-01
-5.96774e-01
0
Visualization
59
AI VIETNAM
All-in-One Course
