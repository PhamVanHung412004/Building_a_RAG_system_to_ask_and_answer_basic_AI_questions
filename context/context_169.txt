Softmax Regression
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
Year 2023
â¢Motivation
â¢Model Construction
â¢Loss Function
â¢Generalization (Further Reading)
â¢Another Approach  (Further Reading) 
Outline
Linear Regression
â– Prediction
Find the line à·œğ‘¦= ğœ½ğ‘‡ğ’™ that is best fitting to given 
data, then use à·œğ‘¦ to predict for new data 
ğ’™
ğ’š
error
error
error
error
à·œğ‘¦= ğœ½ğ‘‡ğ’™= ğ‘¤ğ‘¥+ ğ‘
à·œğ‘¦âˆˆâˆ’âˆ 
+ âˆ 
Feature
Label
Area-based House Price Data
Training data
Model
construct
AI VIETNAM
All-in-One Course
1
Logistic Regression
â– Binary Classification
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
Assign numbers 
to categories
Feature
Category 0
Category 1
Plot data
Feature
A line is not suitable 
for this data
AI VIETNAM
All-in-One Course
2
Idea of Logistic Regression
â– Binary Classification
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
Assign numbers 
to categories
Sigmoid function 
could fit the data
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğœ½ğ‘‡ğ’™= ğ’™ğ‘‡ğœ½
à·œğ‘¦âˆˆ0 1 
1
1 + ğ‘’âˆ’ğœ½ğ‘‡ğ’™ 
Feature
AI VIETNAM
All-in-One Course
L = âˆ’ğ‘¦logà·œğ‘¦âˆ’1 âˆ’ğ‘¦log 1 âˆ’à·œğ‘¦
Binary cross-entropy
3
Motivation
4
AI VIETNAM
All-in-One Course
Model
Label
Loss
ğ‘¥
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘¤
ğ‘
Model decides x belongs to the 
category  (label=1) with the belief of p
ğ‘ƒğ‘¦= 0|ğ‘¥= 1 âˆ’ğ‘
Implicitly conclude that
1
Petal
length
ğ‘¤
ğ‘
p
ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘
ğ‘¥
sigmoid
Convert to probability
Feature
Label
Motivation
5
AI VIETNAM
All-in-One Course
1
Petal
length
p
ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘
ğ‘¥
Model decides x belongs to category 
(y=1) with the belief of p
ğ‘ƒğ‘™ğ‘ğ‘ğ‘’ğ‘™= 0|ğ‘¥= 1 âˆ’ğ‘
Implicitly extract that
sigmoid
ğ‘¤
ğ‘
Training
optimize
How to have explicitly ğ‘ƒğ‘¦= 0|ğ‘¥? 
Optimize ğ‘¤ and ğ‘ for ğ‘ƒğ‘™ğ‘ğ‘ğ‘’ğ‘™= 1|ğ‘¥ affects ğ‘ƒğ‘™ğ‘ğ‘ğ‘’ğ‘™= 0|ğ‘¥ and vice versa 
Problem!
Feature
Label
Motivation
6
AI VIETNAM
All-in-One Course
1
Petal
length
p
ğ‘¥
Convert to 
probabilities
ğ‘¤0
ğ‘0
??? Softmax function
Explicitly output ğ‘ƒğ‘¦= 0|ğ‘¥ and ğ‘ƒğ‘¦= 1|ğ‘¥ 
1
Petal
length
ğ‘¤1
ğ‘ƒğ‘¦= 0|ğ‘¥
ğ‘¥
Convert to 
probabilities
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘ƒğ‘¦= 1|ğ‘¥
Optimize for ğ‘ƒğ‘¦= 0|ğ‘¥ 
Optimize for ğ‘ƒğ‘¦= 1|ğ‘¥ 
ğ‘§0
ğ‘§1
Feature
Label
Problem!
Motivation
7
AI VIETNAM
All-in-One Course
Softmax function
ğ‘§1 = 1.0
ğ‘§2 = 3.0
Softmax
ğ‘“(ğ‘§1) = 0.12
ğ‘“(ğ‘§2) = 0.88
Input
Probability
ğ‘§1 = 1.0
ğ‘§2 = 2.0
Softmax
ğ‘“(ğ‘§1) = 0.09
ğ‘“(ğ‘§2) = 0.24
Input
Probability
ğ‘§3 = 3.0
ğ‘“(ğ‘§3) = 0.67
ğ‘ƒğ‘–= ğ‘“ğ‘§ğ‘–=
ğ‘’ğ‘§ğ‘–
Ïƒğ‘—ğ‘’ğ‘§ğ‘—
0 â‰¤ğ‘“ğ‘§ğ‘–â‰¤1
à·
ğ‘–
ğ‘“ğ‘§ğ‘–= 1
Softmax function
Chuyá»ƒn cÃ¡c giÃ¡ trá»‹ cá»§a má»™t vector thÃ nh cÃ¡c giÃ¡ trá»‹ xÃ¡c suáº¥t
ğ‘“ğ‘¥ğ‘–=
ğ‘’ğ‘¥ğ‘–
Ïƒğ‘—ğ‘’ğ‘¥ğ‘—
0 â‰¤ğ‘“ğ‘¥ğ‘–â‰¤1
à·
ğ‘–
ğ‘“ğ‘¥ğ‘–= 1
Formula
ğ‘¥1 = 1.0
ğ‘¥2 = 2.0
Softmax
ğ‘“(ğ‘¥1) = 0.09
ğ‘“(ğ‘¥2) = 0.24
Input
Probability
ğ‘¥3 = 3.0
ğ‘“(ğ‘¥3) = 0.67
GiÃ¡ trá»‹ nan vÃ¬ ğ‘’ğ‘¥ vÆ°á»£t giá»›i háº¡n lÆ°u trá»¯ cá»§a biáº¿n 
HÃ m mÅ© tÄƒng ráº¥t 
nhanh khi x tÄƒng
ğ‘’ğ‘¥
ğ‘¥
ğ‘’ğ‘¥âˆˆ[0, +âˆ)
ğ‘¥âˆˆ[âˆ’âˆ, +âˆ)
Implementation 
(straightforward)
8
Softmax function (stable)
ğ‘¥1 = 1.0
ğ‘¥2 = 2.0
X
ğ‘¥3 = 3.0
Softmax
ğ‘“(ğ‘¥1) = 0.09
ğ‘“(ğ‘¥2) = 0.24
Probability
ğ‘“(ğ‘¥3) = 0.67
ğ‘“ğ‘¥ğ‘–=
ğ‘’(ğ‘¥ğ‘–âˆ’ğ‘š)
Ïƒğ‘—ğ‘’(ğ‘¥ğ‘—âˆ’ğ‘š)
(Stable) Formula
ğ‘š= max(ğ’™)
ğ‘¥1 = âˆ’2.0
ğ‘¥2 = âˆ’1.0
X-m
ğ‘¥3 = 0
ğ‘’ğ‘¥
ğ‘¥
ğ‘’ğ‘¥âˆˆ(0, 1)
ğ‘¥âˆˆ(âˆ’âˆ, 0)
Implementation 
(stable)
9
Motivation
AI VIETNAM
All-in-One Course
How about loss function?
ğ¿(ğœ½) = âˆ’ylogà·œy1âˆ’(1âˆ’y)log(à·œy0)
Softmax function
ğ‘ƒğ‘–= ğ‘“ğ‘§ğ‘–=
ğ‘’ğ‘§ğ‘–
Ïƒğ‘—ğ‘’ğ‘§ğ‘—
0 â‰¤ğ‘“ğ‘§ğ‘–â‰¤1
à·
ğ‘–
ğ‘“ğ‘§ğ‘–= 1
Explicitly output ğ‘ƒğ‘¦= 1|ğ‘¥ and ğ‘ƒğ‘¦= 0|ğ‘¥ 
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥
ğ‘¥
Softmax
function
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
Feature
Label
10
â¢Motivation
â¢Model Construction
â¢Loss Function
â¢Generalization (Further Reading)
â¢Another Approach  (Further Reading) 
Outline
Model Construction
AI VIETNAM
All-in-One Course
â– 1-D Feature and two classes
Feature is with one dimension
â†’ Need one node for input
Two categories
â†’ Need two node for output
1
ğ‘¥
ğ‘§0
ğ‘§2
ğ‘§1
Softmax
à·œy0 = ğ‘ƒğ‘¦= 0
à·œy1 = ğ‘ƒğ‘¦= 1
Model
Feature
Label
#class=2
#feature=1
11
Model Construction
AI VIETNAM
All-in-One Course
â– 1-D Feature and three classes
Feature is with one dimension
â†’ Need one node for input
Three categories
â†’ Need three nodes for output
1
ğ‘¥
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0 = ğ‘ƒğ‘¦= 0
à·œy1 = ğ‘ƒğ‘¦= 1
à·œy2 = ğ‘ƒğ‘¦= 2
Model
Feature
Label
#class=3
#feature=1
12
Model Construction
AI VIETNAM
All-in-One Course
â– 4-D Feature and three classes
Feature is with two dimensions
â†’ Need two nodes for input
Three categories
â†’ Need three nodes for output
1
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0 = ğ‘ƒğ‘¦= 0
à·œy1 = ğ‘ƒğ‘¦= 1
à·œy2 = ğ‘ƒğ‘¦= 2
Model
ğ‘¥1
ğ‘¥2
Feature
Label
#class=3
#feature=2
13
Model Construction
AI VIETNAM
All-in-One Course
Feature
Label
â– 4-D Feature and three classes
Feature is with four dimensions
â†’ Need four nodes for input
Three categories
â†’ Need three nodes for output
1
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0 = ğ‘ƒğ‘¦= 0
à·œy1 = ğ‘ƒğ‘¦= 1
à·œy2 = ğ‘ƒğ‘¦= 2
Model
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
#class=3
#feature=4
14
â¢Motivation
â¢Model Construction
â¢Loss Function
â¢Generalization (Further Reading)
â¢Another Approach  (Further Reading) 
Outline
Loss function
AI VIETNAM
All-in-One Course
â– Simple illustration
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥
Model
ğ‘§0
ğ‘§1
ğ‘§0 = ğ‘¥ğ‘¤0 + ğ‘0
ğ‘§1 = ğ‘¥ğ‘¤1 + ğ‘1
One-hot encoding for label
ğ‘¦= 0 â†’ğ’š= [1 0]
ğ‘¦= 1 â†’ğ’š= [0 1]
ğ‘¦0 ğ‘¦1
vector
scalar
à·œy0 =
ğ‘’ğ‘§0
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
à·œy1 =
ğ‘’ğ‘§1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
A vector is by default a column vector ğœ½0 = ğ‘0
ğ‘¤0  
vector transpose ğœ½0
ğ‘‡= ğ‘0 ğ‘¤0
ğ’›= ğ‘§0
ğ‘§1 = ğ‘0 ğ‘¤0
ğ‘1 ğ‘¤1
1
ğ‘¥= ğœ½0
ğ‘‡
ğœ½1
ğ‘‡
1
ğ‘¥= ğœ½ğ‘‡ğ’™
à·œğ²= à·œy0
à·œy1 =
1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ‘’ğ‘§0
ğ‘’ğ‘§1 =
ğ‘’ğ’›
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
Feature
Label
Loss function
AI VIETNAM
All-in-One Course
Feature
Label
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥
Model
ğ‘§0
ğ‘§1
One-hot encoding for label
ğ‘¦= 0 â†’ğ’š= [1 0]
ğ‘¦= 1 â†’ğ’š= [0 1]
ğ‘¦0 ğ‘¦1
vector
scalar
ğ‘§0 = ğ‘¥ğ‘¤0 + ğ‘0
ğ‘§1 = ğ‘¥ğ‘¤1 + ğ‘1
à·œy0 =
ğ‘’ğ‘§0
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
à·œy1 =
ğ‘’ğ‘§1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ’›= ğ‘§0
ğ‘§1 = ğ‘0 ğ‘¤0
ğ‘1 ğ‘¤1
1
ğ‘¥= ğœ½0
ğ‘‡
ğœ½1
ğ‘‡
1
ğ‘¥= ğœ½ğ‘‡ğ’™
à·œğ²= à·œy0
à·œy1 =
1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ‘’ğ‘§0
ğ‘’ğ‘§1 =
ğ‘’ğ’›
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ¿ğœ½= âˆ’ğ‘¦0logà·œy0âˆ’ğ‘¦1logà·œy1 = âˆ’à·
ğ‘–=0
1
ğ‘¦ğ‘–logà·œğ‘¦ğ‘–= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š
â– Simple illustration
Loss function
AI VIETNAM
All-in-One Course
1
ğ‘¤1
à·œy0
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1
Model
ğ‘§0
ğ‘§1
ğ¿(ğœ½) = âˆ’ğ‘¦0logà·œy0âˆ’ğ‘¦1logà·œy1 = âˆ’à·
ğ‘–=0
1
ğ‘¦ğ‘–logà·œğ‘¦ğ‘–= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š
à·œy0 =
ğ‘’ğ‘§0
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
à·œy1 =
ğ‘’ğ‘§1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğœ•à·œğ‘¦ğ‘–
ğœ•ğ‘§ğ‘—
= àµà·œğ‘¦ğ‘–1 âˆ’à·œğ‘¦ğ‘– 
ğ‘–ğ‘“ ğ‘–= ğ‘—
âˆ’à·œğ‘¦ğ‘–à·œğ‘¦ğ‘— 
ğ‘–ğ‘“ ğ‘–â‰ ğ‘— 
Derivative
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= âˆ’à·
ğ‘˜
ğ‘¦ğ‘˜
ğœ•log(à·œğ‘¦ğ‘˜)
ğœ•ğ‘§ğ‘–
= âˆ’à·
ğ‘˜
ğ‘¦ğ‘˜
ğœ•log(à·œğ‘¦ğ‘˜)
ğœ•à·œğ‘¦ğ‘˜
ğœ•à·œğ‘¦ğ‘˜
ğœ•ğ‘§ğ‘–
= âˆ’à·
ğ‘˜
ğ‘¦ğ‘˜
1
à·œğ‘¦ğ‘˜
ğœ•à·œğ‘¦ğ‘˜
ğœ•ğ‘§ğ‘–
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= âˆ’ğ‘¦ğ‘–(1 âˆ’à·œğ‘¦ğ‘–) âˆ’à·
ğ‘˜â‰ ğ‘–
ğ‘¦ğ‘˜
1
à·œğ‘¦ğ‘˜
(âˆ’à·œğ‘¦ğ‘˜à·œğ‘¦ğ‘–)
= âˆ’ğ‘¦ğ‘–1 âˆ’à·œğ‘¦ğ‘–+ à·
ğ‘˜â‰ ğ‘–
ğ‘¦ğ‘˜à·œğ‘¦ğ‘–
= âˆ’ğ‘¦ğ‘–+ ğ‘¦ğ‘–à·œğ‘¦ğ‘–+ à·
ğ‘˜â‰ ğ‘–
ğ‘¦ğ‘˜à·œğ‘¦ğ‘–
= à·œğ‘¦ğ‘–
ğ‘¦ğ‘–+ à·
ğ‘˜â‰ ğ‘–
ğ‘¦ğ‘˜
âˆ’ğ‘¦ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
Loss function
AI VIETNAM
All-in-One Course
One-hot encoding for label
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
vector
scalar
ğ‘§0 = ğ‘¥ğ‘¤0 + ğ‘0
ğ‘§1 = ğ‘¥ğ‘¤1 + ğ‘1
à·œy0 =
ğ‘’ğ‘§0
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
à·œy1 =
ğ‘’ğ‘§1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ’›= ğ‘§0
ğ‘§1 = ğ‘0 ğ‘¤0
ğ‘1 ğ‘¤1
1
ğ‘¥= ğœ½0
ğ‘‡
ğœ½1
ğ‘‡
1
ğ‘¥= ğœ½ğ‘‡ğ’™
à·œğ²= à·œy0
à·œy1 =
1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ‘’ğ‘§0
ğ‘’ğ‘§1 =
ğ‘’ğ’›
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ¿ğœ½= âˆ’à·
ğ‘–=0
1
ğ‘¦ğ‘–logà·œğ‘¦ğ‘–= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š
1
ğ‘¤1
à·œy0
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1
Model
ğ‘§0
ğ‘§1
Derivative
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•à·œğ‘¦ğ‘–
ğœ•ğ‘§ğ‘—
= àµà·œğ‘¦ğ‘–1 âˆ’à·œğ‘¦ğ‘– ğ‘–ğ‘“ ğ‘–= ğ‘—
âˆ’à·œğ‘¦ğ‘–à·œğ‘¦ğ‘— 
ğ‘–ğ‘“ ğ‘–â‰ ğ‘— 
ğœ•ğ¿
ğœ•à·œğ‘¦ğ‘–
= âˆ’ğ‘¦ğ‘–
à·œğ‘¦ğ‘–
Simple Illustration - Summary
Feature
Label
1
ğ‘¤1
à·œy0
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1
Model
ğ‘§0
ğ‘§1
One-hot encoding for label
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
vector
scalar
ğœ½= ğ‘0 ğ‘1
ğ‘¤0 ğ‘¤1
ğ’™= 1
ğ‘¥
AI VIETNAM
All-in-One Course
ğ’›= ğœ½ğ‘‡ğ’™
à·œğ²=
ğ‘’ğ’›
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
1. Forward computation
ğ¿(ğœ½) = âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š
2. Loss function
3. Derivative
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğ›»ğœ½ğ¿= ğ’™à·œğ²âˆ’ğ’šğ‘‡
4. Update
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
â€²
ğœ‚is learning rate
Explaining Cross-entropy in another way
Motivation
AI VIETNAM
All-in-One Course
Model
Label
Loss
ğ‘¥
âˆ’ylogà·œyâˆ’(1âˆ’y)log(1âˆ’à·œy )
ğ‘¦
à·œğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’âˆ’ğ‘§
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘¤
ğ‘
Model decides x belongs to the 
category  (label=1) with the belief of p
ğ‘ƒğ‘¦= 0|ğ‘¥= 1 âˆ’ğ‘
Implicitly conclude that
1
Petal
length
ğ‘¤
ğ‘
p
ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘
ğ‘¥
sigmoid
Convert to probability
Feature
Label
21
Outputs of Model
AI VIETNAM
All-in-One Course
Softmax function
à·œyğ‘–=
ğ‘’ğ‘§ğ‘–
Ïƒğ‘—ğ‘’ğ‘§ğ‘—
0 â‰¤ğ‘“ğ‘§ğ‘–â‰¤1
à·
ğ‘–
ğ‘“ğ‘§ğ‘–= 1
Feature
Label
Explicitly output ğ‘ƒğ‘¦= 1|ğ‘¥ and ğ‘ƒğ‘¦= 0|ğ‘¥ 
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥
Softmax
function
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
22
For a Given Sample
AI VIETNAM
All-in-One Course
Softmax function
à·œyğ‘–=
ğ‘’ğ‘§ğ‘–
Ïƒğ‘—ğ‘’ğ‘§ğ‘—
0 â‰¤ğ‘“ğ‘§ğ‘–â‰¤1
à·
ğ‘–
ğ‘“ğ‘§ğ‘–= 1
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
Feature
Label
Given a sample (ğ‘¥= 1.3, ğ‘¦= 0)
With (ğ‘¥= 1.3, ğ‘¦= 0), model becomes better when ğ‘0 increases 
                                                                            and ğ‘1 decreases
Differences between increasing ğ‘0 and decreasing ğ‘1?
23
Softmax function
à·œyğ‘–=
ğ‘’ğ‘§ğ‘–
Ïƒğ‘—ğ‘’ğ‘§ğ‘—
0 â‰¤ğ‘“ğ‘§ğ‘–â‰¤1
à·
ğ‘–
ğ‘“ğ‘§ğ‘–= 1
Feature
Label
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
Given a sample (ğ‘¥= 1.3, ğ‘¦= 0)
With (ğ‘¥= 1.3, ğ‘¦= 0), model becomes better when ğ‘0 increases 
                                                                            and ğ‘1 decreases
Increasing ğ‘0:
à·œy0 =
ğ‘’ğ‘§0
ğ‘’ğ‘§0 + ğ‘’ğ‘§1
increasing z0 
decreasing z1
Decreasing ğ‘1:
à·œy1 =
ğ‘’ğ‘§1
ğ‘’ğ‘§0 + ğ‘’ğ‘§1
increasing z0 
decreasing z1
For a Given 
Sample
24
Feature
Label
Observation
1
Petal
length
ğ‘¤1
ğ‘¥= 1.3
Softmax
function
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
or    
increasing z0 
decreasing z1
With (ğ‘¥= 1.3, ğ‘¦= 0), 
model becomes better 
when ğ‘0 increases and 
ğ‘1 decreases
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
either 
Feature
Label
Loss 
Computation
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
selected 
increasing z0 
decreasing z1
With (ğ‘¥= 1.3, ğ‘¦= 0), 
model becomes better 
when ğ‘0 increases and 
ğ‘1 decreases
ğ‘0 âˆˆ0,1
When ğ‘0 = 0, the model (or Î¸) is worst
When ğ‘0 = 1, the model (or Î¸) is perfect
-log(à·œğ‘¦)
a0  â†’1 
smaller losses
ğ‘0
ğ‘™ğ‘œğ‘ ğ‘ 
-log(1-à·œğ‘¦)
ğ‘0
a0  â†’1 
larger losses
ğ‘™ğ‘œğ‘ ğ‘ 
Which one?
Feature
Label
Loss 
Computation
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
increasing z0 
decreasing z1
With (ğ‘¥= 1.3, ğ‘¦= 0), 
model becomes better 
when ğ‘0 increases and 
ğ‘1 decreases
ğ‘0 âˆˆ0,1
When ğ‘0 = 0, the model (or Î¸) is worst
When ğ‘0 = 1, the model (or Î¸) is perfect
-log(ğ‘0)
a0  â†’1 
smaller losses
ğ‘0
ğ‘™ğ‘œğ‘ ğ‘ 
Loss function
ğ¿(ğœ½) = âˆ’log(à·œy0)
27
Feature
Label
Another 
Sample
1
Petal
length
ğ‘¤1
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¥= 4.1
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
increasing z1 
decreasing z0
With (ğ‘¥= 4.1, ğ‘¦= 1), 
model becomes better 
when ğ‘1 increases and 
ğ‘0 decreases
ğ‘1 âˆˆ0,1
When ğ‘1 = 0, the model (or Î¸) is worst
When ğ‘1 = 1, the model (or Î¸) is perfect
-log(ğ‘1)
a1  â†’1 
smaller losses
ğ‘1
ğ‘™ğ‘œğ‘ ğ‘ 
Loss function
ğ¿(ğœ½) = âˆ’log(à·œy1)
28
Observation
1
Petal
length
ğ‘¤1
à·œy0 = ğ‘ƒğ‘¦= 0|ğ‘¥= ğ‘0
ğ‘¥= 1.3
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
ğ‘¦= 0
ğ¿(ğœ½) = âˆ’log(à·œy0)
With (ğ‘¥= â‹¯, ğ‘¦=?), 
model becomes better 
when ğ‘? increases
Feature
Label
1
Petal
length
ğ‘¤1
à·œy1 = ğ‘ƒğ‘¦= 1|ğ‘¥= ğ‘1
ğ‘¥= 4.1
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ‘§0
ğ‘§1
ğ‘¦= 1
ğ¿(ğœ½) = âˆ’log(à·œy1)
Loss function
ğ¿(ğœ½) = âˆ’ylogà·œy1âˆ’(1âˆ’y)log(à·œy0)
What about 3+ classes?
AI VIETNAM
All-in-One Course
Feature
Label
#classes = 3
#features = 1
ğ‘¦âˆˆ0,1,2
1
ğ‘¥
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0
à·œy1
à·œy2
Model
y = 0 â†’L(ğ›‰) = âˆ’log(à·œy0)
y = 1 â†’L(ğ›‰) = âˆ’log(à·œy1)
y = 2 â†’L(ğ›‰) = âˆ’log(à·œy2)
How to convert into a 
single function?
30
A Suggested Function
AI VIETNAM
All-in-One Course
Feature
Label
#classes = 3
#features = 1
ğ‘¦âˆˆ0,1,2
1
ğ‘¥
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0
à·œy1
à·œy2
Model
L(ğ›‰) = âˆ’y(1âˆ’y)
âˆ’2
log(à·œy2)âˆ’y(2âˆ’y)log(à·œy1)âˆ’(1âˆ’y)( 2âˆ’y
2 )log(à·œy0)
ğ‘¦= 2
ğ‘¦= 1
ğ‘¦= 0
Ok! but awkward!!!    â€¦  and how to improve?
31
Using One-Hot Encoding
AI VIETNAM
All-in-One Course
1
ğ‘¥
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0
à·œy1
à·œy2
Model
L(ğ›‰) = âˆ’ğ‘¦2log(à·œy2)âˆ’ğ‘¦1log(à·œy1)âˆ’ğ‘¦0log(à·œy0)
Loss function
= âˆ’à·
ğ‘–
ğ‘¦ğ‘–log(à·œyi)
Feature
Label
#classes = 3
#features = 1
ğ‘¦âˆˆ0,1,2
One-hot encoding for label
ğ‘¦= 0 â†’ğ’š=
1
0
0
ğ‘¦= 1 â†’ğ’š=
0
1
0
ğ‘¦= 2 â†’ğ’š=
0
0
1
ğ’š=
ğ‘¦0
ğ‘¦1
ğ‘¦2
ğ‘¦ğ‘–âˆˆ0,1
à·
ğ‘–
ğ‘¦ğ‘–= 1
Summary
ğ‘§0 = ğ‘¥ğ‘¤0 + ğ‘0
ğ‘§1 = ğ‘¥ğ‘¤1 + ğ‘1
à·œy0 =
ğ‘’ğ‘§0
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
à·œy1 =
ğ‘’ğ‘§1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ’›= ğ‘§0
ğ‘§1 = ğ‘0 ğ‘¤0
ğ‘1 ğ‘¤1
1
ğ‘¥= ğœ½0
ğ‘‡
ğœ½1
ğ‘‡
1
ğ‘¥= ğœ½ğ‘‡ğ’™
à·œğ²= à·œy0
à·œy1 =
1
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ‘’ğ‘§0
ğ‘’ğ‘§1 =
ğ‘’ğ’›
Ïƒğ‘—=0
1
ğ‘’ğ‘§ğ‘—
ğ¿ğœ½= âˆ’à·
ğ‘–=0
1
ğ‘¦ğ‘–logà·œğ‘¦ğ‘–= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š
1
ğ‘¤1
à·œy0
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1
Model
ğ‘§0
ğ‘§1
Derivative
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•à·œğ‘¦ğ‘–
ğœ•ğ‘§ğ‘—
= àµà·œğ‘¦ğ‘–1 âˆ’à·œğ‘¦ğ‘– ğ‘–ğ‘“ ğ‘–= ğ‘—
âˆ’à·œğ‘¦ğ‘–à·œğ‘¦ğ‘— 
ğ‘–ğ‘“ ğ‘–â‰ ğ‘— 
ğœ•ğ¿
ğœ•à·œğ‘¦ğ‘–
= âˆ’ğ‘¦ğ‘–
à·œğ‘¦ğ‘–
One-hot encoding for label
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
vector
scalar
ğ’™= 1
ğ‘¥
ğœ½=
ğ‘0 ğ‘1
ğ‘¤0 ğ‘¤1
â¢Motivation
â¢Model Construction
â¢Loss Function
â¢Generalization (Further Reading)
â¢Another Approach  (Further Reading) 
Outline
Softmax Regression â€“ NaÃ¯ve
Model
Label
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
ğ‘¤0
ğ‘0
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
ğ‘¤1
ğ‘1
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
Feature
Label
Category A
Category B
1
ğ‘¥
ğ‘§0
ğ‘§1
Softmax
à·œy0 = ğ‘ƒğ‘™ğ‘ğ‘ğ‘’ğ‘™= 0|ğ‘¥
à·œy1 = ğ‘ƒğ‘™ğ‘ğ‘ğ‘’ğ‘™= 1|ğ‘¥
Model
Training 
data
One-hot 
encoding 
for labels
ğ‘¦= 0 â†’ğ’šğ‘‡= [1, 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0, 1]
0 1
index
AI VIETNAM
All-in-One Course
34
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
Model
Label
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
ğ‘¤0
ğ‘0
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
ğ‘¤1
ğ‘1
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¥= 1.4
ğ’š= 1
0
Feature
Label
Training data
One-hot encoding for label
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
#class=2
#feature=1
ğ‘¥, ğ‘¦= 1.4, 0
Training example
35
Softmax Regression NaÃ¯ve
Feature
Label
Training data
#class=2
#feature=1
One-hot encoding for label
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
ğ‘¥, ğ‘¦= 1.4, 0
Training example
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.2
0.1
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.1
0.05
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ’™= 1.4
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ’›0 = 0.38
ğ’›1 = âˆ’0.09
à·ğ’š0 = 0.615
à·ğ’š1 = 0.385
ğ¿= âˆ’log0.615 = 0.486
ğ’š= 1
0
36
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
ğœ•ğ¿
ğœ•ğ’›0
= à·œğ‘¦0 âˆ’1
= 0.615 âˆ’1 = âˆ’0.385
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
Derivative
ğœ•ğ¿
ğœ•ğ’›1
= à·œğ‘¦1 âˆ’0 = 0.385
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.2
0.1
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.1
0.05
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğœ•ğ‘³
ğœ•ğ’›0
= âˆ’0.385
ğœ•ğ‘³
ğœ•ğ’›1
= 0.385
ğ’™= 1.4
à·ğ’š0 = 0.615
à·ğ’š1 = 0.385
ğ¿= âˆ’log0.615 = 0.486
ğ’š= 1
0
37
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
ğœ•ğ‘³
ğœ•ğ‘0
=
à·œğ‘¦0 âˆ’1 = âˆ’0.385
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
Derivative
ğœ•ğ‘³
ğœ•ğ‘1
=
à·œğ‘¦1 âˆ’0 = 0.385
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.2
0.1
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.1
0.05
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğœ•ğ‘³
ğœ•ğ‘0
= âˆ’0.385
ğœ•ğ‘³
ğœ•ğ‘1
= 0.385
ğ’™= 1.4
à·ğ’š0 = 0.615
à·ğ’š1 = 0.385
ğ¿= âˆ’log0.615 = 0.486
ğ’š= 1
0
38
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
ğœ•ğ¿
ğœ•ğ‘§ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘¤ğ‘–
= ğ‘¥à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
ğœ•ğ¿
ğœ•ğ‘ğ‘–
= à·œğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–
Derivative
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
ğœ•ğ‘³
ğœ•ğ‘¤0
= ğ‘¥(à·œğ‘¦0 âˆ’1 )
= âˆ’0.385âˆ—1.4=âˆ’0.539
ğœ•ğ‘³
ğœ•ğ‘¤1
= ğ‘¥(à·œğ‘¦1 âˆ’0 )
= 0.385âˆ—1.4=0.539
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.2
0.1
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.1
0.05
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğœ•ğ‘³
ğœ•ğ‘¤0
= âˆ’0.539
ğœ•ğ‘³
ğœ•ğ‘¤0
= 0.539
ğœ•ğ‘³
ğœ•ğ‘0
= âˆ’0.385
ğœ•ğ‘³
ğœ•ğ‘1
= 0.385
ğ’™= 1.4
à·ğ’š0 = 0.615
à·ğ’š1 = 0.385
ğ¿= âˆ’log0.615 = 0.486
ğ’š= 1
0
39
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
Update parameters
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
â€²
ğœ‚ is learning rate
ğœ½= ğ‘0 ğ‘1
ğ‘¤0 ğ‘¤1
ğœ‚= 0.1
ğ¿ğœ½
â€² =
ğœ•ğ¿
ğœ•ğ‘0
 ğœ•ğ¿
ğœ•ğ‘1
ğœ•ğ¿
ğœ•ğ‘¤0
 
ğœ•ğ¿
ğœ•ğ‘¤1
ğœ½= 0.1 
0.05
0.2 âˆ’0.1 âˆ’0.1 âˆ’0.385 0.385
âˆ’0.539 0.539
=
0.138 
0.011
0.253 âˆ’0.153
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.253
0.138
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.153
0.011
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğœ•ğ‘³
ğœ•ğ‘¤0
= âˆ’0.539
ğœ•ğ‘³
ğœ•ğ‘¤0
= 0.539
ğœ•ğ‘³
ğœ•ğ‘0
= âˆ’0.385
ğœ•ğ‘³
ğœ•ğ‘1
= 0.385
ğ’™= 1.4
à·ğ’š0 = 0.615
à·ğ’š1 = 0.385
ğ¿= âˆ’log0.615 = 0.486
ğ’š= 1
0
40
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
Feature
Label
Training data
One-hot encoding for label
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ‘¦0 ğ‘¦1
ğ‘¥, ğ‘¦= 1.4, 0
Training example
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.253
0.138
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.153
0.011
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğ’™= 1.4
à·ğ’š0 = 0.667
à·ğ’š1 = 0.332
ğ¿= âˆ’log0.667 = 0.403
ğ’›0 = 0.49
ğ’›1 = âˆ’0.20
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
= à·œğ‘¦ğ‘œâˆ’ğ‘¦ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
= à·œğ‘¦1 âˆ’ğ‘¦1
ğ’š= 1
0
ğ’›0 = 0.38
ğ’›1 = âˆ’0.09
à·ğ’š0 = 0.615
à·ğ’š1 = 0.385
ğ¿= âˆ’log0.615 = 0.486
losses reduce!!!
41
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
ğ‘¥
L = âˆ’ğ‘¦0logà·œğ‘¦0 âˆ’ğ‘¦1logà·œğ‘¦1
ğ‘§0 = ğ‘¤0ğ‘¥+ ğ‘0
0.253
0.138
ğ‘¦
ğ‘§1 = ğ‘¤1ğ‘¥+ ğ‘1
âˆ’0.153
0.011
à·œğ‘¦0 =
ğ‘’ğ‘§0
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
à·œğ‘¦1 =
ğ‘’ğ‘§1
Ïƒğ‘–=0
1
ğ‘’ğ‘§ğ‘–
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
contribute
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
= à·œğ‘¦ğ‘œâˆ’ğ‘¦ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
= à·œğ‘¦1 âˆ’ğ‘¦1
ğœ•ğ¿
ğœ•ğ‘¤0
= ğ‘¥à·œğ‘¦0 âˆ’ğ‘¦0
ğœ•ğ¿
ğœ•ğ‘0
= à·œğ‘¦0 âˆ’ğ‘¦0
ğœ•ğ¿
ğœ•ğ‘¤1
= ğ‘¥à·œğ‘¦1 âˆ’ğ‘¦1
ğœ•ğ¿
ğœ•ğ‘1
= à·œğ‘¦1 âˆ’ğ‘¦1
42
Softmax Regression - NaÃ¯ve
AI VIETNAM
All-in-One Course
ğ’™= 1
ğ‘¥
ğ¿ğœ½
â€² =
ğœ•ğ¿
ğœ•ğ‘0
 
ğœ•ğ¿
ğœ•ğ‘1
ğœ•ğ¿
ğœ•ğ‘¤0
 
ğœ•ğ¿
ğ‘¤1
=
1 ğœ•ğ¿
ğœ•ğ‘§0
 
1 ğœ•ğ¿
ğœ•ğ‘§1
ğ‘¥ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
 
ğ‘¥ğœ•ğ¿
ğœ•ğ‘§1
ğ¿ğœ½
â€² =
ğœ•ğ¿
ğœ•ğ‘0
 
ğœ•ğ¿
ğœ•ğ‘1
ğœ•ğ¿
ğœ•ğ‘¤0
 
ğœ•ğ¿
ğ‘¤1
=
1 ğœ•ğ¿
ğœ•ğ‘§0
 
1 ğœ•ğ¿
ğœ•ğ‘§1
ğ‘¥ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
 
ğ‘¥ğœ•ğ¿
ğœ•ğ‘§1
ğ¿ğ’›â€² =
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
ğ›»ğœ½ğ¿= ğ’™à·œğ²âˆ’ğ’šğ‘‡
1
ğ‘¤1
à·œy0
ğ‘¥
Softmax
function
ğ‘¤0
ğ‘0
ğ‘¤1
ğ‘1
à·œy1
ğ‘§0
ğ‘§1
ğœ½= ğœ½ğ‘œ 
ğœ½1 =
ğ‘0 ğ‘1
ğ‘¤0 ğ‘¤1
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
= à·œğ‘¦ğ‘œâˆ’ğ‘¦ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
= à·œğ‘¦1 âˆ’ğ‘¦1
ğœ•ğ¿
ğœ•ğ‘¤0
= ğ‘¥à·œğ‘¦0 âˆ’ğ‘¦0
ğœ•ğ¿
ğœ•ğ‘0
= à·œğ‘¦0 âˆ’ğ‘¦0
ğœ•ğ¿
ğœ•ğ‘¤1
= ğ‘¥à·œğ‘¦1 âˆ’ğ‘¦1
ğœ•ğ¿
ğœ•ğ‘1
= à·œğ‘¦1 âˆ’ğ‘¦1
43
Softmax Regression - Vectorization
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
= à·œğ‘¦ğ‘œâˆ’ğ‘¦ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
= à·œğ‘¦1 âˆ’ğ‘¦1
ğœ•ğ¿
ğœ•ğ‘¤0
= ğ‘¥ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
ğœ•ğ¿
ğœ•ğ‘0
= ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
ğœ•ğ¿
ğœ•ğ‘¤1
= ğ‘¥ğœ•ğ¿
ğœ•ğ‘§1
ğœ•ğ¿
ğœ•ğ‘1
= ğœ•ğ¿
ğœ•ğ‘§1
ğ’™= ğ‘¥0
ğ‘¥1
ğ¿ğœ½
â€² =
ğœ•ğ¿
ğœ•ğ‘¤00
 
ğœ•ğ¿
ğœ•ğ‘¤10
ğœ•ğ¿
ğœ•ğ‘¤01
 
ğœ•ğ¿
ğœ•ğ‘¤11
=
ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§0
 ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§1
ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
 ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§1
ğ¿ğœ½
â€² =
ğœ•ğ¿
ğœ•ğ‘¤00
 
ğœ•ğ¿
ğœ•ğ‘¤10
ğœ•ğ¿
ğœ•ğ‘¤01
 
ğœ•ğ¿
ğœ•ğ‘¤11
=
ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§0
 ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§1
ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
 ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§1
ğ¿ğ’›â€² =
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
ğœ•ğ¿
ğœ•ğ‘§1
ğ›»ğœ½ğ¿= ğ’™à·œğ²âˆ’ğ’šğ‘‡
1
ğ‘¤1
à·œy0
ğ‘¥
Softmax
function
ğ‘¤01
ğ‘¤00
ğ‘¤11
ğ‘¤10
à·œy1
ğ‘§0
ğ‘§1
ğœ½= ğœ½ğ‘œ 
ğœ½1 = ğ‘¤00 ğ‘¤10
ğ‘¤01 ğ‘¤11
44
Softmax Regression - Vectorization
4) TÃ­nh Ä‘áº¡o hÃ m
5) Cáº­p nháº­t tham sá»‘
1) Pick a sample from training data   
2) TÃ­nh output à·œğ‘¦
3) TÃ­nh loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š
ğ›»ğœ½ğ¿= ğ’™à·œğ²âˆ’ğ’šğ‘‡
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
â€²
ğœ‚is learning rate
ğ’›= ğœ½ğ‘‡ğ’™
à·ğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is Hadamard
division
ğœ½= ğœ½ğ‘œ 
ğœ½1 = ğ‘¤00 ğ‘¤10
ğ‘¤01 ğ‘¤11
ğ’™= ğ‘¥0
ğ‘¥1
à·ğ’š= à·œğ‘¦0
à·œğ‘¦1
ğ‘¦= 0 â†’ğ’šğ‘‡= [1 0]
ğ‘¦= 1 â†’ğ’šğ‘‡= [0 1]
ğ¿ğœ½= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š= âˆ’ğ‘™ğ‘œğ‘”à·ğ’š0
ğ¿ğœ½= âˆ’ğ’šğ‘‡ğ‘™ğ‘œğ‘”à·ğ’š= âˆ’ğ‘™ğ‘œğ‘”à·ğ’š1
ğ¿ğœ½
â€² = ğ‘¥0
ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§0
 
ğœ•ğ¿
ğœ•ğ‘§1
=
ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§0
 ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§1
ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
 ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§1
ğœ½= ğ‘¤00 ğ‘¤10
ğ‘¤01 ğ‘¤11 âˆ’ğœ‚
ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§0
 ğ‘¥0
ğœ•ğ¿
ğœ•ğ‘§1
ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§ğ‘œ
 ğ‘¥1
ğœ•ğ¿
ğœ•ğ‘§1
à·ğ’š=
ğ‘’ğ’›
Ïƒğ‘—ğ‘’ğ‘¥ğ‘—
45
â¢Motivation
â¢Model Construction
â¢Loss Function
â¢Generalization (Further Reading)
â¢Another Approach  (Further Reading) 
Outline
Softmax Regression - Batch
AI VIETNAM
All-in-One Course
4) TÃ­nh Ä‘áº¡o hÃ m
5) Cáº­p nháº­t tham sá»‘
1) Pick N samples from training data   
2) TÃ­nh output à·œğ‘¦
3) TÃ­nh loss (cross-entropy)
ğ¿ğœ½= ğŸ(âˆ’ğ’šâ¨€ğ‘™ğ‘œğ‘”à·ğ’šğŸ)
ğ¿ğœ½
â€² = ğ’™ğ‘‡à·œğ²âˆ’ğ’š
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
â€²
ğ‘
ğœ‚is learning rate
ğ’›= ğ’™ğœ½
à·ğ’š= (ğŸâˆ…ğ’…)ğ‘’ğ’›
ğ’…= ğ‘’ğ’›ğŸ
âˆ…is Hadamard
division
1
ğ‘§0
ğ‘§1
ğ‘§2
Softmax
à·œy0 = ğ‘ƒğ‘¦= 0
à·œy1 = ğ‘ƒğ‘¦= 1
à·œy2 = ğ‘ƒğ‘¦= 2
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
#class = 3
#feature = 4
ğœ½= ğœ½0 ğœ½1 ğœ½2
=
ğ‘¤00 ğ‘¤10 ğ‘¤20
ğ‘¤01 ğ‘¤11 ğ‘¤21
ğ‘¤02 ğ‘¤12 ğ‘¤22
ğ‘¤03 ğ‘¤13 ğ‘¤23
ğ‘¤04 ğ‘¤14 ğ‘¤24
ğ’™= ğ‘¥0
(1) ğ‘¥1
(1) ğ‘¥2
(1) ğ‘¥3
1  ğ‘¥4
(1)
ğ‘¥0
(2) ğ‘¥1
(2) ğ‘¥2
(2) ğ‘¥3
2  ğ‘¥4
(2)
ğ’š= 1 
0 
0
0 
1 
0
Friday - Pytorch
AI VIETNAM
All-in-One Course
