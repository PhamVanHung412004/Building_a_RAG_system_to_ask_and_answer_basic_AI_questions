Introduction to KNN
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
Machine Learning
ï¶Definition
AI VIETNAM
All-in-One 2023
1
Machine Learning
ï¶Supervised learning
ï¶Data
AI VIETNAM
All-in-One 2023
Input and output 
data are provided
Training data
Cats
Dogs
From Cat-Dog dataset
2
Machine Learning
ï¶Supervised learning
ï¶Data
AI VIETNAM
All-in-One 2023
Machine 
learning model
Training 
data
Used to teach
Training phase
Machine 
learning modelThis is a catThis is a dog
Testing phase
Testing data (â‰ training data)
Machine 
learning model
Make decision
Cat or Dog?
From Cat-Dog dataset
3
K-Nearest Neighbors
AI VIETNAM
All-in-One Course
Overview
From TA ThÃ¡i
4
K-Nearest Neighbors
ï¶Procedure
5
1. Initialize the value of k
2. Iterate from 1 to total number of training
data points. Calculate the distance between test
data and each row of training dataset.
3. Sort the calculated distances in ascending
order based on distance values
4. Get top k rows from the sorted array
5. Get the most frequent class of these rows
6. Return the predicted class
Data processing 
and select K
Compute distances
Sort distances
Get top K data 
points
Vote and return 
majority
K-NN 
Algorithm
Prepare data 
and select K
Compute distances 
between a testing 
point and points in 
training data
Take the K nearest 
neighbors
Voting
Output
category 1
category 2
Training data
category 1
Test data
category 2
?
?
6
KNN
ï¶Example
AI VIETNAM
All-in-One Course
7
New input data
x_test = 2.4
k=1
ïƒ y_test = 1
k=3
ïƒ y_test = 0
KNN
ï¶Example
AI VIETNAM
All-in-One Course
9
New input data
x_test = (2.4, 0.8)
K = 1
K = 3
11
Example (1)
Unnormalized 
2D data
Example (2)
Unnormalize
d 2D data
12
Training Data 1
Training Data 2
Data normalization
ğ‘‘=
ğ‘¥1
ğ‘¡ğ‘’ğ‘ ğ‘¡âˆ’ğ‘¥1
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›2 + ğ‘¥2
ğ‘¡ğ‘’ğ‘ ğ‘¡âˆ’ğ‘¥2
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘¥1
ğ‘¥2
ğ‘‘
ğ‘¥1
ğ‘¥2
ğ‘‘
ğ‘¥= ğ‘¥âˆ’Ò§ğ‘¥
ğœ
https://www.quora.com/Why-is-the-mean-of-normal-distribution-equal-to-zero
13
Example (3)
normalized 
2D data
KNN
ï¶Implementation
15
Text classification with KNN
Vectorization with Bag of Words
Text Representation
â–Bag of words
doc1 = â€œdeep learning bookâ€ 
doc2 = â€œmachine learning algorithmâ€
doc3 = â€œlearning ai from scratchâ€ 
doc4 = â€œai vietnamâ€ 
Corpus
[â€˜deepâ€™, â€˜learningâ€™, â€˜bookâ€™] 
[â€˜machineâ€™, â€˜learningâ€™, â€˜algorithmâ€™]
[â€˜learningâ€™, â€˜aiâ€™, â€˜fromâ€™, â€˜scratchâ€™] 
[â€˜aiâ€™, â€˜vietnamâ€™] 
Tokenization
Vocabulary = 
deep
learning
book
machine
algorithm
ai
from
scratch
vietnam
ğŸ‘‰Given a string = â€œvietnam machine learning deep learning bookâ€
deep
learning
book
machine
algorithm
ai
from
scratch
vietnam
BoW
1
2
1
1
0
0
0
0
1
Binary BoW
1
1
1
1
0
0
0
0
1
AI VIETNAM
All-in-One 2023
17
Doc
Label
gÃ³p giÃ³ gáº·t bÃ£o
1
cÃ³ lÃ m má»›i cÃ³ Äƒn
1
Ä‘áº¥t lÃ nh chim Ä‘áº­u
1
Äƒn chÃ¡o Ä‘Ã¡ bÃ¡t
0
gáº­y Ã´ng Ä‘áº­p lÆ°ng Ã´ng
0
qua cáº§u rÃºt vÃ¡n
0
Training data
Test data
negative (0)
positive (1)
?
?
doc_0
doc_1
doc_2
doc_3
doc_4
doc_5
bÃ¡t
0
0
0
1
0
0
bÃ£o
1
0
0
0
0
0
chim
0
0
1
0
0
0
chÃ¡o
0
0
0
1
0
0
cÃ³
0
2
0
0
0
0
cáº§u
0
0
0
0
0
1
giÃ³
1
0
0
0
0
0
gÃ³p
1
0
0
0
0
0
gáº­y
0
0
0
0
1
0
gáº·t
1
0
0
0
0
0
lÃ m
0
1
0
0
0
0
lÃ nh
0
0
1
0
0
0
lÆ°ng
0
0
0
0
1
0
má»›i
0
1
0
0
0
0
qua
0
0
0
0
0
1
rÃºt
0
0
0
0
0
1
vÃ¡n
0
0
0
0
0
1
Ã´ng
0
0
0
0
2
0
Äƒn
0
1
0
1
0
0
Ä‘Ã¡
0
0
0
1
0
0
Ä‘áº¥t
0
0
1
0
0
0
Ä‘áº­p
0
0
0
0
1
0
Ä‘áº­u
0
0
1
0
0
0
Tokenization
Vocabulary 
bÃ¡t
bÃ£o
chim
chÃ¡o
cÃ³
cáº§u
giÃ³
gÃ³p
gáº­y
gáº·t
lÃ m
lÃ nh
lÆ°ng
má»›i
qua
rÃºt
vÃ¡n
Ã´ng
Äƒn
Ä‘Ã¡
Ä‘áº¥t
Ä‘áº­p
Ä‘áº­u
BoW vectors
â€˜quaâ€™  â€˜cáº§uâ€™
â€˜gÃ³pâ€™  â€˜giÃ³â€™  â€˜gáº·tâ€™  â€˜bÃ£oâ€™
â€˜cÃ³â€™  â€™lÃ mâ€™  â€™má»›iâ€™  â€™cÃ³â€™
â€˜Ä‘áº¥tâ€™  â€™lÃ nhâ€™
â€™chimâ€™  â€˜Ä‘áº­uâ€™
â€˜Äƒnâ€™  â€˜chÃ¡oâ€™  â€˜Ä‘Ã¡â€™ â€˜bÃ¡tâ€™
â€˜gáº­yâ€™  â€˜Ã´ngâ€™
â€˜Ä‘áº­pâ€™  â€˜lÆ°ngâ€™  â€˜Ã´ngâ€™
â€˜rÃºtâ€™  â€˜vÃ¡nâ€™
gáº­y Ã´ng Ä‘áº­p lÆ°ng Ã´ng
â€˜Äƒnâ€™
18
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
Compute distance 
between test vector 
and training vectors
Select K 
(K=3)
Output
Take the K 
nearest neighbors 
and Voting
Training 
vectors
Doc
Label
Distance
gÃ³p giÃ³ gáº·t bÃ£o
1
2.645
cÃ³ lÃ m má»›i cÃ³ Äƒn
1
2.449
Ä‘áº¥t lÃ nh chim Ä‘áº­u
1
2.236
Äƒn chÃ¡o Ä‘Ã¡ bÃ¡t
0
2.236
gáº­y Ã´ng Ä‘áº­p lÆ°ng Ã´ng
0
3.162
qua cáº§u rÃºt vÃ¡n
0
2.645
positive
negative
test point
positive
negative
test point
positive
negative
positive
negative
test point
KhÃ´ng lÃ m cáº¡p 
Ä‘áº¥t mÃ  Äƒn
Test text
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
1
0
0
Vocab
Transform
19
Text classification with KNN
TF-IDF vectorizer (extension)
Doc
Label
gÃ³p giÃ³ gáº·t bÃ£o
0
cÃ³ lÃ m má»›i cÃ³ Äƒn
0
Ä‘áº¥t lÃ nh chim Ä‘áº­u
0
Äƒn chÃ¡o Ä‘Ã¡ bÃ¡t
1
gáº­y Ã´ng Ä‘áº­p lÆ°ng Ã´ng
1
qua cáº§u rÃºt vÃ¡n
1
doc_0
doc_1
doc_2
doc_3
doc_4
doc_5
bÃ¡t
0
0
0
1
0
0
bÃ£o
1
0
0
0
0
0
chim
0
0
1
0
0
0
chÃ¡o
0
0
0
1
0
0
cÃ³
0
2
0
0
0
0
cáº§u
0
0
0
0
0
1
giÃ³
1
0
0
0
0
0
gÃ³p
1
0
0
0
0
0
gáº­y
0
0
0
0
1
0
gáº·t
1
0
0
0
0
0
lÃ m
0
1
0
0
0
0
lÃ nh
0
0
1
0
0
0
lÆ°ng
0
0
0
0
1
0
má»›i
0
1
0
0
0
0
qua
0
0
0
0
0
1
rÃºt
0
0
0
0
0
1
vÃ¡n
0
0
0
0
0
1
Ã´ng
0
0
0
0
2
0
Äƒn
0
1
0
1
0
0
Ä‘Ã¡
0
0
0
1
0
0
Ä‘áº¥t
0
0
1
0
0
0
Ä‘áº­p
0
0
0
0
1
0
Ä‘áº­u
0
0
1
0
0
0
Training data
Test data
negative (0)
positive (1)
?
?
Doc-term matrix
ğ¼ğ·ğ¹ğ‘¡= ğ‘™ğ‘œğ‘”
ğ‘+ 1
ğ·ğ¹ğ‘¡+ 1 + 1
Smothing
IDF vector
2.25
2.25
2.25
2.25
1.84
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
1.84
1.84
2.25
2.25
2.25
2.25
ğ‘™ğ‘œğ‘”6 + 1
1 + 1 + 1
ğ‘™ğ‘œğ‘”6 + 1
2 + 1 + 1
Clean 
data
Build 
Doc-Term 
matrix
Compute 
IDF 
vector
N = number of documents
20
doc_0
doc_1
doc_2
doc_3
doc_4
doc_5
bÃ¡t
0
0
0
1
0
0
bÃ£o
1
0
0
0
0
0
chim
0
0
1
0
0
0
chÃ¡o
0
0
0
1
0
0
cÃ³
0
2
0
0
0
0
cáº§u
0
0
0
0
0
1
giÃ³
1
0
0
0
0
0
gÃ³p
1
0
0
0
0
0
gáº­y
0
0
0
0
1
0
gáº·t
1
0
0
0
0
0
lÃ m
0
1
0
0
0
0
lÃ nh
0
0
1
0
0
0
lÆ°ng
0
0
0
0
1
0
má»›i
0
1
0
0
0
0
qua
0
0
0
0
0
1
rÃºt
0
0
0
0
0
1
vÃ¡n
0
0
0
0
0
1
Ã´ng
0
0
0
0
2
0
Äƒn
0
1
0
1
0
0
Ä‘Ã¡
0
0
0
1
0
0
Ä‘áº¥t
0
0
1
0
0
0
Ä‘áº­p
0
0
0
0
1
0
Ä‘áº­u
0
0
1
0
0
0
Doc-term matrix
ğ‘‡ğ¹(ğ‘¡,ğ‘‘) = ğ‘™ğ‘œğ‘”ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘¡, ğ‘‘+ 1
doc_0
doc_1
doc_2
doc_3
doc_4
doc_5
0.0
0
0
0.69
0
0
0.69
0
0
0
0
0
0
0
0.69
0
0
0
0
0
0
0.69
0
0
0
1.09
0
0
0
0
0
0
0
0
0
0.69
0.69
0
0
0
0
0
0.69
0
0
0
0
0
0
0
0
0
0.69
0
0.69
0
0
0
0
0
0
0.69
0
0
0
0
0
0
0.69
0
0
0
0
0
0
0
0.69
0
0
0.69
0
0
0
0
0
0
0
0
0
0.69
0
0
0
0
0
0.69
0
0
0
0
0
0.69
0
0
0
0
1.09
0
0
0.69
0
0.69
0
0
0
0
0
0.69
0
0
0
0
0.69
0
0
0
0
0
0
0
0.69
0
0
0
0.69
0
0
0
TF matrix
ğ‘™ğ‘œğ‘”0 + 1
ğ‘™ğ‘œğ‘”1 + 1
Compute TF 
matrix
21
doc_0
doc_1
doc_2
doc_3
doc_4
doc_5
0.0
0
0
0.69
0
0
0.69
0
0
0
0
0
0
0
0.69
0
0
0
0
0
0
0.69
0
0
0
1.09
0
0
0
0
0
0
0
0
0
0.69
0.69
0
0
0
0
0
0.69
0
0
0
0
0
0
0
0
0
0.69
0
0.69
0
0
0
0
0
0
0.69
0
0
0
0
0
0
0.69
0
0
0
0
0
0
0
0.69
0
0
0.69
0
0
0
0
0
0
0
0
0
0.69
0
0
0
0
0
0.69
0
0
0
0
0
0.69
0
0
0
0
1.09
0
0
0.69
0
0.69
0
0
0
0
0
0.69
0
0
0
0
0.69
0
0
0
0
0
0
0
0.69
0
0
0
0.69
0
0
0
TF matrix
IDF vector
2.25
2.25
2.25
2.25
1.84
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
2.25
1.84
1.84
2.25
2.25
2.25
2.25
doc_0
doc_1
doc_2
doc_3
doc_4
doc_5
0.0
0
0
0.52
0
0
0.5
0
0
0
0
0
0
0
0.5
0
0
0
0
0
0
0.52
0
0
0
0.62
0
0
0
0
0
0
0
0
0
0.5
0.5
0
0
0
0
0
0.5
0
0
0
0
0
0
0
0
0
0.46
0
0.5
0
0
0
0
0
0
0.47
0
0
0
0
0
0
0.5
0
0
0
0
0
0
0
0.46
0
0
0.47
0
0
0
0
0
0
0
0
0
0.5
0
0
0
0
0
0.5
0
0
0
0
0
0.5
0
0
0
0
0.6
0
0
0.39
0
0.42
0
0
0
0
0
0.52
0
0
0
0
0.5
0
0
0
0
0
0
0
0.46
0
0
0
0.5
0
0
0
0
0
0
0
2.02
0
0
0
0
0
1.56
0
0
1.56
0
0
0
0
1.28
0
0
0
0
=
=
=
ğ¿2_ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘£) =
ğ‘£
ğ‘£2
2.02
(2.022 + 1.562 + 1.562 + 1.282)
1.56
(2.022 + 1.562 + 1.562 + 1.282)
TF-IDF Matrix
Compute and 
normalize TF-IDF 
vectors
ğ‘‡ğ¹ğ¼ğ·ğ¹(ğ‘¡,ğ‘‘) = ğ‘‡ğ¹(ğ‘¡,ğ‘‘) Ã— ğ¼ğ·ğ¹ğ‘¡
x
x
x
22
KhÃ´ng lÃ m cáº¡p 
Ä‘áº¥t mÃ  Äƒn
Test text
Compute 
TF
Compute 
TF-IDF
Normalize
0
0
0
0
0
0
0
0
0
0
0.61
0
0
0
0
0
0
0
0.5
0
0.61
0
0
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
positive
negative
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
Compute distance 
between test vector 
and training vectors
Select K 
(K=3)
Output
Take the K 
nearest neighbors 
and Voting
Test vector
test point
positive
negative
test point
Training 
vectors
positive
negative
test point
positive
negative
Doc
Label
Distance
gÃ³p giÃ³ gáº·t bÃ£o
1
1.41
cÃ³ lÃ m má»›i cÃ³ Äƒn
1
1.01
Ä‘áº¥t lÃ nh chim Ä‘áº­u
1
1.17
Äƒn chÃ¡o Ä‘Ã¡ bÃ¡t
0
1.25
gáº­y Ã´ng Ä‘áº­p lÆ°ng Ã´ng
0
1.41
qua cáº§u rÃºt vÃ¡n
0
1.41
23
Entropy
ï¶Motivation
AI VIETNAM
All-in-One 2023
p A = 9
10 = 0.9
A: Get a red ball
B: Get a blue ball
p B = 1
10 = 0.1
Experiment 1
Got a red ball 
E: Pick a ball from the basket
Experiment 2
Got a blue ball 
Which experiment makes you more surprised?
How to measure 
the surprises?
ğ‘†ğ‘¢ğ‘Ÿğ‘ğ‘Ÿğ‘–ğ‘ ğ‘’(ğ¸) =
1
ğ‘(ğ¸)
Observation
ğ‘†ğ‘¢ğ‘Ÿğ‘ğ‘Ÿğ‘–ğ‘ ğ‘’(ğ¸)
ğ‘(ğ¸)
Problem?
Monotonic decrease of the function surprise(E) 
ğ‘™ğ‘œğ‘”ğ‘†ğ‘¢ğ‘Ÿğ‘ğ‘Ÿğ‘–ğ‘ ğ‘’(ğ¸) = ğ‘™ğ‘œğ‘”
1
ğ‘(ğ¸)
Information(ğ‘¥) = âˆ’ğ‘™ğ‘œğ‘”ğ‘(ğ‘¥)
1
ğ‘(ğ¸)
âˆ’ğ‘™ğ‘œğ‘”ğ‘(ğ¸)
âˆ’ğ‘™ğ‘œğ‘”ğ‘(ğ¸)
In information theory
= âˆ’ğ‘™ğ‘œğ‘”ğ‘(ğ¸)
25
H ğ‘‹âˆ¶= âˆ’à·
ğ‘¥âˆˆğ‘‹
ğ‘(ğ‘¥) ğ‘™ğ‘œğ‘”ğ‘(ğ‘¥)
Entropy: Average of information
p X = 0 = 9
10 = 0.9
p X = 1 = 1
10 = 0.1
H ğ‘‹= âˆ’à·
ğ‘¥âˆˆğ‘‹
ğ‘(ğ‘¥) ğ‘™ğ‘œğ‘”ğ‘(ğ‘¥)
p X = 0 = 5
10 = 0.5
p X = 1 = 5
10 = 0.5
H ğ‘‹= âˆ’à·
ğ‘¥âˆˆğ‘‹
ğ‘(ğ‘¥) ğ‘™ğ‘œğ‘”ğ‘(ğ‘¥)
= âˆ’0.9ğ‘™ğ‘œğ‘”0.9 âˆ’0.1ğ‘™ğ‘œğ‘”0.1
= 0.468
= âˆ’0.5ğ‘™ğ‘œğ‘”0.5 âˆ’0.5ğ‘™ğ‘œğ‘”0.5
= 1.0
Entropy
AI VIETNAM
All-in-One 2023
