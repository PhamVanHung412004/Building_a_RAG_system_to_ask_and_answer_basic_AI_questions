Insight into 
Logistic Regression
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
➢Vectorization
➢Optimiztion for 1+ samples
➢Logistic Regression – Mini-batch
➢Logistic Regression – Batch
➢BCE and MSE Loss Functions
➢Sigmoid and Tanh Function (Optional)
Outline
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Implementation - One Sample
Feature
Label
if #features changes, which functions are affected?
1
demo
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
𝜕𝐿
𝜕𝑤𝑖
= 𝑥𝑖(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤𝑖= 𝑤𝑖−𝜂𝜕𝐿
𝜕𝑤𝑖
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
2
demo
How to solve the problem?
Vector/Matrix Operations
Multiply with a number
𝛼𝑢= 𝛼
𝑢1
…
𝑢𝑛
=
𝛼𝑢1
…
𝛼𝑢𝑛
=
data 
1
2
3
2
*
result 
2
4
6
1
3
2
4
T
1
2
3
4
A =
𝑎11 …  𝑎1𝑛
… … . . .
𝑎𝑚1 …  𝑎𝑚𝑛
A𝑇=
𝑎11 …  𝑎𝑚1
… … …
𝑎1𝑛 …  𝑎𝑚𝑛
Ԧ𝑣=
𝑣1
…
𝑣𝑛
Ԧ𝑣𝑇= 𝑣1 … 𝑣𝑛
1
2
T
1
2
Transpose
3
Vector/Matrix Operations
Dot product
Ԧ𝑣=
𝑣1
…
𝑣𝑛
𝑢=
𝑢1
…
𝑢𝑛
Ԧ𝑣∙𝑢= 𝑣1 × 𝑢1 + ⋯+ 𝑣𝑛× 𝑢𝑛
v 
1
2
2
3
w 
=
result 
8
4
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Vectorization
AI VIETNAM
All-in-One Course
𝑧= 𝑤𝑥+ 𝑏1 = 𝑏 𝑤
1
𝑥= 𝜽𝑇𝒙
dot product
Traditional
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝑤𝑥+ 𝑏
𝜽= 𝑏
𝑤
→𝜽𝑇= 𝑏 𝑤
Feature
Label
𝑥
𝑦
5
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝑤𝑥+ 𝑏
𝑧= 𝜽𝑇𝒙
𝐿ො𝑦, 𝑦= −ylogොy−(1−y)log(1−ොy )
numbers
What will we do?
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
6
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Traditional
Vectorization
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝑤𝑥+ 𝑏
ො𝑦−𝑦× 1
ො𝑦−𝑦× 𝑥 =
ො𝑦−𝑦
1 
𝑥 =
ො𝑦−𝑦𝒙=
𝜕𝐿
𝜕𝑏
𝜕𝐿
𝜕𝑤
= 𝛻𝜽𝐿
common factor
→
𝛻𝜽𝐿= 𝒙( Ƹ𝑦−𝑦)
𝜕𝐿
𝜕𝑤= 𝑥ො𝑦−𝑦=
ො𝑦−𝑦× 𝑥
𝜕𝐿
𝜕𝑏=
ො𝑦−𝑦 =
ො𝑦−𝑦× 1
7
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝜽𝑇𝒙
𝑤 =  𝑤 − 𝜂𝜕𝐿
𝜕𝑤
𝑏 =  𝑏 − 𝜂𝜕𝐿
𝜕𝑏
𝛻𝜽𝐿=
𝜕𝐿
𝜕𝑏
𝜕𝐿
𝜕𝑤
𝜽
𝜽
𝛻𝜽𝐿
→
𝜽= 𝜽−𝜂𝛻𝜽𝐿
8
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (𝐱, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝒙(ො𝑦−𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
Traditional
Vectorized
Vectorization
AI VIETNAM
All-in-One Course
𝑧= 𝑤𝑥+ 𝑏
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
9
Vectorization
AI VIETNAM
All-in-One Course
❖ Implementation (using Numpy)
# Given X and y
4) Compute derivative
5) Update parameters
1) Pick a sample (𝒙, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝒙(ො𝑦−𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
10
𝒙=
1
𝑥1
𝑥2
=
1
1.4
0.2
Given 𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
𝜂= 0.01
1
Model
Input
Label
Loss
𝒙
𝜽=
0.1
0.5
−0.1
ො𝑦= 𝜎𝜽𝑇𝒙 = 0.6856
𝐿= 1.1573
𝑦= 0
3
Dataset
4) Compute derivative
5) Update parameters
1) Pick a sample (𝒙, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝒙(ො𝑦−𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝛻𝜽𝐿= 𝒙ො𝑦−𝑦=
1
1.4
0.2
0.6856 =
0.6856
0.9599
0.1371
=
𝐿𝑏
′
𝐿𝑤1
′
𝐿𝑤2
′
4
𝛉−ηL𝛉
′ =
0.1
0.5
−0.1
−η
0.6856
0.9599
0.1371
=
0.093
0.499
−0.101
5
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (𝒙, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿𝜽= −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝐱(ොy −𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
𝑧= 𝜽𝑇𝒙
Demo
AI VIETNAM
All-in-One Course
𝒙 =
1
1.4
0.2
𝒚= 0
Dataset
12
➢Vectorization
➢Optimiztion for 1+ samples
➢Logistic Regression – Mini-batch
➢Logistic Regression – Batch
➢BCE and MSE Loss Functions
➢Sigmoid and Tanh Function (Optional)
Outline
Optimization for One+  Samples 
❖ Equations for partial gradients
AI VIETNAM
All-in-One Course
𝑑𝑓
𝑑𝑎= 𝑥
𝑑𝑔
𝑑𝑎= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑎= 2𝑥𝑓−𝑦
𝑑𝑓
𝑑𝑏= 1
𝑑𝑔
𝑑𝑏= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑏= 2 𝑓−𝑦
𝑑𝑔
𝑑𝑓= 2 𝑓−𝑦
During looking for optimal a 
and b, at a given time, a and b 
have concrete values
𝑓𝑥𝑖= 𝑎𝑥𝑖+ 𝑏
𝑔𝑓𝑖= 𝑓𝑖−𝑦𝑖2
(𝑥2=2, 𝑦2=7)
(𝑥1=1, 𝑦1=5)
𝑏
𝑓𝑖
𝑑𝑓𝑖
𝑑𝑏
𝑎
𝑑𝑓𝑖
𝑑𝑎
𝑥𝑖
𝑔𝑖
𝑑𝑔𝑖
𝑑𝑓𝑖
𝑑𝑔𝑖
𝑑𝑓𝑖
𝑑𝑓𝑖
𝑑𝑎
𝑦𝑖
𝑑𝑔𝑖
𝑑𝑓𝑖
𝑑𝑓𝑖
𝑑𝑏
13
illustration
𝑏
𝑓1
𝑑𝑓1
𝑑𝑏
𝑎
𝑑𝑓1
𝑑𝑎
𝑥1
𝑔1
𝑑𝑔1
𝑑𝑓1
𝑑𝑔1
𝑑𝑓1
𝑑𝑓1
𝑑𝑎
𝑦1
𝑑𝑔1
𝑑𝑓1
𝑑𝑓1
𝑑𝑏
𝑏
𝑓2
𝑑𝑓2
𝑑𝑏
𝑎
𝑑𝑓2
𝑑𝑎
𝑥2
𝑔2
𝑑𝑔2
𝑑𝑓2
𝑑𝑔2
𝑑𝑓2
𝑑𝑓2
𝑑𝑎
𝑦2
𝑑𝑔2
𝑑𝑓2
𝑑𝑓2
𝑑𝑏
❖ Optimization for a composite function
෍
𝑖
𝑑𝑔𝑖
𝑑𝑎= 𝑑𝑔1
𝑑𝑓1
𝑑𝑓1
𝑑𝑎+ 𝑑𝑔2
𝑑𝑓2
𝑑𝑓2
𝑑𝑎
෍
𝑖
𝑑𝑔𝑖
𝑑𝑏= 𝑑𝑔1
𝑑𝑓1
𝑑𝑓1
𝑑𝑏+ 𝑑𝑔2
𝑑𝑓2
𝑑𝑓2
𝑑𝑏
Find a and b so that g(f(x)) is minimum
𝑑𝑔
𝑑𝑎= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑎= 2𝑥𝑓−𝑦
𝑑𝑔
𝑑𝑏= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑏= 2 𝑓−𝑦
Partial derivative functions
𝑓𝑥𝑖= 𝑎𝑥𝑖+ 𝑏
𝑔𝑓𝑖= 𝑓𝑖−𝑦𝑖2
(𝑥2=2,𝑦2=7)
(𝑥1=1,𝑦1=5)
Optimization
❖ How to use gradient information
AI VIETNAM
All-in-One Course
info 1
info 2
1
update at 
time t
update at 
time t+1
info 1
info 2
2
update
Compute partial 
gradient at a, b
Move a, b  
opposite to db, db
Initialize a, b
𝜂= 0.01
𝑑𝑔
𝑑𝑎= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑎= 2𝑥𝑓−𝑦
𝑑𝑔
𝑑𝑏= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑏= 2 𝑓−𝑦
info 1
info 2
1
Summary
16
Compute partial 
gradient at a, b
Move a, b  
opposite to db, db
Initialize a, b
𝜂= 0.01
𝑑𝑔
𝑑𝑎= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑎= 2𝑥𝑓−𝑦
𝑑𝑔
𝑑𝑏= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑏= 2 𝑓−𝑦
Summary
info 1
info 2
2
update
17
Compute partial 
gradient at a, b
Move a, b  
opposite to db, db
Initialize a, b
𝜂= 0.001
𝑑𝑔
𝑑𝑎= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑎= 2𝑥𝑓−𝑦
𝑑𝑔
𝑑𝑏= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑏= 2 𝑓−𝑦
Summary
info 1
info 2
2
update
18
➢Vectorization
➢Optimiztion for 1+ samples
➢Logistic Regression – Mini-batch
➢Logistic Regression – Batch
➢BCE and MSE Loss Functions
➢Sigmoid and Tanh Function (Optional)
Outline
Linear Regression (m-samples)
AI VIETNAM
All-in-One Course
𝒙 = 1 1.5 0.2
1 4.1 1.3
𝒚= 0
1
𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
❖ Construct formulas
𝒛= 𝑧(1)
𝑧(2) = 𝑤1𝑥1
(1) + 𝑤2𝑥2
(1) + 𝑏
𝑤1𝑥1
(2) + 𝑤2𝑥2
(2) + 𝑏
𝒙 = 1 1.5 0.2
1 4.1 1.3 = 1 𝑥1
(1) 𝑥2
(1)
1 𝑥1
(2) 𝑥2
(2)
𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
2) Compute output ො𝑦
Dataset
= 1 𝑥1
(1) 𝑥2
(1)
1 𝑥1
(2) 𝑥2
(2)
𝑏
𝑤1
𝑤2
= 𝒙𝜽= 0.83
2.02
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝒛= 𝜽𝑇𝒙
19
Linear Regression (m-samples)
AI VIETNAM
All-in-One Course
𝒙 = 1 1.5 0.2
1 4.1 1.3
𝒚= 0
1
𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
❖ Construct formulas
ෝ𝒚= 𝜎𝒛= ො𝑦1
ො𝑦2
=
1
1 + 𝑒−𝑧(1)
1
1 + 𝑒−𝑧(2)
=
1
1 + 𝑒−𝒛= 0.69
0.88
20
𝒙 = 1 1.5 0.2
1 4.1 1.3 = 1 𝑥1
(1) 𝑥2
(1)
1 𝑥1
(2) 𝑥2
(2)
𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
2) Compute output ො𝑦
Dataset
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝒛= 𝜽𝑇𝒙
𝒛= 𝒙𝜽= 0.83
2.02
Numpy perspective
Dataset
Linear Regression (m-samples)
AI VIETNAM
All-in-One Course
❖ Construct formulas
21
3) Compute loss
𝐿(ෝ𝒚, 𝒚) = 𝐿(1) ො𝑦(1), 𝑦(1) + 𝐿(2) ො𝑦(2), 𝑦(2)
m
𝒙 = 1 1.5 0.2
1 4.1 1.3
𝒚= 0
1
𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
𝐿(ෝ𝒚, 𝒚) = 1
m −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝐿(1) ො𝑦(1), 𝑦(1) = −𝑦(1)logො𝑦(1) −(1−𝑦(1))log(1−ො𝑦(1))
𝐿(2) ො𝑦(2), 𝑦(2) = −𝑦(2)logො𝑦(2) −(1−𝑦(2))log(1−ො𝑦(2))
+
𝐲Tlogො𝐲
(1−y)Tlog(1−ො𝐲 )
22
4) Compute derivative
𝜕𝐿(1)
𝜕𝑏
= (ො𝑦(1) −𝑦(1))
𝜕𝐿(1)
𝜕𝑤1
= 𝑥1
(1)(ො𝑦(1) −𝑦(1))
𝜕𝐿(1)
𝜕𝑤2
= 𝑥2
(1)(ො𝑦(1) −𝑦(1))
𝜕𝐿(2)
𝜕𝑏
= (ො𝑦(2) −𝑦(2))
𝜕𝐿(2)
𝜕𝑤1
= 𝑥1
(2)(ො𝑦(2) −𝑦(2))
𝜕𝐿(2)
𝜕𝑤2
= 𝑥2
(2)(ො𝑦(2) −𝑦(2))
𝜕𝐿
𝜕𝑏=
𝜕𝐿(1)
𝜕𝑏+ 𝜕𝐿(2)
𝜕𝑏
𝑚
= (ො𝑦(1) −𝑦(1)) + (ො𝑦(2) −𝑦(2))
𝑚
sample 1
sample 2
= 1
𝑚1 ∗(ො𝑦(1) −𝑦(1)) + 1 ∗(ො𝑦(2) −𝑦(2))
= 1
𝑚𝑥0
(1) 
𝑥0
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
𝜕𝐿
𝜕𝑤1
=
𝜕𝐿(1)
𝜕𝑤1 + 𝜕𝐿(2)
𝜕𝑤1
𝑚
= 𝑥1
(1)(ො𝑦(1) −𝑦(1)) + 𝑥1
(2)(ො𝑦(2) −𝑦(2))
𝑚
= 1
𝑚𝑥1
(1) 
𝑥1
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
𝜕𝐿
𝜕𝑤2
=
𝜕𝐿(1)
𝜕𝑤2 + 𝜕𝐿(2)
𝜕𝑤2
𝑚
= 𝑥2
(1)(ො𝑦(1) −𝑦(1)) + 𝑥2
(2)(ො𝑦(2) −𝑦(2))
𝑚
= 1
𝑚𝑥2
(1) 
𝑥2
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
𝑥0
(1) = 1
𝑥0
(2) = 1
23
4) Compute derivative
𝜕𝐿(1)
𝜕𝑏
= (ො𝑦(1) −𝑦(1))
𝜕𝐿(1)
𝜕𝑤1
= 𝑥1
(1)(ො𝑦(1) −𝑦(1))
𝜕𝐿(1)
𝜕𝑤2
= 𝑥2
(1)(ො𝑦(1) −𝑦(1))
𝜕𝐿(2)
𝜕𝑏
= (ො𝑦(2) −𝑦(2))
𝜕𝐿(2)
𝜕𝑤1
= 𝑥1
(2)(ො𝑦(2) −𝑦(2))
𝜕𝐿(2)
𝜕𝑤2
= 𝑥2
(2)(ො𝑦(2) −𝑦(2))
𝜕𝐿
𝜕𝑏= 1
𝑚𝑥0
(1) 
𝑥0
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
sample 1
sample 2
𝜕𝐿
𝜕𝑤1
= 1
𝑚𝑥1
(1) 
𝑥1
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
𝜕𝐿
𝜕𝑤2
= 1
𝑚𝑥2
(1) 
𝑥2
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
𝛻𝜽𝑳=
𝜕𝐿
𝜕𝑏
𝜕𝐿
𝜕𝑤1
𝜕𝐿
𝜕𝑤2
= 1
𝑚
𝑥0
(1) 
𝑥0
(2)
𝑥1
(1) 
𝑥1
(2)
𝑥2
(1) 
𝑥2
(2)
ො𝑦(1) −𝑦(1)
ො𝑦(2) −𝑦(2)
𝒙(1) 𝑇
𝒙(2) 𝑇
ො𝐲
𝐲
𝛻𝜽𝐿= 1
𝑚𝒙𝑇(ො𝐲−𝒚)
24
5) Update parameters
𝛻𝜽𝑳=
𝜕𝐿
𝜕𝑏
𝜕𝐿
𝜕𝑤1
𝜕𝐿
𝜕𝑤2
= 1
𝑚𝒙𝑇(ො𝐲−𝒚)
𝒙 = 1 1.5 0.2
1 4.1 1.3
𝒚= 0
1
𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
Dataset
𝜽
𝜽
𝛻𝜽𝑳
𝑏=  𝑏 − 𝜂𝜕𝐿
𝜕𝑏 
𝑤1 = 𝑤1 − 𝜂𝜕𝐿
𝜕𝑤1
𝑤2 = 𝑤2 − 𝜂𝜕𝐿
𝜕𝑤2
𝜽= 𝜽−𝜂𝛻𝜽𝐿
Logistic Regression - Minibatch
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
m −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
m 𝐱T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
𝒛= 𝒙𝜽
AI VIETNAM
All-in-One Course
Model
Input
Label
Loss
𝒙
𝜽=
𝑏
𝑤1
𝑤2
ෝ𝒚= 𝜎𝒙𝜽 
𝐿(𝜽) =…
𝒚
Mini-batch m=2
𝜽=
𝑏
𝑤1
𝑤2
𝒙 = 1 𝑥1
(1) 𝑥2
(1)
1 𝑥1
(2) 𝑥2
(2)
25
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
m −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
m 𝐱T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝒛= 𝒙𝜽
Model
Loss
𝒙= 1 1.5 0.2
1 4.1 1.3
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽 = 0.6963
0.8828
𝐿(𝜽) = …
𝒚= 0
1
Mini-batch m=2
Dataset
2
= 𝜎
0.83
2.02
= 0.6963
0.8828
ෝ𝒚= 𝜎𝒙𝜽= 𝜎
1 1.5 0.2
1 4.1 1.3
0.1
0.5
−0.1
26
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
m −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
m 𝐱T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝒛= 𝒙𝜽
Model
Loss
𝒙= 1 1.5 0.2
1 4.1 1.3
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽 = 0.6963
0.8828
𝐿(𝜽) = 0.65815
𝒚= 0
1
Mini-batch m=2
3
𝐿(𝜽) = 1
m −0 1
log0.6963
log0.8828 −1 0
log(1 −0.6963)
log(1 −0.8828)
= 1
m −log0.8828−log(1 −0.6963)
= 0.1246 + 1.1917
m
= 0.65815
𝑦= log(𝑥)
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
m −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
m 𝐱T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝒛= 𝒙𝜽
Model
Loss
𝒙= 1 1.5 0.2
1 4.1 1.3
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽 = 0.6963
0.8828
𝐿(𝜽) = 0.65815
𝒚= 0
1
Mini-batch m=2
Dataset
4
𝛻𝜽𝐿= 1
m
1 
1 
1.5 
4.1
0.2 
1.3
0.6963
0.8828 −0
1
= 1
m
1 
1 
1.5 
4.1
0.2 
1.3
0.6963
−0.1171 =
0.28961
0.28217
−0.0064
3
4) Compute derivative
5) Update parameters 
1) Pick m samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
m −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
m 𝐱T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝒛= 𝒙𝜽
Model
Loss
𝒙= 1 1.5 0.2
1 4.1 1.3
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽 = 0.6963
0.8828
𝐿(𝜽) = 0.65815
𝒚= 0
1
Mini-batch m=2
Dataset
4
𝛻𝜽𝐿=
0.28961
0.28217
−0.0064
𝛉−ηL𝛉
′ =
0.1
0.5
−0.1
−η
0.28961
0.28217
−0.0064
=
0.0971
0.4971
−0.099
5
3
29
➢Vectorization
➢Optimiztion for 1+ samples
➢Logistic Regression – Mini-batch
➢Logistic Regression – Batch
➢BCE and MSE Loss Functions
➢Sigmoid and Tanh Function (Optional)
Outline
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ො𝑦
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
𝑧= 𝒙𝜽
Logistic Regression - Batch
Model
Input
Label
Loss
𝒙
𝜽=
𝑏
𝑤1
𝑤2
ෝ𝒚= 𝜎𝜽𝑇𝒙 
𝐿(𝜽) =…
𝒚
𝜽=
𝑏
𝑤1
𝑤2
𝒙 =
1 𝑥1
(1) 𝑥2
(1)
1 𝑥1
(2) 𝑥2
(2)
1 𝑥1
(3) 𝑥2
(3)
1 𝑥1
(4) 𝑥2
(4)
30
Dataset
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝑧= 𝒙𝜽
𝒙 =
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
𝒚=
0
0
1
1
Model
Input
Label
Loss
𝒙
𝜽=
𝑏
𝑤1
𝑤2
ෝ𝒚= 𝜎𝒙𝜽 
𝐿(𝜽) =…
𝒚
𝜽=
𝑏
𝑤1
𝑤2
Logistic Regression - Batch
31
Model
Loss
𝒙
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽=
1
1 + 𝑒−𝒛
𝐿(𝜽) = …
𝒚
2
= 𝜎
0.78
0.83
1.49
2.02
=
0.6856
0.6963
0.8160
0.8828
ෝ𝒚= 𝜎𝒙𝜽= 𝜎
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
0.1
0.5
−0.1
Dataset
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝑧= 𝒙𝜽
𝒙 =
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
32
3
𝐿(𝜽) = 1
N
−
0
0
1
1
T
log
0.6856
0.6963
0.8160
0.8828
−
1
1
1
1
−
0
0
1
1
𝑇
log
1−
0.6856
0.6963
0.8160
0.8828
𝑦= log(𝑥)
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝑧= 𝒙𝜽
Model
Loss
𝒙
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽=
1
1 + 𝑒−𝒛
𝐿(𝜽) = …
𝒚
𝒚=
0
0
1
1
ෝ𝒚=
0.6856
0.6963
0.8160
0.8828
33
3
𝐿(𝜽) = 1
N
−
0
0
1
1
T
log
0.6856
0.6963
0.8160
0.8828
−
1
1
0
0
𝑇
log
0.3144
0.3037
0.1840
0.1172
= 1
N −log0.8160−log0.8828 −log0.3144−log0.3037
𝑦= log(𝑥)
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝑧= 𝒙𝜽
= 0.6691
Model
Loss
𝒙
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽=
1
1 + 𝑒−𝒛
𝐿(𝜽) = 0.6691
𝒚
𝒚=
0
0
1
1
ෝ𝒚=
0.6856
0.6963
0.8160
0.8828
34
4
𝛻𝜽𝐿= 1
N
1 
1 
1 
1 
1.4 1.5 3.0 4.1
0.2 0.2 1.1 1.3
0.6856
0.6963
0.8160
0.8828
−
0
0
1
1
= 1
N
1 
1 
1 
1 
1.4 1.5 3.0 4.1
0.2 0.2 1.1 1.3
0.6856
0.6963
−0.184
−0.117
=
0.2702
0.2431
−0.019
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝑧= 𝒙𝜽
𝒙 =
1 1.4 0.2
1 1.5 0.2
1 3.0 1.1
1 4.1 1.3
ෝ𝒚=
0.6856
0.6963
0.8160
0.8828
Model
Loss
𝒙
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽=
1
1 + 𝑒−𝒛
𝐿(𝜽) = 0.6691
𝒚
𝒚=
0
0
1
1
3
Model
Loss
𝜽=
0.1
0.5
−0.1
ෝ𝒚= 𝜎𝒙𝜽=
1
1 + 𝑒−𝒛
𝐿(𝜽) = 0.6691
4
𝛻𝜽𝐿=
0.2702
0.2431
−0.019
𝛉−ηL𝛉
′ =
0.1
0.5
−0.1
−η
0.2702
0.2431
−0.019
=
0.0971
0.4971
−0.099
5
3
Dataset
4) Compute derivative
5) Update parameters 
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(ො𝐲, 𝒚) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝑧= 𝒙𝜽
𝒙
𝒚
➢Vectorization
➢Optimiztion for 1+ samples
➢Logistic Regression – Mini-batch
➢Logistic Regression – Batch
➢BCE and MSE Loss Functions
➢Sigmoid and Tanh Function (Optional)
Outline
Hessian Matrices
❖ Definition
AI VIETNAM
All-in-One Course
The Hessian matrix or Hessian is a
square matrix of second-order partial
derivatives of a scalar-valued function
https://en.wikipedia.org/wiki/Hessian_matrix
Given 𝑓(𝑥, 𝑦)
𝑓: 𝑅2 →𝑅
𝐻𝑓=
𝜕2𝑓
𝜕𝑥2 
𝜕2𝑓
𝜕𝑥𝜕𝑦
𝜕2𝑓
𝜕𝑥𝜕𝑦 
𝜕2𝑓
𝜕𝑦2
Given 𝑓𝑥, 𝑦= 𝑥2 + 2𝑥2𝑦+ 𝑦3
𝐻𝑓= 2 + 4𝑦 
4𝑥
4𝑥 
6𝑦
𝜕𝑓
𝜕𝑥= 2𝑥+ 4𝑥𝑦
𝜕𝑓
𝜕𝑦= 2𝑥2 + 3𝑦2
37
Binary Cross-entropy
❖ Convex function
L = −𝑦log ො𝑦−(1 −𝑦)log 1 −ො𝑦
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝜽𝑇𝒙
Model and Loss
𝜕𝐿
𝜕𝜃𝑖
= 𝜕𝐿
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃𝑖
𝜕ො𝑦
𝜕𝑧= ො𝑦(1 −ො𝑦)
𝜕𝑧
𝜕𝜃𝑖
= 𝑥𝑖
Derivative
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖(ො𝑦−𝑦)
𝜕𝐿
𝜕ො𝑦= −𝑦
ො𝑦+ 1 −𝑦
1 −ො𝑦=
ො𝑦−𝑦
ො𝑦(1 −ො𝑦)
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖(ො𝑦−𝑦)
𝜕2𝐿
𝜕𝜃𝑖
2 = 𝜕
𝜕𝜃𝑖
𝑥𝑖(ො𝑦−𝑦) = 𝑥𝑖
2(ො𝑦−ො𝑦2) ≥0
𝑥𝑖
2 ≥0
ො𝑦−ො𝑦2 ∈0, 1
4
Logistic Regression-MSE
L = (ො𝑦−𝑦)2
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
Model and Loss
𝜕𝐿
𝜕𝜃𝑖
= 2𝑥𝑖(ො𝑦−𝑦)ො𝑦(1 −ො𝑦)
𝜕𝐿
𝜕𝜃𝑖
= 𝜕𝐿
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃𝑖
𝜕ො𝑦
𝜕𝑧= ො𝑦(1 −ො𝑦)
𝜕𝑧
𝜕𝜃𝑖
= 𝑥𝑖
Derivative
𝜕𝐿
𝜕ො𝑦= 2(ො𝑦−𝑦)
❖ Construct loss
AI VIETNAM
All-in-One Course
39
Mean Squared Error
L = (ො𝑦−𝑦)2
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
Model and Loss
𝜕𝐿
𝜕𝜃𝑖
= 2𝑥𝑖(ො𝑦−𝑦)ො𝑦(1 −ො𝑦)
𝜕𝐿
𝜕𝜃𝑖
= 𝜕𝐿
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃𝑖
𝜕ො𝑦
𝜕𝑧= ො𝑦(1 −ො𝑦)
𝜕𝑧
𝜕𝜃𝑖
= 𝑥𝑖
Derivative
𝜕𝐿
𝜕ො𝑦= 2(ො𝑦−𝑦)
𝜕𝐿
𝜕𝜃𝑖= 2𝑥𝑖(ො𝑦−𝑦)ො𝑦(1 −ො𝑦) = 2𝑥𝑖−ො𝑦3 + ො𝑦2 −𝑦ො𝑦+ 𝑦ො𝑦2
𝜕2𝐿
𝜕𝜃𝑖
2 = 𝜕
𝜕𝜃𝑖
2𝑥𝑖−ො𝑦3 + ො𝑦2 −𝑦ො𝑦+ 𝑦ො𝑦2
= 2𝑥𝑖−3ො𝑦2𝑥𝑖ො𝑦1 −ො𝑦+ 2𝑥𝑖ො𝑦ො𝑦1 −ො𝑦−𝑦𝑥𝑖ො𝑦(1 −ො𝑦) + 2𝑥𝑖𝑦ො𝑦ො𝑦1 −ො𝑦
= 2𝑥𝑖
2ො𝑦(1 −ො𝑦) −3ො𝑦2 + 2ො𝑦−𝑦+ 2𝑦ො𝑦
40
Mean Squared Error
𝜕2𝐿
𝜕𝜃𝑖
2 = 2𝑥𝑖
2 ො𝑦(1 −ො𝑦) −3ො𝑦2 + 2ො𝑦−𝑦+ 2𝑦ො𝑦
𝑥𝑖
2 ≥0
ො𝑦(1 −ො𝑦) ∈0, 1
4
𝑦= 0
𝑓(ො𝑦) = −3ො𝑦2 + 2ො𝑦
𝑦= 1
𝑓ො𝑦= −3ො𝑦2 + 4ො𝑦−1
𝑓(ො𝑦) = −3ො𝑦2 + 2ො𝑦
𝑓ො𝑦= −3ො𝑦2 + 4ො𝑦−1
41
MSE and BCE
AI VIETNAM
All-in-One Course
❖ Visualization
Mean Squared Error
Binary Cross-Entropy
42
Sigmoid and Tanh Functions
AI VIETNAM
All-in-One Course
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥
sigmoid 𝑥=
1
1 + 𝑒−𝑥
35
1
2
3
4
Sigmoid and Tanh Functions
AI VIETNAM
All-in-One Course
tanh 𝑥= 2 ×
1
1 + 𝑒−2𝑥−1
sigmoid 2𝑥=
1
1 + 𝑒−2𝑥
tanh 𝑥= 2 × sigmoid 2𝑥−1
➢Vectorization
➢Optimiztion for 1+ samples
➢Logistic Regression – Mini-batch
➢Logistic Regression – Batch
➢BCE and MSE Loss Functions
➢Sigmoid and Tanh Function (Optional)
Outline
Tanh function
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥= 1 −𝑒−2𝑥
1 + 𝑒−2𝑥
= −𝑒−2𝑥−1
𝑒−2𝑥+ 1 =
2
𝑒−2𝑥+ 1 −1
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥= 𝑒2𝑥−1
𝑒2𝑥+ 1
= 1 −
2
𝑒2𝑥+ 1
AI VIETNAM
All-in-One Course
Tanh function
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥= 𝑒2𝑥−1
𝑒2𝑥+ 1 = 1 −
2
𝑒2𝑥+ 1
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥= 1 −𝑒−2𝑥
1 + 𝑒−2𝑥= −𝑒−2𝑥−1
𝑒−2𝑥+ 1 =
2
𝑒−2𝑥+ 1 −1
𝑡𝑎𝑛ℎ′(𝑥) =
𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥
′
= 𝑒𝑥+ 𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥−𝑒𝑥−𝑒−𝑥
𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥2
= 𝑒𝑥+ 𝑒−𝑥2 −𝑒𝑥−𝑒−𝑥2
𝑒𝑥+ 𝑒−𝑥2
= 1 −𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥
2
= 1 −𝑡𝑎𝑛ℎ2(𝑥)
AI VIETNAM
All-in-One Course
Tanh function
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥= 𝑒2𝑥−1
𝑒2𝑥+ 1 = 1 −
2
𝑒2𝑥+ 1
tanh 𝑥= 𝑒𝑥−𝑒−𝑥
𝑒𝑥+ 𝑒−𝑥= 1 −𝑒−2𝑥
1 + 𝑒−2𝑥= −𝑒−2𝑥−1
𝑒−2𝑥+ 1 =
2
𝑒−2𝑥+ 1 −1
𝑡𝑎𝑛ℎ′(𝑥) =
2
𝑒−2𝑥+ 1 −1 
′
=
4𝑒−2𝑥
𝑒−2𝑥+ 1 2 = 4 𝑒−2𝑥+ 1 −1
𝑒−2𝑥+ 1 2  
= 4
1
𝑒−2𝑥+ 1 −
1
𝑒−2𝑥+ 1 2
= −
4
𝑒−2𝑥+ 1 2 −
4
𝑒−2𝑥+ 1
= −
4
𝑒−2𝑥+ 1 2 −
4
𝑒−2𝑥+ 1 + 1 −1
= 1 −
2
𝑒−2𝑥+ 1 −1
2
= 1 −𝑡𝑎𝑛ℎ2(𝑥)
AI VIETNAM
All-in-One Course
Logistic Regression
Tanh
❖ Construct loss
tanh 𝑥=
2
1 + 𝑒−2𝑥−1
L = −𝑦log ො𝑦𝑠−(1 −𝑦)log 1 −ො𝑦𝑠
ො𝑦= 𝑡𝑎𝑛ℎ(𝑧) = 𝑒𝑧−𝑒−𝑧
𝑒𝑧+ 𝑒−𝑧
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
Model and Loss
ො𝑦𝑠= ො𝑦+ 1
2
𝜕𝐿
𝜕𝜃𝑖
= 𝜕𝐿
𝜕ො𝑦𝑠
𝜕ො𝑦𝑠
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃𝑖
𝜕ො𝑦
𝜕𝑧= 1 −ො𝑦2
𝜕𝑧
𝜕𝜃𝑖
= 𝑥𝑖
Derivative
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖
(ො𝑦𝑠−𝑦)(1 −ො𝑦2)
2ො𝑦𝑠(1 −ො𝑦𝑠)
𝜕𝐿
𝜕ො𝑦𝑠
= −𝑦
ො𝑦𝑠
+ 1 −𝑦
1 −ො𝑦𝑠
=
ො𝑦𝑠−𝑦
ො𝑦𝑠(1 −ො𝑦𝑠)
𝜕ො𝑦𝑠
𝜕ො𝑦= 1
2
Logistic Regression
Tanh
❖ Construct loss
tanh 𝑥=
2
1 + 𝑒−2𝑥−1
L = −𝑦log ො𝑦𝑠−(1 −𝑦)log 1 −ො𝑦𝑠
ො𝑦= 𝑡𝑎𝑛ℎ(𝑧) = 𝑒𝑧−𝑒−𝑧
𝑒𝑧+ 𝑒−𝑧
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
Model and Loss
ො𝑦𝑠= ො𝑦+ 1
2
𝜕𝐿
𝜕𝜃𝑖
= 𝜕𝐿
𝜕ො𝑦𝑠
𝜕ො𝑦𝑠
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃𝑖
𝜕ො𝑦
𝜕𝑧= 1 −ො𝑦2
𝜕𝑧
𝜕𝜃𝑖
= 𝑥𝑖
Derivative
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖
(ො𝑦𝑠−𝑦)(1 −ො𝑦2)
2ො𝑦𝑠(1 −ො𝑦𝑠)
𝜕𝐿
𝜕ො𝑦𝑠
= −𝑦
ො𝑦𝑠
+ 1 −𝑦
1 −ො𝑦𝑠
=
ො𝑦𝑠−𝑦
ො𝑦𝑠(1 −ො𝑦𝑠)
𝜕ො𝑦𝑠
𝜕ො𝑦= 1
2
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖
(ො𝑦+ 1
2
−𝑦)(1 −ො𝑦2)
2 ො𝑦+ 1
2
(1 −ො𝑦+ 1
2
)
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖
(ො𝑦+ 1 −2𝑦)(1 −ො𝑦2)
(ො𝑦+ 1)(1 −ො𝑦)
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖(ො𝑦+ 1 −2𝑦)
Summary
𝑥
+∞
−∞
𝑥1
𝑥2
𝒚
𝑦1
𝑦2
𝑦=
1
1 + 𝑒−𝑥
Sigmoid function
4) Compute derivative
5) Update parameters
1) Pick all the samples from training data   
2) Compute output ෝ𝒚
3) Compute loss (binary cross-entropy)
ෝ𝒚= 𝜎(𝒛) =
1
1 + 𝑒−𝒛
𝐿(𝜽) = 1
N −𝐲Tlogො𝐲−(1−y)Tlog(1−ො𝐲 )
𝛻𝜽𝐿= 1
N 𝒙T(ො𝐲−𝒚)
𝜽= 𝜽−𝜂𝐿𝜽
′
𝜂is learning rate
𝒛= 𝒙𝜽
