Multilayer Perceptron
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) – Background
(2) – Multilayer Perceptron
(3) – Classification Application
1 - Background
!
3
Sigmoid Function
𝑧
𝜎
𝜎(𝑧) =
1
1 + 𝑒!"
𝑧∈−∞
+ ∞
𝜎(z) ∈0
1
Sigmoid function
Threshold
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
Hours
Score
0.5
0
1.0
0
1.5
1
2.0
1
2.5
2
3.0
2
3.5
3
4.0
3
Classes: {0, 1}
Binary Classification
Classes: {0, 1, 2, 3}
Multi-class Classification
1 - Background
!
4
Softmax Function
𝑧! = 1.0
𝑧" = 3.0
Softmax
𝑓(𝑧!) = 0.12
𝑓(𝑧") = 0.88
Input
Probability
𝑧! = 1.0
𝑧" = 2.0
Softmax
𝑓(𝑧!) = 0.09
𝑓(𝑧") = 0.24
Input
Probability
𝑧# = 3.0
𝑓(𝑧#) = 0.67
𝑃! = 𝑓𝑧! =
𝑒"!
∑# 𝑒""
0 ≤𝑓𝑧! ≤1
*
!
𝑓𝑧! = 1
1 - Background
!
5
Softmax Regression
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
𝑥
𝑧!
𝑧"
𝑧#
Softmax
#y! = 𝑃𝑦= 0
#y" = 𝑃𝑦= 1
Softmax Regression
𝐰𝟎
b𝟎
w𝟏
b𝟏
1 - Background
!
6
Softmax Regression
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
𝒚=
𝑦#
…
𝑦$
𝑦% ∈0,1
/
%
𝑦% = 1
C = #classes
𝑦= 0 →𝒚= 1
0
𝑦= 1 →𝒚= 0
1
Hours
Score
0.5
0
1.5
1
3.0
2
Classes: {0, 1, 2}
Multi-class Classification
#feature: 1
#class: 3
𝑦= 0 →𝒚=
1
0
0
𝑦= 1 →𝒚=
0
1
0
𝑦= 2 →𝒚=
0
0
1
One-hot Encoding
1
𝑥
𝑧!
𝑧"
Sigmoid
#y
1 - Background
!
7
Softmax Regression
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
𝑥
𝑧!
𝑧"
𝑧#
Softmax
#y! = 𝑃𝑦= 0
#y" = 𝑃𝑦= 1
𝑦
y = 0
Logistic Regression
One-Hot 
Encoding
𝑦
y = 0
Softmax Regression
L(𝛉) = −ylog:y−(1−y)log(1−:y )
w
b
w&
b&
w!
b!
𝑦! = 1
L(𝛉) = −𝑦!log(:y!)−𝑦&log(:y&)
𝑦" = 0
= −<
'
𝑦'log(:y()
1 - Background
!
8
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output #𝑦
3) Compute loss (cross-entropy)
𝐿𝜽= −𝒚#𝑙𝑜𝑔1𝒚
∇$L = 𝒙#𝐲−𝒚#
𝜽= 𝜽−𝜂∇$L
𝜂is learning rate
𝒛= 𝜽#𝒙
1𝒚= 𝑒𝒛∅𝒅
𝒅= 1 … 1 𝑒𝒛
∅is 
Hadamard
division
𝜃= [b&
b!
w&
w!]
𝒙) = [1
0.5]
Data #1
𝒚= 0
𝜂= 0.1
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
𝜃= [0.1
0.2
0.3
0.4]
One-hot encoding for label
𝑦= 0 →𝒚) = [1 0]
𝑦= 1 →𝒚) = [0 1]
1 - Background
!
9
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output #𝑦
3) Compute loss (cross-entropy)
𝐿𝜽= −𝒚#𝑙𝑜𝑔1𝒚
∇$L = 𝒙#𝐲−𝒚#
𝜽= 𝜽−𝜂∇$L
𝜂is learning rate
𝒛= 𝜽#𝒙
1𝒚= 𝑒𝒛∅𝒅
𝒅= 1 … 1 𝑒𝒛
∅is 
Hadamard
division
𝜃= [b&
b!
w&
w!]
𝒙) = [1
0.5]
𝜂= 0.1
= [0.1
0.2
0.3
0.4]
𝑦= 0 →𝒚) = [1 0]
L = −𝑦#log7𝑦# −𝑦&log7𝑦&
𝑧& = 𝑤&𝑥+ 𝑏&
0.3
0.1
𝑦
𝑧! = 𝑤!𝑥+ 𝑏!
0.4
0.2
:𝑦& =
𝑒*!
∑'+&
!
𝑒*"
:𝑦! =
𝑒*#
∑'+&
!
𝑒*"
𝑤!
𝑏!
𝑤#
𝑏#
𝒛! = 0.25
𝒛# = 0.4
7𝒚! = 0.46
7𝒚# = 0.54
𝒚= 1
0
𝐿= 0.77
∇$L = 𝐱#y −𝑦&
= −0.54
0.54
−0.27
0.27
𝜽= 𝜽−𝜂𝐿𝜽
(
= 0.105
0.194
0.302
0.397
1 - Background
!
10
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output #𝑦
3) Compute loss (cross-entropy)
𝐿𝜽= −𝒚#𝑙𝑜𝑔1𝒚
∇$L = 𝒙#𝐲−𝒚#
𝜽= 𝜽−𝜂∇$L
𝜂is learning rate
𝒛= 𝜽#𝒙
1𝒚= 𝑒𝒛∅𝒅
𝒅= 1 … 1 𝑒𝒛
∅is 
Hadamard
division
𝜃= [b&
b!
w&
w!]
𝒙) = [1
0.5]
𝜂= 0.1
= [0.1
0.2
0.3
0.4]
𝑦= 0 →𝒚) = [1 0]
L = −𝑦#log7𝑦# −𝑦&log7𝑦&
𝑧& = 𝑤&𝑥+ 𝑏&
0.302
0.105
𝑦
𝑧! = 𝑤!𝑥+ 𝑏!
0.397
0.194
:𝑦& =
𝑒*!
∑'+&
!
𝑒*"
:𝑦! =
𝑒*#
∑'+&
!
𝑒*"
𝑤!
𝑏!
𝑤#
𝑏#
𝒛! = 0.257
𝒛# = 0.393
7𝒚! = 0.466
7𝒚# = 0.534
𝒚= 1
0
𝐿= 0.763
𝜽= 𝜽−𝜂𝐿𝜽
(
= 0.105
0.194
0.302
0.397
1 - Background
!
11
Softmax Regression
Hours
Pass
0.25
???
4.5
???
Prediction
𝑦'()*: 0
𝑧& = 𝑤&𝑥+ 𝑏&
0.302
0.105
𝑧! = 𝑤!𝑥+ 𝑏!
0.397
0.194
:𝑦& =
𝑒*!
∑'+&
!
𝑒*"
:𝑦! =
𝑒*#
∑'+&
!
𝑒*"
𝑤!
𝑏!
𝑤#
𝑏#
2 – Multilayer Perceptron
!
12
Motivation
1
𝑥
𝑧!
𝑧"
𝑧#
Softmax
#y! = 𝑃𝑦= 0
#y" = 𝑃𝑦= 1
w&
b&
w!
b!
1
𝑥
𝑧!
𝑧"
Sigmoid
#y
w
b
Input Layer
Output Layer
2 – Multilayer Perceptron
!
13
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
w&
b&
w!
b!
#parameters: 4
2 – Multilayer Perceptron
!
14
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
#parameters: 12
2 – Multilayer Perceptron
!
15
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
#parameters: 12
2 – Multilayer Perceptron
!
16
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
#y
2 – Multilayer Perceptron
!
17
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
ℎ= [1.0 x]W, = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
#y
2 – Multilayer Perceptron
!
18
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W) = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W+ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
#y
2 – Multilayer Perceptron
!
19
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W) = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W+ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
#y = 0.5449
#y = σ z
= 𝜎0.18
= 0.5449
2 – Multilayer Perceptron
!
20
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W) = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W+ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
#y = 0.5449
#y = σ z
= 𝜎0.18
= 0.5449
L = 0.7872
2 – Multilayer Perceptron
!
21
Backward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.0946
0.0946
0.0946
0.0891
0.0946
0.0891
W+ = W+
=
0.0455
0.0782
0.0782
x = 1.0
2.0
y = 0
#y = 0.5449
#y = σ z
= 𝜎0.18
= 0.5449
L = 0.7872
2 – Multilayer Perceptron
!
22
Activation
vSigmoid Function
2 – Multilayer Perceptron
!
23
Activation
vSigmoid Function
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
Sigmoid
Sigmoid
2 – Multilayer Perceptron
!
24
Activation
vReLU Function
2 – Multilayer Perceptron
!
25
Activation
vReLU Function
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
ReLU
ReLU
2 – Multilayer Perceptron
!
26
Activation
vTanh Function
2 – Multilayer Perceptron
!
27
Activation
vTanh Function
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
Tanh
Tanh
2 – Multilayer Perceptron
!
28
Loss
vBCELoss()
2 – Multilayer Perceptron
!
29
Loss
vCrossEntropyLoss()
With Softmax Funtion
2 – Multilayer Perceptron
!
30
Optimizer
vSGD()
3 – Classification using MLP
!
31
Pipeline
Train 
Dataset
Data
Preparation
Model
Feature Extraction
Normalization
Convert to Tensor
Parameter 
Initialization
Optimizer: SGD
Loss: CrossEntropyLoss
Metric: Accuracy
Trained
Model
Training
Test 
Dataset
Evaluation
Score: Accuracy
vIris Dataset
3 – Classification using NN
!
32
Load Iris Dataset
v Load Iris Dataset from sklearn
v Train: Test = 0.7 : 0.3
3 – Classification using NN
!
33
Data Preparation
v Normalization: StandardScaler()
v Convert to tensor
3 – Classification using NN
!
34
Model
3 – Classification using NN
!
35
Loss Function, Optimizer
3 – Classification using NN
!
36
Training
3 – Classification using NN
!
37
Evaluation
Thanks!
Any questions?
38
