<<<<<<< HEAD
Node ID: 62fe2f59-dc94-42da-a053-20e15da8082f
Text: AI VIETNAM aivietnam.edu.vn 66 start_positions.append(0) 67
end_positions.append(0) 68 else: 69 # Nếu không, gán vị trí bắt đầu và
kết thúc dựa trên 70 # vị trí của các mã thông tin 71 idx =
context_start 72 while idx <= context_end and offset[idx][0] <=
start_char: 73 idx += 1 74 start_positions.append(idx - 1) 75 76 idx =
context_end 77 while ...
=======
AI VIETNAM aivietnam.edu.vn
66 start_positions.append(0)
67 end_positions.append(0)
68 else:
69 # Nếu không, gán vị trí bắt đầu và kết thúc dựa trên
70 # vị trí của các mã thông tin
71 idx = context_start
72 while idx <= context_end and offset[idx][0] <= start_char:
73 idx += 1
74 start_positions.append(idx - 1)
75
76 idx = context_end
77 while idx >= context_start and offset[idx][1] >= end_char:
78 idx -= 1
79 end_positions.append(idx + 1)
80
81 # Thêm thông tin vị trí bắt đầu và kết thúc vào inputs
82 inputs["start_positions"] = start_positions
83 inputs["end_positions"] = end_positions
84
85 return inputs
Sau đó ta sẽ chạy đoạn hàm trên với từng dòng trong raw_dataset của tập train:
1 # Tạo một biến train_dataset và gán cho nó giá trị sau khi áp dụng
2 # hàm preprocess_training_examples lên tập dữ liệu "train"
3 #
4 # Bật chế độ xử lý theo từng batch bằng cách đặt batched=True
5 #
6 # Loại bỏ các cột không cần thiết trong
7 # tập dữ liệu "train" bằng cách sử dụng remove_columns
8
9 train_dataset = raw_datasets["train"].map(
10 preprocess_training_examples,
11 batched=True,
12 remove_columns=raw_datasets["train"].column_names,
13 )
14
15 # In ra độ dài của tập dữ liệu "train" ban đầu và
16 # độ dài của tập dữ liệu đã được xử lý (train_dataset)
17 len(raw_datasets["train"]), len(train_dataset)
• Tokenize val set:Ta sẽ làm tương tự với tập val, hàm preprocess_validation_examples
thực hiện việc tiền xử lý dữ liệu cho quá trình đánh giá mô hình. Hàm chuẩn bị danh
sách câu hỏi, mã hóa các câu hỏi và văn bản liên quan bằng cách sử dụng tokenizer. 
>>>>>>> HEAD@{1}
