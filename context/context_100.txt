Momentum
Gradient Descent with Momentum
Nesterov Momentum
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Multilayer Perceptron
(2) â€“ Gradient Descent with Momentum
(3) â€“ Nesterov Momentum
1 â€“ Multilayer Perceptron
!
3
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
#parameters: 12
1 â€“ Multilayer Perceptron
!
4
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
#parameters: 12
1 â€“ Multilayer Perceptron
!
5
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W! = W!"
W!#
=
0.1
0.1
0.1
0.1
0.1
0.1
W$ = W$
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
)y
1 â€“ Multilayer Perceptron
!
6
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W! = W!"
W!#
=
0.1
0.1
0.1
0.1
0.1
0.1
W$ = W$
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W! = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W$ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
)y = 0.5449
)y = Ïƒ z
= ğœ0.18
= 0.5449
L = 0.7872
1 â€“ Multilayer Perceptron
!
7
Backward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W! = W!"
W!#
=
0.0946
0.0946
0.0946
0.0891
0.0946
0.0891
W$ = W$
=
0.0455
0.0782
0.0782
x = 1.0
2.0
y = 0
)y = 0.5449
)y = Ïƒ z
= ğœ0.18
= 0.5449
L = 0.7872
1 â€“ Multilayer Perceptron
!
8
Activation
vSigmoid Function
vReLU Function
vTanh Function
1 â€“ Multilayer Perceptron
!
9
Loss
vBCELoss()
vCrossEntropyLoss()
1 â€“ Multilayer Perceptron
!
10
Optimizer
vSGD()
1 â€“ Multilayer Perceptron
!
11
Classification using Multilayer Perceptron
Train 
Dataset
Data
Preparation
Model
Feature Extraction
Normalization
Convert to Tensor
Parameter 
Initialization
Optimizer: SGD
Loss: CrossEntropyLoss
Metric: Accuracy
Trained
Model
Training
Test 
Dataset
Evaluation
Score: Accuracy
vIris Dataset
2 â€“ GD with Momentum
!
12
Gradient Descent
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡#L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
ğ‘§= ğœ½$ğ’™
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output &ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š$ğ‘™ğ‘œğ‘”=ğ’š
âˆ‡#L = ğ’™&ğ²âˆ’ğ’š$
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
ğ’›= ğœ½$ğ’™
=ğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ GD with Momentum
!
13
Gradient Descent
Ã˜ Objective function: 2ğ‘¥C
Ã˜ Derivative: 2ğ‘¥
Ã˜ Gradient:
ğ‘¥= ğ‘¥âˆ’ğœ‚âˆ—ğ‘“D(ğ‘¥)
2 â€“ GD with Momentum
!
14
Gradient Descent
Ã˜ Objective function: 2ğ‘¥C
Ã˜ Derivative: 2ğ‘¥
Ã˜ Gradient:
ğ‘¥= ğ‘¥âˆ’ğœ‚âˆ—ğ‘“D(ğ‘¥)
Initial weight
Minimum cost
Incremental step
Gradient
ğ‘¥= âˆ’2.0
ğœ‚= 0.1
ğ‘¥= âˆ’2.0 âˆ’0.1 âˆ—2 âˆ—âˆ’2.0
= âˆ’1.6
2 â€“ GD with Momentum
!
15
Gradient Descent
Ã˜ Objective function: 2ğ‘¥C
Ã˜ Derivative: 2ğ‘¥
Ã˜ Gradient:
ğ‘¥= ğ‘¥âˆ’ğœ‚âˆ—ğ‘“D(ğ‘¥)
Minimum cost
Incremental step
2 â€“ GD with Momentum
!
16
Gradient Descent
2 â€“ GD with Momentum
!
17
Gradient Descent
2 â€“ GD with Momentum
!
18
Gradient Descent
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient:
ğ‘¥= ğ‘¥âˆ’ğœ‚âˆ—ğ‘“D(ğ‘¥)
2 â€“ GD with Momentum
!
19
Gradient Descent
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient:
ğ‘¥= ğ‘¥âˆ’ğœ‚âˆ—ğ‘“D(ğ‘¥)
ğ‘¥= âˆ’2.0
ğœ‚= 0.1
ğ‘¥= âˆ’2.0 âˆ’0.1 âˆ—âˆ’12
= âˆ’0.8
2 â€“ GD with Momentum
!
20
Gradient Descent
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient:
ğ‘¥= ğ‘¥âˆ’ğœ‚âˆ—ğ‘“D(ğ‘¥)
x = âˆ’2.0
Î· = 0.1
num_epochs = 5
Local minimum
Global 
minimum
Cost = 2.05
2 â€“ GD with Momentum
!
21
Gradient Descent
2 â€“ GD with Momentum
!
22
Gradient Descent
2 â€“ GD with Momentum
!
23
Gradient Descent
Local minimum
Global 
minimum
2 â€“ GD with Momentum
!
24
GD with Momentum
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI + ğœ‚ğ‘“D(ğ‘¥)
ğ‘¥= ğ‘¥âˆ’ğ‘£G
ğ›¾: momentum
2 â€“ GD with Momentum
!
25
GD with Momentum
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI + ğœ‚ğ‘“D(ğ‘¥)
ğ‘¥= ğ‘¥âˆ’ğ‘£G
ğ‘¥= âˆ’2. 0
ğ‘£! = 0.8 âˆ—0.0 + 0.1 âˆ—âˆ’12 = âˆ’1.2
ğœ‚= 0.1
ğ›¾= 0.8
ğ‘¥= âˆ’2.0 âˆ’(âˆ’1.2) = âˆ’0.8
ğ‘£" = 0.0
2 â€“ GD with Momentum
!
26
GD with Momentum
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI + ğœ‚ğ‘“D(ğ‘¥)
ğ‘¥= ğ‘¥âˆ’ğ‘£G
ğ‘¥= âˆ’2. 0
ğ‘£! = 0.8 âˆ—(âˆ’1.2) + 0.1 âˆ—3.072 = âˆ’0.65
ğœ‚= 0.1
ğ›¾= 0.8
ğ‘¥= âˆ’0.8 âˆ’(âˆ’0.65) = âˆ’0.15
ğ‘£! = âˆ’1.2
2 â€“ GD with Momentum
!
27
GD with Momentum
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI + ğœ‚ğ‘“D(ğ‘¥)
ğ‘¥= ğ‘¥âˆ’ğ‘£G
Cost = 0.12
num_epochs = 5
ğ‘¥= âˆ’2. 0
ğœ‚= 0.1
ğ›¾= 0.8
ğ‘£" = 0.0
2 â€“ GD with Momentum
!
28
GD with Momentum
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI + ğœ‚ğ‘“D(ğ‘¥)
ğ‘¥= ğ‘¥âˆ’ğ‘£G
Negative of Gradient
Real movement
Momentum
Movement = Negative of Gradient + Momentum
2 â€“ GD with Momentum
!
29
GD with Momentum
2 â€“ GD with Momentum
!
30
GD with Momentum
3 â€“ Nesterov Momentum
!
31
Problem of GD with Momentum
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI + ğœ‚ğ‘“D(ğ‘¥)
ğ‘¥= ğ‘¥âˆ’ğ‘£G
num_epochs = 10
ğ‘¥= âˆ’2. 0
ğœ‚= 0.1
ğ›¾= 0.9
ğ‘£" = 0.0
3 â€“ Nesterov Momentum
!
32
Nesterov Momentum
Ã˜ Objective function:
xE + xF âˆ’2xC âˆ’2x + 2
Ã˜ Derivative: 
4ğ‘¥F + 3ğ‘¥C âˆ’4ğ‘¥
Ã˜ Gradient with Momentum:
ğ‘£G = ğ›¾ğ‘£GHI âˆ’ğœ‚ğ‘“D(ğ‘¥+ ğ›¾âˆ—ğ‘£GHI)
ğ‘¥= ğ‘¥+ ğ‘£G
ğ‘¥= âˆ’2. 0
ğœ‚= 0.1
ğ›¾= 0.9
ğ‘£" = 0.0
3 â€“ Nesterov Momentum
!
33
Source
3 â€“ Nesterov Momentum
!
34
3 â€“ Nesterov Momentum
!
35
Thanks!
Any questions?
36
