Transformer Insight
Year 2023
Quang-Vinh Dinh
Ph.D. in Computer Science
AI VIETNAM
All-in-One Course
➢Revisiting RNNs, MLPs, and CNNs
➢From RNNs to Transformers
➢Self-Attention
➢Masked Self-Attention
➢Cross-Attention
Outline
-
50,000 movie review for sentiment analysis
-
Consist of:
+ 25,000 movie review for training
+ 25,000 movie review for testing
-
Label: positive – negative
“A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-
BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire 
piece…..”
positive
“This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 
years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, 
and it's continued its decline further to the complete waste of time it is today….”
negative
“I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air 
conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is 
witty and the characters are likable (even the well bread suspected serial killer)….”
positive
“BTW Carver gets a very annoying sidekick who makes you wanna shoot him the first three minutes 
he's on screen.”
negative
Text Classification
AI VIETNAM
All-in-One Course
❖ IMDB dataset
1
Review
❖Text classification model
AI VIETNAM
All-in-One Course
Text 
Embedding
The first von 
Trier movie i've 
ever seen …
Text 
Encoding
Text 
Vectorization
Output
Data
Convert words to 
indices
Map indices to 
vectors
Understand 
context
Linear & 
Softmax
Classify
2
-
50,000 movie review for sentiment analysis
-
Consist of:
+ 25,000 movie review for training
+ 25,000 movie review for testing
-
Label: positive – negative
Text 
Classification
❖ IMDB dataset
Using RNN
Word-1
Word-2
Word-500
RNN Cell
RNN Cell
RNN Cell
RNN Cell
RNN Cell
RNN Cell
hidden_dim=64
dim=2
embed_dim = 128
50
55
60
65
70
75
80
85
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Train Accuracy
Test Accuracy
Test accuracy: ~68%
4
Text Deep Models
❖Bidirectional RNN/LSTM
AI VIETNAM
All-in-One Course
50
60
70
80
90
100
110
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Train Accuracy
Test Accuracy
(LSTM) Test accuracy: ~88%
5
Text Deep Models
❖Multilayer perceptron
Output
Text 
Sample
Preprocessing
(standardization & 
vectorization)
Preprocessing
𝑧1
Softmax
Fully-connected layer
Embedding
…
𝑧2
6
Text Deep Models
❖Using Conv1D
Embedding
W
C
W
C
H=1
Global Pooling
Max pooling
AI VIETNAM
All-in-One Course
Data
𝑣1 
𝑣2 
𝑣3 
𝑣4
𝑣5 
𝑣6 
𝑣7 
𝑣8
𝑣9 𝑣10 𝑣11 𝑣12
𝑣13 𝑣14 𝑣15 𝑣16
𝑚1 = max 𝑣1, 𝑣2 , 𝑣5 , 𝑣6
𝑚2 = max 𝑣3, 𝑣4 , 𝑣7 , 𝑣8
𝑚3 = max 𝑣9, 𝑣10 , 𝑣13 , 𝑣14
𝑚4 = max 𝑣11, 𝑣12 , 𝑣15 , 𝑣16
𝑣1 
𝑣2 
𝑣3 
𝑣4
𝑣5 
𝑣6 
𝑣7 
𝑣8
𝑣9 𝑣10 𝑣11 𝑣12
𝑣13 𝑣14 𝑣15 𝑣16
𝑚1 𝑚2
𝑚3 𝑚4
(                       ) = 
2x2 max 
pooling 
max_pool1d(input_size)
Data
𝑣1 
𝑣2 
𝑣3 
𝑣4
𝑣5 
𝑣6 
𝑣7 
𝑣8
𝑣9 𝑣10 𝑣11 𝑣12
𝑣13 𝑣14 𝑣15 𝑣16
global max 
pooling 
𝑣1 
𝑣2 
𝑣3 
𝑣4
𝑣5 
𝑣6 
𝑣7 
𝑣8
𝑣9 𝑣10 𝑣11 𝑣12
𝑣13 𝑣14 𝑣15 𝑣16
(                       ) = 𝑚
𝑚= max 𝑣1, 𝑣2, … , 𝑣16
Global max pooling
8
Text Deep Models
❖Conv1D + GlobalMaxPooling1D 
AI VIETNAM
All-in-One Course
9
➢Revisiting RNNs, MLPs, and CNNs
➢From RNNs to Transformers
➢Self-Attention
➢Masked Self-Attention
➢Cross-Attention
Outline
From RNNs to Transformers
AI VIETNAM
All-in-One Course
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
new
representation
❖ RNN/LSTM/GRU Limitations
10
From RNNs to Transformers
AI VIETNAM
All-in-One Course
❖ Desire properties
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
new
representation
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
𝛼00
𝛼01
𝛼0𝑛
11
𝐷0
𝐷1
𝐷𝑛
…
𝑌0
𝑌1
𝑌𝑛
…
new
representation
Desire Property
𝛼00
𝛼01
𝛼0𝑛
𝛼00
𝛼01
𝛼0𝑚
+
+
𝑌0  =
𝛼0𝑖=
∙
12
𝛼00
𝛼01
𝛼0𝑛
+
+
𝑌0  =
𝛼00 =
∙
𝛼01 =
∙
𝛼0𝑚=
∙
Desire Property
∙
=
𝛼00 𝛼01 …  𝛼0𝑛 
…
𝛼00 =
∙
𝛼01 =
∙
𝛼0𝑛=
∙
Context Awareness
AI VIETNAM
All-in-One Course
14
𝛼10 =
∙
𝛼11 =
∙
𝛼1𝑛=
∙
∙
=
𝛼10 𝛼11 …  𝛼1𝑛 
…
Context Awareness
AI VIETNAM
All-in-One Course
15
𝛼𝑛0 =
∙
𝛼𝑛1 =
∙
𝛼𝑛𝑛=
∙
∙
=
𝛼𝑛0 𝛼𝑛1 …  𝛼𝑛𝑛 
…
Context Awareness
AI VIETNAM
All-in-One Course
16
∙
=
𝛼10 𝛼11 …  𝛼1𝑛 
…
∙
=
𝛼00 𝛼01 …  𝛼0𝑛 
…
∙
=
𝛼𝑛0 𝛼𝑛1 …  𝛼𝑛𝑛 
…
…
…
…
∙
=
𝛼00 𝛼01 …  𝛼0𝑛
𝛼10 𝛼11 …  𝛼1𝑛
… 
𝛼𝑛0 𝛼𝑛1 …  𝛼𝑛𝑛
 
𝛼= 𝐷𝐷𝑇
=
𝛼0
𝛼1
… 
𝛼𝑛
 
17
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
new
representation
𝛼00
𝛼01
𝛼0𝑛
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
weight: determine how 
much 𝑋0 contributes to 𝑌0
(𝛼00, 𝛼01 , … , 𝛼0𝑛)
(𝛼10, 𝛼11 , … , 𝛼1𝑛)
… 
(𝛼𝑛0, 𝛼𝑛1 , … , 𝛼𝑛𝑛)
softmax(𝛼00, 𝛼01 , … , 𝛼0𝑛)
softmax(𝛼10, 𝛼11 , … , 𝛼1𝑛)
… 
softmax(𝛼𝑛0, 𝛼𝑛1 , … , 𝛼𝑛𝑛)
𝛼= softmax(𝐷𝐷𝑇)
normalization
1
𝐷0
𝐷1
𝐷𝑚
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑚
…
new
representation
𝛼00
𝛼01
𝛼0𝑛
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
weight: determine how 
much 𝑋0 contributes to 𝑌0
(𝛼00, 𝛼01 , … , 𝛼0𝑛)
(𝛼10, 𝛼11 , … , 𝛼1𝑛)
… 
(𝛼𝑚0, 𝛼𝑚1 , … , 𝛼𝑛𝑛)
softmax(𝛼00, 𝛼01 , … , 𝛼0𝑛)
softmax(𝛼10, 𝛼11 , … , 𝛼1𝑛)
… 
softmax(𝛼𝑛0, 𝛼𝑛1 , … , 𝛼𝑛𝑛)
𝛼= softmax(𝐷𝐷𝑇
𝑑
)
weight vector
2
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
weight: determine how 
much 𝑋0 contributes to 𝑌0
(𝛼00, 𝛼01 , … , 𝛼0𝑛)
(𝛼10, 𝛼11 , … , 𝛼1𝑛)
… 
(𝛼𝑛0, 𝛼𝑛1 , … , 𝛼𝑛𝑛)
softmax(𝛼00, 𝛼01 , … , 𝛼0𝑛)
softmax(𝛼10, 𝛼11 , … , 𝛼1𝑛)
… 
softmax(𝛼𝑛0, 𝛼𝑛1 , … , 𝛼𝑛𝑛)
𝛼= softmax(𝐷𝐷𝑇
𝑑
)
weight vector
𝑌= 𝛼𝐷= softmax(𝐷𝐷𝑇
𝑑
)𝐷
Contextualized representation
𝑌=
𝛼0𝑖 𝛼01 …  𝛼0𝑛
𝛼1𝑖 𝛼11 …  𝛼1𝑛
… 
𝛼𝑛𝑖 𝛼𝑛1 …  𝛼𝑛𝑛
𝐷0
𝐷1
… 
𝐷𝑛
=
𝑌0
𝑌1
… 
𝑌𝑛
Context Awareness
AI VIETNAM
All-in-One Course
❖ Contextualized representation
𝑋=
𝑋0
𝑋1
. . .
𝑋𝑛
∈ℛ𝑛×𝑑
𝑊𝑄= 𝜃𝑞0 
𝜃𝑞1 …  𝜃𝑞𝑚∈ℛ𝑑×𝑚
𝑊𝐾= 𝜃𝑘0 
𝜃𝑘1 …  𝜃𝑘𝑚∈ℛ𝑑×𝑚
𝑊𝑉= 𝜃𝑣0 
𝜃𝑣1 …  𝜃𝑣𝑚∈ℛ𝑑×𝑚
Query 𝑄= 𝑋𝑊𝑄∈ℛ𝑛×𝑚
Key 𝐾= 𝑋𝑊𝐾∈ℛ𝑛×𝑚
Value 𝑉= 𝑋𝑊𝑉∈ℛ𝑛×𝑚
𝑊𝑂= 𝜃0 
𝜃1 …  𝜃𝑜∈ℛ𝑚×𝑜
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Vectorization 
& Embedding
21
𝑌= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
𝑉
𝑌= 𝛼𝐷= softmax(𝐷𝐷𝑇
𝑑
)𝐷
Contextualized representation
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
22
Vectorization 
& Embedding
𝐴= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
𝑉
𝑌= 𝐴𝑊𝑂
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Self-attention
𝑌0
𝑌1
𝑌𝑛
…
Self-Attention
23
𝑋= −0.1 0.1 0.3
𝑊𝑄=
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝑊𝐾=
−0.49 −0.68 0.18
−0.44 −0.46 0.18
 0.07 −0.10 0.44
𝑊𝑉=
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
𝑊𝑂=
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
𝑄= 𝑋𝑊𝑄= −0.1 0.1 0.3
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
= −0.08 −0.14 −0.24
𝐾= 𝑋𝑊𝐾= −0.1 0.1 0.3
−0.49 
−0.68 
0.18
−0.44 
−0.46 
0.18
 0.07 
−0.10 
0.44
= 0.02 −0.01 0.13
𝑉= 𝑋𝑊𝑉= −0.1 0.1 0.3
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
= −0.16 −0.08 −0.05
head = 1
Self-Attention
❖ Example 1
24
𝑌= 𝐴𝑊𝑂= −0.16 −0.08 −0.05
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
= −0.16 −0.08 −0.05
𝐴= softmax 𝑄𝐾𝑇
𝑑
𝑉
= softmax −0.08 −0.14 −0.24
0.02
−0.01
0.13
1
𝑑
) −0.16 −0.08 −0.05
= −0.16 −0.08 −0.05
approximately
Self-Attention
AI VIETNAM
All-in-One Course
❖ Example 1
= softmax −0.0198
−0.16 −0.08 −0.05
25
head = 1
Self-Attention
❖ Example 2
𝑋= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
𝑊𝑄=
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝑊𝐾=
−0.49 
−0.68 0.18
−0.44 
−0.46 0.18
 0.07 
−0.10 0.44
𝑊𝑉=
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
𝑊𝑂=
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
𝑄= 𝑋𝑊𝑄= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝐾= 𝑋𝑊𝐾= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.49 
−0.68 
0.18
−0.44 
−0.46 
0.18
 0.07 
−0.10 
0.44
𝑉= 𝑋𝑊𝑉= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
= −0.08 −0.14 −0.24
−0.39 0.77 0.69
= 0.02 −0.01 0.13
0.27 0.27 −0.26
= −0.16 −0.08 −0.05
−0.02 −0.02 0.05
𝐴= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
𝑉
26
approximately
Self-Attention
AI VIETNAM
All-in-One Course
❖ Example 2
𝑌= 𝐴𝑊𝑂= 0.14 0.09 0.07
0.12 0.08 0.06
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
= −0.029 −0.028 0.065
−0.025 −0.025 0.058
𝐴= softmax 𝑄𝐾𝑇
𝑑
𝑉
= softmax
−0.08 −0.14 −0.24
−0.39 
0.77 
0.69
 0.02 
0.27
−0.01 
0.27
 0.13 −0.26
1
𝑑
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05
= 0.49 0.51
0.52 0.48
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05 = 0.14 0.09 0.07
0.12 0.08 0.06
= softmax
−0.019 
0.002
 0.043 
−0.046
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05
27
Softmax
n
d
n
d
d
d
n
n
n
n
n
𝑑𝑞
𝑑𝑘
𝑑𝑣
𝑑𝑞
𝑑𝑘
𝑑𝑣
𝑊𝑞
𝑊𝑘
𝑊𝑣
Q
K
V
𝑄𝐾𝑇
Y
A
n
n
𝑑𝑞= 𝑑𝑘= 𝑑𝑣
Embedding size
Sequence length
Input
𝑑𝑣
Self-Attention
28
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Self-attention
𝑌0
𝑌1
𝑌𝑛
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Self-attention
𝑌0
𝑌1
𝑌𝑛
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Self-attention
𝑌0
𝑌1
𝑌𝑛
𝑌𝑖= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝑖𝐾𝑖
𝑇
𝑑
𝑉𝑖
𝑂= concat(… 𝑌𝑖… )𝑊𝑂
𝑊𝑂= 𝜃0 
𝜃1 …  𝜃𝑜∈ℛℎ∗𝑚×𝑜
29
➢Revisiting RNNs, MLPs, and CNNs
➢From RNNs to Transformers
➢Self-Attention
➢Masked Self-Attention
➢Cross-Attention
Outline
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Transformer
AI VIETNAM
All-in-One Course
❖ Masked self-attention
30
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
𝛼00
𝛼01
𝛼0𝑛
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
𝑌0 = 𝛼00𝐷0 + 0 × 𝐷1 + ⋯+ 0 × 𝐷𝑛
𝛼0 = softmax(𝐷0𝐷𝑇
𝑑
) =
𝛼00
𝛼01
…
𝛼0𝑛
𝛼00
0
…
0
How to obtain 
kind of →
𝛼0 = softmax 𝐷0𝐷𝑇
𝑑
∗
1
0
…
0
=
𝛼00
0
…
0
?
Masked 
self-attention
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
𝛼00
𝛼01
𝛼0𝑛
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
𝑌0 = 𝛼00𝐷0 + 0 × 𝐷1 + ⋯+ 0 × 𝐷𝑛
𝛼0 = softmax(𝐷0𝐷𝑇
𝑑
) =
𝛼00
𝛼01
…
𝛼0𝑛
𝛼00
0
…
0
How to obtain 
kind of →
𝛼0 = softmax
𝐷0𝐷𝑇
𝑑
∗
1
0
…
0
=
𝛼00
0
…
0
?
Mask 
self-attention
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
𝛼00
𝛼01
𝛼0𝑛
𝑌0 = 𝛼00𝐷0 + 𝛼01𝐷1 + ⋯+ 𝛼0𝑛𝐷𝑛
𝑌0 = 𝛼00𝐷0 + 0 × 𝐷1 + ⋯+ 0 × 𝐷𝑛
𝛼0 = softmax(𝐷0𝐷𝑇
𝑚) =
𝛼00
𝛼01
…
𝛼0𝑛
𝛼00
0
…
0
How to obtain 
kind of →
𝛼0 = softmax
𝐷0𝐷𝑇
𝑑
+
0
−∞
…
−∞
=
𝛼00
0
…
0
?
Mask 
self-attention
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
𝛼10
𝛼11
𝛼1𝑛
𝑌1 = 𝛼10𝐷0 + 𝛼11𝐷1 + ⋯+ 𝛼1𝑛𝐷𝑛
𝑌1 = 𝛼10𝐷0 + 𝛼11𝐷1 + ⋯+ 0 × 𝐷𝑛
𝛼1 = softmax
𝐷1𝐷𝑇
𝑑
+
0
0
−∞
…
−∞
=
𝛼10
𝛼11
0
…
0
Mask 
self-attention
34
𝐷0
𝐷1
𝐷𝑛
…
a word vector
a word vector
a word vector
𝑌0
𝑌1
𝑌𝑛
…
𝛼𝑛0
𝛼𝑛1
𝛼𝑛𝑛
𝑌𝑛= 𝛼𝑛0𝐷0 + 𝛼𝑛1𝐷1 + ⋯+ 𝛼𝑛𝑛𝐷𝑛
?
𝛼𝑛= softmax
𝐷𝑛𝐷𝑇
𝑑
+
0
0
0
…
0
=
𝛼𝑛0
𝛼𝑛1
𝛼𝑛2
…
𝛼𝑛𝑛
Mask 
self-attention
35
𝐷0
𝐷1
𝐷𝑛
…
𝑌0
𝑌1
𝑌𝑛
…
𝐷0
𝐷1
𝐷𝑛
…
𝑌0
𝑌1
𝑌𝑛
…
𝛼= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝐷𝐷𝑇
𝑑
𝛼= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝐷𝐷𝑇
𝑑
+ 𝑀
𝑀=
0 
−∞ 
−∞… −∞
0 
0 
−∞… −∞
…
0 
0 
0 
…  
0
𝑌= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
+ 𝑀𝑉
𝑀=
0 
−∞ 
−∞… −∞
0 
0 
−∞… −∞
…
0 
0 
0 
…  
0
Mask self-attention
𝑌0
𝑌1
𝑌𝑛
…
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
37
𝐴= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
+ 𝑀𝑉
𝑌= 𝐴𝑊𝑂
Mask self-attention
𝑌0
𝑌1
𝑌𝑛
…
word-0
word-1
word-n
…
𝑋0
𝑋1
𝑋𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
Masked 
Self-Attention
38
𝑋= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
𝑊𝑄=
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝑊𝐾=
−0.49 
−0.68 0.18
−0.44 
−0.46 0.18
 0.07 
−0.10 0.44
𝑊𝑉=
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
𝑊𝑂=
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
𝑄= 𝑋𝑊𝑄= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝐾= 𝑋𝑊𝐾= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.49 
−0.68 
0.18
−0.44 
−0.46 
0.18
 0.07 
−0.10 
0.44
𝑉= 𝑋𝑊𝑉= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
head = 1
Masked Multi-
head Attention
𝑀= 0 
−∞
0 
0
= −0.08 −0.14 −0.24
−0.39 0.77 0.69
= 0.02 −0.01 0.13
0.27 0.27 −0.26
= −0.16 −0.08 −0.05
−0.02 −0.02 0.05
39
approximately
Masked Multi-head Attention
AI VIETNAM
All-in-One Course
❖ Example 
𝑌= 𝐴𝑊𝑂= −0.16 −0.08 −0.05
 0.12 
0.08 
0.06
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
=
0.03 0.02 −0.06
−0.02 −0.02 0.05
𝐴= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
+ 𝑀𝑉
= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑
−0.08 −0.14 −0.24
−0.39 
0.77 
0.69
 0.02 
0.27
−0.01 
0.27
 0.13 −0.26
1
𝑑
+ 0 
−∞
0 
0
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05
=
1.0 
0.0
0.52 0.48
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05 = −0.16 −0.08 −0.05
 0.12 
0.08 0.06
= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑
−0.019 
0.002
 0.043 
−0.046 + 0 
−∞
0 
0
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05
40
Softmax
n
d
n
d
d
d
n
n
n
n
n
𝑑𝑞
𝑑𝑘
𝑑𝑣
𝑑𝑞
𝑑𝑘
𝑑𝑣
𝑊𝑞
𝑊𝑘
𝑊𝑣
Q
K
V
𝑄𝐾𝑇
Y
A
n
n
𝑑𝑞= 𝑑𝑘= 𝑑𝑣
Embedding size
Sequence length
Input
𝑑𝑣
Masked Self-Attention
−∞
0
41
𝐴= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑄𝐾𝑇
𝑑
𝑉
𝑌= 𝐴𝑊𝑂
𝐸0
𝐸1
𝐸𝑛
…
𝑄0
𝐾0
𝑉0
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄1
𝐾1
𝑉1
𝑊𝑄
𝑊𝐾
𝑊𝑉
𝑄𝑛
𝐾𝑛
𝑉𝑛
𝑊𝑄
𝑊𝐾
𝑊𝑉
One head cross-attention
𝑌0
𝑌1
𝑌𝑛
…
𝐷0
𝐷1
𝐷𝑛
Cross-Attention
❖ Example
42
head = 1
Cross-Attention
❖ Example
𝑋= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
𝑊𝑄=
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝑊𝐾=
−0.49 
−0.68 0.18
−0.44 
−0.46 0.18
 0.07 
−0.10 0.44
𝑊𝑉=
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
𝑊𝑂=
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
𝑄= 𝐶𝑊𝑄= −0.6 
0.3 −0.4
 0.5 
0.9 −0.5
−0.35 
0.51 
0.50
 0.36 −0.47 −0.29
−0.51 −0.14 −0.56
𝐾= 𝑋𝑊𝐾= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.49 
−0.68 
0.18
−0.44 
−0.46 
0.18
 0.07 
−0.10 
0.44
𝑉= 𝑋𝑊𝑉= −0.1 
0.1 
0.3
 0.4 −1.1 −0.3
−0.41 
0.39 −0.65
−0.40 −0.07 −0.34
−0.55 −0.13 −0.29
= 0.52 −0.39 −0.16
0.40 −0.09 
0.27
= 0.02 −0.01 0.13
0.27 0.27 −0.26
= −0.16 −0.08 −0.05
−0.02 −0.02 0.05
𝐶= −0.6 
0.3 −0.4
 0.5 
0.9 −0.5
43
approximately
Cross-Attention
AI VIETNAM
All-in-One Course
❖ Example
𝑌= 𝐴𝑊𝑂= 0.15 0.10 0.07
0.13 0.09 0.07
−0.36 −0.08 0.32
 0.27 
0.05 0.15
−0.05 −0.28 0.05
= −0.0305 −0.0296 0.0677
−0.0281 −0.0277 0.0630
𝐴= softmax 𝑄𝐾𝑇
𝑑
𝑉
= softmax
0.52 −0.39 −0.16
0.40 −0.09 
0.27
 0.02 
0.27
−0.01 
0.27
 0.13 −0.26
1
𝑑
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05
= 0.49 0.51
0.51 0.49
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05 = 0.15 0.10 0.07
0.13 0.09 0.07
= softmax
−0.0028 0.0470
 0.0278 0.0075
−0.16 −0.08 −0.05
−0.02 −0.02 
0.05
44
Softmax
n
d
n
d
d
d
n
m
m
n
m
𝑑𝑞
𝑑𝑘
𝑑𝑣
𝑑𝑞
𝑑𝑘
𝑑𝑣
𝑊𝑞
𝑊𝑘
𝑊𝑣
Q
K
V
𝑄𝐾𝑇
Z
A
n
m
𝑑𝑞= 𝑑𝑘
Embedding size
Sequence length
Input 1
(Decoder hidden state)
m
d
Sequence length
Input 2
(Encoder output)
𝑑𝑣
Cross-Attention
45
