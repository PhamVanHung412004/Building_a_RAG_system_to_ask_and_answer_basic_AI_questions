<<<<<<< HEAD
Node ID: 82a91dee-ea3e-433e-bfa6-71ff479f624a
Text: __init__ () 37 self . dim = dim 38 39 def forward (self , time
): 40 device = time . device 41 half_dim = self . dim // 2 42
embeddings = math . log (10000) / ( half_dim - 1) 43 embeddings =
torch . exp ( torch . arange ( half_dim , device = device ) * -
embeddings ) 44 embeddings = time [: , None ] * embeddings [None , :]
45 embeddings = torch ...
=======
__init__ ()
37 self . dim = dim
38
39 def forward (self , time ):
40 device = time . device
41 half_dim = self . dim // 2
42 embeddings = math . log (10000) / ( half_dim - 1)
43 embeddings = torch . exp ( torch . arange ( half_dim , device = device ) * - embeddings )
44 embeddings = time [: , None ] * embeddings [None , :]
45 embeddings = torch . cat (( embeddings . sin () , embeddings . cos ()), dim = -1)
46 # TODO : Double check the ordering here
47 return embeddings
48
49
50 class SimpleUnet (nn. Module ):
51 """
52 A simplified variant of the Unet architecture .
53 """
54 def __init__ ( self ):
55 super (). __init__ ()
56 image_channels = 3
57 down_channels = (64 , 128 , 256 , 512 , 1024)
58 up_channels = (1024 , 512 , 256 , 128 , 64)
59 out_dim = 3
60 time_emb_dim = 32
61
62 # Time embedding
63 self . time_mlp = nn. Sequential (
64 SinusoidalPositionEmbeddings ( time_emb_dim ),
65 nn. Linear ( time_emb_dim , time_emb_dim ),
66 nn. ReLU ()
67 )
68
69 # Initial projection
70 self . conv0 = nn. Conv2d ( image_channels , down_channels [0] , 3, padding =1)
71
72 # Downsample
73 self . downs = nn. ModuleList ([ Block ( down_channels [i], down_channels [i+1] , \
74 time_emb_dim ) \
75 for i in range ( len ( down_channels ) -1) ])
76 # Upsample
77 self . ups = nn. ModuleList ([ Block ( up_channels [i], up_channels [i+1] , \
78 time_emb_dim , up= True ) \
79 for i in range ( len ( up_channels ) -1) ])
80
81 # Edit : Corrected a bug found by Jakub C ( see YouTube comment )
82 self . output = nn. Conv2d ( up_channels [ -1] , out_dim , 1)
6
>>>>>>> HEAD@{1}
