Machine Learning
Softmax Regression
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Background
(2) â€“ Softmax Regression
(3) â€“ Code
1 â€“ Background
Linear Regression
!
3
Level
Salary
0
8
1
15
2
18
3
22
4
26
5
30
6
38
7
47
Data
Visualization
y = 6x + 7
y = f(x): linear function
Modeling
y = wx + b
Find w and b to fit 
the data
1 â€“ Background
!
4
Linear Regression
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute the output &ğ‘¦
3) Compute loss
!ğ‘¦= ğ‘¤ğ‘¥+ ğ‘
ğ¿= (!ğ‘¦âˆ’ğ‘¦)!
ğœ•ğ¿
ğœ•ğ‘¤= 2ğ‘¥(!ğ‘¦âˆ’ğ‘¦)
ğœ•ğ¿
ğœ•ğ‘= 2(!ğ‘¦âˆ’ğ‘¦)
ğ‘¤= ğ‘¤âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘¤
ğ‘= ğ‘âˆ’ğœ‚ğœ•ğ¿
ğœ•ğ‘
ğœ‚is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from training data   
2) Compute output &ğ‘¦
3) Compute loss
!ğ‘¦= ğœ½"ğ’™= ğ’™"ğœ½
ğ¿= (!ğ‘¦âˆ’ğ‘¦)!
âˆ‡#L = 2ğ’™(!ğ‘¦âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡#L
ğœ‚is learning rate
Traditional
Basic Python
Vectorized
Numpy
1 â€“ Background
!
5
Logistic Regression
Data #1
Hours
Pass
0.25
???
4.5
???
Learning
Prediction
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
!
6
Logistic Regression
Data #1
Visualization
Modeling
y = f(x)
Find a function to 
fit the data
Sigmoid function
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
1 â€“ Background
1 â€“ Background
!
7
Sigmoid Function
ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(z) âˆˆ0
1
âˆ€ğ‘§!ğ‘§" âˆˆğ‘ğ‘and ğ‘§! â‰¤ğ‘§"
â†’ğœ(ğ‘§!) â‰¤ğœ(ğ‘§")
ğ‘§
+âˆ
âˆ’âˆ
ğ‘§!
ğ‘§"
ğœ
ğœ!
ğœ"
Sigmoid function
Property
1 â€“ Background
!
8
Sigmoid Function
ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ‘§= ğœ½"ğ’™
ğ‘§= ğœ½"ğ’™
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(ğ‘§) âˆˆ0
1
ğ‘¥
ğ‘§
ğ‘¥
ğ‘§
ğ‘§
ğœ
ğ‘§
ğœ
1 â€“ Background
!
9
Logistic Regression using Gradient Descent
ğœ½! = [ğ‘
ğ‘¤" ğ‘¤#]
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
ğ’š= 0
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡&L = ğ±(&y âˆ’ğ‘¦)
ğœ½= ğœ½âˆ’ğœ‚âˆ‡&L
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğœ‚= 0.1
Data #1
Day
Hours
Pass
1
0.5
0
2
1.0
0
3
1.5
1
2
2.0
0
1
2.5
0
2
3.0
1
1
3.5
1
2
4.0
1
1 â€“ Background
!
10
Logistic Regression using Gradient Descent
ğ’š= 0
Model
ğ‘
ğ‘¤B
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘§= 0.35
)ğ‘¦= 0.59
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡&L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤C
ğœ½= ğœ½âˆ’ğœ‚âˆ‡&L
1 â€“ Background
!
11
Logistic Regression using Gradient Descent
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤B
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog!yâˆ’(1âˆ’y)log(1âˆ’!y )
ğ‘§= 0.35
)ğ‘¦= 0.59
L = 0.883
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡&L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤C
ğœ½= ğœ½âˆ’ğœ‚âˆ‡&L
1 â€“ Background
!
12
Logistic Regression using Gradient Descent
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤B
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog!yâˆ’(1âˆ’y)log(1âˆ’!y )
ğ‘§= 0.35
)ğ‘¦= 0.59
L = 0.883
âˆ‡!L = ğ±()y âˆ’ğ‘¦)
=
1
1.0
0.5
0.59 =
0.59
0.59
0.295
=
ğ¿"
#
ğ¿$#
#
ğ¿$$
#
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡&L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤C
ğœ½= ğœ½âˆ’ğœ‚âˆ‡&L
1 â€“ Background
!
13
Logistic Regression using Gradient Descent
ğ’š= 0
Model
Loss
ğ‘¦
ğ‘¦= 0
ğ‘
ğ‘¤B
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
âˆ’ylog!yâˆ’(1âˆ’y)log(1âˆ’!y )
ğ‘§= 0.35
)ğ‘¦= 0.59
L = 0.883
âˆ‡!L = ğ±()y âˆ’ğ‘¦)
=
1
1.0
0.5
0.59 =
0.59
0.59
0.295
=
ğ¿"
#
ğ¿$#
#
ğ¿$$
#
ğ‘= 0.1 âˆ’ğœ‚0.59 = 0.041
ğ‘¤%= 0.2 âˆ’ğœ‚0.59 =0.141
ğœ‚= 0.1
0.1
0.2
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡&L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.1
0.2 0.1]
0.1
ğ‘¤C
ğ‘¤&= 0.1 âˆ’ğœ‚0.295
=0.0706
ğœ½= ğœ½âˆ’ğœ‚âˆ‡&L
1 â€“ Background
!
14
Logistic Regression using Gradient Descent
ğ’š= 0
Model
ğ‘
ğ‘¤B
!ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§= ğ‘¤ğ‘¥+ ğ‘
ğ‘= 0.1 âˆ’ğœ‚0.59 = 0.041
ğ‘¤%= 0.2 âˆ’ğœ‚0.59 =0.141
ğœ‚= 0.1
0.041
0.141
4) Compute derivative
5) Update parameters
1) Pick a sample (ğ‘¥, ğ‘¦) from 
training data   
2) Compute output &ğ‘¦
3) Compute loss
&ğ‘¦= ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
âˆ‡&L = ğ±(&y âˆ’ğ‘¦)
ğœ‚is learning rate
ğ‘§= ğœ½!ğ’™
ğ’™! = [1.
1.0 0.5]
ğœ½! = [0.041
0.141 0.0706]
0.0706
ğ‘¤C
ğ‘¤&= 0.1 âˆ’ğœ‚0.295
=0.0705
ğœ½= ğœ½âˆ’ğœ‚âˆ‡&L
1 â€“ Background
!
15
Prediction
Prediction
Prediction
&ğ‘¦= ğœğ‘§=
1
1 + ğ‘’$% = 0.58
ğ‘§= ğ‘¤ğ‘¥+ ğ‘= 0.3417
Prediction
Threshold = 0.5
ğ‘¦*+,-: 1
Day
Hours
Pass
2
0.25
???
1
4.5
???
Day
Hours
Pass
2
0.25
???
1
4.5
???
Model
ğ‘
ğ‘¤B
0.041
0.141
0.0706
ğ‘¤C
Prediction
&ğ‘¦= ğœğ‘§=
1
1 + ğ‘’$% = 0.622
ğ‘§= ğ‘¤ğ‘¥+ ğ‘= 0.5
Threshold = 0.5
ğ‘¦*+,-: 1
Model
ğ‘
ğ‘¤B
0.041
0.141
0.0706
ğ‘¤C
2 â€“ Softmax Regression
!
16
Problem
ğ‘§
ğœ
ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(z) âˆˆ0
1
Sigmoid function
Threshold
2 â€“ Softmax Regression
!
17
Problem
ğ‘§
ğœ
ğœ(ğ‘§) =
1
1 + ğ‘’$%
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(z) âˆˆ0
1
Sigmoid function
Threshold
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
Hours
Score
0.5
0
1.0
0
1.5
1
2.0
1
2.5
2
3.0
2
3.5
3
4.0
3
Classes: {0, 1}
Binary Classification
Classes: {0, 1, 2, 3}
Multi-class Classification
2 â€“ Softmax Regression
!
18
Problem
Hours
Pass
0.5
0
2.0
1
Hours
Score
0.5
0
1.5
1
3.0
2
4.0
3
Classes: {0, 1}
Binary Classification
Classes: {0, 1, 2, 3}
Multi-class Classification
Input 
x=0.5
P(0|X) = 0.3
P(1|X) = 0.7
Compute
z
Function
&y = f(z)
Sigmoid function
Input 
x=0.5
P(0|X) = 0.3
P(2|X) = 0.4
Compute
z
Function
&y = f(z)
P(1|X) = 0.2
P(3|X) = 0.1
Class: 1
Class: 2
Softmax Function
2 â€“ Softmax Regression
!
19
Softmax Function
ğ‘§" = 1.0
ğ‘§# = 3.0
Softmax
ğ‘“(ğ‘§") = 0.12
ğ‘“(ğ‘§#) = 0.88
Input
Probability
ğ‘§" = 1.0
ğ‘§# = 2.0
Softmax
ğ‘“(ğ‘§") = 0.09
ğ‘“(ğ‘§#) = 0.24
Input
Probability
ğ‘§' = 3.0
ğ‘“(ğ‘§') = 0.67
ğ‘ƒ! = ğ‘“ğ‘§! =
ğ‘’"=
âˆ‘# ğ‘’">
0 â‰¤ğ‘“ğ‘§! â‰¤1
*
!
ğ‘“ğ‘§! = 1
1
ğ‘¥
ğ‘§%
ğ‘§"
Sigmoid
)y
2 â€“ Softmax Regression
!
20
Parameters
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
ğ‘¥
ğ‘§%
ğ‘§"
ğ‘§!
Softmax
)y' = ğ‘ƒğ‘¦= 0
)y% = ğ‘ƒğ‘¦= 1
Logistic Regression
Softmax Regression
ğ°
b
ğ°ğŸ
bğŸ
wğŸ
bğŸ
1
ğ‘¥
ğ‘§%
ğ‘§"
Sigmoid
)y
2 â€“ Softmax Regression
!
21
Loss Function
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
ğ‘¥
ğ‘§%
ğ‘§"
ğ‘§!
Softmax
)y' = ğ‘ƒğ‘¦= 0
)y% = ğ‘ƒğ‘¦= 1
ğ‘¦
y = 0
Logistic Regression
How to 
compute loss?
ğ‘¦
y = 0
Softmax Regression
ğ¿(ğœ½) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
w
b
w*
b*
w"
b"
2 â€“ Softmax Regression
!
22
One-Hot Encoding
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
ğ’š=
ğ‘¦.
â€¦
ğ‘¦/
ğ‘¦0 âˆˆ0,1
F
0
ğ‘¦0 = 1
C = #classes
ğ‘¦= 0 â†’ğ’š= 1
0
ğ‘¦= 1 â†’ğ’š= 0
1
Hours
Score
0.5
0
1.5
1
3.0
2
Classes: {0, 1, 2}
Multi-class Classification
#feature: 1
#class: 3
ğ‘¦= 0 â†’ğ’š=
1
0
0
ğ‘¦= 1 â†’ğ’š=
0
1
0
ğ‘¦= 2 â†’ğ’š=
0
0
1
1
ğ‘¥
ğ‘§%
ğ‘§"
Sigmoid
)y
2 â€“ Softmax Regression
!
23
Loss Function
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
ğ‘¥
ğ‘§%
ğ‘§"
ğ‘§!
Softmax
)y' = ğ‘ƒğ‘¦= 0
)y% = ğ‘ƒğ‘¦= 1
ğ‘¦
y = 0
Logistic Regression
One-Hot 
Encoding
ğ‘¦
y = 0
Softmax Regression
L(ğ›‰) = âˆ’ylog&yâˆ’(1âˆ’y)log(1âˆ’&y )
w
b
w*
b*
w"
b"
ğ‘¦' = 1
L(ğ›‰) = âˆ’ğ‘¦"log(&y")âˆ’ğ‘¦*log(&y*)
ğ‘¦% = 0
= âˆ’O
+
ğ‘¦+log(&y,)
2 â€“ Softmax Regression
!
24
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
0.5]
Data #1
ğ’š= 0
ğœ‚= 0.1
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
ğœƒ= [0.1
0.2
0.3
0.4]
One-hot encoding for label
ğ‘¦= 0 â†’ğ’š! = [1 0]
ğ‘¦= 1 â†’ğ’š! = [0 1]
2 â€“ Softmax Regression
!
25
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.25
ğ’›! = 0.4
=ğ’š% = 0.46
=ğ’š! = 0.54
ğ’š= 1
0
2 â€“ Softmax Regression
!
26
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.25
ğ’›! = 0.4
=ğ’š% = 0.46
=ğ’š! = 0.55
ğ’š= 1
0
ğ¿= 0.77
2 â€“ Softmax Regression
!
27
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.25
ğ’›! = 0.4
=ğ’š% = 0.46
=ğ’š! = 0.54
ğ’š= 1
0
ğ¿= 0.77
âˆ‡!L = ğ±)y âˆ’ğ‘¦*
= âˆ’0.54
0.54
âˆ’0.27
0.27
2 â€“ Softmax Regression
!
28
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.25
ğ’›! = 0.4
=ğ’š% = 0.46
=ğ’š! = 0.54
ğ’š= 1
0
ğ¿= 0.77
âˆ‡!L = ğ±)y âˆ’ğ‘¦*
= âˆ’0.54
0.54
âˆ’0.27
0.27
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
#
= 0.105
0.194
0.302
0.397
2 â€“ Softmax Regression
!
29
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.302
0.105
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.397
0.194
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.257
ğ’›! = 0.393
=ğ’š% = 0.466
=ğ’š! = 0.534
ğ’š= 1
0
ğ¿= 0.763
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
#
= 0.105
0.194
0.302
0.397
2 â€“ Softmax Regression
!
30
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
1.0]
Data #1
ğ’š= 0
ğœ‚= 0.1
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
ğœƒ= [0.1
0.2
0.3
0.4]
One-hot encoding for label
ğ‘¦= 0 â†’ğ’š! = [1 0]
ğ‘¦= 1 â†’ğ’š! = [0 1]
2 â€“ Softmax Regression
!
31
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ Softmax Regression
!
32
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ Softmax Regression
!
33
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
1.0]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.4
ğ’›! = 0.6
=ğ’š% = 0.45
=ğ’š! = 0.55
ğ’š= 1
0
2 â€“ Softmax Regression
!
34
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ Softmax Regression
!
35
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
1.0]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.4
ğ’›! = 0.6
=ğ’š% = 0.45
=ğ’š! = 0.55
ğ’š= 1
0
ğ¿= 0.8
2 â€“ Softmax Regression
!
36
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ Softmax Regression
!
37
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
1.0]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.4
ğ’›! = 0.6
=ğ’š% = 0.45
=ğ’š! = 0.55
ğ’š= 1
0
ğ¿= 0.8
âˆ‡!L = ğ±)y âˆ’ğ‘¦*
= âˆ’0.55
0.55
âˆ’0.55
0.55
2 â€“ Softmax Regression
!
38
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ Softmax Regression
!
39
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b*
b"
w*
w"]
ğ’™! = [1
1.0]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š! = [1 0]
L = âˆ’ğ‘¦.log!ğ‘¦. âˆ’ğ‘¦1log!ğ‘¦1
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.3
0.1
ğ‘¦
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.4
0.2
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
ğ’›% = 0.4
ğ’›! = 0.6
=ğ’š% = 0.45
=ğ’š! = 0.55
ğ’š= 1
0
ğ¿= 0.8
âˆ‡!L = ğ±)y âˆ’ğ‘¦*
= âˆ’0.55
0.55
âˆ’0.55
0.55
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
#
= 0.105
0.195
0.305
0.395
2 â€“ Softmax Regression
!
40
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output )ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š(ğ‘™ğ‘œğ‘”Eğ’š
âˆ‡!L = ğ’™)ğ²âˆ’ğ’š(
ğœ½= ğœ½âˆ’ğœ‚âˆ‡!L
ğœ‚is learning rate
ğ’›= ğœ½(ğ’™
Eğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
2 â€“ Softmax Regression
!
41
Prediction
Hours
Pass
0.25
???
4.5
???
Prediction
ğ‘¦*+,-: 0
ğ‘§* = ğ‘¤*ğ‘¥+ ğ‘*
0.305
0.105
ğ‘§" = ğ‘¤"ğ‘¥+ ğ‘"
0.394
0.194
&ğ‘¦* =
ğ‘’%!
âˆ‘+-*
"
ğ‘’%"
&ğ‘¦" =
ğ‘’%#
âˆ‘+-*
"
ğ‘’%"
ğ‘¤%
ğ‘%
ğ‘¤!
ğ‘!
2 â€“ Softmax Regression
!
42
Prediction
Thanks!
Any questions?
43
