1
Text Classification with Mamba - Project
Year 2023
TA Minh-Duc Bui
STA Khai-Xuan Trinh
2
Outline
1. Motivation
2. State Space Models
3. Mamba
4. Coding
3
Outline
1. Motivation
2. State Space Models
3. Mamba
4. Coding
4
RNNs vs. Transformers
RNNs
â€¢
train stage: ğ‘¶(ğ‘µ), non-parallelable
â€¢
inference stage: ğ‘¶(ğ‘µ)
â€¢
ğ‘¶ğŸper token
Transformers
â€¢
train stage: ğ‘¶(ğ‘µğŸ), parallelable
â€¢
inference stage: ğ‘¶ğ‘µğŸ
â€¢
ğ‘¶ğ‘µper token
Ideal Models
â€¢
train stage: ğ‘¶(ğ‘µ), parallelable
â€¢
inference stage: ğ‘¶(ğ‘µ)
â€¢
ğ‘¶ğŸper token
5
RNNs vs. Transformers
Mamba Encoder
Transformer Encoder
6
Mamba Block
Norm
Multi-Head
Attention
Norm
MLP
+
+
L x
Linear
Linear
SSM
Conv
Ã—
Linear
ğœ
ğœ
L x
7
Mamba Architecture
Tokenizer
Mamba Encoder
MLP 
Head
Class
Positive
Negative
tÃ´i
há»c
AI
táº¡i
AI
VN
8
Outline
1. Motivation
2. State Space Models
3. Mamba
4. Coding
9
State Space Models (SSMs)
What
The SSMs are traditionally used
in control theory to model a
dynamic
system
via
state
variables.
How
Why
â€¢
train stage: ğ‘¶(ğ‘µ), parallelable
â€¢
inference stage: ğ‘¶(ğ‘µ)
â€¢
ğ‘¶ğŸper token
Benefits
â€¢
suitable for long-range tasks
â€¢
faster training and inferencing
â€¢
low computational cost
â€¢
less memory
10
Discretize SSMs
Discretize
Continuous
Recurrent
11
Discretize SSMs
1
2
when     is small
3
12
Example of Recurrent Representation
Recurrent Representation
13
Example of Recurrent Representation
â„0 =
Ã—
+
Ã—
â„âˆ’1
tÃ´i
ğ‘Ã— ğ·
ğ‘Ã— ğ·
ğ‘Ã— 1
1 Ã— ğ·
ğ‘Ã— ğ‘
ğ‘Ã— ğ·
ğ‘¦0 =
Ã—
1 Ã— ğ‘
ğ‘Ã— ğ·
â„0
=
1 Ã— ğ·
=
ğ‘Ã— ğ·
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
ğ‘¥
input: ğ¿Ã— ğ·
14
Example of Recurrent Representation
3 Ã— 1
3 Ã— 3
1 Ã— 3
2
3
1
1
2
1
2
3
2
3
4
1
1
3
5
tÃ´i
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: 3 Ã— 4
1
2
1
4
5
1
7
3
4
3
2
4
3 Ã— 1
3 Ã— 3
1 Ã— 3
1
5
3
0.5
1
6
1
2
3
2
5
4
1
3
5
ğ´
ğµ
1
15
Example of Recurrent Representation
â„0 =
Ã—
+
Ã—
â„âˆ’1
tÃ´i
3 Ã— 4
3 Ã— 4
3 Ã— 1
1 Ã— 4
3 Ã— 3
3 Ã— 4
ğ‘¦0 =
Ã—
1 Ã— 3
3 Ã— 4
â„0
=
1 Ã— 4
=
3 Ã— 4
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: 3 Ã— 4
1
2
1
4
5
1
7
3
4
3
2
4
1
2
1
4
ğ‘¥0 (ğ‘¡Ã´ğ‘–)
2
3
1
1
2
1
2
3
2
3
4
1
0
0
0
0
0
0
0
0
0
0
0
0
2
4
2
8
3
6
3
12
1
2
1
4
2
4
2
8
3
6
3
12
1
2
1
4
1
3
5
16
32
16
64
16
Example of Recurrent Representation
â„1 =
Ã—
+
Ã—
â„0
tÃ´i
3 Ã— 4
3 Ã— 4
3 Ã— 1
1 Ã— 4
3 Ã— 3
3 Ã— 4
ğ‘¦1 =
Ã—
1 Ã— 3
3 Ã— 4
â„1
=
1 Ã— 4
=
3 Ã— 4
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: 3 Ã— 4
1
2
1
4
5
1
7
3
4
3
2
4
5
1
7
3
ğ‘¥1 (Ä‘ğ‘–)
2
3
1
1
2
1
2
3
2
3
4
1
2
4
2
8
3
6
3
12
1
2
1
4
19
20
23
42
30
31
36
69
24
39
26
79
19
20
23
42
30
31
36
69
24
39
26
79
1
3
5
229
308
261
644
17
Example of Recurrent Representation
tÃ´i
Ä‘i
há»c
1
2
1
4
5
1
7
3
4
3
2
4
SSM Block
16
32
16
64
5
1
7
3
229
308
261
644
...
18
Discretize SSMs
Recurrent Representation
Learnable Params
â€¢
ğ´, ğµ, ğ¶, ğ·,
19
Convolutional Representation of SSMs
t=0
t=1
t=2
t=k
...
...
20
Convolutional Representation of SSMs
t=k
21
Example of Convolutional Representation
ğ‘ªà´¥ğ‘¨ğŸà´¥ğ‘©
ğ‘ªà´¥ğ‘¨à´¥ğ‘©
ğ‘ªà´¥ğ‘©
tÃ´i
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: ğ¿Ã— ğ·
ğŸ
ğŸ
ğ‘¥0
ğ‘¥1
ğ‘¥2
Kernel
Input
Output
Padding
22
Example of Convolutional Representation
ğ‘ªà´¥ğ‘¨ğŸà´¥ğ‘©
ğ‘ªà´¥ğ‘¨à´¥ğ‘©
ğ‘ªà´¥ğ‘©
tÃ´i
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: ğ¿Ã— ğ·
ğŸ
ğŸ
ğ‘¥0
ğ‘¥1
ğ‘¥2
ğ‘¦0
Kernel
Input
Output
Step 1
ğ‘¦0 = ğ‘ªà´¥ğ‘©ğ‘¥0
23
Example of Convolutional Representation
ğ‘ªà´¥ğ‘¨ğŸà´¥ğ‘©
ğ‘ªà´¥ğ‘¨à´¥ğ‘©
ğ‘ªà´¥ğ‘©
tÃ´i
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: ğ¿Ã— ğ·
ğŸ
ğŸ
ğ‘¥0
ğ‘¥1
ğ‘¥2
ğ‘¦0
ğ‘¦1
Kernel
Input
Output
Step 2
ğ‘¦1 = ğ‘ªğ‘¨ğ‘©ğ‘¥0 + ğ‘ªà´¥ğ‘©ğ‘¥1
24
Example of Convolutional Representation
ğ‘ªà´¥ğ‘¨ğŸà´¥ğ‘©
ğ‘ªà´¥ğ‘¨à´¥ğ‘©
ğ‘ªà´¥ğ‘©
tÃ´i
ğ¿: sequence len
ğ·: hidden state
ğ‘: SSM dimension
Ä‘i
há»c
input: ğ¿Ã— ğ·
ğŸ
ğŸ
ğ‘¥0
ğ‘¥1
ğ‘¥2
ğ‘¦0
ğ‘¦1
ğ‘¦2
Kernel
Input
Output
Step 3
ğ‘¦2 = ğ‘ªà´¥ğ‘¨ğŸà´¥ğ‘©ğ‘¥0 + ğ‘ªğ‘¨ğ‘©ğ‘¥1 + ğ‘ªà´¥ğ‘©ğ‘¥2
25
Example of Convolutional Representation
worker 1
worker 2
worker 3
ğ‘¦0
ğ‘¦1
ğ‘¦2
Can be computed in parallel!
26
SSMs is Ideal Models
Recurrent
Convolutional
27
SSMs is Ideal Models
28
The importance of the A matrix
Convolutional
SSMs still have some of the same issues as
RNNs, like vanishing/exploding gradients.
Recurrent
solution
The performance in the Sequencial MNIST
increased from 60% to 98%.
29
30
Outline
1. Motivation
2. State Space Models
3. Mamba
4. Coding
31
Mamba Motivation
ğ‘„= ğ‘‹ğ‘Šğ‘„
ğ‘Šğ¾
ğ‘Šğ‘‰
ğ‘‹= {ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4} Ã—
Ã—
Ã—
ğ‘Šğ‘„
ğ¾= ğ‘‹ğ‘Šğ¾
ğ‘‰= ğ‘‹ğ‘Šğ‘‰
ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›ğ‘„, ğ¾, ğ‘‰= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥
ğ‘„ğ¾ğ‘‡
ğ‘‘ğ‘˜
ğ‘‰
A step back: Attention in Transformer
32
Mamba Motivation
A step back: S4
33
Mamba Contribution #1: Selection Mechanism
34
Mamba Contribution #1: Selection Mechanism
Compare S4 and Mamba (S6)
35
S4
Mamba
Mamba Contribution #1: Selection Mechanism
36
Convolutional
Mamba becomes time-varying
Mamba Contribution #1: Selection Mechanism
37
How does GPU work?
Compute
SRAM
Compute
SRAM
...
HBM
Streaming Multiprocessors
GPU 
SRAM
GPU HBM
Main Memory
(CPU DRAM)
SRAM: 19 TB/s (20 MB)
HBM: 1.5 TB/s (40 GB)
DRAM: 12.8 GB/s (>1 TB)
Memory Hierarchy with
Bandwidth & Memory Size
GPU
38
How does GPU work?
Blog: Making Deep Learning Go Brrrr From First Principles
Memory & Compute
39
How does GPU work?
Blog: Making Deep Learning Go Brrrr From First Principles
Memory & Compute
40
How does GPU work?
Blog: Making Deep Learning Go Brrrr From First Principles
NVIDIA A100 Tensor Core GPU
Memory & Compute
41
Mamba Contribution #2: Hardware-aware Algorithm
Kernel Fusion
Blog: Making Deep Learning Go Brrrr From First Principles
42
Mamba Contribution #2: Hardware-aware Algorithm
Recomputation
43
Mamba Contribution #2: Hardware-aware Algorithm
9
6
7
10
8
7
9
15
22
32
40
47
ğ‘¥0
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5
â„0
â„1
â„2
â„3
â„4
â„5
Initial array
Pre-fix sum
Model input
Scan output
Prefix-sum
Parallel Scan
Recurrent
Parallel Scan
44
Mamba Contribution #2: Hardware-aware Algorithm
Parallel Scan
Mamba Encoder
Transformer Encoder
45
Mamba Block
Norm
Multi-Head
Attention
Norm
MLP
+
+
L x
Linear
Linear
SSM
Conv
Ã—
Linear
ğœ
ğœ
L x
46
Mamba Architecture
Tokenizer
Mamba Encoder
MLP 
Head
Class
Positive
Negative
tÃ´i
há»c
AI
táº¡i
AI
VN
47
Mamba Arch. & Transformer Arch.
Mamba
ViT
48
Outline
1. Motivation
2. State Space Models
3. Mamba
4. Coding
Custom Trainer
(MambaTrainer)
Mamba Model
(MambaTextClassification)
49
Coding
Mamba Backbone
Mamba Head
(MambaClassificationHead)
Custom Trainer
(MambaTrainer)
Mamba Model
(MambaTextClassification)
50
Coding
Mamba Backbone
Mamba Head
(MambaClassificationHead)
Custom Trainer
(MambaTrainer)
Mamba Model
(MambaTextClassification)
51
Coding
Mamba Backbone
Mamba Head
(MambaClassificationHead)
Custom Trainer
(MambaTrainer)
Mamba Model
(MambaTextClassification)
52
Coding
Mamba Backbone
Mamba Head
(MambaClassificationHead)
53
Coding
Custom Trainer
(MambaTrainer)
Mamba Model
(MambaTextClassification)
Mamba Backbone
Mamba Head
(MambaClassificationHead)
54
Summary
Thanks!
Any questions?
55
