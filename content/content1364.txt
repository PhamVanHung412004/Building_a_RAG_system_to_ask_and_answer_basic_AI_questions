Module for normalization .
43 """
44 return GroupNorm32 (32 , channels )
45
46
47
48 def checkpoint (func , inputs , params , flag ):
49 """
50 Evaluate a function without caching intermediate activations , allowing for
51 reduced memory at the expense of extra compute in the backward pass .
52
53 : param func : the function to evaluate .
54 : param inputs : the argument sequence to pass to ‘func ‘.
55 : param params : a sequence of parameters ‘func ‘ depends on but does not
56 explicitly take as arguments .
57 : param flag : if False , disable gradient checkpointing .
58 """
59 if flag :
60 args = tuple ( inputs ) + tuple ( params )
61 return CheckpointFunction . apply (func , len ( inputs ), * args )
62 else :
63 return func (* inputs )
64
65
66 class CheckpointFunction ( torch . autograd . Function ):
67 @staticmethod
68 def forward (ctx , run_function , length , * args ):
69 ctx . run_function = run_function
70 ctx . input_tensors = list ( args [: length ])
71 ctx . input_params = list ( args [ length :])
72 with torch . no_grad ():
73 output_tensors = ctx . run_function (* ctx . input_tensors )
74 return output_tensors
75
76 @staticmethod
77 def backward (ctx , * output_grads ):
78 ctx . input_tensors = [x. detach (). requires_grad_ ( True ) for x in ctx .
input_tensors ]
79 with torch . enable_grad ():
80 # Fixes a bug where the first op in run_function modifies the
81 # Tensor storage in place , which is not allowed for detach () ’d
82 # Tensors .
83 shallow_copies = [x. view_as (x) for x in ctx . input_tensors ]
84 output_tensors = ctx . run_function (* shallow_copies )
85 input_grads = torch . autograd . 