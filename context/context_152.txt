Multilayer Perceptron
Year 2023
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
CONTENT
(1) â€“ Background
(2) â€“ Multilayer Perceptron
(3) â€“ Classification Application
1 - Background
!
3
Sigmoid Function
ğ‘§
ğœ
ğœ(ğ‘§) =
1
1 + ğ‘’!"
ğ‘§âˆˆâˆ’âˆ
+ âˆ
ğœ(z) âˆˆ0
1
Sigmoid function
Threshold
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
Hours
Score
0.5
0
1.0
0
1.5
1
2.0
1
2.5
2
3.0
2
3.5
3
4.0
3
Classes: {0, 1}
Binary Classification
Classes: {0, 1, 2, 3}
Multi-class Classification
1 - Background
!
4
Softmax Function
ğ‘§! = 1.0
ğ‘§" = 3.0
Softmax
ğ‘“(ğ‘§!) = 0.12
ğ‘“(ğ‘§") = 0.88
Input
Probability
ğ‘§! = 1.0
ğ‘§" = 2.0
Softmax
ğ‘“(ğ‘§!) = 0.09
ğ‘“(ğ‘§") = 0.24
Input
Probability
ğ‘§# = 3.0
ğ‘“(ğ‘§#) = 0.67
ğ‘ƒ! = ğ‘“ğ‘§! =
ğ‘’"!
âˆ‘# ğ‘’""
0 â‰¤ğ‘“ğ‘§! â‰¤1
*
!
ğ‘“ğ‘§! = 1
1 - Background
!
5
Softmax Regression
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
ğ‘¥
ğ‘§!
ğ‘§"
ğ‘§#
Softmax
#y! = ğ‘ƒğ‘¦= 0
#y" = ğ‘ƒğ‘¦= 1
Softmax Regression
ğ°ğŸ
bğŸ
wğŸ
bğŸ
1 - Background
!
6
Softmax Regression
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
ğ’š=
ğ‘¦#
â€¦
ğ‘¦$
ğ‘¦% âˆˆ0,1
/
%
ğ‘¦% = 1
C = #classes
ğ‘¦= 0 â†’ğ’š= 1
0
ğ‘¦= 1 â†’ğ’š= 0
1
Hours
Score
0.5
0
1.5
1
3.0
2
Classes: {0, 1, 2}
Multi-class Classification
#feature: 1
#class: 3
ğ‘¦= 0 â†’ğ’š=
1
0
0
ğ‘¦= 1 â†’ğ’š=
0
1
0
ğ‘¦= 2 â†’ğ’š=
0
0
1
One-hot Encoding
1
ğ‘¥
ğ‘§!
ğ‘§"
Sigmoid
#y
1 - Background
!
7
Softmax Regression
Hours
Pass
0.5
0
2.0
1
Classes: {0, 1}
Binary Classification
#feature: 1
#class: 2
1
ğ‘¥
ğ‘§!
ğ‘§"
ğ‘§#
Softmax
#y! = ğ‘ƒğ‘¦= 0
#y" = ğ‘ƒğ‘¦= 1
ğ‘¦
y = 0
Logistic Regression
One-Hot 
Encoding
ğ‘¦
y = 0
Softmax Regression
L(ğ›‰) = âˆ’ylog:yâˆ’(1âˆ’y)log(1âˆ’:y )
w
b
w&
b&
w!
b!
ğ‘¦! = 1
L(ğ›‰) = âˆ’ğ‘¦!log(:y!)âˆ’ğ‘¦&log(:y&)
ğ‘¦" = 0
= âˆ’<
'
ğ‘¦'log(:y()
1 - Background
!
8
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output #ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š#ğ‘™ğ‘œğ‘”1ğ’š
âˆ‡$L = ğ’™#ğ²âˆ’ğ’š#
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚is learning rate
ğ’›= ğœ½#ğ’™
1ğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b&
b!
w&
w!]
ğ’™) = [1
0.5]
Data #1
ğ’š= 0
ğœ‚= 0.1
Hours
Pass
0.5
0
1.0
0
1.5
1
2.0
1
ğœƒ= [0.1
0.2
0.3
0.4]
One-hot encoding for label
ğ‘¦= 0 â†’ğ’š) = [1 0]
ğ‘¦= 1 â†’ğ’š) = [0 1]
1 - Background
!
9
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output #ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š#ğ‘™ğ‘œğ‘”1ğ’š
âˆ‡$L = ğ’™#ğ²âˆ’ğ’š#
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚is learning rate
ğ’›= ğœ½#ğ’™
1ğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b&
b!
w&
w!]
ğ’™) = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š) = [1 0]
L = âˆ’ğ‘¦#log7ğ‘¦# âˆ’ğ‘¦&log7ğ‘¦&
ğ‘§& = ğ‘¤&ğ‘¥+ ğ‘&
0.3
0.1
ğ‘¦
ğ‘§! = ğ‘¤!ğ‘¥+ ğ‘!
0.4
0.2
:ğ‘¦& =
ğ‘’*!
âˆ‘'+&
!
ğ‘’*"
:ğ‘¦! =
ğ‘’*#
âˆ‘'+&
!
ğ‘’*"
ğ‘¤!
ğ‘!
ğ‘¤#
ğ‘#
ğ’›! = 0.25
ğ’›# = 0.4
7ğ’š! = 0.46
7ğ’š# = 0.54
ğ’š= 1
0
ğ¿= 0.77
âˆ‡$L = ğ±#y âˆ’ğ‘¦&
= âˆ’0.54
0.54
âˆ’0.27
0.27
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
(
= 0.105
0.194
0.302
0.397
1 - Background
!
10
Softmax Regression
4) Compute derivative
5) Update parameters
1) Pick a sample from training data   
2) Compute output #ğ‘¦
3) Compute loss (cross-entropy)
ğ¿ğœ½= âˆ’ğ’š#ğ‘™ğ‘œğ‘”1ğ’š
âˆ‡$L = ğ’™#ğ²âˆ’ğ’š#
ğœ½= ğœ½âˆ’ğœ‚âˆ‡$L
ğœ‚is learning rate
ğ’›= ğœ½#ğ’™
1ğ’š= ğ‘’ğ’›âˆ…ğ’…
ğ’…= 1 â€¦ 1 ğ‘’ğ’›
âˆ…is 
Hadamard
division
ğœƒ= [b&
b!
w&
w!]
ğ’™) = [1
0.5]
ğœ‚= 0.1
= [0.1
0.2
0.3
0.4]
ğ‘¦= 0 â†’ğ’š) = [1 0]
L = âˆ’ğ‘¦#log7ğ‘¦# âˆ’ğ‘¦&log7ğ‘¦&
ğ‘§& = ğ‘¤&ğ‘¥+ ğ‘&
0.302
0.105
ğ‘¦
ğ‘§! = ğ‘¤!ğ‘¥+ ğ‘!
0.397
0.194
:ğ‘¦& =
ğ‘’*!
âˆ‘'+&
!
ğ‘’*"
:ğ‘¦! =
ğ‘’*#
âˆ‘'+&
!
ğ‘’*"
ğ‘¤!
ğ‘!
ğ‘¤#
ğ‘#
ğ’›! = 0.257
ğ’›# = 0.393
7ğ’š! = 0.466
7ğ’š# = 0.534
ğ’š= 1
0
ğ¿= 0.763
ğœ½= ğœ½âˆ’ğœ‚ğ¿ğœ½
(
= 0.105
0.194
0.302
0.397
1 - Background
!
11
Softmax Regression
Hours
Pass
0.25
???
4.5
???
Prediction
ğ‘¦'()*: 0
ğ‘§& = ğ‘¤&ğ‘¥+ ğ‘&
0.302
0.105
ğ‘§! = ğ‘¤!ğ‘¥+ ğ‘!
0.397
0.194
:ğ‘¦& =
ğ‘’*!
âˆ‘'+&
!
ğ‘’*"
:ğ‘¦! =
ğ‘’*#
âˆ‘'+&
!
ğ‘’*"
ğ‘¤!
ğ‘!
ğ‘¤#
ğ‘#
2 â€“ Multilayer Perceptron
!
12
Motivation
1
ğ‘¥
ğ‘§!
ğ‘§"
ğ‘§#
Softmax
#y! = ğ‘ƒğ‘¦= 0
#y" = ğ‘ƒğ‘¦= 1
w&
b&
w!
b!
1
ğ‘¥
ğ‘§!
ğ‘§"
Sigmoid
#y
w
b
Input Layer
Output Layer
2 â€“ Multilayer Perceptron
!
13
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
w&
b&
w!
b!
#parameters: 4
2 â€“ Multilayer Perceptron
!
14
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
#parameters: 12
2 â€“ Multilayer Perceptron
!
15
Multilayer Perceptron
Input Layer
Output Layer
Activation
1
1
Hidden Layer
#parameters: 12
2 â€“ Multilayer Perceptron
!
16
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
#y
2 â€“ Multilayer Perceptron
!
17
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
â„= [1.0 x]W, = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
#y
2 â€“ Multilayer Perceptron
!
18
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W) = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W+ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
#y
2 â€“ Multilayer Perceptron
!
19
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W) = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W+ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
#y = 0.5449
#y = Ïƒ z
= ğœ0.18
= 0.5449
2 â€“ Multilayer Perceptron
!
20
Forward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.1
0.1
0.1
0.1
0.1
0.1
W+ = W+
=
0.1
0.1
0.1
x = 1.0
2.0
y = 0
h = [1.0 x]W) = 1.0
1.0 2.0
0.1
0.1
0.1
0.1
0.1
0.1
= 0.4
0.4
z = [1.0 h]W+ = 1.0
0.4 0.4
0.1
0.1
0.1
= 0.18
#y = 0.5449
#y = Ïƒ z
= ğœ0.18
= 0.5449
L = 0.7872
2 â€“ Multilayer Perceptron
!
21
Backward
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
1.0
2.0
h1
h2
z
W) = W)"
W)*
=
0.0946
0.0946
0.0946
0.0891
0.0946
0.0891
W+ = W+
=
0.0455
0.0782
0.0782
x = 1.0
2.0
y = 0
#y = 0.5449
#y = Ïƒ z
= ğœ0.18
= 0.5449
L = 0.7872
2 â€“ Multilayer Perceptron
!
22
Activation
vSigmoid Function
2 â€“ Multilayer Perceptron
!
23
Activation
vSigmoid Function
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
Sigmoid
Sigmoid
2 â€“ Multilayer Perceptron
!
24
Activation
vReLU Function
2 â€“ Multilayer Perceptron
!
25
Activation
vReLU Function
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
ReLU
ReLU
2 â€“ Multilayer Perceptron
!
26
Activation
vTanh Function
2 â€“ Multilayer Perceptron
!
27
Activation
vTanh Function
Input Layer
Output Layer
Sigmoid
1
1
Hidden Layer
Tanh
Tanh
2 â€“ Multilayer Perceptron
!
28
Loss
vBCELoss()
2 â€“ Multilayer Perceptron
!
29
Loss
vCrossEntropyLoss()
With Softmax Funtion
2 â€“ Multilayer Perceptron
!
30
Optimizer
vSGD()
3 â€“ Classification using MLP
!
31
Pipeline
Train 
Dataset
Data
Preparation
Model
Feature Extraction
Normalization
Convert to Tensor
Parameter 
Initialization
Optimizer: SGD
Loss: CrossEntropyLoss
Metric: Accuracy
Trained
Model
Training
Test 
Dataset
Evaluation
Score: Accuracy
vIris Dataset
3 â€“ Classification using NN
!
32
Load Iris Dataset
v Load Iris Dataset from sklearn
v Train: Test = 0.7 : 0.3
3 â€“ Classification using NN
!
33
Data Preparation
v Normalization: StandardScaler()
v Convert to tensor
3 â€“ Classification using NN
!
34
Model
3 â€“ Classification using NN
!
35
Loss Function, Optimizer
3 â€“ Classification using NN
!
36
Training
3 â€“ Classification using NN
!
37
Evaluation
Thanks!
Any questions?
38
