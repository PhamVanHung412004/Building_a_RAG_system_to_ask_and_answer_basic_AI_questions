From Linear Regression
to Logistic Regression
Year 2023
Quang-Vinh Dinh
PhD in Computer Science
AI VIETNAM
All-in-One Course
➢Optimization Review
➢Linear Regression Review
➢Logistic Regression
➢Examples
➢Vectorization
➢Implementation (optional)
Outline
Optimization
❖ Gradient descent
1
AI VIETNAM
All-in-One Course
𝑑
𝑑𝑥𝐽(𝑥) = lim
∆𝑥→0
𝐽𝑥 + ∆𝑥 −𝐽(𝑥 )
∆𝑥 
𝐽
𝑥
𝑥+ ∆𝑥𝑥
J(x)
∆𝑥
∆𝐽
𝑥
𝐽
𝑥op
𝑥2
𝐝
𝐝𝐱 𝐉𝐱𝟐< 𝟎
𝐝
𝐝𝐱 𝐉𝐱𝟏> 𝟎
𝑥1
x𝑛𝑒𝑤= x𝑜𝑙𝑑− 𝜂d
dx J x𝑜𝑙𝑑
Derivate at x𝑜𝑙𝑑
learning rate
Optimization
𝐉𝜃
Khởi tạo giá trị 𝜽
d
d𝜃J 𝜃> 0
Dịch chuyển 𝜃 
về phía trái
𝜃 value
Di chuyển 𝜽 ngược hướng đạo hàm
d
d𝜃J 𝜃> 0
Dịch chuyển 𝜃 
về phía trái
d
d𝜃J 𝜃> 0
Dịch chuyển 𝜃 
về phía trái
𝜃 value
J 𝜃
𝜃 value
d
d𝜃J 𝜃< 0
Dịch chuyển 𝜃 
về phía phải
Cứ tiếp tục di chuyển ngược 
hướng đạo hàm
d
d𝜃J 𝜃> 0
Dịch chuyển 𝜃 về phía trái
J 𝜃
❖ Gradient descent
2
Optimization
AI VIETNAM
All-in-One Course
❖ Square function
Compute 
derivative at x
Move x 
opposite to dx
Initialize x
𝑓𝑥= 𝑥2
−100 ≤𝑥≤100
𝑥∈ℕ
𝑥𝑡= 𝑥𝑡−1 −𝜂𝑓′(𝑥𝑡−1)
3
Optimization
❖ Square function
𝑥0 = 99.0
𝜂= 0.1
𝑥𝑡= 𝑥𝑡−1 −𝜂𝑓′(𝑥)
𝑓𝑥= 𝑥2
4
Optimization
❖ Square function
𝑥0 = 99.0
𝜂= 0.001
𝑥𝑡= 𝑥𝑡−1 −𝜂𝑓′(𝑥)
𝑓𝑥= 𝑥2
5
Optimization
❖ Square function
𝑥0 = 99.0
𝜂= 0.8
𝑥𝑡= 𝑥𝑡−1 −𝜂𝑓′(𝑥)
𝑓𝑥= 𝑥2
6
Optimization
❖ Square function
𝑥0 = 99.0
𝜂= 1. 1
𝑥𝑡= 𝑥𝑡−1 −𝜂𝑓′(𝑥)
𝑓𝑥= 𝑥2
7
Optimization
AI VIETNAM
All-in-One Course
❖ Optimization: 2D function
−100 ≤𝑥, 𝑦≤100
𝑥, 𝑦∈ℕ
𝑓𝑥, 𝑦= 𝑥2 + 𝑦2
8
Derivative
AI VIETNAM
All-in-One Course
❖ Optimization: 2D function
−100 ≤𝑥, 𝑦≤100
𝑥, 𝑦∈ℕ
𝑓𝑥, 𝑦= 𝑥2 + 𝑦2
𝑥= 𝑥−𝜂𝜕𝑓(𝑥, 𝑦)
𝜕𝑥
𝑦= 𝑦−𝜂𝜕𝑓(𝑥, 𝑦)
𝜕𝑦
𝑥0 = 3.0
𝑦0 = 4.0
𝜕𝑓(𝑥0, 𝑦0)
𝜕𝑥
= 6.0
𝜕𝑓(𝑥0, 𝑦0)
𝜕y
= 8.0
𝑥1 = 2.0
𝑦1 = 3.0
𝜕𝑓(𝑥1, 𝑦1)
𝜕𝑥
= 4.0
𝜕𝑓(𝑥1, 𝑦1)
𝜕y
= 6.0
𝑥2 = 1.0
𝑦2 = 2.0
𝜕𝑓(𝑥2, 𝑦2)
𝜕𝑥
= 2.0
𝜕𝑓(𝑥2, 𝑦2)
𝜕y
= 4.0
𝑥3 = 0.0
𝑦3 = 1.0
𝜕𝑓(𝑥3, 𝑦3)
𝜕𝑥
= 0.0
𝜕𝑓(𝑥3, 𝑦3)
𝜕y
= 0.0
𝑥4 = 0.0
𝑦4 = 0.0
𝜂= 1.0
9
Optimization
❖ For composite function
AI VIETNAM
All-in-One Course
𝑓𝑥= 2𝑥−1
𝑔𝑓= 𝑓−3 2
𝑔𝑓𝑥
𝑥
𝑓
𝑑
𝑑𝑥𝑓𝑥
𝑥
𝑓
𝑔
𝑑
𝑑𝑓𝑔𝑓
𝑑
𝑑𝑥𝑓𝑥
𝑑
𝑑𝑥𝑔𝑓𝑥
=
𝑑
𝑑𝑓𝑔𝑓
∗
𝑑
𝑑𝑥𝑓𝑥
𝑥
𝑔
𝑓𝑥
𝑔𝑓
Optimization
❖ For composite function
AI VIETNAM
All-in-One Course
𝑥
𝑓
𝑑
𝑑𝑥𝑓𝑥
𝑥
𝑓
𝑔
𝑑
𝑑𝑓𝑔𝑓
𝑑
𝑑𝑥𝑓𝑥
𝑑
𝑑𝑥𝑔𝑓𝑥
=
𝑑
𝑑𝑓𝑔𝑓
∗
𝑑
𝑑𝑥𝑓𝑥
𝑓𝑥= 2𝑥−1
𝑔𝑓= 𝑓−3 2
𝑔𝑥= 2𝑥−1 −3 2
= 2𝑥−4 2
𝑔′ 𝑥= 4 2𝑥−4
= 8𝑥−16
11
Optimization
❖ For composite function and chain rule
AI VIETNAM
All-in-One Course
𝑥
𝑓
𝑑
𝑑𝑥𝑓𝑥
𝑥
𝑓
𝑔
𝑑
𝑑𝑓𝑔𝑓
𝑑
𝑑𝑥𝑓𝑥
𝑑
𝑑𝑥𝑔𝑓𝑥
=
𝑑
𝑑𝑓𝑔𝑓
∗
𝑑
𝑑𝑥𝑓𝑥
𝑓𝑥= 2𝑥−1
𝑔𝑓= 𝑓−3 2
𝑑𝑔
𝑑𝑥= 𝑑𝑔
𝑑𝑓
𝑑𝑓
𝑑𝑥
𝑓′ 𝑥= 2
𝑔′ 𝑓= 2 𝑓−3
= 2 𝑓−3 2
= 4 2𝑥−1 −3
= 8𝑥−16
12
➢Optimization Review
➢Linear Regression Review
➢Logistic Regression
➢Examples
➢Vectorization
➢Implementation (optional)
Outline
House 
Price 
Prediction
Feature
Label
House price data
price = w∗area + 𝑏
if area=6.0, price=7.28
price = 𝑤1 ∗area + 𝑏1
price = 𝑤2 ∗area + 𝑏2
price = 𝑤3 ∗area + 𝑏3
if area=6.0, price=?
Feature
Label
House price data
Linear Regression
AI VIETNAM
All-in-One Course
predicted_price = w ∗area + 𝑏
error = predicted_price −real_price 2
❖ Area-based house price prediction
𝐿(ො𝑦, 𝑦) = ො𝑦−𝑦2
ො𝑦= 𝑤𝑥+ 𝑏
w = -0.34
b = 0.04
14
w = 1.17
b = 0.26
Linear Regression
AI VIETNAM
All-in-One Course
❖ Area-based house price prediction
predicted_price = w ∗area + 𝑏
error = predicted_price −real_price 2
𝐿(ො𝑦, 𝑦) = ො𝑦−𝑦2
ො𝑦= 𝑤𝑥+ 𝑏
15
Linear Regression
❖ Area-based house 
     price prediction
w = -0.34
b = 0.04
w = 1.17
b = 0.26
𝐿(ො𝑦, 𝑦) =
ො𝑦−𝑦2
ො𝑦= 𝑤𝑥+ 𝑏
AI VIETNAM
All-in-One Course
How to change w and b 
so that 𝐿(ො𝑦, 𝑦) reduces
16
Linear Regression
❖ Understanding the loss function
AI VIETNAM
All-in-One Course
Different b values with a fixed w value
Different w values with a fixed b value
𝐿(ො𝑦, 𝑦) =
ො𝑦−𝑦2
ො𝑦= 𝑤𝑥+ 𝑏
How to change w and b 
so that 𝐿(ො𝑦, 𝑦𝑖) reduces
17
Linear Regression
AI VIETNAM
All-in-One Course
Error (loss) computation
Idea: compare predicted values ො𝑦 and label values y 
Squared loss
L(ො𝑦, 𝑦) = (ො𝑦−𝑦)2
Linear equation
ො𝑦= 𝑤𝑥+ 𝑏
where ො𝑦 is a predicted value,
            𝑤 and 𝑏 are parameters
            and 𝑥 is input feature
Compute output ො𝑦
Compute loss
Compute derivative 
for each parameter
Update parameters
Training 
data
Pick a sample 
(x, y)
x=area and y=price
Initialize 𝑤 and 𝑏
18
Use gradient descent to minimize the loss function
𝜕𝐿
𝜕𝑤= 𝜕𝐿
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑤= 2𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 𝜕𝐿
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑏= 2(ො𝑦−𝑦)
Compute derivate for each parameter
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
Update parameters
is learning rate
𝜂
Linear Regression
AI VIETNAM
All-in-One Course
Find better w and b
Error (loss) computation
Idea: compare predicted values ො𝑦 and label values y 
Squared loss
L(ො𝑦, 𝑦) = (ො𝑦−𝑦)2
Linear equation
ො𝑦= 𝑤𝑥+ 𝑏
where ො𝑦 is a predicted value,
            𝑤 and 𝑏 are parameters
            and 𝑥 is input feature
Linear Regression
❖ Example
Compute output ො𝑦
Compute loss
Compute derivative 
for each parameter
Update parameters
Training 
data
Pick a sample 
(x, y)
x=area and y=price
Initialize 𝑤 and 𝑏
Feature
Label
Model
Input
Label
Loss
Parameters
𝑥
𝑤
𝑏
ො𝑦= 𝑤𝑥+ 𝑏
(ො𝑦−𝑦)2
𝑦
20
Given
sample
data
Model
𝑥 = 6.7
𝑏= 0.04
w = -0.34
𝑦 = 9.1
ො𝑦= 𝑥𝑤+ 𝑏 = -2.238 
Input
Label
Loss
ො𝑦−𝑦2 = 128.5
Parameters
Forward 
propagation
House price prediction
Linear Regression
AI VIETNAM
All-in-One Course
Initialize 
b=0.04 and 
w=-0.34
Feature
Label
1
21
Model
𝑥 = 6.7
𝑏= 0.26676
𝑤 = 1.17929
𝑦 = 9.1
ො𝑦= 𝑥𝑤+ 𝑏 = -2.238 
Input
Label
Loss
Parameters
Backpropagation
ො𝑦−𝑦2 = 128.5
𝜕𝐿
𝜕𝑤= 2𝑥ෝ𝑦−𝑦
 
= −151.9292
𝜕𝐿
𝜕𝑏= 2 ෝ𝑦−𝑦
 
= −22.676
𝜂= 0.01
New w and b help 
the loss reduce
Forward 
propagation
Model
𝑥 = 6.7
𝑦 = 9.1
ො𝑦= 𝑥𝑤+ 𝑏 = -2.238 
Input
Label
Loss
Parameters
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
ො𝑦−𝑦2 = 0.868
𝑏= 0.26676
𝑤 = 1.17929
w = w −𝜂𝜕𝐿
𝜕𝑤
Linear Regression
AI VIETNAM
All-in-One Course
3
2
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
w = w −𝜂𝜕𝐿
𝜕𝑤
w = -0.34
b = 0.04
L = 128.55
w = 1.179292 b = 0.26676
L = 0.868
Model prediction before and after the first update
Before updating
After updating
Linear Regression
AI VIETNAM
All-in-One Course
❖ Toy example
23
Linear Regression
AI VIETNAM
All-in-One Course
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
ො𝑦= 𝑤𝑥+ 𝑏
𝐿(ො𝑦, 𝑦) = (ො𝑦−𝑦)2
𝜕𝐿
𝜕𝑤= 2𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= 2(ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
❖ Summary (one feature and one sample)
Model
Input
Label
Loss
Parameters
𝑥
𝑤
𝑏
ො𝑦= 𝑤𝑥+ 𝑏
(ො𝑦−𝑦)2
𝑦
24
➢Optimization Review
➢Linear Regression Review
➢Logistic Regression
➢Examples
➢Vectorization
➢Implementation (optional)
Outline
Idea of Logistic Regression
ො𝑦= 𝜽𝑇𝒙= 𝑤𝑥+ 𝑏
ො𝑦∈−∞ 
+ ∞ 
❖ Linear regression
Find the line ො𝑦= 𝜽𝑇𝒙 that is best fitting to given 
data, then use ො𝑦 to predict for new data 
Feature
Label
Area-based House Price Data
𝒙
𝒚
error
error
error
error
Training data
Model
construct
AI VIETNAM
All-in-One Course
25
Idea of Logistic Regression
❖ Given a new kind of data
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
Assign numbers 
to categories
Feature
Feature
Category 0
Category 1
Plot data
A line is not suitable 
for this data
AI VIETNAM
All-in-One Course
26
Idea of Logistic Regression
𝜎(𝑢) =
1
1 + 𝑒−𝑧
𝑧∈−∞ 
+ ∞ 
𝜎(𝑢) ∈0 1 
∀𝑧1𝑧2 ∈𝑎 𝑏 and 𝑧1 ≤𝑧2
→𝜎(𝑧1) ≤𝑧(𝑢1)
𝑧
+∞
−∞
𝑧1
𝑧2
𝜎
𝜎1
𝜎2
Sigmoid function
Property
AI VIETNAM
All-in-One Course
27
Idea of Logistic Regression
𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝑧= 𝑤𝑥+ 𝑏
𝑧∈−∞ 
+ ∞ 
𝜎(𝑧) ∈0 1 
𝑥
𝑧
𝑥
𝑧
𝑧
𝜎
𝑧
𝜎
AI VIETNAM
All-in-One Course
Idea of Logistic Regression
𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝑧= 𝑤𝑥+ 𝑏
𝑧∈−∞ 
+ ∞ 
𝜎(𝑧) ∈0 1 
𝑥
𝑧
𝑥
𝑧
𝑧
𝜎
𝑧
𝜎
AI VIETNAM
All-in-One Course
Feature
Label
Category 0
Category 1
Idea of Logistic Regression
𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝜎(𝑧) ∈0 1 
𝑧= 0.535 ∗𝑥−0.654
𝑥
𝜎
𝑧
𝜎(𝑧)
AI VIETNAM
All-in-One Course
30
Feature
Label
Category 0
Category 1
Idea of Logistic Regression
𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝜎(𝑧) ∈0 1 
𝑧= 2.331 ∗𝑥−5.156
𝑥
𝜎
𝑧
𝜎(𝑧)
AI VIETNAM
All-in-One Course
31
Feature
Label
Category 0
Category 1
Idea of Logistic Regression
𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝜎(𝑧) ∈0 1 
𝑧(𝑥)
𝑥
AI VIETNAM
All-in-One Course
32
How to evaluate the performance of a model?
𝜎(𝑧)
𝑦= 𝑒𝑥
𝑦= 2𝑥
𝑦=
1
2
𝑥
𝑦=
1
6
𝑥
❖ Suggested Functions
𝑦= log(𝑥)
𝑦= −log(𝑥)
𝑦= log(1 −𝑥)
𝑦= −log(1 −𝑥)
𝑦= 𝑥2
Idea of Logistic Regression
❖ Loss function
AI VIETNAM
All-in-One Course
𝐿(ො𝑦) = −log(1 −ො𝑦)
if y = 0 
𝐿(ො𝑦) = −log(ො𝑦)
if y = 1 
-log(ො𝑦)
with y = 1
𝑒𝑟𝑟𝑜𝑟
ො𝑦
-log(1-ො𝑦)
with y = 0
𝑒𝑟𝑟𝑜𝑟
ො𝑦
How to 
remove if?
Feature
Label
Category 0
Category 1
𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝜎(𝑧) ∈0 1 
Idea of Logistic Regression
❖ Loss function
AI VIETNAM
All-in-One Course
Feature
Label
Output
-log(1-ො𝑦)
with y = 0
-log(ො𝑦)
with y = 1
𝐿(ො𝑦) = −log(ො𝑦)
if y = 1 
𝐿(ො𝑦) = −log(1 −ො𝑦)
if y = 0 
ො𝑦
ො𝑦
L(y, ො𝑦) = −𝑦logො𝑦−1 −𝑦log 1 −ො𝑦
Binary cross-entropy
𝑒𝑟𝑟𝑜𝑟
𝑒𝑟𝑟𝑜𝑟
Introduce the loss function in another way
Idea of Logistic Regression
❖ Given a new kind of data
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
Assign numbers 
to categories
Sigmoid function 
could fit the data
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
ො𝑦∈0 1 
1
1 + 𝑒−𝜽𝑇𝒙 
Feature
Error
if 𝑦= 1
error = 1 −ො𝑦
if 𝑦= 0
error = ො𝑦
error =1-ො𝑦
error = ො𝑦
Feature
For some 𝜽
AI VIETNAM
All-in-One Course
36
Idea of Logistic Regression
error =1-ො𝑦
error = ො𝑦
Error
if 𝑦= 1
error = 1 −ො𝑦
if 𝑦= 0
error = ො𝑦
belief = ො𝑦
belief = 1 −ො𝑦
Belief
if 𝑦= 1
belief = ො𝑦
if 𝑦= 0
belief = 1 −ො𝑦
Minimize error ~ maximize belief ~ Minimize (-belief)
𝑃= ො𝑦 𝑦1 −ො𝑦1−𝑦
AI VIETNAM
All-in-One Course
❖ Construct loss
37
Idea of Logistic Regression
❖ Construct loss
belief = ො𝑦
belief = 1 −ො𝑦
Belief
belief = 𝑃
log_belief = log𝑃
log_belief = 𝑦logො𝑦+ 1 −𝑦log 1 −ො𝑦
loss = −log _belief
L(ො𝑦, 𝑦) = −𝑦logො𝑦−1 −𝑦log 1 −ො𝑦
Binary cross-entropy
= −[𝑦logො𝑦+ 1 −𝑦log 1 −ො𝑦]
if 𝑦= 1
belief = ො𝑦
if 𝑦= 0
belief = 1 −ො𝑦
𝑃= ො𝑦 𝑦1 −ො𝑦1−𝑦
One sample
AI VIETNAM
All-in-One Course
38
log𝑎𝑎= 1
log𝑎𝑥𝑦= log𝑎𝑥+ log𝑎𝑦
Công thức phổ biến
Hàm log là hàm đơn điệu (~thứ tự 
không thay đổi)
∀𝑥1𝑥2 ∈𝑎 𝑏 và 𝑥1 ≤𝑥2
Logarithm
→log(𝑥1) ≤log(𝑥1)
Ứng dụng trong Machine Learning
𝑓𝑥
𝑓𝑥
log 𝑓𝑥
log 𝑓𝑥
Tìm bộ tham số 𝛉 cho một model sao 
cho model mô tả được dữ liệu training 
argmax
θ
𝑓θ = argmax 𝑃θ training data
Với data sample được thu nhập độc lập với nhau
argmax
θ
𝑃θ sample_1 ∗⋯∗𝑃θ sample_𝑛
argmax
θ
𝑓θ =
Dùng hàm log
argmax
θ
log𝑓θ = argmax
θ
[log 𝑃θ sample_1 + ⋯+ log𝑃θ sample_𝑛]
Ví trí cực đại của hàm 𝒇𝜽 và 𝐥𝐨𝐠𝒇𝜽 không thay đổi 
Idea of Logistic Regression
belief = ො𝑦
belief = 1 −ො𝑦
Belief
𝑃𝑖= ො𝑦𝑖
𝑦𝑖1 −ො𝑦𝑖1−𝑦𝑖
belief = ෑ
𝑖=1
𝑛
𝑃𝑖
log_belief = ෍
𝑖=1
𝑛
log𝑃𝑖
log_belief = ෍
𝑖=1
𝑛
𝑦𝑖logො𝑦𝑖+ 1 −𝑦𝑖log 1 −ො𝑦𝑖
loss = −log _belief
since iid
L = 1
𝑁−𝒚𝑇𝑙𝑜𝑔ෝ𝒚−(𝟏−𝒚𝑇)𝑙𝑜𝑔𝟏−ෝ𝒚
Binary cross-entropy
= −෍
𝑖=1
𝑛
𝑦𝑖logො𝑦𝑖+ 1 −𝑦𝑖log 1 −ො𝑦𝑖
if 𝑦𝑖= 1
belief = ො𝑦𝑖
if 𝑦𝑖= 0
belief = 1 −ො𝑦𝑖
N samples
❖ Construct loss
AI VIETNAM
All-in-One Course
40
𝜕𝐿
𝜕𝜃𝑖
= 𝜕𝐿
𝜕ො𝑦
𝜕ො𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃𝑖
𝜕ො𝑦
𝜕𝑧= ො𝑦(1 −ො𝑦)
𝜕𝑧
𝜕𝜃𝑖
= 𝑥𝑖
Derivative
𝜕𝐿
𝜕𝜃𝑖
= 𝑥𝑖(ො𝑦−𝑦)
𝜕𝐿
𝜕ො𝑦= −𝑦
ො𝑦+ 1 −𝑦
1 −ො𝑦=
ො𝑦−𝑦
ො𝑦(1 −ො𝑦)
Idea of Logistic Regression
L(ො𝑦−𝑦) = −𝑦log ො𝑦−(1 −𝑦)log 1 −ො𝑦
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
Model and Loss
❖ Construct loss
AI VIETNAM
All-in-One Course
41
-log(ො𝑦)
with y = 1
𝑒𝑟𝑟𝑜𝑟
ො𝑦
-log(1-ො𝑦)
with y = 0
𝑒𝑟𝑟𝑜𝑟
ො𝑦
Idea of Logistic Regression
Feature
Label
Category 0
Category 1
Feature
Label
Category 0
Category 1
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
1
1 + 𝑒−𝑧
1
1 + 𝑒−𝑧
AI VIETNAM
All-in-One Course
42
➢Optimization Review
➢Linear Regression Review
➢Logistic Regression
➢Examples
➢Vectorization
➢Implementation (optional)
Outline
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝑧= 𝑤𝑥+ 𝑏
𝜽𝑇 = [𝑏 𝑤]
𝒙𝑇 = [1 
𝑥]
Model
Label
Loss
−ylogොy−(1−y)log(1−ොy )
𝑦
𝑥
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
𝑏
𝑤
AI VIETNAM
All-in-One Course
43
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
Logistic Regression-Stochastic
𝒙 =
1
1.4
𝒚= 0
Dataset
Model
Loss
0.1
𝑦
𝑥
-0.1
𝑥= 1.4
𝑧= −0.0399
ො𝑦= 0.49
𝑦= 0
L = 0.6733
𝑏
𝑤
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
−ylogොy−(1−y)log(1−ොy )
AI VIETNAM
All-in-One Course
44
Logistic Regression-Stochastic
𝒙 =
1
1.4
𝒚= 0
Dataset
𝜂= 0.01
𝐿𝑏
′
𝐿w
′
=
1 ∗0.49
1.4 ∗0.49 =
0.49
0.686
𝑏= 0.1 −𝜂0.49=0.095
Model
Loss
0.1
𝑦
𝑥
-0.1
𝑥= 1.4
𝑧= −0.0399
ො𝑦= 0.49
L = 0.6733
𝑏
𝑤
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
−ylogොy−(1−y)log(1−ොy )
𝑦= 0
𝑤 = −0.1 −𝜂0.686=−0.1068
AI VIETNAM
All-in-One Course
45
Logistic Regression-Stochastic
𝒙 =
1
1.4
𝒚= 0
Dataset
Model
Loss
0.095
𝑦
𝑥
−0.1068
𝑥= 1.4
𝑧= −0.0545
ො𝑦= 0.486
L = 0.666
𝑏
𝑤
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤𝑥+ 𝑏
−ylogොy−(1−y)log(1−ොy )
𝑦= 0
previous L = 0.6733
AI VIETNAM
All-in-One Course
46
Another example
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝑤𝑖= 𝑤𝑖−𝜂𝜕𝐿
𝜕𝑤𝑖
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
𝜽𝑇 = [𝑏 𝑤1 𝑤2]
𝒙𝑇 = [1 𝑥1 𝑥2]
Model
Label
Loss
𝑥1
−ylogොy−(1−y)log(1−ොy )
𝑦
𝑥2
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
𝑤1
𝑏
𝑤2
AI VIETNAM
All-in-One Course
𝜕𝐿
𝜕𝑤𝑖
= 𝑥𝑖(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
Model
Label
Loss
𝑥1
0.5
0.1
𝑦
𝑥2
-0.1
𝑥1 = 1.4
𝑥2 = 0.2
𝑧= 0.78
ො𝑦= 0.6856
𝑦= 0
𝐿= 1.1573
𝑤1
𝑏
𝑤2
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
Logistic Regression-Stochastic
−ylogොy−(1−y)log(1−ොy )
AI VIETNAM
All-in-One Course
𝒙 =
1
1.4
0.2
𝒚= 0
Dataset
48
Logistic Regression-Stochastic
Model
Loss
𝑥1
0.5
0.1
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
𝑦
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑥2
-0.1
𝜂= 0.01
𝑤1
𝑏
𝑤2
𝐿𝑏
′
𝐿𝑤1
′
𝐿𝑤2
′
=
1 ∗0.6856
1.4 ∗0.6856
0.2 ∗0.6856
=
0.6856
0.9599
0.1371
𝑏= 0.1 −𝜂0.6856
𝑤1= 0.5 −𝜂0.9598
𝑤2= −0.1+𝜂0.1371
𝐿𝑏
′
𝐿𝑤1
′
𝐿𝑤2
′
𝑧= 0.78
ො𝑦= 0.6856
𝑦= 0
𝐿= 1.1573
−ylogොy−(1−y)log(1−ොy )
𝑥1 = 1.4
𝑥2 = 0.2
AI VIETNAM
All-in-One Course
𝒙 =
1
1.4
0.2
𝒚= 0
Dataset
=0.0931
=0.4990
=−0.1013
Logistic Regression-Stochastic
Model
Loss
𝑥1
0.4904
0.0931
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
𝑦
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑥2
-0.1013
𝜂= 0.01
𝑤1
𝑏
𝑤2
𝐿𝑏
′
𝐿𝑤1
′
𝐿𝑤2
′
ො𝑦= 0.6856
𝐿= 1.1573
−ylogොy−(1−y)log(1−ොy )
𝑧= 0.78
𝑦= 0
AI VIETNAM
All-in-One Course
𝒙 =
1
1.4
0.2
𝒚= 0
Dataset
𝐿𝑏
′
𝐿𝑤1
′
𝐿𝑤2
′
=
1 ∗0.6856
1.4 ∗0.6856
0.2 ∗0.6856
=
0.6856
0.9599
0.1371
𝑏= 0.1 −𝜂0.6856
𝑤1= 0.5 −𝜂0.9598
𝑤2= −0.1+𝜂0.1371
=0.0931
=0.4990
=−0.1013
𝑥1 = 1.4
𝑥2 = 0.2
Logistic Regression-Stochastic
Model
Loss
𝑥1
0.4904
0.0931
𝑧= 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
𝑦
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝑥2
-0.1013
𝑤1
𝑏
𝑤2
ො𝑦= 0.6812
𝐿= 1.1432
−ylogොy−(1−y)log(1−ොy )
𝑧= 0.75
𝒙 =
1
1.4
0.2
𝒚= 0
Dataset
previous 𝑳= 1.1573
𝑥1 = 1.4
𝑥2 = 0.2
𝑦= 0
AI VIETNAM
All-in-One Course
51
➢Optimization Review
➢Linear Regression Review
➢Logistic Regression
➢Examples
➢Vectorization
➢Implementation (optional)
Outline
Review
Multiply with a number
𝛼𝑢= 𝛼
𝑢1
…
𝑢𝑛
=
𝛼𝑢1
…
𝛼𝑢𝑛
=
data 
1
2
3
2
*
result 
2
4
6
1
3
2
4
T
1
2
3
4
A =
𝑎11 …  𝑎1𝑛
… … . . .
𝑎𝑚1 …  𝑎𝑚𝑛
A𝑇=
𝑎11 …  𝑎𝑚1
… … …
𝑎1𝑛 …  𝑎𝑚𝑛
Ԧ𝑣=
𝑣1
…
𝑣𝑛
Ԧ𝑣𝑇= 𝑣1 … 𝑣𝑛
1
2
T
1
2
AI VIETNAM
All-in-One Course
Transpose
52
Review
Dot product
Ԧ𝑣=
𝑣1
…
𝑣𝑛
𝑢=
𝑢1
…
𝑢𝑛
Ԧ𝑣∙𝑢= 𝑣1 × 𝑢1 + ⋯+ 𝑣𝑛× 𝑢𝑛
v 
1
2
2
3
w 
=
result 
8
53
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Vectorization
AI VIETNAM
All-in-One Course
𝑧= 𝑤𝑥+ 𝑏1 = 𝑏 𝑤
1
𝑥= 𝜽𝑇𝒙
dot product
Traditional
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝑤𝑥+ 𝑏
𝜽= 𝑏
𝑤
→𝜽𝑇= 𝑏 𝑤
Feature
Label
𝑥
𝑦
54
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝑤𝑥+ 𝑏
𝑧= 𝜽𝑇𝒙
𝐿ො𝑦, 𝑦= (ො𝑦−𝑦)2
numbers
What will we do?
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
55
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Traditional
Vectorization
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝑤𝑥+ 𝑏
ො𝑦−𝑦× 1
ො𝑦−𝑦× 𝑥 =
ො𝑦−𝑦
1 
𝑥 =
ො𝑦−𝑦𝒙=
𝜕𝐿
𝜕𝑏
𝜕𝐿
𝜕𝑤
= 𝛻𝜽𝐿
common factor
→
𝛻𝜽𝐿= 2𝒙( Ƹ𝑦−𝑦)
𝜕𝐿
𝜕𝑤= 𝑥ො𝑦−𝑦=
ො𝑦−𝑦× 𝑥
𝜕𝐿
𝜕𝑏=
ො𝑦−𝑦 =
ො𝑦−𝑦× 1
56
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝑧= 𝑤𝑥+ 𝑏
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
Traditional
Vectorization
AI VIETNAM
All-in-One Course
𝒙= 1
𝑥
𝜽= 𝑏
𝑤
𝑧= 𝜽𝑇𝒙
𝑤 =  𝑤 − 𝜂𝜕𝐿
𝜕𝑤
𝑏 =  𝑏 − 𝜂𝜕𝐿
𝜕𝑏
𝛻𝜽𝐿=
𝜕𝐿
𝜕𝑏
𝜕𝐿
𝜕𝑤
𝜽
𝜽
𝛻𝜽𝐿
→
𝜽= 𝜽−𝜂𝛻𝜽𝐿
57
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute the output ො𝑦
3) Compute loss
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝜕𝐿
𝜕𝑤= 𝑥(ො𝑦−𝑦)
𝜕𝐿
𝜕𝑏= (ො𝑦−𝑦)
𝑤= 𝑤−𝜂𝜕𝐿
𝜕𝑤
𝑏= 𝑏−𝜂𝜕𝐿
𝜕𝑏
𝜂is learning rate
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝒙(ො𝑦−𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
Traditional
Vectorized
Vectorization
AI VIETNAM
All-in-One Course
𝑧= 𝑤𝑥+ 𝑏
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
Vectorization
AI VIETNAM
All-in-One Course
❖ Implementation (using Numpy)
# Given X and y
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝒙(ො𝑦−𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
59
Model
Input
Label
Loss
𝒙
𝜽=
0.1
0.5
−0.1
ො𝑦= 𝜎𝜽𝑇𝒙 = 0.6856
𝐿= 1.1573
𝑦= 0
𝒙=
1
𝑥1
𝑥2
=
1
1.4
0.2
Given 𝜽=
𝑏
𝑤1
𝑤2
=
0.1
0.5
−0.1
𝛻𝜽𝐿= 𝒙ො𝑦−𝑦=
1
1.4
0.2
0.6856 =
0.6856
0.9599
0.1371
=
𝐿𝑏
′
𝐿𝑤1
′
𝐿𝑤2
′
𝛉−ηL𝛉
′ =
0.1
0.5
−0.1
−η
0.6856
0.9599
0.1371
=
0.093
0.499
−0.101
𝜂= 0.01
1
3
Dataset
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
𝑧= 𝜽𝑇𝒙= 𝒙𝑇𝜽
𝐿(ො𝑦, 𝑦) = −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝒙(ො𝑦−𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
4
5
Logistic Regression-Stochastic
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿𝜽= −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝐱(ොy −𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
𝑧= 𝜽𝑇𝒙
Demo
AI VIETNAM
All-in-One Course
𝒙 =
1
1.4
0.2
𝒚= 0
Dataset
61
4) Compute derivative
5) Update parameters
1) Pick a sample (𝑥, 𝑦) from training data   
2) Compute output ො𝑦
3) Compute loss
ො𝑦= 𝜎(𝑧) =
1
1 + 𝑒−𝑧
𝐿𝜽= −ylogොy−(1−y)log(1−ොy )
𝛻𝜽𝐿= 𝐱(ොy −𝑦)
𝜽= 𝜽−𝜂𝛻𝜽𝐿
𝜂is learning rate
𝑧= 𝜽𝑇𝒙
