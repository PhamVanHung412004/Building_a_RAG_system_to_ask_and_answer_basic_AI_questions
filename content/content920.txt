• Entropy: Sử dụng hàm entropy để tính độ bất định. vời hàm entropy được định nghĩa như
sau:
u = arg max
i
−
kX
j=1
Pθ(aj|qi) lnPθ(aj|qi) (2)
Trong đóPθ(aj|qi) là tần số xuất hiện của một câu trả lời j nhất định trong tập tất cả các
dự đoán. Entropy càng cao chứng tỏ độ bất định càng lớn.
• Variance: Sử dụng variance cho các câu hỏi liên quan đến toán học. Các tác giả nhận ra
rằng độ variance trong các câu trả lời là rất lớn. Vì độ variance trong các câu trả lời là rất
lớn, ta cần phải normalize các câu trả lời lại.
• Self-Confidence: Sử dụng một mô hình LLM để tạo sinh ra thang điểm cho độ bất định
của các câu trả lời. Thang điểm này hoạt động không quá tốt do các mô hình LLM có xu hứ
tự tin thái quá vào các câu trả lời.
Nhìn chung disagreement và entropy là 2 phương pháp tính độ bất định cho ra kết quả tốt nhất.
Trong đó disagreement cho ra kết quả tốt hơn entropy trong nhiều task.
13 Directional Stimulus Prompting
Đây là một kĩ thuật prompting giúp LLM có thể tóm tắt văn bản theo ý muốn. Kĩ thuật này được
giới thiệu trong paper "Guiding Large Language Models via Directional Stimulus Prompting" Li
et al., 2023. DSP sử dụng các kích thích/chỉ dẫn hướng (directional stimulus, trong trường hợp này
là các từ khóa), để cung cấp hướng dẫn cụ thể cho từng trường hợp cho các mô hình ngôn ngữ lớn
18