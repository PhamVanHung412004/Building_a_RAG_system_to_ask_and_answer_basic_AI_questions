Module 10 - Exercise
Text Summarization 
with Human Feedback (RLHF)
Year 2023
AI VIET NAM
Nguyen Quoc Thai
AI VIETNAM
All-in-One Course
1
2
Objective
Ã˜ ChatGPT
Ã˜ Text Summarization
Ã˜ Human Feedback
3
ChatGPT
4
ChatGPT
5
Ã˜ Large Language Models
Ã˜ InstructGPT (RLHF)
Ã˜ Text Summarization using RLHF
Outline
6
Large Language Models
ChatGPT
!
7
Large Language Models
Large Language Models
!
Ã˜ Medium-sized models: BERT/RoBERTa models (100M or 300M), T5 models (220M, 
700M, 3B, 11B)
Ã˜ â€œVeryâ€ large LMs: models of 100+ billion parameters
GPT3 (175B), BLOOM (176B), PaLM (540B), GLaM (1200B)â€¦
Ã˜ Larger model sizes => Larger compute, more expensive during inference
8
Large Language Models
Large Language Models
!
Ã˜ Data scale: usually in the order of trillions of tokens
GPT3 (0.5 trillion tokens), LLaMA (1.4 trillion tokens), â€¦
Ã˜ Training data: Low-quality data 
9
Large Language Models
Large Language Models
!
10
Large Language Models
Large Language Models
!
Ã˜ Pre-training: trained on huge amounts of unlabeled text using â€œself-supervisedâ€ 
training objective
Ã˜ Adaptation: how to use a pre-trained model for downstream task?
11
Large Language Models
Large Language Models
!
Ã˜ The promise: one single model to solve 
many NLP tasks
Ã˜ Emergent properties in LLMs
12
Large Language Models
Prompts
!
Ã˜ Prompts involve instructions and context passed to a language model to achieve a
desired task
Ã˜ Prompt engineering is the practice of developing and optimizing prompts to
efficiently use language models (LMs) for a variety of applications
13
Large Language Models
Elements of Prompts
!
Prompt
Response
Language
Model
TASK DESCRIPTION
CURRENT INPUT
OUTPUT INDICATOR
EXAMPLE 1
EXAMPLE 2
One-shot
Few-shot
14
Large Language Models
Three setting for In-Context-Learning
!
15
Ã˜ Large Language Models
Ã˜ InstructGPT (RLHF)
Ã˜ Text Summarization using RLHF
Outline
16
InstructGPT (RLHF)
Overview
!
17
InstructGPT (RLHF)
Pre-training LLMs
!
18
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ Goal: optimize the LLM to generate the response that users are 
looking for
19
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ Goal: optimize the LLM to generate the response that users are 
looking for
20
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ A large collections of prompts:
q Labeler-written prompts
Plain
Labelers to come up with  an arbitrary task, while 
ensuring diversity of tasks
Few-shot
Labelers to come up with an instruction and multiple 
query/response pairs for that instruction
â€œGiven the sentiment for a tweetâ€
User-based
Collect use-cases stated in applications to the 
OpenAI API. Labelers to come up with prompts 
correspponding to these use-cases
21
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ A large collections of prompts:
q Labeler-written prompts
q API user prompts (From OpenAI GPT3 Playground)
- 200 prompts / per organization
- 10 use cases
22
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ A large collections of prompts:
23
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ A large collections of prompts:
24
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ Fine tune the model, call this model SFT Model
q Initialized with pretrained GPT3 175B model
q Trained for 16 epochs on demonstration data
q Notation:
ğœ‹!"#
25
InstructGPT (RLHF)
Supervised Fine-Tuning (SFT)
!
Ã˜ Task: language modeling
Ã˜ Training data: high-quality in the format of (prompt, response)
Ã˜ Data scale: 10,000 â€“ 100,000 (prompt, response) pairs
Ã˜ Model: LLMs
- Input: prompt
- Output: response for this prompt
Ã˜ Loss: cross entropy
26
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ Training a model to output a score on a given input (a pair of 
prompt â€“ response)
Ã˜ A classification or regression task
27
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ Given K = 4 to 9 outputs to rank for each prompt
28
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ Given K = 4 to 9 outputs to rank for each prompt
Ã˜ For 4 ranked responses:  D > C > A = B
=> 5 ranked pairs: (D > C), (D > A), (D > B), (C > A), (C > B)
29
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
x: the prompt, yw: the better completion, yl: the worse completion
loss ğœƒ= E %,'!,'" ~) log ğœr* x, y+ âˆ’r* x, y,
Reward on better 
completion
Reward on worse 
completion
30
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
Ã˜ Overfitting problem
Each prompt has K completions => K choose 2 pairs to compare
Each completion can appear in K-1 gradient updates
31
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
Ã˜ Overfitting problem
Each prompt has K completions => K choose 2 pairs to compare
Each completion can appear in K-1 gradient updates
Ã˜ Solution: train on all comparisons from each prompt as a single 
batch element
Ã˜ Normalization in loss with -1/(K choose 2):
loss ğœƒ= âˆ’1
K
2
E %,'!,'" ~) log ğœr* x, y+ âˆ’r* x, y,
32
InstructGPT (RLHF)
Reward Modeling
!
Ã˜ The reward model: ğ’“ğœ½
Ã˜ Training data: high-quality data
x: the prompt, yw: the better completion, yl: the worse completion
Ã˜ Data scale: 100K â€“ 1M examples
InstructGPT: 50,000 prompts (each prompt: 4 to 9 responses) => 
300K to 1.8M training examples
Ã˜ Training sample (x, yw, yl )
loss ğœƒ= âˆ’1
K
2
E %,'!,'" ~) log ğœr* x, y+ âˆ’r* x, y,
33
InstructGPT (RLHF)
Reinforcement Learning
!
Ã˜ Goal: train the SFT model to generate output responses that will 
maximize the scores by the RM model
Ã˜ Training data: randomly selected prompts
Ã˜ Data sacle: 10,000 â€“ 100,0000 prompts
34
InstructGPT (RLHF)
Reinforcement Learning
!
ML Task: Reinforcement Learning
Ã˜ Action space: the vocabulary of tokens the LLM uses. Taking 
action means choosing a token to generate
Ã˜ Observation space: the distribution over all possible prompts
Ã˜ Policy: the probability distribution over all actions to take (all 
tokens to generate) given an observation (a  prompt)
Ã˜ Reward function: the reward model from stage 2
35
InstructGPT (RLHF)
Reinforcement Learning
!
Ã˜ DRL: the distribution of prompts used for RL model
Ã˜ LLM ğœ‹-
./: the model being trained with RL, parameterized by ğœ™
Ã˜ For each x from DRL: y: ğœ‹-
./(x)
objective0 x, y; ğœ™= ğ’“ğœ½(x, y)
Ã˜ For all training data DRL
objective0 x, y; ğœ™= E(2,')~4#$./ğ’“ğœ½(x, y)
36
InstructGPT (RLHF)
Reinforcement Learning
!
Ã˜ Worse reward esimates: as RLHF is updated, its outputs become 
very different from what the RM was trained on
Ã˜ Solution: add a KL penalty that makes sure PPO model output 
does not deviate too far from SFT model
objective0 x, y; ğœ™= E %,5 ~4#$67 ğ’“ğœ½x, y âˆ’ğ›½log
ğœ‹-
./ ğ‘¦ğ‘¥
ğœ‹89: ğ‘¦ğ‘¥
37
InstructGPT (RLHF)
Reinforcement Learning
!
Ã˜ Just using RL objective leads to performance degradation 
on many NLP tasks
Ã˜ Solution: add a auxiliary LM ojective on the pretraining data. 
Call this variant PPO-ptx
Ã˜ Dpretrain: the distribution of the pretraining data for the pretrain 
model
objective; x<=>?=@AB; ğœ™= ğ›¾E2~4%&'(&)*+ log ğ…ğ“
./ x
38
InstructGPT (RLHF)
Reinforcement Learning
!
Ã˜ Maximize the objective function in RL training:
objective ğ“= objective0 x, y; ğœ™+ objective; x<=>?=@AB; ğœ™
objective ğ“= E %,5 ~4#$67 ğ’“ğœ½x, y âˆ’ğ›½log
ğœ‹-
./ ğ‘¦ğ‘¥
ğœ‹89: ğ‘¦ğ‘¥
+ ğ›¾E2~4%&'(&)*+ log ğ…ğ“
./ x
39
InstructGPT (RLHF)
Summary
!
40
Ã˜ Large Language Models
Ã˜ InstructGPT (RLHF)
Ã˜ Text Summarization using RLHF
Outline
41
Text Summarization using RLHF
Text Summarization
!
Document
Summary
42
Text Summarization using RLHF
Text Summarization
!
43
Text Summarization using RLHF
Types of Text Summarization
!
Ã˜ Based on Output Type
Document
Summary
Document
Summary
Extrective
Abstractive
44
Text Summarization using RLHF
Types of Text Summarization
!
Ã˜ Based on Input Type
Document
Summary
Document
Summary
Single-Document
Multi-Document
45
Text Summarization using RLHF
Dataset
!
CarperAI/openai_summarize_tldr
46
Text Summarization using RLHF
Dataset
!
CarperAI/openai_summarize_comparisions
47
Text Summarization using RLHF
Pipeline
!
Document
Summary
LM
Document + Summary
Reward Model
Reward
Document + Summary
LM
LM
log-probs
log-probs
Reward
KL-div
PPO
-
+
Active Model
Reference Model
Policy gradients optimize model
48
Text Summarization using RLHF
Pipeline â€“ Supervised Fine-Tuning
!
Document
Summary
LM
Task: Text Generation
Dataset: CarperAI/openai_summarize_tldr
Metric: ROUGE
Tokenization
Generation
Document + Summary
49
Text Summarization using RLHF
Pipeline â€“ Reward Modeling
!
Reward Model
Reward
Task: Text Classification (Document-Level)
Dataset:CarperAI/openai_summarize_comparisions
Metric: Accuracy
Tokenization
Classification
Chosen: 1
Rejected: 0
50
Text Summarization using RLHF
Pipeline â€“ PPO
!
Task: Reinforcement Learning
Dataset: CarperAI/openai_summarize_tldr
Document + Summary
LM
LM
log-probs
Log-probs
Reward
KL-div
PPO
-
+
Active Model
Policy gradients optimize model
51
Text Summarization using RLHF
Experiment
!
52
Summary
Thanks!
Any questions?
53
