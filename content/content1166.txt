AI VIETNAM aivietnam.edu.vn
178 h = in_rest (x)
179 h = self . h_upd (h)
180 x = self . x_upd (x)
181 h = in_conv (h)
182 else :
183 h = self . in_layers (x)
184 emb_out = self . emb_layers ( emb ). type (h. dtype )
185 while len ( emb_out . shape ) < len (h. shape ):
186 emb_out = emb_out [... , None ]
187 if self . use_scale_shift_norm :
188 out_norm , out_rest = self . out_layers [0] , self . out_layers [1:]
189 scale , shift = torch . chunk ( emb_out , 2, dim =1)
190 h = out_norm (h) * (1 + scale ) + shift
191 h = out_rest (h)
192 else :
193 h = h + emb_out
194 h = self . out_layers (h)
195 return self . skip_connection (x) + h
196
197 class AttentionBlock (nn. Module ):
198 """
199 An attention block that allows spatial positions to attend to each other .
200 Originally ported from here , but adapted to the N-d case .
